{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxOzVIYCVTOc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "\n",
        "# Reset random seed for reproducibility\n",
        "np.random.seed(2025)\n",
        "\n",
        "# =====================================================\n",
        "# 1. UNIFIED TOUCHPOINTS (DESIGN + INSURANCE + RISK)\n",
        "# =====================================================\n",
        "TOUCHPOINTS = [\n",
        "    \"Clinical_Strategy\",     # 0: Bed counts, ROI, Service lines\n",
        "    \"Risk_Underwriting\",     # 1: Liability profile, Premium forecasting\n",
        "    \"Site_Catastrophe_Risk\", # 2: Flood plains, Seismic, Seismic Insurance\n",
        "    \"Medical_Planning\",      # 3: Patient flow, Fall risk, Med-Mal exposure\n",
        "    \"Equipment_Assets\",      # 4: MRI/CT (High-value equipment floater)\n",
        "    \"Structural_Integrity\",  # 5: Seismic resilience vs. Deductibles\n",
        "    \"MEP_Systems\",           # 6: Life safety, HVAC (HEPA), Power backup\n",
        "    \"ICT_Cyber_Security\",    # 7: EMR Security, Ransomware insurance\n",
        "    \"Infection_Control\",     # 8: Biohazard risk, Liability mitigation\n",
        "    \"Claims_History_Proxy\",  # 9: Simulated historical risk data\n",
        "    \"Cost_Engineering\",      # 10: CAPEX vs. OPEX (Premium costs)\n",
        "    \"Regulatory_Compliance\", # 11: JCI/FGI, License to operate\n",
        "    \"BIM_Digital_Twin\",      # 12: Precision underwriting data\n",
        "    \"Reinsurance_Layer\",     # 13: Risk transfer for 100M+ assets\n",
        "    \"Operational_Policy\"     # 14: Final Policy Issuance / Approval\n",
        "]\n",
        "\n",
        "D_GRAPH = len(TOUCHPOINTS)\n",
        "\n",
        "# =====================================================\n",
        "# 2. THE DSM (DEPENDENCY STRUCTURE MATRIX)\n",
        "# =====================================================\n",
        "# This matrix represents the \"Gordian Knot\" where Insurance (Risk)\n",
        "# and Design (Asset) are deeply coupled.\n",
        "DSM = np.zeros((D_GRAPH, D_GRAPH))\n",
        "\n",
        "# Key Couplings:\n",
        "# Risk Underwriting (1) affects Strategy (0) and Cost (10)\n",
        "DSM[0, 1] = 0.8; DSM[1, 0] = 0.5\n",
        "# Med Planning (3) affects Infection Control (8) and Claims (9)\n",
        "DSM[3, 8] = 0.9; DSM[3, 9] = 0.7\n",
        "# Equipment (4) affects Structural (5) and Reinsurance (13)\n",
        "DSM[4, 5] = 1.0; DSM[4, 13] = 0.8\n",
        "# ICT Cyber (7) has a loop with Operational Policy (14)\n",
        "DSM[7, 14] = 0.6; DSM[14, 7] = 0.9\n",
        "# Regulatory (11) must approve almost everything\n",
        "DSM[11, [0, 3, 6, 8]] = 1.0\n",
        "\n",
        "# =====================================================\n",
        "# 3. METRIC CONFIGURATION (KPIs)\n",
        "# =====================================================\n",
        "METRIC_KEYS = [\n",
        "    \"Loss_Ratio_Prevention\", # Insurance perspective\n",
        "    \"Clinical_Safety\",       # Hospital perspective\n",
        "    \"Capital_Efficiency\",    # Financial perspective\n",
        "    \"Regulatory_Alpha\",      # Compliance perspective\n",
        "]\n",
        "\n",
        "# Mapping: Node -> [Primary Feature, Impacted Metric]\n",
        "METRIC_MAP = {\n",
        "    \"Clinical_Strategy\":     [\"target_roi\", \"Capital_Efficiency\"],\n",
        "    \"Risk_Underwriting\":     [\"premium_density\", \"Loss_Ratio_Prevention\"],\n",
        "    \"Site_Catastrophe_Risk\": [\"pml_score\", \"Loss_Ratio_Prevention\"], # Probable Max Loss\n",
        "    \"Medical_Planning\":      [\"nurse_travel_dist\", \"Clinical_Safety\"],\n",
        "    \"Equipment_Assets\":      [\"asset_replacement_val\", \"Capital_Efficiency\"],\n",
        "    \"Structural_Integrity\":  [\"seismic_drift_ratio\", \"Clinical_Safety\"],\n",
        "    \"MEP_Systems\":           [\"redundancy_n_plus_1\", \"Clinical_Safety\"],\n",
        "    \"ICT_Cyber_Security\":    [\"encryption_level\", \"Loss_Ratio_Prevention\"],\n",
        "    \"Infection_Control\":     [\"isolation_room_ratio\", \"Clinical_Safety\"],\n",
        "    \"Claims_History_Proxy\":  [\"incident_frequency\", \"Loss_Ratio_Prevention\"],\n",
        "    \"Cost_Engineering\":      [\"total_insured_value\", \"Capital_Efficiency\"],\n",
        "    \"Regulatory_Compliance\": [\"compliance_score\", \"Regulatory_Alpha\"],\n",
        "    \"BIM_Digital_Twin\":      [\"data_fidelity_lod\", \"Regulatory_Alpha\"],\n",
        "    \"Reinsurance_Layer\":     [\"risk_retention_level\", \"Capital_Efficiency\"],\n",
        "    \"Operational_Policy\":    [\"policy_limit_million\", \"Regulatory_Alpha\"]\n",
        "}\n",
        "\n",
        "# =====================================================\n",
        "# 4. UNIFIED DATA GENERATORS\n",
        "# =====================================================\n",
        "def get_generators():\n",
        "    return {\n",
        "        \"Clinical_Strategy\":     lambda: {\"target_roi\": np.random.uniform(0.08, 0.18)},\n",
        "        \"Risk_Underwriting\":     lambda: {\"premium_density\": np.random.uniform(0.02, 0.05)},\n",
        "        \"Site_Catastrophe_Risk\": lambda: {\"pml_score\": np.random.beta(2, 5)},\n",
        "        \"Medical_Planning\":      lambda: {\"nurse_travel_dist\": np.random.randint(20, 100)},\n",
        "        \"Equipment_Assets\":      lambda: {\"asset_replacement_val\": np.random.uniform(50, 500)}, # Millions\n",
        "        \"Structural_Integrity\":  lambda: {\"seismic_drift_ratio\": np.random.uniform(0.005, 0.02)},\n",
        "        \"MEP_Systems\":           lambda: {\"redundancy_n_plus_1\": np.random.choice([0, 1, 2])},\n",
        "        \"ICT_Cyber_Security\":    [\"low\", \"med\", \"high\"], # Handled in formula\n",
        "        \"Infection_Control\":     lambda: {\"isolation_room_ratio\": np.random.uniform(0.1, 0.3)},\n",
        "        \"Claims_History_Proxy\":  lambda: {\"incident_frequency\": np.random.poisson(2)},\n",
        "        \"Cost_Engineering\":      lambda: {\"total_insured_value\": np.random.randint(200, 1500)},\n",
        "        \"Regulatory_Compliance\": lambda: {\"compliance_score\": np.random.uniform(0.85, 1.0)},\n",
        "        \"BIM_Digital_Twin\":      lambda: {\"data_fidelity_lod\": np.random.choice([300, 350, 400, 500])},\n",
        "        \"Reinsurance_Layer\":     lambda: {\"risk_retention_level\": np.random.uniform(0.1, 0.5)},\n",
        "        \"Operational_Policy\":    lambda: {\"policy_limit_million\": np.random.randint(500, 2000)}\n",
        "    }\n",
        "def get_generators():\n",
        "    return {\n",
        "        \"Strategy\":     lambda s=False: {\"roi_score\": np.random.uniform(0.08, 0.18)},\n",
        "        \"Underwriting\": lambda s=False: {\"cost_rate\": np.random.uniform(0.02, 0.05)},\n",
        "        \"Hazard\":       lambda s=False: {\"pml_score\": np.random.beta(2, 5)},\n",
        "        \"Operations\":   lambda s=False: {\"efficiency_ratio\": np.random.random()},\n",
        "        \"Assets\":       lambda s=False: {\"asset_value\": np.random.randint(200, 1500)},\n",
        "        \"Resilience\":   lambda s=False: {\"resilience_score\": np.random.uniform(0.4, 0.9)},\n",
        "        \"MEP\":          lambda s=False: {\"redundancy_level\": np.random.choice([0, 1, 2])},\n",
        "        \"Cyber\":        lambda s=False: {\n",
        "            \"risk_level\": np.random.choice([\"low\", \"med\", \"high\"]),\n",
        "            \"cyber_score\": np.random.uniform(0.4, 0.9)\n",
        "        },\n",
        "        \"Safety\":       lambda s=False: {\"safety_score\": np.random.random()},\n",
        "        \"Claims\":       lambda s=False: {\"incident_rate\": np.random.poisson(2)},\n",
        "        \"Costing\":      lambda s=False: {\"opex_cost\": np.random.randint(5, 50)},\n",
        "        \"Compliance\":   lambda s=False: {\"compliance_score\": np.random.uniform(0.85, 1.0)},\n",
        "        \"DigitalTwin\":  lambda s=False: {\"fidelity_lod\": np.random.choice([300, 350, 400, 500])},\n",
        "        \"RiskTransfer\": lambda s=False: {\"retention_ratio\": np.random.uniform(0.1, 0.5)},\n",
        "        \"Policy\":       lambda s=False: {\"cap_limit\": np.random.randint(500, 2000)}\n",
        "    }\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# 5. COMPLEXITY SCORING & NORMALIZATION FORMULAS\n",
        "# =====================================================\n",
        "# Transforms raw engineering/insurance data into 0.0-1.0 scores\n",
        "FORMULAS = {\n",
        "    \"Clinical_Strategy\":     lambda x: x / 0.2,\n",
        "    \"Risk_Underwriting\":     lambda x: 1.0 - (x * 10),\n",
        "    \"Site_Catastrophe_Risk\": lambda x: 1.0 - x, # Lower PML is better\n",
        "    \"Medical_Planning\":      lambda x: max(0, 1.0 - (x/120)),\n",
        "    \"Equipment_Assets\":      lambda x: x / 500.0,\n",
        "    \"Structural_Integrity\":  lambda x: 1.0 - (x / 0.03),\n",
        "    \"MEP_Systems\":           lambda x: x / 2.0,\n",
        "    \"ICT_Cyber_Security\":    lambda x: 1.0 if x == \"high\" else 0.5,\n",
        "    \"Infection_Control\":     lambda x: x / 0.4,\n",
        "    \"Claims_History_Proxy\":  lambda x: max(0, 1.0 - (x/10)),\n",
        "    \"Cost_Engineering\":      lambda x: x / 2000.0,\n",
        "    \"Regulatory_Compliance\": lambda x: (x - 0.8) / 0.2,\n",
        "    \"BIM_Digital_Twin\":      lambda x: x / 500.0,\n",
        "    \"Reinsurance_Layer\":     lambda x: 1.0 - x,\n",
        "    \"Operational_Policy\":    lambda x: x / 2000.0\n",
        "}\n",
        "\n",
        "# =====================================================\n",
        "# 6. STRUCTURAL SUMMARY\n",
        "# =====================================================\n",
        "print(\"=== UNIFIED INSURANCE-HOSPITAL DESIGN TEMPLATE ===\")\n",
        "print(f\"Total Nodes: {len(TOUCHPOINTS)}\")\n",
        "print(f\"Metrics Tracked: {METRIC_KEYS}\")\n",
        "\n",
        "# Identifying the 'Heart' of the Insurance Complexity\n",
        "print(\"\\n--- CORE INSURANCE LOOPS (Coupled Nodes) ---\")\n",
        "# 1. Financial Loop: Strategy <-> Risk Underwriting\n",
        "# 2. Technical Loop: Cyber Security <-> Operational Policy\n",
        "# 3. Asset Loop: Equipment <-> Structural <-> Reinsurance\n",
        "# ======================================================\n",
        "# UNIFIED GENERATOR MAP (15 TOUCHPOINTS)\n",
        "# ======================================================\n",
        "# This map links each Touchpoint to its specific data generator function.\n",
        "# It handles both the 12 design nodes and the 3 new insurance/risk nodes.\n",
        "\n",
        "GENERATOR_MAP = {\n",
        "    # --- DESIGN & CLINICAL NODES ---\n",
        "    \"Clinical_Strategy\":     lambda: {\n",
        "        \"target_roi\": np.random.uniform(0.08, 0.18),\n",
        "        \"icu_bed_count\": np.random.randint(10, 60)\n",
        "    },\n",
        "    \"Site_Catastrophe_Risk\": lambda: {\n",
        "        \"pml_score\": np.random.beta(2, 5), # Probable Max Loss\n",
        "        \"seismic_zone\": np.random.choice([1, 2, 3, 4])\n",
        "    },\n",
        "    \"Medical_Planning\":      lambda: {\n",
        "        \"nurse_travel_dist\": np.random.randint(20, 100),\n",
        "        \"fall_risk_index\": np.random.uniform(0.1, 0.9)\n",
        "    },\n",
        "    \"Equipment_Assets\":      lambda: {\n",
        "        \"asset_replacement_val\": np.random.uniform(50, 500), # Millions\n",
        "        \"mri_shielding_req\": np.random.uniform(0.5, 1.0)\n",
        "    },\n",
        "    \"Structural_Integrity\":  lambda: {\n",
        "        \"seismic_drift_ratio\": np.random.uniform(0.005, 0.02),\n",
        "        \"vibration_sensitivity\": np.random.choice([\"VC-A\", \"VC-B\", \"VC-C\"])\n",
        "    },\n",
        "    \"MEP_Systems\":           lambda: {\n",
        "        \"redundancy_n_plus_1\": np.random.choice([0, 1, 2]),\n",
        "        \"ach_operating_room\": np.random.randint(15, 25)\n",
        "    },\n",
        "    \"ICT_Cyber_Security\":    lambda: {\n",
        "        \"encryption_level\": np.random.choice([\"low\", \"med\", \"high\"]),\n",
        "        \"vulnerability_count\": np.random.randint(0, 50)\n",
        "    },\n",
        "    \"Infection_Control\":     lambda: {\n",
        "        \"isolation_room_ratio\": np.random.uniform(0.1, 0.3),\n",
        "        \"hepa_coverage\": np.random.uniform(0.7, 1.0)\n",
        "    },\n",
        "    \"Claims_History_Proxy\":  lambda: {\n",
        "        \"incident_frequency\": np.random.poisson(2),\n",
        "        \"projected_annual_claims\": np.random.randint(1, 15)\n",
        "    },\n",
        "    \"Cost_Engineering\":      lambda: {\n",
        "        \"total_insured_value\": np.random.randint(200, 1500),\n",
        "        \"ve_savings_ratio\": np.random.uniform(0.05, 0.20)\n",
        "    },\n",
        "    \"Regulatory_Compliance\": lambda: {\n",
        "        \"compliance_score\": np.random.uniform(0.85, 1.0),\n",
        "        \"fgi_compliance_gap\": np.random.randint(0, 5)\n",
        "    },\n",
        "    \"BIM_Digital_Twin\":      lambda: {\n",
        "        \"data_fidelity_lod\": np.random.choice([300, 350, 400, 500]),\n",
        "        \"clash_resolution_rate\": np.random.uniform(0.6, 0.99)\n",
        "    },\n",
        "\n",
        "    # --- INSURANCE & GOVERNANCE NODES ---\n",
        "    \"Risk_Underwriting\":     lambda: {\n",
        "        \"premium_density\": np.random.uniform(0.02, 0.05),\n",
        "        \"liability_exposure_index\": np.random.uniform(0.1, 0.8)\n",
        "    },\n",
        "    \"Reinsurance_Layer\":     lambda: {\n",
        "        \"risk_retention_level\": np.random.uniform(0.1, 0.5),\n",
        "        \"reinsurance_attachment_point\": np.random.randint(10, 50)\n",
        "    },\n",
        "    \"Operational_Policy\":    lambda: {\n",
        "        \"policy_limit_million\": np.random.randint(500, 2000),\n",
        "        \"policy_compliance_score\": np.random.uniform(0.7, 1.0)\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "\n",
        "# Seed for 2025 consistency\n",
        "np.random.seed(2025)\n",
        "\n",
        "# =====================================================\n",
        "# 1. UNIFIED TOUCHPOINTS: THE \"INSURANCE-DESIGN\" STACK\n",
        "# =====================================================\n",
        "# We expand the list to include the \"Risk Transfer\" and \"Claims\" layers.\n",
        "TOUCHPOINTS = [\n",
        "    \"ClinicalProgramming\",     # 0: Bed count / Service lines\n",
        "    \"Risk_Underwriting\",       # 1: Liability profile / Premium base\n",
        "    \"Site_Catastrophe_Scoring\",# 2: Flood, Seismic, Wind PML (Probable Max Loss)\n",
        "    \"MedicalEquipPlanning\",    # 3: MRI/CT/Linac (Asset Replacement Value)\n",
        "    \"ArchitecturalLayout\",     # 4: Adjacency vs. Med-Mal Exposure\n",
        "    \"Structural_Seismic\",      # 5: Resilience vs. Business Interruption Insurance\n",
        "    \"Specialized_MEP\",         # 6: Life Safety / Redundancy (Loss of Utilities Risk)\n",
        "    \"ICT_Cyber_Security\",      # 7: Ransomware / EMR Liability\n",
        "    \"InfectionControlPlan\",    # 8: Biohazard risk / Liability mitigation\n",
        "    \"Claims_Sim_Proxy\",        # 9: Actuarial incident projection\n",
        "    \"ValueEngineering_Gate\",   # 10: Cost cuts vs. Deductible spikes\n",
        "    \"Regulatory_Accred\",       # 11: JCI/FGI compliance (License to Operate)\n",
        "    \"BIM_Digital_Twin\",        # 12: Precision Underwriting Data (LOD 500)\n",
        "    \"Reinsurance_Layer\",       # 13: Risk transfer for catastrophic assets\n",
        "    \"Operational_Policy\"       # 14: Final Binding Contract\n",
        "]\n",
        "\n",
        "# =====================================================\n",
        "# 2. THE DSM: FINANCIAL-TECHNICAL COUPLING\n",
        "# =====================================================\n",
        "D = len(TOUCHPOINTS)\n",
        "DSM = np.zeros((D, D))\n",
        "\n",
        "# The \"Liability Loop\": Clinical Programming (0) -> Underwriting (1) -> Claims (9) -> Regulatory (11) -> 0\n",
        "DSM[0, 1] = 0.8; DSM[1, 9] = 0.7; DSM[9, 11] = 0.9; DSM[11, 0] = 1.0\n",
        "\n",
        "# The \"Asset Protection Loop\": Med Equip (3) -> Structural (5) -> Reinsurance (13) -> Value Eng (10) -> 3\n",
        "DSM[3, 5] = 1.0; DSM[5, 13] = 0.8; DSM[13, 10] = 0.7; DSM[10, 3] = 0.9\n",
        "\n",
        "# The \"Digital/Cyber Loop\": ICT (7) -> Cyber Liability (14) -> BIM (12)\n",
        "DSM[7, 14] = 0.9; DSM[14, 12] = 0.5; DSM[12, 1] = 0.6\n",
        "\n",
        "# =====================================================\n",
        "# 3. UNIFIED DATA CONVERSIONS (ENGINEERING TO ACTUARIAL)\n",
        "# =====================================================\n",
        "# This function defines how we map physical data to insurance metrics.\n",
        "\n",
        "def engineering_to_actuarial_map():\n",
        "    return {\n",
        "        \"ClinicalProgramming\":   [\"bed_count\", \"Liability_Exposure\"],\n",
        "        \"Risk_Underwriting\":     [\"premium_base\", \"Financial_Risk\"],\n",
        "        \"Site_Catastrophe_Scoring\":[\"pml_index\", \"Natural_Hazard_Risk\"],\n",
        "        \"MedicalEquipPlanning\":  [\"asset_tiv\", \"Property_Risk\"], # Total Insured Value\n",
        "        \"Structural_Seismic\":    [\"resilience_rating\", \"BI_Risk\"], # Business Interruption\n",
        "        \"ICT_Cyber_Security\":    [\"vulnerability_score\", \"Cyber_Liability\"],\n",
        "        \"InfectionControlPlan\":  [\"airflow_safety\", \"Clinical_Risk\"],\n",
        "        \"Claims_Sim_Proxy\":      [\"incident_rate\", \"Actuarial_Risk\"]\n",
        "    }\n",
        "\n",
        "# =====================================================\n",
        "# 4. UNIFIED METRIC CALCULATOR: PREMIUM DELTA\n",
        "# =====================================================\n",
        "def calculate_premium_impact(data_df):\n",
        "    \"\"\"\n",
        "    Calculates how design decisions change the Insurance Premium.\n",
        "    Baseline: $1.0M Premium.\n",
        "    Design 'Wins' (high safety) reduce it; Risk 'Spikes' increase it.\n",
        "    \"\"\"\n",
        "    baseline_premium = 1000000\n",
        "\n",
        "    # Example logic: High Resilience in Structual (Node 5) reduces BI Insurance cost\n",
        "    struct_resilience = data_df[data_df['Touchpoint'] == 'Structural_Seismic']['value'].mean()\n",
        "    premium_mod = 1.0\n",
        "\n",
        "    if struct_resilience > 0.8:\n",
        "        premium_mod -= 0.15  # 15% discount for high resilience\n",
        "    elif struct_resilience < 0.4:\n",
        "        premium_mod += 0.30  # 30% penalty for seismic risk\n",
        "\n",
        "    # Example logic: Cyber vulnerabilities (Node 7)\n",
        "    cyber_risk = data_df[data_df['Touchpoint'] == 'ICT_Cyber_Security']['value'].mean()\n",
        "    premium_mod += (cyber_risk * 0.5) # Up to 50% increase for bad IT security\n",
        "\n",
        "    return baseline_premium * premium_mod\n",
        "\n",
        "# =====================================================\n",
        "# 5. CORE SIMULATION SUMMARY\n",
        "# =====================================================\n",
        "print(\"=== UNIFIED INSURANCE-INFRASTRUCTURE OPTIMIZER ===\")\n",
        "print(f\"Unified Nodes: {D}\")\n",
        "print(f\"Interdependency Density: {DSM.sum() / (D*D):.2%}\")\n",
        "\n",
        "# Detect Loops where Insurance dictates Design\n",
        "G = nx.DiGraph()\n",
        "for i in range(D):\n",
        "    for j in range(D):\n",
        "        if DSM[i,j] > 0: G.add_edge(TOUCHPOINTS[i], TOUCHPOINTS[j])\n",
        "\n",
        "SCCs = list(nx.strongly_connected_components(G))\n",
        "loops = [s for s in SCCs if len(s) > 1]\n",
        "\n",
        "print(\"\\n[ANALYSIS] DETECTED ACTUARIAL FEEDBACK LOOPS:\")\n",
        "for i, loop in enumerate(loops):\n",
        "    print(f\"  Cycle {i+1}: {loop}\")\n",
        "    if \"Risk_Underwriting\" in loop:\n",
        "        print(\"    *Financial Impact: Risk Management is forcing Design Rework.*\")\n",
        "\n",
        "print(\"\\n[GOAL] This model allows the user to see how adding one more HEPA filter\")\n",
        "print(\"reduces the Professional Liability Premium by calculating the clinical risk delta.\")"
      ],
      "metadata": {
        "id": "TyiLuN-7VTz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# ======================================================\n",
        "# 1. METRIC KEYS (15 Touchpoints: Design + Insurance)\n",
        "# ======================================================\n",
        "METRIC_KEYS = [\n",
        "    \"ClinicalProgramming\",    # 0\n",
        "    \"SiteLogistics\",          # 1\n",
        "    \"MedicalEquipPlanning\",   # 2\n",
        "    \"ArchitecturalLayout\",    # 3\n",
        "    \"InfectionControl\",       # 4\n",
        "    \"StructuralSeismic\",      # 5\n",
        "    \"SpecializedMEP\",         # 6\n",
        "    \"ICT_SmartHospital\",      # 7\n",
        "    \"Regulatory_Accred\",      # 8\n",
        "    \"ValueEngineering\",       # 9\n",
        "    \"BIM_DigitalTwin\",        # 10\n",
        "    \"Risk_Underwriting\",      # 11: NEW - Liability & Premiums\n",
        "    \"Reinsurance_Layer\",      # 12: NEW - Catastrophic Risk Transfer\n",
        "    \"Operational_Policy\",     # 13: NEW - Governance & Procedures\n",
        "    \"Aggregate_Safety\"        # 14: Summary Index\n",
        "]\n",
        "\n",
        "# ======================================================\n",
        "# 2. FEATURE KEYS (Expanded for Insurance Domain)\n",
        "# ======================================================\n",
        "FEATURE_KEYS = [\n",
        "    # Clinical / Site\n",
        "    'icu_bed_count', 'acuity_level', 'theatre_complexity',\n",
        "    'ambulance_access_rating', 'helipad_clearance',\n",
        "    # Technical\n",
        "    'mri_shielding_req', 'radiation_protection_mm',\n",
        "    'clinical_adjacency_score', 'infection_control_zoning',\n",
        "    'vibration_sensitivity_vc', 'seismic_importance_factor',\n",
        "    'ach_operating_room', 'medgas_redundancy',\n",
        "    'telemetry_coverage', 'nurse_call_latency',\n",
        "    # Compliance / Process\n",
        "    'fgi_compliance_gap', 'jci_safety_score',\n",
        "    've_savings_ratio', 'clash_resolution_rate',\n",
        "    'handover_completeness',\n",
        "    # Insurance / Financial (NEW)\n",
        "    'premium_base_rate', 'liability_exposure_index',\n",
        "    'risk_retention_limit', 'reinsurance_attachment_point',\n",
        "    'policy_compliance_score'\n",
        "]\n",
        "\n",
        "# ======================================================\n",
        "# 3. MAPPING (Node -> Features)\n",
        "# ======================================================\n",
        "metric_feature_map = {\n",
        "    \"ClinicalProgramming\":   ['icu_bed_count', 'theatre_complexity'],\n",
        "    \"SiteLogistics\":         ['ambulance_access_rating', 'helipad_clearance'],\n",
        "    \"MedicalEquipPlanning\":  ['mri_shielding_req', 'radiation_protection_mm'],\n",
        "    \"ArchitecturalLayout\":   ['clinical_adjacency_score', 'infection_control_zoning'],\n",
        "    \"InfectionControl\":      ['ach_operating_room', 'infection_control_zoning'],\n",
        "    \"StructuralSeismic\":     ['vibration_sensitivity_vc', 'seismic_importance_factor'],\n",
        "    \"SpecializedMEP\":        ['medgas_redundancy', 'ach_operating_room'],\n",
        "    \"ICT_SmartHospital\":     ['telemetry_coverage', 'nurse_call_latency'],\n",
        "    \"Regulatory_Accred\":     ['fgi_compliance_gap', 'jci_safety_score'],\n",
        "    \"ValueEngineering\":      ['ve_savings_ratio', 'theatre_complexity'],\n",
        "    \"BIM_DigitalTwin\":       ['clash_resolution_rate', 'handover_completeness'],\n",
        "    \"Risk_Underwriting\":     ['premium_base_rate', 'liability_exposure_index'],\n",
        "    \"Reinsurance_Layer\":     ['risk_retention_limit', 'reinsurance_attachment_point'],\n",
        "    \"Operational_Policy\":    ['policy_compliance_score', 'jci_safety_score'],\n",
        "    \"Aggregate_Safety\":      ['jci_safety_score', 'clash_resolution_rate', 'liability_exposure_index']\n",
        "}\n",
        "\n",
        "# ======================================================\n",
        "# 4. FORMULAS (Physics, Economics & Actuarial)\n",
        "# ======================================================\n",
        "METRIC_FORMULAS = [\n",
        "    lambda x: np.log(x + 1),            # 0. Clinical\n",
        "    lambda x: np.exp(-x),               # 1. Site\n",
        "    lambda x: x**1.8 / 10.0,            # 2. MedEquip\n",
        "    lambda x: x * 1.2,                  # 3. Arch\n",
        "    lambda x: 3.0 * x,                  # 4. Infection\n",
        "    lambda x: 2.5 * x,                  # 5. Structural\n",
        "    lambda x: np.tanh(x) * 2.0,         # 6. MEP\n",
        "    lambda x: 1.0 / (1.0 + x),          # 7. ICT\n",
        "    lambda x: np.exp(-x * 3.0),         # 8. Regulatory\n",
        "    lambda x: 1.0 / (x + 0.1),          # 9. ValueEng\n",
        "    lambda x: (np.arctan(x) * 2)/np.pi, # 10. BIM\n",
        "    lambda x: 1.0 / (x * 0.5 + 0.1),    # 11. Underwriting: Lower exposure = Higher Score\n",
        "    lambda x: x * 0.8,                  # 12. Reinsurance: Higher retention capacity is good\n",
        "    lambda x: np.log(x + 2),            # 13. Policy: Log benefit of strict governance\n",
        "    lambda x: np.mean(x),               # 14. Aggregate\n",
        "]\n",
        "\n",
        "# ======================================================\n",
        "# 5. TARGET MATRIX (15x15 Interaction Graph)\n",
        "# ======================================================\n",
        "# Row = Source Node, Column = Target Metric Impact\n",
        "# 0-14 Indices\n",
        "METRIC_TARGET = [\n",
        "    # 0  1  2  3  4  5  6  7  8  9  10 11 12 13 14\n",
        "    [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1], # 0: Clinical -> VE, Underwriting\n",
        "    [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1], # 1: Site -> VE, Underwriting\n",
        "    [0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1], # 2: MedEquip -> Reinsurance (High Asset Value)\n",
        "    [0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1], # 3: Arch -> Policy (Flows)\n",
        "    [0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1], # 4: Infection -> Underwriting, Policy\n",
        "    [0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1], # 5: Structural -> Reinsurance\n",
        "    [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1], # 6: MEP -> Policy\n",
        "    [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1], # 7: ICT -> Underwriting (Cyber), Policy\n",
        "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1], # 8: Regulatory (Global Constraint)\n",
        "    [1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1], # 9: Value Eng (Feedback Loop)\n",
        "    [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], # 10: BIM (Global Coordination)\n",
        "    [1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1], # 11: Underwriting -> Feedbacks to Strategy/Reins\n",
        "    [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1], # 12: Reinsurance -> Feedbacks to MedEquip/Struct\n",
        "    [0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1], # 13: Policy -> Feedbacks to Operations\n",
        "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], # 14: Aggregate\n",
        "]\n",
        "\n",
        "# ======================================================\n",
        "# 6. OPTIMIZER (Branch & Bound)\n",
        "# ======================================================\n",
        "class BranchBoundOptimizer:\n",
        "    def __init__(self, tol=1e-3, max_depth=20, minimize=True, value_range=(0.0, 10.0)):\n",
        "        self.tol = tol\n",
        "        self.max_depth = max_depth\n",
        "        self.minimize = minimize\n",
        "        self.value_range = value_range\n",
        "\n",
        "    def optimize(self, features, y=None, metric_mask=None):\n",
        "        y = np.zeros(3) if y is None else np.array(y[:3])\n",
        "        base = np.mean(list(features.values())) + np.mean(y)\n",
        "\n",
        "        a0, b0 = self.value_range\n",
        "        a0 += base\n",
        "        b0 += base\n",
        "\n",
        "        work = [(a0, b0, 0)]\n",
        "        best_x = None\n",
        "        best_score = np.inf if self.minimize else -np.inf\n",
        "\n",
        "        def better(s1, s2):\n",
        "            return s1 < s2 if self.minimize else s1 > s2\n",
        "\n",
        "        while work:\n",
        "            a, b, depth = work.pop()\n",
        "            mid = 0.5 * (a + b)\n",
        "\n",
        "            # Apply Formula if mask is active\n",
        "            mv = [f(mid) if m else 0.0 for f, m in zip(METRIC_FORMULAS, metric_mask)]\n",
        "            score = sum(mv)\n",
        "\n",
        "            if best_x is None or better(score, best_score):\n",
        "                best_x = mid\n",
        "                best_score = score\n",
        "\n",
        "            if depth >= self.max_depth or (b - a) < self.tol:\n",
        "                continue\n",
        "\n",
        "            work.append((a, mid, depth + 1))\n",
        "            work.append((mid, b, depth + 1))\n",
        "\n",
        "        return best_x\n",
        "\n",
        "# ======================================================\n",
        "# 8. EVALUATOR\n",
        "# ======================================================\n",
        "class MetricsEvaluator:\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_matrix,\n",
        "        metric_formulas=METRIC_FORMULAS,\n",
        "        metric_feature_map=metric_feature_map,\n",
        "        feature_keys=FEATURE_KEYS,\n",
        "        feature_target=None,\n",
        "        metric_target=None,\n",
        "        tol=1e-3,\n",
        "        max_depth=20,\n",
        "        minimize=False,\n",
        "        value_range=(0.0, 5.0)\n",
        "    ):\n",
        "        self.data_matrix = data_matrix\n",
        "        self.metric_formulas = metric_formulas\n",
        "        self.metric_feature_map = metric_feature_map\n",
        "        self.feature_keys = feature_keys\n",
        "\n",
        "        # Defaults\n",
        "        self.feature_target = feature_target or [[1]*len(feature_keys) for _ in range(data_matrix.shape[0])]\n",
        "        self.metric_target = metric_target or METRIC_TARGET\n",
        "        self.num_nodes = data_matrix.shape[0]\n",
        "\n",
        "        self.optimizer = BranchBoundOptimizer(\n",
        "            tol=tol,\n",
        "            max_depth=max_depth,\n",
        "            minimize=minimize,\n",
        "            value_range=value_range\n",
        "        )\n",
        "\n",
        "    def extract_features(self, node_idx):\n",
        "        if node_idx >= len(self.data_matrix): return {}\n",
        "        row = self.data_matrix[node_idx]\n",
        "        mask = self.feature_target[node_idx]\n",
        "        features = {k: v for k, v, m in zip(self.feature_keys, row, mask) if m}\n",
        "        return features\n",
        "\n",
        "    def compute_node_metrics(self, node_idx, y=None):\n",
        "        features = self.extract_features(node_idx)\n",
        "        metric_mask = self.metric_target[node_idx] if node_idx < len(self.metric_target) else [0]*len(METRIC_KEYS)\n",
        "        metric_values = {}\n",
        "\n",
        "        for key, formula, mask in zip(METRIC_KEYS, self.metric_formulas, metric_mask):\n",
        "            if mask and key in self.metric_feature_map:\n",
        "                relevant_keys = self.metric_feature_map[key]\n",
        "                relevant_features = [features[f] for f in relevant_keys if f in features]\n",
        "                x = np.mean(relevant_features) if relevant_features else 0.0\n",
        "\n",
        "                # Optimize\n",
        "                opt_value = self.optimizer.optimize(features={key: x}, y=y, metric_mask=[1])\n",
        "                metric_values[key] = formula(opt_value)\n",
        "            else:\n",
        "                metric_values[key] = 0.0\n",
        "\n",
        "        metric_values['score'] = sum(metric_values.values())\n",
        "        return metric_values\n",
        "# ======================================================\n",
        "# 8. EXECUTION\n",
        "# ======================================================\n",
        "\n",
        "# Generate Mock Data (15 Nodes x 25 Features)\n",
        "# ======================================================\n",
        "# 9. DIMENSIONALITY CONFIGURATION (15 Nodes)\n",
        "# ======================================================\n",
        "\n",
        "# Dimensions assigned based on node complexity and coupling depth.\n",
        "# \"Hub\" nodes and Financial nodes get higher dimensions.\n",
        "\n",
        "candidate_dims = [\n",
        "    [4],   # 0: ClinicalProgramming (Input)\n",
        "    [4],   # 1: SiteLogistics (Input)\n",
        "    [16],  # 2: MedicalEquipPlanning (HUB: High Value Assets)\n",
        "    [12],  # 3: ArchitecturalLayout (Coupled)\n",
        "    [12],  # 4: InfectionControl (Coupled)\n",
        "    [12],  # 5: StructuralSeismic (Coupled)\n",
        "    [16],  # 6: SpecializedMEP (HUB: The Engine)\n",
        "    [8],   # 7: ICT_SmartHospital (High Tech)\n",
        "    [12],  # 8: Regulatory_Accred (Constraint)\n",
        "    [8],   # 9: ValueEngineering (Feedback)\n",
        "    [12],  # 10: BIM_DigitalTwin (Coordination Layer)\n",
        "    [16],  # 11: Risk_Underwriting (NEW: High Complexity Financial)\n",
        "    [16],  # 12: Reinsurance_Layer (NEW: High Value/Complex)\n",
        "    [8],   # 13: Operational_Policy (Process)\n",
        "    [4]    # 14: Aggregate_Safety (Output)\n",
        "]\n",
        "\n",
        "D_graph = len(candidate_dims)"
      ],
      "metadata": {
        "id": "Q0fGU91EVabm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Ensure TOUCHPOINTS match the GENERATOR_MAP keys exactly\n",
        "TOUCHPOINTS = [\n",
        "    \"Clinical_Strategy\",     # Matches Generator Map\n",
        "    \"Risk_Underwriting\",\n",
        "    \"Site_Catastrophe_Risk\",\n",
        "    \"Medical_Planning\",\n",
        "    \"Equipment_Assets\",\n",
        "    \"Structural_Integrity\",\n",
        "    \"MEP_Systems\",\n",
        "    \"ICT_Cyber_Security\",\n",
        "    \"Infection_Control\",\n",
        "    \"Claims_History_Proxy\",\n",
        "    \"Cost_Engineering\",\n",
        "    \"Regulatory_Compliance\",\n",
        "    \"BIM_Digital_Twin\",\n",
        "    \"Reinsurance_Layer\",\n",
        "    \"Operational_Policy\"\n",
        "]\n",
        "\n",
        "# 2. Re-run the Data Matrix generation\n",
        "num_samples = 100\n",
        "feature_list = []\n",
        "\n",
        "for tp in TOUCHPOINTS:\n",
        "    # Now tp (\"Clinical_Strategy\") will correctly find the key in GENERATOR_MAP\n",
        "    samples = [GENERATOR_MAP[tp]() for _ in range(num_samples)]\n",
        "    df = pd.DataFrame(samples)\n",
        "\n",
        "    # Encode categorical text (like 'high'/'med'/'low') into numbers\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype == 'object':\n",
        "            df[col] = pd.factorize(df[col])[0]\n",
        "\n",
        "    feature_list.append(df)\n",
        "\n",
        "# 3. Build and Normalize\n",
        "DATA_MATRIX_RAW = pd.concat(feature_list, axis=1).to_numpy()\n",
        "DATA_MATRIX = (DATA_MATRIX_RAW - DATA_MATRIX_RAW.min(axis=0)) / (np.ptp(DATA_MATRIX_RAW, axis=0) + 1e-8)\n",
        "\n",
        "print(f\"Success! Data Matrix Shape: {DATA_MATRIX.shape}\")\n",
        "# =====================================================\n",
        "# 3. HIGH-FIDELITY SYNTHETIC TARGET GENERATOR\n",
        "# =====================================================\n",
        "def generate_synthetic_targets_per_node(DATA_MATRIX, candidate_dims):\n",
        "    \"\"\"\n",
        "    Transforms the normalized feature space into node-specific target signatures.\n",
        "    Uses reflective padding to preserve variance in high-dimensional risk nodes.\n",
        "    \"\"\"\n",
        "    dims_flat = [i[0] for i in candidate_dims]\n",
        "    num_nodes = len(dims_flat)\n",
        "    targets = []\n",
        "    current_col = 0\n",
        "\n",
        "    for node_idx in range(num_nodes):\n",
        "        dim = dims_flat[node_idx]\n",
        "\n",
        "        # Select the feature row for this specific touchpoint\n",
        "        # Wrapping ensures we don't index out of bounds if num_nodes > num_samples\n",
        "        row = DATA_MATRIX[node_idx % DATA_MATRIX.shape[0]]\n",
        "\n",
        "        # Extract a contiguous block of data corresponding to the node's dimensionality\n",
        "        block = row[current_col : current_col + dim]\n",
        "\n",
        "        # Padding logic: Crucial for nodes requiring higher dimensions (16)\n",
        "        # than their direct feature count. Reflective padding maintains the\n",
        "        # 'extreme' values essential for catastrophic risk modeling.\n",
        "        if len(block) < dim:\n",
        "            block = np.pad(block, (0, dim - len(block)), mode='reflect')\n",
        "\n",
        "        targets.append({\n",
        "            'node_id': node_idx,\n",
        "            'touchpoint': TOUCHPOINTS[node_idx],\n",
        "            'dim_required': dim,\n",
        "            'target': np.round(block, 4)\n",
        "        })\n",
        "\n",
        "        # Increment the column pointer with a modular wrap to recycle feature variance\n",
        "        current_col = (current_col + dim) % DATA_MATRIX.shape[1]\n",
        "\n",
        "    return targets\n",
        "\n",
        "# Execute target generation\n",
        "synthetic_targets = generate_synthetic_targets_per_node(DATA_MATRIX, candidate_dims)\n",
        "\n",
        "# =====================================================\n",
        "# 4. DIAGNOSTIC OUTPUT\n",
        "# =====================================================\n",
        "print(f\"--- 15-Node Optimization Matrix Created ---\")\n",
        "print(f\"Total Combined Features: {DATA_MATRIX.shape[1]}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for t in synthetic_targets:\n",
        "    node_desc = f\"{t['node_id']:02d} | {t['touchpoint']:<22}\"\n",
        "    print(f\"Node {node_desc} | Size: {t['dim_required']:02d} | Sig: {t['target'][:4]}...\")"
      ],
      "metadata": {
        "id": "rCIcB9UMVjhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "# Target Mask (which metrics apply to which node)\n",
        "\n",
        "def top_k_masked_probs(weights, k):\n",
        "    \"\"\"\n",
        "    Keep only top-k weights, zero out the rest, renormalize.\n",
        "    \"\"\"\n",
        "    if k >= len(weights):\n",
        "        return weights / (weights.sum() + 1e-12)\n",
        "\n",
        "    idx = np.argpartition(weights, -k)[-k:]\n",
        "    mask = np.zeros_like(weights)\n",
        "    mask[idx] = weights[idx]\n",
        "\n",
        "    s = mask.sum()\n",
        "    if s > 0:\n",
        "        mask /= s\n",
        "    return mask\n",
        "def get_all_paths(G, C, node_types, start=0, end=None):\n",
        "    \"\"\"\n",
        "    Return all simple paths that respect coupling constraints.\n",
        "    \"\"\"\n",
        "    if end is None:\n",
        "        end = G.shape[0] - 1\n",
        "\n",
        "    D = G.shape[0]\n",
        "    paths = []\n",
        "\n",
        "    def coupling_ok(i, j):\n",
        "        label = C[i, j]\n",
        "\n",
        "        # Default direct coupling\n",
        "        if \"->\" in label and \"->C\" not in label:\n",
        "            src, dst = label.split(\"->\")\n",
        "            return src == node_types[i] and dst == node_types[j]\n",
        "\n",
        "        # Escalation case (B->B->C)\n",
        "        if label == \"B->B->C\":\n",
        "            return node_types[i] == \"B\" and node_types[j] == \"B\"\n",
        "\n",
        "        return False\n",
        "\n",
        "    def dfs(node, path, visited):\n",
        "        if node == end:\n",
        "            paths.append(path.copy())\n",
        "            return\n",
        "\n",
        "        for nxt in range(D):\n",
        "            if G[node, nxt] > 0 and nxt not in visited:\n",
        "                if not coupling_ok(node, nxt):\n",
        "                    continue\n",
        "\n",
        "                visited.add(nxt)\n",
        "                dfs(nxt, path + [nxt], visited)\n",
        "                visited.remove(nxt)\n",
        "\n",
        "    dfs(start, [start], {start})\n",
        "    return paths\n",
        "\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# EXTERNAL DEPENDENCIES & CONFIGURATION\n",
        "# =============================================================================\n",
        "# These variables are referenced in the original code but not defined.\n",
        "# Assumed to be present in the execution environment.\n",
        "# -----------------------------------------------------------------------------\n",
        "# D_graph = ...\n",
        "# DATA_MATRIX = ...\n",
        "# METRIC_KEYS = [...]\n",
        "# METRIC_TARGET = [...]\n",
        "# METRIC_FORMULAS = [...]\n",
        "# METRIC_INVERSES = {...}\n",
        "# synthetic_targets = ...\n",
        "# MetricsEvaluator = ... (Class)\n",
        "# -----------------------------------------------------------------------------\n",
        "# =============================================================================\n",
        "# NEW: DETERMINISTIC DFS & PATH EVALUATOR\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "def evaluate_fixed_paths(paths, node_metrics, beta=0.3):\n",
        "    \"\"\"\n",
        "    Replaces collect_and_select_best_walks.\n",
        "    Scores the specific paths found by DFS.\n",
        "    \"\"\"\n",
        "    walks = []\n",
        "    best = None\n",
        "\n",
        "    for path in paths:\n",
        "        cost = 0.0\n",
        "        quality = 0.0\n",
        "\n",
        "        # Calculate Path Metrics\n",
        "        for node_idx in path:\n",
        "            m = node_metrics[node_idx]\n",
        "            cost += m.get(\"cost\", 0.0)\n",
        "            quality += m.get(\"quality\", 0.0)\n",
        "\n",
        "        score = quality - 1.0 * cost # using default lambda_cost=1.0\n",
        "\n",
        "        # DFS paths are unique, so count is always 1\n",
        "        count = 1\n",
        "        adjusted_score = score / (1 + beta * count)\n",
        "\n",
        "        record = {\n",
        "            \"path\": path,\n",
        "            \"score\": score,\n",
        "            \"adjusted_score\": adjusted_score,\n",
        "            \"cost\": cost,\n",
        "            \"quality\": quality,\n",
        "            \"count\": count\n",
        "        }\n",
        "        walks.append(record)\n",
        "\n",
        "        if best is None or adjusted_score > best[\"adjusted_score\"]:\n",
        "            best = record\n",
        "\n",
        "    return walks, best\n",
        "\n",
        "def dsm_walk(G, node_metrics, start=0, max_steps=50, lambda_cost=1.0):\n",
        "    D = G.shape[0]\n",
        "    target_node = D - 1  # Define Nlast\n",
        "\n",
        "    # Force start at 0 if not provided\n",
        "    current = start\n",
        "    path = [current]\n",
        "    visited = {current}\n",
        "\n",
        "    cost = 0.0\n",
        "    quality = 0.0\n",
        "\n",
        "    # Metrics for the start node\n",
        "    m_start = node_metrics[current]\n",
        "    cost += m_start.get(\"cost\", 0.0)\n",
        "    quality += m_start.get(\"quality\", 0.0)\n",
        "\n",
        "    reached_target = False\n",
        "\n",
        "    for _ in range(max_steps):\n",
        "        # If we reached the last node, stop successfully\n",
        "        if current == target_node:\n",
        "            reached_target = True\n",
        "            break\n",
        "\n",
        "        weights = np.zeros(D)\n",
        "\n",
        "        for j in range(D):\n",
        "            # Allow visiting the target even if visited (though unlikely to revisit in DAG)\n",
        "            # But generally prevent cycles\n",
        "            if j == 0:\n",
        "                continue\n",
        "            if j in visited:\n",
        "                continue\n",
        "\n",
        "            q = node_metrics[j].get(\"quality\", 0.0)\n",
        "            c = node_metrics[j].get(\"cost\", 0.0)\n",
        "\n",
        "            # Standard probability weight\n",
        "            weights[j] = G[current, j] * (q / (1 + c))\n",
        "\n",
        "        if weights.sum() == 0:\n",
        "            break\n",
        "\n",
        "        weights /= weights.sum()\n",
        "        nxt = np.random.choice(D, p=weights)\n",
        "\n",
        "        m = node_metrics[nxt]\n",
        "        cost += m.get(\"cost\", 0.0)\n",
        "        quality += m.get(\"quality\", 0.0)\n",
        "\n",
        "        path.append(nxt)\n",
        "        visited.add(nxt)\n",
        "        current = nxt\n",
        "\n",
        "    # Recalculate score based on success\n",
        "    score = quality - lambda_cost * cost\n",
        "\n",
        "    # Heavy penalty if the walk did not reach Nlast\n",
        "    if not reached_target:\n",
        "        score = -1e9\n",
        "\n",
        "    return path, score, cost, quality\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "def collect_and_select_best_walks(G, node_metrics, n_walks=300, beta=0.3):\n",
        "    walks = []\n",
        "    best = None\n",
        "    path_counts = defaultdict(int)\n",
        "    D = G.shape[0]\n",
        "\n",
        "    for _ in range(n_walks):\n",
        "        # Force start at 0\n",
        "        start = 0\n",
        "        path, score, cost, quality = dsm_walk(\n",
        "            G, node_metrics, start\n",
        "        )\n",
        "\n",
        "        # Only record if it successfully reached the last node\n",
        "        # (Score is -1e9 if it failed, per updated dsm_walk)\n",
        "        if path[-1] != (D - 1):\n",
        "            continue\n",
        "\n",
        "        key = tuple(path)\n",
        "        path_counts[key] += 1\n",
        "\n",
        "        # novelty penalty\n",
        "        adjusted_score = score / (1 + beta * path_counts[key])\n",
        "\n",
        "        record = {\n",
        "            \"path\": path,\n",
        "            \"score\": score,\n",
        "            \"adjusted_score\": adjusted_score,\n",
        "            \"cost\": cost,\n",
        "            \"quality\": quality,\n",
        "            \"count\": path_counts[key]\n",
        "        }\n",
        "        walks.append(record)\n",
        "\n",
        "        if best is None or adjusted_score > best[\"adjusted_score\"]:\n",
        "            best = record\n",
        "\n",
        "    return walks, best\n",
        "\n",
        "class CouplingState:\n",
        "    \"\"\"\n",
        "    One-step coupling automaton.\n",
        "    C = transient escalation after B->B->C\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.state = \"NORMAL\"\n",
        "\n",
        "    def update(self, coupling):\n",
        "        if coupling == \"B->B->C\":\n",
        "            self.state = \"C\"\n",
        "        else:\n",
        "            self.state = \"NORMAL\"\n",
        "\n",
        "    def allows(self, next_node_type):\n",
        "        \"\"\"\n",
        "        Rule:\n",
        "        C  random, but NOT B\n",
        "        \"\"\"\n",
        "        if self.state == \"C\" and next_node_type == \"B\":\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "def infer_node_types(node_metrics):\n",
        "    \"\"\"\n",
        "    A = quality-dominant\n",
        "    B = cost-dominant\n",
        "    \"\"\"\n",
        "    node_types = []\n",
        "    for m in node_metrics:\n",
        "        q = m.get(\"quality\", 0.0)\n",
        "        c = m.get(\"cost\", 0.0)\n",
        "        node_types.append(\"A\" if q >= c else \"B\")\n",
        "    return node_types\n",
        "\n",
        "def build_coupling_matrix(node_types):\n",
        "    \"\"\"\n",
        "    Builds a D x D coupling-label matrix.\n",
        "    C exists only as a coupling escalation (B->B->C).\n",
        "    \"\"\"\n",
        "    D = len(node_types)\n",
        "    C = np.empty((D, D), dtype=object)\n",
        "\n",
        "    for i in range(D):\n",
        "        for j in range(D):\n",
        "            src = node_types[i]\n",
        "            dst = node_types[j]\n",
        "\n",
        "            if src == \"B\" and dst == \"B\":\n",
        "                C[i, j] = \"B->B->C\"\n",
        "            else:\n",
        "                C[i, j] = f\"{src}->{dst}\"\n",
        "\n",
        "    return C\n",
        "\n",
        "\n",
        "def coupling_weight(coupling_label, metrics_i, metrics_j):\n",
        "    \"\"\"\n",
        "    OHMIC GF: I = V / R\n",
        "    Treats the transition as a vector field.\n",
        "    \"\"\"\n",
        "    # Vector extraction\n",
        "    v_i = np.array(list(metrics_i.values())) if isinstance(metrics_i, dict) else np.array(metrics_i)\n",
        "    v_j = np.array(list(metrics_j.values())) if isinstance(metrics_j, dict) else np.array(metrics_j)\n",
        "\n",
        "    # 1. Voltage (Potential Gain): Only conduct on improvement\n",
        "    v_diff = np.maximum(v_j - v_i, 0)\n",
        "    voltage = np.linalg.norm(v_diff) + 1e-6\n",
        "\n",
        "    # 2. Resistance (Node Friction): Destination cost/complexity\n",
        "    resistance = np.mean(v_j) + 0.1\n",
        "\n",
        "    # 3. Current (Conductance)\n",
        "    conductance = voltage / resistance\n",
        "\n",
        "    # Type A = High Conductance, Type B = High Damping\n",
        "    valve = 1.3 if \"A\" in coupling_label else 0.7\n",
        "    return max(conductance * valve, 0.0001)\n",
        "\n",
        "# Configuration\n",
        "#candidate_dims = [[2], [2], [2], [2], [2], [2], [2], [2], [1]]\n",
        "outer_generations = 1\n",
        "inner_learning = 0.1\n",
        "gamma_interlayer = 1\n",
        "top_k = 21\n",
        "\n",
        "# Initialize random state\n",
        "np.random.seed()\n",
        "seed = None  # Placeholder as per original logic\n",
        "\n",
        "# Placeholder for Data Matrix generation (from original snippet)\n",
        "# new_DATA_MATRIX = np.random.rand(D_graph, DATA_MATRIX.shape[1])\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# HELPER CLASSES\n",
        "# =============================================================================\n",
        "class DSM_Tracker:\n",
        "    \"\"\"\n",
        "    Tracks a DSM (Design Structure Matrix) layer and its residual.\n",
        "    \"\"\"\n",
        "    def __init__(self, multiplex_layer):\n",
        "        self.layer = multiplex_layer\n",
        "        self.primary_dsm = None\n",
        "        self.residual_dsm = None\n",
        "        self.update_dsms()\n",
        "\n",
        "    def update_dsms(self):\n",
        "        if self.primary_dsm is None:\n",
        "            # First time: store current DSM as reference\n",
        "            self.primary_dsm = self.layer.chosen_Gmat.copy()\n",
        "            self.residual_dsm = np.zeros_like(self.primary_dsm)\n",
        "        else:\n",
        "            # Update residual: current DSM minus primary\n",
        "            current = self.layer.chosen_Gmat\n",
        "            self.residual_dsm = current - self.primary_dsm\n",
        "\n",
        "    def get_matrices(self):\n",
        "        return self.primary_dsm, self.residual_dsm\n",
        "\n",
        "    def print_matrices(self):\n",
        "        print(\"\\n--- Primary DSM ---\")\n",
        "        print(self.primary_dsm)\n",
        "        print(\"\\n--- Residual DSM ---\")\n",
        "        print(self.residual_dsm)\n",
        "\n",
        "\n",
        "class DSM_Layer_Decomposer:\n",
        "    \"\"\"\n",
        "    Manages the additive decomposition of DSM layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, baseline_matrix, mode='additive'):\n",
        "        self.baseline_matrix = baseline_matrix.copy()\n",
        "        self.current_total = baseline_matrix.copy()\n",
        "        self.mode = mode\n",
        "        self.layers = []\n",
        "        self.residuals = []\n",
        "\n",
        "    def add_snapshot(self, new_total_matrix):\n",
        "        \"\"\"\n",
        "        Calculates the DELTA (change) between the new state and the previous state,\n",
        "        stores that delta as a layer.\n",
        "        \"\"\"\n",
        "        delta = new_total_matrix - self.current_total\n",
        "        self.layers.append(delta.copy())\n",
        "\n",
        "        # Update current tracker\n",
        "        self.current_total = new_total_matrix.copy()\n",
        "\n",
        "        # Calculate residual (Difference from the baseline)\n",
        "        residual = self.current_total - self.baseline_matrix\n",
        "        self.residuals.append(residual)\n",
        "\n",
        "        layer_id = len(self.layers) - 1\n",
        "        print(f\"\\n=== DSM LAYER {layer_id} CAPTURED ===\")\n",
        "        print(f\"Layer Contribution (Delta):\\n{np.round(delta, 3)}\")\n",
        "\n",
        "        return delta\n",
        "\n",
        "    def get_reconstruction(self):\n",
        "        return np.sum(self.layers, axis=0)\n",
        "\n",
        "\n",
        "class SVM:\n",
        "    \"\"\"\n",
        "    Metric-inverse multi-output SVM with epsilon-insensitive loss.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, output_dim=None, metric_keys=None, lr=0.001, epsilon=0.1):\n",
        "        self.input_dim = input_dim\n",
        "        self.metric_keys = metric_keys\n",
        "\n",
        "        if metric_keys is not None:\n",
        "            self.output_dim = len(metric_keys)\n",
        "        elif output_dim is not None:\n",
        "            self.output_dim = output_dim\n",
        "        else:\n",
        "            self.output_dim = 1\n",
        "\n",
        "        self.lr = lr\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        # Lazy initialization\n",
        "        self.X_train = None\n",
        "        self.y_train = None\n",
        "        self.alpha = None\n",
        "        self.b = None\n",
        "\n",
        "    def train_step(self, X, y_true):\n",
        "        X = np.array(X)\n",
        "        y_true = np.array(y_true)\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        if self.X_train is None:\n",
        "            self.X_train = X.copy()\n",
        "            self.y_train = y_true.copy()\n",
        "            self.alpha = np.zeros((X.shape[1], self.output_dim))\n",
        "            self.b = np.zeros(self.output_dim)\n",
        "\n",
        "        # Linear kernel\n",
        "        y_pred = X.dot(self.alpha) + self.b\n",
        "\n",
        "        # Epsilon-insensitive loss\n",
        "        diff = y_pred - y_true\n",
        "        mask = np.abs(diff) > self.epsilon\n",
        "        diff *= mask\n",
        "\n",
        "        grad_alpha = X.T.dot(diff) / n_samples\n",
        "        grad_b = diff.mean(axis=0)\n",
        "\n",
        "        self.alpha -= self.lr * grad_alpha\n",
        "        self.b -= self.lr * grad_b\n",
        "\n",
        "        loss = np.mean(np.maximum(0, np.abs(y_pred - y_true) - self.epsilon))\n",
        "        return loss\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.array(X)\n",
        "        y_pred = X.dot(self.alpha) + self.b\n",
        "\n",
        "        if self.metric_keys is None:\n",
        "            return y_pred\n",
        "\n",
        "        # Apply metric inverses\n",
        "        y_transformed = np.zeros_like(y_pred)\n",
        "        for i, key in enumerate(self.metric_keys):\n",
        "            inverse_fn = METRIC_INVERSES[key]\n",
        "            # Handle potential list return from inverse_fn\n",
        "            val_func = lambda y: inverse_fn(y)[0] if isinstance(inverse_fn(y), list) else inverse_fn(y)\n",
        "            y_transformed[:, i] = np.array([val_func(y) for y in y_pred[:, i]])\n",
        "\n",
        "        return y_transformed\n",
        "\n",
        "\n",
        "class InterLayer:\n",
        "    def __init__(self, D_graph, max_inner_dim, inter_dim=None, edge_threshold=0.02, gamma=1.0, seed=42):\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "\n",
        "        self.D_graph = D_graph\n",
        "        self.edge_threshold = edge_threshold\n",
        "        self.gamma = gamma\n",
        "\n",
        "        # Handle list vs int inputs for dimensions\n",
        "        m_dim = max_inner_dim[0] if isinstance(max_inner_dim, list) else max_inner_dim\n",
        "\n",
        "        if inter_dim is not None:\n",
        "            self.inter_dim = inter_dim[0] if isinstance(inter_dim, list) else inter_dim\n",
        "        else:\n",
        "            self.inter_dim = m_dim\n",
        "\n",
        "        self.max_input = 2 * m_dim\n",
        "\n",
        "        # Initialize weights\n",
        "        self.weights = {}\n",
        "        self.bias = {}\n",
        "        for i in range(D_graph):\n",
        "            for j in range(D_graph):\n",
        "                if i != j:\n",
        "                    w_init = np.random.uniform(-0.1, 0.1, (self.inter_dim, self.max_input))\n",
        "                    self.weights[(i, j)] = w_init\n",
        "                    self.bias[(i, j)] = np.zeros(self.inter_dim)\n",
        "\n",
        "    def compute_edge_activation(self, i, j, nested_reps):\n",
        "        concat = np.concatenate([nested_reps[i], nested_reps[j]])\n",
        "        # Pad or truncation\n",
        "        if len(concat) < self.max_input:\n",
        "            concat = np.pad(concat, (0, self.max_input - len(concat)))\n",
        "        else:\n",
        "            concat = concat[:self.max_input]\n",
        "\n",
        "        # Normalize\n",
        "        concat = (concat - np.mean(concat)) / (np.std(concat) + 1e-12)\n",
        "\n",
        "        # Activation\n",
        "        v = self.weights[(i, j)].dot(concat) + self.bias[(i, j)]\n",
        "        input_strength = np.clip(np.mean(np.abs(concat)), 0, 1)\n",
        "        v = v * input_strength\n",
        "\n",
        "        return 1 / (1 + np.exp(-v))\n",
        "\n",
        "    def build_activations(self, Gmat, nested_reps):\n",
        "        acts = {}\n",
        "        for i in range(self.D_graph):\n",
        "            for j in range(self.D_graph):\n",
        "                if i == j:\n",
        "                    continue\n",
        "                if abs(Gmat[i, j]) > self.edge_threshold:\n",
        "                    acts[(i, j)] = self.compute_edge_activation(i, j, nested_reps)\n",
        "        return acts\n",
        "\n",
        "    @staticmethod\n",
        "    def pairwise_squared_corr(acts):\n",
        "        if len(acts) < 2:\n",
        "            return 0.0\n",
        "        A = np.stack(list(acts.values()))\n",
        "        A_centered = A - A.mean(axis=1, keepdims=True)\n",
        "        stds = np.sqrt(np.sum(A_centered**2, axis=1) / (A.shape[1]-1) + 1e-12)\n",
        "        cov = A_centered @ A_centered.T / (A.shape[1]-1)\n",
        "        corr = cov / (np.outer(stds, stds) + 1e-12)\n",
        "        np.fill_diagonal(corr, 0)\n",
        "        return float((corr**2).sum())\n",
        "\n",
        "    def mi_for_graph(self, Gmat, nested_reps):\n",
        "        acts = self.build_activations(Gmat, nested_reps)\n",
        "        if not acts:\n",
        "            return 0.0\n",
        "        return self.gamma * self.pairwise_squared_corr(acts)\n",
        "\n",
        "def build_dsm_from_walks(D, paths):\n",
        "    \"\"\"\n",
        "    Constructs a DSM where entry [i,j] is the probability\n",
        "    that a successful process moves from i to j.\n",
        "    \"\"\"\n",
        "    flow = np.zeros((D, D))\n",
        "\n",
        "    # Count transitions\n",
        "    for path in paths:\n",
        "        for k in range(len(path) - 1):\n",
        "            u, v = path[k], path[k+1]\n",
        "            flow[u, v] += 1\n",
        "\n",
        "    # Normalize by the total number of successful walks.\n",
        "    # This prevents 'saturation'if an edge is rarely used, it stays small.\n",
        "    n_paths = len(paths)\n",
        "    if n_paths > 0:\n",
        "        flow = flow / n_paths\n",
        "\n",
        "    np.fill_diagonal(flow, 0.0)\n",
        "    return flow\n",
        "# =============================================================================\n",
        "# MAIN OPTIMIZER CLASS\n",
        "# =============================================================================\n",
        "class MetricDrivenRandomWalk:\n",
        "    def __init__(self, coupling_matrix, node_metrics, max_steps=50):\n",
        "        self.C = coupling_matrix\n",
        "        self.node_metrics = node_metrics\n",
        "        self.D = coupling_matrix.shape[0]\n",
        "        self.max_steps = max_steps\n",
        "\n",
        "    def transition_probs(self, i, k=2):\n",
        "        w = np.zeros(self.D)\n",
        "\n",
        "        for j in range(self.D):\n",
        "            if i == j:\n",
        "                continue\n",
        "\n",
        "            weight = coupling_weight(self.C[i, j], self.node_metrics[j])\n",
        "\n",
        "            # Directional heuristic (keep yours)\n",
        "            if j > i:\n",
        "                weight *= 1.2\n",
        "            elif j < i:\n",
        "                weight *= 0.8\n",
        "\n",
        "            w[j] = weight\n",
        "\n",
        "        # If nothing viable, fallback to uniform\n",
        "        if w.sum() == 0:\n",
        "            return np.ones(self.D) / self.D\n",
        "\n",
        "        # Emphasize strong edges\n",
        "        w = w ** 2\n",
        "\n",
        "        #  HARD SPARSITY CONSTRAINT (TOP-K)\n",
        "        w = top_k_masked_probs(w, k)\n",
        "\n",
        "        return w\n",
        "\n",
        "\n",
        "    def run(self, start=0):\n",
        "        path = [start]\n",
        "        current = start\n",
        "        target_node = self.D - 1\n",
        "\n",
        "        for _ in range(self.max_steps):\n",
        "            if current == target_node:\n",
        "                break\n",
        "\n",
        "            probs = self.transition_probs(current)\n",
        "\n",
        "            # Move\n",
        "            current = np.random.choice(self.D, p=probs)\n",
        "            path.append(current)\n",
        "\n",
        "            if current == target_node:\n",
        "                break\n",
        "\n",
        "        return path\n",
        "class MetricDrivenRandomWalk:\n",
        "    def __init__(self, coupling_matrix, node_metrics, max_steps=50, top_k=2):\n",
        "        self.C = coupling_matrix\n",
        "        self.node_metrics = node_metrics\n",
        "        self.D = coupling_matrix.shape[0]\n",
        "        self.max_steps = max_steps\n",
        "        self.top_k = top_k  # <--- FIX: Store the parameter\n",
        "\n",
        "    def transition_probs(self, i):\n",
        "        w = np.zeros(self.D)\n",
        "\n",
        "        for j in range(self.D):\n",
        "            if i == j:\n",
        "                continue\n",
        "\n",
        "            weight = coupling_weight(self.C[i, j], self.node_metrics[j])\n",
        "\n",
        "            # Directional heuristic\n",
        "            if j > i:\n",
        "                weight *= 1.2\n",
        "            elif j < i:\n",
        "                weight *= 0.8\n",
        "\n",
        "            w[j] = weight\n",
        "\n",
        "        if w.sum() == 0:\n",
        "            return np.ones(self.D) / self.D\n",
        "\n",
        "        w = w ** 2\n",
        "\n",
        "        #  FIX: Use the stored self.top_k\n",
        "        w = top_k_masked_probs(w, self.top_k)\n",
        "\n",
        "        return w\n",
        "\n",
        "    def run(self, start=0):\n",
        "        path = [start]\n",
        "        current = start\n",
        "        target_node = self.D - 1\n",
        "\n",
        "        for _ in range(self.max_steps):\n",
        "            if current == target_node:\n",
        "                break\n",
        "\n",
        "            probs = self.transition_probs(current)\n",
        "\n",
        "            # Move\n",
        "            current = np.random.choice(self.D, p=probs)\n",
        "            path.append(current)\n",
        "\n",
        "            if current == target_node:\n",
        "                break\n",
        "\n",
        "        return path\n",
        "class MetricDrivenRandomWalk:\n",
        "    def __init__(self, coupling_matrix, node_metrics, metric_targets, max_steps=100, top_k=2):\n",
        "        self.C = coupling_matrix\n",
        "        self.node_metrics = node_metrics\n",
        "        self.metric_targets = metric_targets\n",
        "        self.D = coupling_matrix.shape[0]\n",
        "        self.max_steps = max_steps\n",
        "        self.top_k = top_k\n",
        "\n",
        "        #  Precompute the base Ohmic Conductance (The 'Clean' Pipe)\n",
        "        self.Gamma_base = self._precompute_conductance()\n",
        "\n",
        "    def _precompute_conductance(self):\n",
        "        gamma_mat = np.zeros((self.D, self.D))\n",
        "        for i in range(self.D):\n",
        "            for j in range(self.D):\n",
        "                if i != j and self.metric_targets[i][j] != 0:\n",
        "                    # Calculate I = V / R using the means of the metric vectors\n",
        "                    gamma_mat[i, j] = coupling_weight(self.C[i, j], self.node_metrics[i], self.node_metrics[j])\n",
        "        return gamma_mat\n",
        "\n",
        "    def run(self, start=0):\n",
        "        path = [start]\n",
        "        current = start\n",
        "        target_node = self.D - 1\n",
        "\n",
        "        for _ in range(self.max_steps):\n",
        "            if current == target_node:\n",
        "                break\n",
        "\n",
        "            # 1. Grab precomputed conductances for 'current'\n",
        "            w = self.Gamma_base[current, :].copy()\n",
        "\n",
        "            # 2. APPLY MEANS-BASED FATIGUE (The Singularity Breaker)\n",
        "            # We penalize nodes based on the MEAN of their appearance in the path\n",
        "            for j in range(self.D):\n",
        "                if j in path:\n",
        "                    # Every visit reduces the conductance by a mean-factor\n",
        "                    # This increases the 'Resistance' of the loop\n",
        "                    count = path.count(j)\n",
        "                    w[j] *= (1.0 / (1.0 + count))\n",
        "\n",
        "            # 3. DIRECTIONAL BIAS (The Gravity Jump)\n",
        "            # Ensure the jump 'means' something by favoring forward progress\n",
        "            indices = np.arange(self.D)\n",
        "            w[indices > current] *= 1.5  # Potential to jump ahead\n",
        "            w[indices < current] *= 0.2  # Heavy friction for jumping back\n",
        "\n",
        "            # 4. EXECUTE THE JUMP\n",
        "            if w.sum() == 0:\n",
        "                break # Circuit broken\n",
        "\n",
        "            # Sharpen the distribution and pick the best K\n",
        "            probs = top_k_masked_probs(w**2, self.top_k)\n",
        "            current = np.random.choice(self.D, p=probs)\n",
        "            path.append(current)\n",
        "\n",
        "        return path\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MetricDrivenRandomWalk:\n",
        "    def __init__(self, coupling_matrix, node_metrics, metric_targets, max_steps=100, top_k=2):\n",
        "        self.C = coupling_matrix\n",
        "        self.node_metrics = node_metrics\n",
        "        self.metric_targets = metric_targets\n",
        "        self.D = coupling_matrix.shape[0]\n",
        "        self.max_steps = max_steps\n",
        "        self.top_k = top_k\n",
        "\n",
        "        # 1. Internal Enigma State (The 'Rotor' Configuration)\n",
        "        # This permutation vector re-maps the node indices on every jump\n",
        "        self.enigma_state = np.arange(self.D)\n",
        "\n",
        "        # 2. Precompute the Base Ohmic Conductance (Static Wiring)\n",
        "        self.Gamma_base = self._precompute_conductance()\n",
        "\n",
        "    def _precompute_conductance(self):\n",
        "        \"\"\"Standard Ohmic Flow: Current = Voltage / Mean(Metrics)\"\"\"\n",
        "        gamma_mat = np.zeros((self.D, self.D))\n",
        "        for i in range(self.D):\n",
        "            for j in range(self.D):\n",
        "                if i != j and self.metric_targets[i][j] != 0:\n",
        "                    # Ohmic GF: delta Quality / Mean Friction\n",
        "                    gamma_mat[i, j] = coupling_weight(self.C[i, j], self.node_metrics[i], self.node_metrics[j])\n",
        "        return gamma_mat\n",
        "\n",
        "    def rotate_rotors(self, current_node):\n",
        "        \"\"\"\n",
        "        The Enigma Shift: Permutes the state based on the 'Mean' of the node.\n",
        "        This changes the available 'exits' for the next step.\n",
        "        \"\"\"\n",
        "        metrics = self.node_metrics[current_node]\n",
        "        # Use the mean as the 'Notch' on the rotor\n",
        "        mean_val = np.mean(list(metrics.values())) if isinstance(metrics, dict) else np.mean(metrics)\n",
        "\n",
        "        # Shift the state machine by the intensity of the current node\n",
        "        shift = int(mean_val * 10) % self.D\n",
        "        self.enigma_state = np.roll(self.enigma_state, shift)\n",
        "\n",
        "    def run(self, start=0):\n",
        "        path = [start]\n",
        "        current = start\n",
        "        target_node = self.D - 1\n",
        "\n",
        "        for step in range(self.max_steps):\n",
        "            if current == target_node:\n",
        "                break\n",
        "\n",
        "            #  STEP 1: Rotate the Enigma State Machine\n",
        "            self.rotate_rotors(current)\n",
        "\n",
        "            #  STEP 2: Get Base Weights\n",
        "            w = self.Gamma_base[current, :].copy()\n",
        "\n",
        "            #  STEP 3: Permutation Mapping\n",
        "            # Scramble the available flows through the current Enigma configuration\n",
        "            # This makes the 'Singularity' impossible because the weights shift every visit\n",
        "            w = w[self.enigma_state]\n",
        "\n",
        "            #  STEP 4: Directional Bias (Inertia)\n",
        "            indices = np.arange(self.D)\n",
        "            w[indices > current] *= 2.0  # Forward Pull\n",
        "            w[indices < current] *= 0.1  # Backward Resistance (Rework)\n",
        "\n",
        "            if w.sum() == 0:\n",
        "                break # Open Circuit\n",
        "\n",
        "            #  STEP 5: Jump\n",
        "            probs = top_k_masked_probs(w**2, self.top_k)\n",
        "            current = np.random.choice(self.D, p=probs)\n",
        "            path.append(current)\n",
        "\n",
        "        return path\n",
        "\n",
        "class MetricDrivenRandomWalk:\n",
        "    def __init__(self, coupling_matrix, node_metrics, metric_targets, max_steps=100, top_k=2):\n",
        "        self.C = coupling_matrix\n",
        "        self.node_metrics = node_metrics\n",
        "        self.metric_targets = metric_targets\n",
        "        self.D = coupling_matrix.shape[0]\n",
        "        self.max_steps = max_steps\n",
        "        self.top_k = top_k\n",
        "\n",
        "        #  PRE-COMPUTED MANIFOLD (The Speed Fix)\n",
        "        # We build the 'Enigma' logic directly into the static weights\n",
        "        self.Gamma_Enigma = self._precompute_enigma_manifold()\n",
        "\n",
        "    def _precompute_enigma_manifold(self):\n",
        "        \"\"\"\n",
        "        Calculates the entire state-space once.\n",
        "        Incorporates 'Mean Resistance' into the structure.\n",
        "        \"\"\"\n",
        "        manifold = np.zeros((self.D, self.D))\n",
        "        for i in range(self.D):\n",
        "            m_i = self.node_metrics[i]\n",
        "            # Mean Friction of source node acts as a Rotor Notch\n",
        "            friction_i = np.mean(list(m_i.values())) if isinstance(m_i, dict) else np.mean(m_i)\n",
        "\n",
        "            for j in range(self.D):\n",
        "                if i != j and self.metric_targets[i][j] != 0:\n",
        "                    # Ohmic Conductance\n",
        "                    base_i = coupling_weight(self.C[i, j], m_i, self.node_metrics[j])\n",
        "\n",
        "                    # ENIGMA BIAS: Favor jumps that 'match' the rotor phase\n",
        "                    # This replaces the slow np.roll() inside the run loop\n",
        "                    phase_shift = (i + j) % self.D\n",
        "                    enigma_mod = 1.0 + (0.1 * np.sin(phase_shift * friction_i))\n",
        "\n",
        "                    # GRAVITY BIAS: Forward progress is structurally cheaper\n",
        "                    gravity = 2.0 if j > i else 0.2\n",
        "\n",
        "                    manifold[i, j] = base_i * enigma_mod * gravity\n",
        "        return manifold\n",
        "\n",
        "    def run(self, start=0):\n",
        "        path = [start]\n",
        "        current = start\n",
        "        target = self.D - 1\n",
        "\n",
        "        # Pre-slice Gamma to avoid lookups\n",
        "        G = self.Gamma_Enigma\n",
        "\n",
        "        for _ in range(self.max_steps):\n",
        "            if current == target:\n",
        "                break\n",
        "\n",
        "            #  O(1) LOOKUP: No math, no counts, no state shifts\n",
        "            w = G[current].copy()\n",
        "\n",
        "            # Apply a light novelty penalty only if we hit a loop\n",
        "            if current in path[:-1]:\n",
        "                w *= 0.1 # Instant circuit breaker\n",
        "\n",
        "            if w.sum() == 0:\n",
        "                break\n",
        "\n",
        "            # Probabilistic Jump\n",
        "            probs = top_k_masked_probs(w**2, self.top_k)\n",
        "            current = np.random.choice(self.D, p=probs)\n",
        "            path.append(current)\n",
        "\n",
        "        return path"
      ],
      "metadata": {
        "id": "7Z_mRbD4VpM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def build_coupling_weight(label, m_i, m_j):\n",
        "    \"\"\"\n",
        "    Interpretation: Pascal's Law P = F / A\n",
        "    Force (F) = Potential delta in metrics (Improvement)\n",
        "    Area (A) = Complexity/Friction of the destination node\n",
        "    Valve = Orifice efficiency based on coupling type\n",
        "    \"\"\"\n",
        "    # 1. THE NODAL VECTORS\n",
        "    v_i = np.array(list(m_i.values())) if isinstance(m_i, dict) else np.array(m_i)\n",
        "    v_j = np.array(list(m_j.values())) if isinstance(m_j, dict) else np.array(m_j)\n",
        "\n",
        "    # 2. THE DRIVING FORCE (F)\n",
        "    # The pressure only builds if the destination offers more \"potential\"\n",
        "    # F = || max(0, v_j - v_i) ||\n",
        "    force = np.linalg.norm(np.maximum(v_j - v_i, 0)) + 1e-9\n",
        "\n",
        "    # 3. THE SURFACE AREA (A)\n",
        "    # The 'wider' the destination node (more complex/costly),\n",
        "    # the more the force is distributed, lowering the pressure.\n",
        "    surface_area = np.mean(v_j) + 0.1\n",
        "\n",
        "    # 4. THE VALVE ORIFICE (Efficiency)\n",
        "    # A-type couplings are 'Wide Nozzles' (High pressure translation)\n",
        "    # B-type couplings are 'Constricted Nozzles' (Damped flow)\n",
        "    label_str = str(label) if label is not None else \"\"\n",
        "    if \"A\" in label_str:\n",
        "        motivator = force # High flow efficiency\n",
        "    elif \"B\" in label_str:\n",
        "        motivator =  1.0  # Damped efficiency\n",
        "    else:\n",
        "        motivator = surface_area# Standard atmospheric pressure\n",
        "\n",
        "    # 5. THE SYSTEM PRESSURE (P)\n",
        "    # (i) Power driver\n",
        "    # (iii) Force driver\n",
        "    # (ii) Pressure\n",
        "    pressure = motivator * (force / surface_area)\n",
        "\n",
        "    return max(pressure, 0.0001)\n",
        "def infer_node_types(node_metrics):\n",
        "    \"\"\"\n",
        "    Determines node type based on Cost vs Quality dominance.\n",
        "    A = quality-dominant, B = cost-dominant.\n",
        "    \"\"\"\n",
        "    node_types = []\n",
        "    for m in node_metrics:\n",
        "        # Handle dict or list input\n",
        "        if isinstance(m, dict):\n",
        "            q = m.get(\"quality\", 0.0)\n",
        "            c = m.get(\"cost\", 0.0)\n",
        "        else:\n",
        "            # Assuming [cost, quality] or similar if list,\n",
        "            # but defaulting to A if structure unknown\n",
        "            q, c = 1, 0\n",
        "        node_types.append(\"A\" if q >= c else \"B\")\n",
        "    return node_types\n",
        "\n",
        "def build_coupling_matrix(node_types):\n",
        "    \"\"\"\n",
        "    Builds a D x D coupling-label matrix.\n",
        "    C exists only as a coupling escalation (B->B->C).\n",
        "    \"\"\"\n",
        "    D = len(node_types)\n",
        "    C = np.empty((D, D), dtype=object)\n",
        "\n",
        "    for i in range(D):\n",
        "        for j in range(D):\n",
        "            src = node_types[i]\n",
        "            dst = node_types[j]\n",
        "\n",
        "            if src == \"B\" and dst == \"B\":\n",
        "                C[i, j] = \"B->B->C\"\n",
        "            else:\n",
        "                C[i, j] = f\"{src}->{dst}\"\n",
        "    return C\n",
        "\n",
        "def build_dsm_from_walks(D, paths):\n",
        "    \"\"\"\n",
        "    Constructs a DSM (Design Structure Matrix) from walk paths.\n",
        "    Entry [i,j] is the probability that a process moves from i to j.\n",
        "    \"\"\"\n",
        "    flow = np.zeros((D, D))\n",
        "    for path in paths:\n",
        "        for k in range(len(path) - 1):\n",
        "            u, v = path[k], path[k+1]\n",
        "            flow[u, v] += 1\n",
        "\n",
        "    n_paths = len(paths)\n",
        "    if n_paths > 0:\n",
        "        flow = flow / n_paths\n",
        "\n",
        "    np.fill_diagonal(flow, 0.0)\n",
        "    return flow\n",
        "\n",
        "# =============================================================================\n",
        "# 2. THE METRIC-DRIVEN RANDOM WALKER (Consolidated)\n",
        "# =============================================================================\n",
        "\n",
        "class CouplingState:\n",
        "    \"\"\"\n",
        "    One-step coupling automaton to enforce B->B->C rules.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.state = \"NORMAL\"\n",
        "\n",
        "    def update(self, coupling):\n",
        "        if coupling == \"B->B->C\":\n",
        "            self.state = \"C\"\n",
        "        else:\n",
        "            self.state = \"NORMAL\"\n",
        "\n",
        "    def allows(self, next_node_type):\n",
        "        # Rule: After C state, random is allowed, but strictly NOT B\n",
        "        if self.state == \"C\" and next_node_type == \"B\":\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "class MetricDrivenRandomWalk:\n",
        "    def __init__(self, coupling_matrix, node_metrics, metric_targets=None, max_steps=50, top_k=2, temperature=1.0):\n",
        "        \"\"\"\n",
        "        Unified Walker Class.\n",
        "        :param coupling_matrix: DxD matrix of transition labels.\n",
        "        :param node_metrics: List of dicts/vectors for node values.\n",
        "        :param metric_targets: (Optional) Binary mask or Target Matrix.\n",
        "        :param max_steps: Maximum path length.\n",
        "        :param top_k: Branching factor constraint.\n",
        "        :param temperature: Control randomness (Higher = more random, Lower = more deterministic).\n",
        "        \"\"\"\n",
        "        self.C = coupling_matrix\n",
        "        self.node_metrics = node_metrics\n",
        "        self.node_types = infer_node_types(node_metrics)\n",
        "\n",
        "        self.D = coupling_matrix.shape[0]\n",
        "        self.max_steps = max_steps\n",
        "        self.top_k = top_k\n",
        "        self.temperature = temperature\n",
        "\n",
        "        # Handle Mask: If None, allow all connections (ones)\n",
        "        if metric_targets is not None:\n",
        "            self.mask = np.array(metric_targets)\n",
        "        else:\n",
        "            self.mask = np.ones((self.D, self.D))\n",
        "\n",
        "    def run(self, start=0):\n",
        "        path = [start]\n",
        "        current = start\n",
        "        target_node = self.D - 1\n",
        "\n",
        "        # Initialize Automaton for this specific run\n",
        "        automaton = CouplingState()\n",
        "\n",
        "        for _ in range(self.max_steps):\n",
        "            if current == target_node:\n",
        "                break\n",
        "\n",
        "            # 1. Calculate Raw Weights (Ohmic Conductance)\n",
        "            weights = np.zeros(self.D)\n",
        "            for j in range(self.D):\n",
        "                if current == j: continue\n",
        "\n",
        "                # A. Structural Mask Check (User provided mask)\n",
        "                if self.mask[current, j] == 0:\n",
        "                    continue\n",
        "\n",
        "                # B. Automaton Rule Check\n",
        "                if not automaton.allows(self.node_types[j]):\n",
        "                    continue\n",
        "\n",
        "                # C. Compute Physics-based Weight\n",
        "                base_w = build_coupling_weight(self.C[current, j], self.node_metrics[current], self.node_metrics[j])\n",
        "\n",
        "                # D. Directional Bias (Gravity)\n",
        "                gravity = 1.5 if j > current else 0.4\n",
        "\n",
        "                # E. Loop Friction (Novelty check)\n",
        "                friction = 0.1 if j in path else 1.0\n",
        "\n",
        "                weights[j] = base_w * gravity * friction\n",
        "\n",
        "            # 2. Check for Dead End\n",
        "            if weights.sum() == 0:\n",
        "                break\n",
        "\n",
        "            # 3. Apply Temperature (Softmax-ish scaling) for True Randomness\n",
        "            # Normalize first to avoid overflow in exp\n",
        "            weights = weights / (weights.max() + 1e-12)\n",
        "            exp_weights = np.exp(weights / self.temperature)\n",
        "\n",
        "            # Zero out the ones that were originally zero\n",
        "            exp_weights[weights == 0] = 0\n",
        "\n",
        "            # 4. Top-K Sparsity & Normalization\n",
        "            probs = top_k_masked_probs(exp_weights, self.top_k)\n",
        "\n",
        "            # 5. Stochastic Jump\n",
        "            nxt = np.random.choice(self.D, p=probs)\n",
        "\n",
        "            # Update state\n",
        "            automaton.update(self.C[current, nxt])\n",
        "            path.append(nxt)\n",
        "            current = nxt\n",
        "\n",
        "        return path\n",
        "from collections import deque\n",
        "\n",
        "        #return True\n",
        "# --- Inner Loop (FCM & Learning) ---\n",
        "INNER_FCM_STEPS = 1000       # Iterations per node simulation\n",
        "INNER_LR_X = 1.0             # Learning rate for State X\n",
        "INNER_LR_Y = 0.01           # Learning rate for State Y\n",
        "INNER_LR_W = 1.0             # Learning rate for Weights\n",
        "INNER_SVM_LR = 0.01          # SVM Learning Rate\n",
        "INNER_GAMMA = 1.0            # Inter-layer neural connection strength\n",
        "\n",
        "# --- Random Walk & Pathfinding ---\n",
        "WALK_BETA = 0.3              # Novelty penalty (dampens repeated paths)\n",
        "WALK_LAMBDA_COST = 1.0       # Penalty weight for cost metrics\n",
        "\n",
        "# --- Outer Loop (Topology Optimization) ---\n",
        "OUTER_GENERATIONS = 1        # Iterations per Layer\n",
        "OUTER_COST_LIMIT = 1000      # Normalization ceiling for scores\n",
        "INTER_EDGE_THRESH = 0.02     # Min DSM weight to trigger neural link\n",
        "\n",
        "for ijk in range(7,8):\n",
        "    for jik in range(10,20):\n",
        "        print(50*'_',ijk,50*'-',jik,50*'=')\n",
        "        #===============================================================================\n",
        "        OUTER_N_SIMS = 1000          # More simulations to find the \"Hidden Gem\" paths\n",
        "        WALK_MAX_STEPS = 50          # Let the walker explore complex relationships deeply\n",
        "        DSM_TARGET_EDGES = jik        # Allow HIGHER density (Complexity is allowed!)\n",
        "        OUTER_DSM_LAYERS = 1         # Balanced hierarchy (Structure -> Systems -> Skin)\n",
        "        DSM_ADDITIVE_RATE = 0.9      # Low Learning Rate: Learn slowly, don't panic.\n",
        "        DSM_FEEDBACK_STR = 0.05      # Weak Feedback: Listen to problems, but don't obsess.\n",
        "        WALK_TOP_K = 2               # Soft Sparsity: Consider more options per step.\n",
        "        DSM_FEEDBACK_FILTER = 0.2    # Only react to major issues.\n",
        "        DSM_PRUNE_THRESH = 0.02      # Keep subtle connections.\n",
        "        DSM_INIT_RANGE = 0.2         # Start with a blanker slate.\n",
        "        STARTING_POINT = ijk           # START AT SITE ANALYSIS (Respect the Land).\n",
        "        #=============================================================================\n",
        "        class Fuzzy_Hierarchical_Multiplex:\n",
        "            def __init__(self, candidate_dims, D_graph,\n",
        "                        synthetic_targets,\n",
        "                        gamma_interlayer=1.0, causal_flag=False,\n",
        "                        metrics=METRIC_KEYS, metric_mask=METRIC_TARGET):\n",
        "\n",
        "                self.candidate_dims = candidate_dims\n",
        "                self.D_graph = D_graph\n",
        "                self.synthetic_targets = synthetic_targets\n",
        "                self.causal_flag = causal_flag\n",
        "                self.best_dim_per_node = [len(t)-1 for t in synthetic_targets]\n",
        "                self.MM = metric_mask\n",
        "                self.MK = metrics\n",
        "                self.MKI = metrics + ['score']\n",
        "\n",
        "                self.PLM = [[] for _ in range(self.D_graph)]\n",
        "                self.PLMS = [[] for _ in range(self.D_graph)]\n",
        "                self.nested_reps = [np.zeros(c[0]) for c in candidate_dims]\n",
        "\n",
        "                # Inter-layer setup\n",
        "                self.inter_layer = InterLayer(D_graph, max_inner_dim=max(candidate_dims), gamma=gamma_interlayer)\n",
        "                self.chosen_Gmat = np.random.uniform(0.0, 0.3, (D_graph, D_graph))\n",
        "                np.fill_diagonal(self.chosen_Gmat, 0)\n",
        "\n",
        "                self.l2_before, self.l2_after = [], []\n",
        "                self.max_target_len = max(len(t['target']) for t in synthetic_targets)\n",
        "                self.svm_lr = 0.01\n",
        "\n",
        "                self.metric_traces = {k: [] for k in metrics}\n",
        "                self.metric_traces_per_node = [{} for _ in range(self.D_graph)]\n",
        "\n",
        "                # DSM optimization hyperparameters\n",
        "                self.dsm_lr = 0.1\n",
        "                self.dsm_l1 = 0.02\n",
        "                self.dsm_clip = 1.0\n",
        "                self.dsm_history = []\n",
        "                self.dsm_cost_weight = 0.05\n",
        "\n",
        "            def print_dsm_basic(self):\n",
        "                D = self.D_graph\n",
        "                print(\"\\n=== DESIGN STRUCTURE MATRIX (DSM) : Gmat ===\")\n",
        "                header = \"     \" + \" \".join([f\"N{j:>4}\" for j in range(D)])\n",
        "                print(header)\n",
        "                for i in range(D):\n",
        "                    row = \"N{:>2} | \".format(i)\n",
        "                    for j in range(D):\n",
        "                        row += f\"{self.chosen_Gmat[i, j]:>5.2f} \"\n",
        "                    print(row)\n",
        "\n",
        "            # ---------- INNER LOOP (FCM) ----------\n",
        "            def run_inner(self, node_idx, target, D_fcm,\n",
        "                        steps=INNER_FCM_STEPS, lr_x=INNER_LR_X, lr_y=INNER_LR_Y, lr_W=INNER_LR_W,\n",
        "                        decorrelate_metrics=False):\n",
        "\n",
        "                # --- Initialize activations ---\n",
        "                x = np.random.uniform(-0.6, 0.6, D_fcm)\n",
        "                y = np.random.uniform(-0.1, 0.1, D_fcm)\n",
        "\n",
        "                # L2 tracking\n",
        "                self.l2_before.append(np.linalg.norm(self.nested_reps[node_idx][:len(target)] - target))\n",
        "\n",
        "                # --- FCM updates ---\n",
        "                W = np.random.uniform(-0.6, 0.6, (D_fcm, D_fcm))\n",
        "                np.fill_diagonal(W, 0)\n",
        "\n",
        "                for _ in range(steps):\n",
        "                    z = y.dot(W) + x\n",
        "                    Theta_grad_z = z - target\n",
        "                    Theta_grad_x = Theta_grad_z\n",
        "                    Theta_grad_y = Theta_grad_z.dot(W.T)\n",
        "                    Theta_grad_W = np.outer(y, Theta_grad_z)\n",
        "\n",
        "                    x -= lr_x * np.clip(Theta_grad_x, -0.05, 0.05)\n",
        "                    y -= lr_y * np.clip(Theta_grad_y, -0.05, 0.05)\n",
        "                    W -= lr_W * np.clip(Theta_grad_W, -0.01, 0.01)\n",
        "\n",
        "                    x = np.clip(x, 0, 1)\n",
        "                    y = np.clip(y, 0, 1)\n",
        "                    np.fill_diagonal(W, 0)\n",
        "                    W = np.clip(W, -1, 1)\n",
        "\n",
        "                # --- Update nested representation ---\n",
        "                self.nested_reps[node_idx][:len(x)] = x\n",
        "                self.l2_after.append(np.linalg.norm(x - target))\n",
        "\n",
        "                # --- Extract node features ---\n",
        "                # Assuming MetricsEvaluator is a global or imported class\n",
        "                metrics_evaluator = MetricsEvaluator(data_matrix=DATA_MATRIX)\n",
        "                features = metrics_evaluator.extract_features(node_idx)\n",
        "                feat_vals = np.array(list(features.values()))\n",
        "\n",
        "                # --- Compute metrics scaled by activations + features ---\n",
        "                metric_mask = METRIC_TARGET[node_idx]\n",
        "                metric_values = {}\n",
        "\n",
        "                for key, formula, mask in zip(METRIC_KEYS, METRIC_FORMULAS, metric_mask):\n",
        "                    if mask:\n",
        "                        weighted_input = np.mean(feat_vals)\n",
        "                        # Outer scale check\n",
        "                        outer_scale = getattr(self, 'best_node_weights', {}).get(node_idx, 1.0)\n",
        "                        if isinstance(outer_scale, (list, np.ndarray)):\n",
        "                            # fallback if it was stored incorrectly in previous context\n",
        "                            outer_scale = 1.0\n",
        "\n",
        "                        weighted_input *= outer_scale\n",
        "                        metric_val = formula(weighted_input)\n",
        "                        metric_values[key] = metric_val\n",
        "\n",
        "                        # STORE DATAPOINT\n",
        "                        self.metric_traces[key].append((weighted_input, metric_val))\n",
        "                    else:\n",
        "                        metric_values[key] = 0.0\n",
        "\n",
        "                # --- Total score ---\n",
        "                metric_values['score'] = sum(metric_values.values())\n",
        "\n",
        "                # --- Build SVM Training Data ---\n",
        "                metric_output_vals = np.array(\n",
        "                    [v for k, v in metric_values.items() if k not in ['score', 'x', 'feat_vals']]\n",
        "                )\n",
        "\n",
        "                # Lazy init per-node SVM\n",
        "                if not hasattr(self, \"node_svms\"):\n",
        "                    self.node_svms = {}\n",
        "\n",
        "                if node_idx not in self.node_svms:\n",
        "                    self.node_svms[node_idx] = SVM(\n",
        "                        input_dim=len(self.MK),\n",
        "                        output_dim=self.candidate_dims[node_idx][0],\n",
        "                        lr=self.svm_lr\n",
        "                    )\n",
        "\n",
        "                svm = self.node_svms[node_idx]\n",
        "\n",
        "                # Build SVM Input/Output\n",
        "                x_in_full = np.zeros(len(self.MK))\n",
        "                x_in_full[:len(metric_output_vals)] = metric_output_vals\n",
        "                x_in = x_in_full.reshape(1, -1)\n",
        "\n",
        "                y_out_full = np.zeros(self.candidate_dims[node_idx][0])\n",
        "                y_out_full[:len(x)] = x\n",
        "                y_out = y_out_full.reshape(1, -1)\n",
        "\n",
        "                # Train SVM\n",
        "                _ = svm.train_step(x_in, y_out)\n",
        "\n",
        "                # --- Store PLMS trace ---\n",
        "                self.PLMS[node_idx].append((float(weighted_input), metric_output_vals))\n",
        "\n",
        "                if len(self.PLMS[node_idx]) % 100 == 0:\n",
        "                    print(f\"Node {node_idx}, samples learned:\", len(self.PLMS[node_idx]))\n",
        "\n",
        "                # --- Compute inter-layer MI ---\n",
        "                mi_score = self.inter_layer.mi_for_graph(self.chosen_Gmat, self.nested_reps)\n",
        "\n",
        "                return x, y, W, mi_score, metric_values\n",
        "\n",
        "            # ---------- OUTER LOOP (Topology Optimization) ----------\n",
        "            def run_outer(self, outer_cost_limit=OUTER_COST_LIMIT, alpha=0.0, additive_rate=DSM_ADDITIVE_RATE, n_simulations=OUTER_N_SIMS):\n",
        "                \"\"\"\n",
        "                CORRECTED LOGIC (Distributed Flow):\n",
        "                1. Uses Random Walk to find high-probability metric flows starting from ANY node.\n",
        "                2. Injects FEEDBACK LOOPS to create Coupled Blocks.\n",
        "                3. Prunes weak edges to prevent 'Total Chaos'.\n",
        "\n",
        "                *Update:* Removed 'Start at 0' handicap. Now samples flows from all subsystems.\n",
        "                \"\"\"\n",
        "                node_metrics_list = self.capped_node_metrics\n",
        "                D = self.D_graph\n",
        "\n",
        "                # =========================================================\n",
        "                # 1. METRIC SCORING\n",
        "                # =========================================================\n",
        "                raw_scores = np.array([m['score'] for m in node_metrics_list])\n",
        "                total_raw = raw_scores.sum()\n",
        "                if total_raw > outer_cost_limit:\n",
        "                    scale_factor = outer_cost_limit / total_raw\n",
        "                    for metrics in node_metrics_list:\n",
        "                        for key in self.MKI:\n",
        "                            metrics[key] *= scale_factor\n",
        "                    raw_scores *= scale_factor\n",
        "\n",
        "                fuzzy_tensor = self.compute_fuzzy_metric_tensor(normalize=False)\n",
        "                self.weighted_fmt = fuzzy_tensor.copy()\n",
        "\n",
        "                # Calculate Contributions\n",
        "                node_contributions = np.zeros(D)\n",
        "                for i in range(D):\n",
        "                    own_score = raw_scores[i]\n",
        "                    fmt_contrib = fuzzy_tensor[i, :, :].sum() - fuzzy_tensor[i, i, :].sum()\n",
        "                    node_contributions[i] = own_score + self.inter_layer.gamma * fmt_contrib\n",
        "                self.node_score_contributions = node_contributions\n",
        "                self.correlation_penalty = 0.0\n",
        "\n",
        "                # =========================================================\n",
        "                # 2. PROBABILISTIC PATHFINDING (Distributed Random Walk)\n",
        "                # =========================================================\n",
        "\n",
        "                node_metrics = self.capped_node_metrics\n",
        "                node_types = infer_node_types(node_metrics)\n",
        "                C_matrix = build_coupling_matrix(node_types)\n",
        "                print(node_types)\n",
        "                # Initialize Walker\n",
        "                # Inside run_outer method:\n",
        "\n",
        "# ... (Previous metric scoring logic) ...\n",
        "\n",
        "                # Pass the mask (METRIC_TARGET) explicitly\n",
        "                # Assuming 'self.MM' holds the METRIC_TARGET data structure you provided\n",
        "                walker = MetricDrivenRandomWalk(\n",
        "                    C_matrix,\n",
        "                    node_metrics,\n",
        "                    metric_targets=self.MM, # <--- PASS THE MASK HERE\n",
        "                    max_steps=WALK_MAX_STEPS,\n",
        "                    top_k=WALK_TOP_K\n",
        "                )\n",
        "\n",
        "# ... (Rest of pathfinding logic) ...\n",
        "\n",
        "                # --- VALIDATOR (RELAXED) ---\n",
        "                                # 1. Define the Tiers of Validation\n",
        "                def is_path_valid(path):\n",
        "                    # 1. Topology Checks\n",
        "                    if len(path) < 2: return False        # A path must go somewhere\n",
        "                    if path[-1] != (D - 1): return False  # Must still converge to Project Completion\n",
        "                    if len(set(path)) != len(path): return False # No self-cycles\n",
        "\n",
        "                    # 2. Coupling Constraints\n",
        "                    state_machine = CouplingState()\n",
        "                    for k in range(len(path) - 1):\n",
        "                        u, v = path[k], path[k+1]\n",
        "                        type_u, type_v = node_types[u], node_types[v]\n",
        "\n",
        "                        if not state_machine.allows(type_v): return False\n",
        "\n",
        "                        coupling = \"B->B->C\" if (type_u == \"B\" and type_v == \"B\") else f\"{type_u}->{type_v}\"\n",
        "                        state_machine.update(coupling)\n",
        "                    return True\n",
        "                    def is_path_valid(path, max_violations=1):\n",
        "                        violations = 0\n",
        "                        state_machine = CouplingState()\n",
        "\n",
        "                        for k in range(len(path) - 1):\n",
        "                            u, v = path[k], path[k+1]\n",
        "                            type_v = node_types[v]\n",
        "\n",
        "                            if not state_machine.allows(type_v):\n",
        "                                violations += 1\n",
        "                                if violations > max_violations:\n",
        "                                    return False  # Still fail if it's too messy\n",
        "                                # Continue anyway if under the limit\n",
        "\n",
        "                            # Update state regardless of the violation to keep tracking\n",
        "                            coupling = f\"{node_types[u]}->{type_v}\"\n",
        "                            state_machine.update(coupling)\n",
        "\n",
        "                        return True\n",
        "\n",
        "                # --- DISTRIBUTED SAMPLING ---\n",
        "                valid_paths = []\n",
        "                print(f\" [Optimizer] Sampling {n_simulations} distributed paths (Any Start -> End)...\")\n",
        "\n",
        "                # Potential start nodes: 0 to D-2 (Any node except the final Sink node)\n",
        "                possible_starts = list(range(D - 1))\n",
        "\n",
        "                for _ in range(n_simulations):\n",
        "                    # Randomly select a starting subsystem to ensure \"All Processes Included\"\n",
        "\n",
        "                    path = walker.run(start=STARTING_POINT)\n",
        "\n",
        "                    if is_path_valid(path):\n",
        "                        valid_paths.append(path)\n",
        "\n",
        "                # FALLBACK\n",
        "                if len(valid_paths) == 0:\n",
        "                    print(\" [WARNING] Strict constraints failed. Synthesizing default backbone.\")\n",
        "                    valid_paths.append(list(range(D)))\n",
        "\n",
        "                # Deduplicate\n",
        "                unique_paths = sorted(list(set(tuple(p) for p in valid_paths)), key=lambda x: len(x), reverse=True)\n",
        "                valid_paths = [list(p) for p in unique_paths]\n",
        "\n",
        "                # Deduplicate\n",
        "                unique_paths = sorted(list(set(tuple(p) for p in valid_paths)), key=lambda x: len(x), reverse=True)\n",
        "                valid_paths = [list(p) for p in unique_paths]\n",
        "\n",
        "                # =========================================================\n",
        "                # 3. LAYER CONSTRUCTION & FEEDBACK INJECTION\n",
        "                # =========================================================\n",
        "\n",
        "                G_layer = build_dsm_from_walks(D, valid_paths)\n",
        "\n",
        "                # Feedback Loops\n",
        "                feedback_strength = DSM_FEEDBACK_STR\n",
        "                G_feedback = G_layer.T * feedback_strength\n",
        "\n",
        "                # Filter feedback\n",
        "                G_feedback[G_layer < DSM_FEEDBACK_FILTER] = 0.0\n",
        "\n",
        "                # Combine\n",
        "                G_layer_final = G_layer + G_feedback\n",
        "\n",
        "                # =========================================================\n",
        "                # 4. UPDATE, AMPLIFY & PRUNE\n",
        "                # =========================================================\n",
        "\n",
        "                self.chosen_Gmat = self.chosen_Gmat + (additive_rate * G_layer_final)\n",
        "\n",
        "                if np.max(self.chosen_Gmat) > 0:\n",
        "                    self.chosen_Gmat /= np.max(self.chosen_Gmat)\n",
        "\n",
        "                # Top-K Pruning\n",
        "                TARGET_EDGES = DSM_TARGET_EDGES\n",
        "                flat = self.chosen_Gmat.ravel()\n",
        "                if len(flat) > TARGET_EDGES:\n",
        "                    threshold = np.partition(flat, -TARGET_EDGES)[-TARGET_EDGES]\n",
        "                    self.chosen_Gmat[self.chosen_Gmat < threshold] = 0.0\n",
        "\n",
        "                # Noise Pruning\n",
        "                self.chosen_Gmat[self.chosen_Gmat < DSM_PRUNE_THRESH] = 0.0\n",
        "\n",
        "                density = np.count_nonzero(self.chosen_Gmat)\n",
        "                print(f\" [Optimizer] Matrix Updated. Density: {density} edges. Max Val: {np.max(self.chosen_Gmat):.2f}\")\n",
        "\n",
        "                self.walks, self.best_walk = collect_and_select_best_walks(\n",
        "                    self.chosen_Gmat,\n",
        "                    self.capped_node_metrics,\n",
        "                    beta=WALK_BETA\n",
        "                )\n",
        "\n",
        "                self.print_dsm_basic()\n",
        "\n",
        "                if not hasattr(self, \"_node_contributions_history\"):\n",
        "                    self._node_contributions_history = []\n",
        "                self._node_contributions_history.append(node_contributions.copy())\n",
        "\n",
        "                return node_metrics_list, 0.0, node_contributions\n",
        "\n",
        "            def run(self, outer_generations=OUTER_GENERATIONS, num_dsm_layers=OUTER_DSM_LAYERS):\n",
        "                best_score = -np.inf\n",
        "\n",
        "                # 1. FIX INITIALIZATION:\n",
        "                # Use the random initial state as the baseline.\n",
        "                # This ensures Layer 0 captures the \"Jump\" from noise to structure.\n",
        "                baseline = self.chosen_Gmat.copy()\n",
        "                dsm_decomposer = DSM_Layer_Decomposer(baseline, mode='additive')\n",
        "                dsm_decomposer.current_total = baseline.copy()\n",
        "\n",
        "                print(f\"Starting Optimization: {num_dsm_layers} Layers x {outer_generations} Gens\")\n",
        "\n",
        "                # Define the \"Building Blocks\" for the 3 layers (based on your 12-15 node stack)\n",
        "                # Layer 0: Structure (Nodes 0-5), Layer 1: Systems (Nodes 6-10), Layer 2: Skin/Ops (Nodes 11-14)\n",
        "                nodes_per_layer = np.array_split(range(self.D_graph), num_dsm_layers)\n",
        "\n",
        "                for layer_idx in range(num_dsm_layers):\n",
        "                    print(f\"\\n>>> COMPILING LAYER {layer_idx + 1}: {['STRUCTURE', 'SYSTEMS', 'SKIN'][layer_idx]} <<<\")\n",
        "\n",
        "                    # Determine the nodes active in this specific layer\n",
        "                    active_nodes = nodes_per_layer[layer_idx]\n",
        "\n",
        "                    for gen in range(outer_generations):\n",
        "                        # 1. Inner Loop (Targeting active nodes for this layer)\n",
        "                        node_metrics_list = []\n",
        "                        for node_idx in range(self.D_graph):\n",
        "                            full_target = self.synthetic_targets[node_idx]['target']\n",
        "                            D_fcm = self.candidate_dims[node_idx][0]\n",
        "                            target = full_target[:D_fcm]\n",
        "\n",
        "                            # We simulate everything, but the \"Learning\" is focused on the active layer\n",
        "                            _, _, _, _, metrics = self.run_inner(node_idx, target, D_fcm)\n",
        "                            node_metrics_list.append(metrics)\n",
        "\n",
        "                        self.capped_node_metrics = node_metrics_list\n",
        "\n",
        "                        # 2. Outer Loop (Topology Optimization)\n",
        "                        # We pass the layer_idx to run_outer if you want to adjust the WALK_TOP_K\n",
        "                        # or additive_rate per layer (e.g., higher for structure, lower for skin)\n",
        "                        _, capped_score, _ = self.run_outer()\n",
        "\n",
        "                        best_score = max(best_score, capped_score)\n",
        "                        print(f\" [Gen {gen+1}] Score: {capped_score:.4f}\", end='\\r')\n",
        "\n",
        "                    print(\"\")\n",
        "\n",
        "                    # 3. SNAPSHOT: The Decomposer captures the \"Delta\" for this layer\n",
        "                    # This is where the MUX/DEMUX logic is voucher-ed.\n",
        "                    dsm_decomposer.add_snapshot(self.chosen_Gmat)\n",
        "\n",
        "                self.dsm_layers = dsm_decomposer.layers\n",
        "                print(\"\\nOptimization Complete. All 3 Layers Compiled.\")\n",
        "                return best_score\n",
        "\n",
        "            # ---------- VISUALIZATIONS & ANALYSIS ----------\n",
        "\n",
        "            def plot_pointwise_minmax_elite(self, top_k=21):\n",
        "                plt.figure(figsize=(14, 3))\n",
        "                for i in range(self.D_graph):\n",
        "                    dim_i = self.candidate_dims[i][0]\n",
        "                    base = self.nested_reps[i][:dim_i]\n",
        "                    reps = np.clip(base + np.random.normal(0, 0.05, (top_k, len(base))), 0, 1)\n",
        "                    y_min, y_max = reps.min(axis=0), reps.max(axis=0)\n",
        "                    y_sel = base\n",
        "\n",
        "                    y_true = self.synthetic_targets[i]['target'][:len(y_sel)]\n",
        "                    if len(y_true) < len(y_sel):\n",
        "                        y_true = np.pad(y_true, (0, len(y_sel) - len(y_true)), \"constant\")\n",
        "                    else:\n",
        "                        y_true = y_true[:len(y_sel)]\n",
        "\n",
        "                    plt.subplot(1, self.D_graph, i + 1)\n",
        "                    plt.fill_between(range(len(y_min)), y_min, y_max, color='skyblue', alpha=0.4, label='Elite Interval')\n",
        "                    plt.plot(y_sel, 'k-', lw=2, label='Estimated')\n",
        "                    plt.plot(y_true, 'r--', lw=2, label='True')\n",
        "                    plt.ylim(0, 1.05)\n",
        "                    plt.title(f\"Node {i + 1}\")\n",
        "                    if i == 0: plt.legend()\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "            def plot_nested_activations(self):\n",
        "                plt.figure(figsize=(12, 3))\n",
        "                for i, rep in enumerate(self.nested_reps):\n",
        "                    dim_i = self.candidate_dims[i][0]\n",
        "                    rep_i = rep[:dim_i]\n",
        "                    plt.subplot(1, self.D_graph, i + 1)\n",
        "                    plt.bar(range(len(rep_i)), rep_i, color=plt.cm.plasma(rep_i))\n",
        "                    plt.ylim(0, 1)\n",
        "                    plt.title(f\"Node {i + 1}\")\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "            def plot_outer_fuzzy_graph(self):\n",
        "                G = nx.DiGraph()\n",
        "                for i in range(self.D_graph): G.add_node(i)\n",
        "                for i in range(self.D_graph):\n",
        "                    for j in range(self.D_graph):\n",
        "                        if i != j and abs(self.chosen_Gmat[i, j]) > 0.02:\n",
        "                            G.add_edge(i, j, weight=self.chosen_Gmat[i, j])\n",
        "\n",
        "                node_sizes = [self.best_dim_per_node[i] * 200 for i in range(self.D_graph)]\n",
        "                edge_colors = ['green' if d['weight'] > 0 else 'red' for _, _, d in G.edges(data=True)]\n",
        "                edge_widths = [abs(d['weight']) * 3 for _, _, d in G.edges(data=True)]\n",
        "\n",
        "                pos = nx.spring_layout(G)\n",
        "                plt.figure(figsize=(6, 6))\n",
        "                nx.draw(G, pos, node_size=node_sizes, node_color='skyblue',\n",
        "                        edge_color=edge_colors, width=edge_widths, arrows=True, with_labels=True)\n",
        "                plt.title(\"Outer Fuzzy Multiplex Graph\")\n",
        "                plt.show()\n",
        "\n",
        "            def print_interactions(self, return_tensor=True, verbose=True):\n",
        "                D_graph = self.D_graph\n",
        "                inter_dim = self.inter_layer.inter_dim\n",
        "                inter_tensor = np.zeros((D_graph, D_graph, inter_dim))\n",
        "\n",
        "                acts = self.inter_layer.build_activations(self.chosen_Gmat, self.nested_reps)\n",
        "                if not acts:\n",
        "                    if verbose:\n",
        "                        print(\"No active edges above threshold.\")\n",
        "                    return inter_tensor if return_tensor else None\n",
        "\n",
        "                for (i, j), vec in acts.items():\n",
        "                    inter_tensor[i, j, :] = vec\n",
        "                    if verbose:\n",
        "                        act_str = \", \".join([f\"{v:.3f}\" for v in vec])\n",
        "                        print(f\"Node {i} -> Node {j}: [{act_str}]\")\n",
        "                return inter_tensor if return_tensor else None\n",
        "\n",
        "            def compute_fuzzy_metric_tensor(self, normalize=True, verbose=False):\n",
        "                metrics_keys = self.MK\n",
        "                D = self.D_graph\n",
        "                num_metrics = len(metrics_keys)\n",
        "                tensor = np.zeros((D, D, num_metrics))\n",
        "\n",
        "                metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "\n",
        "                node_metrics = []\n",
        "                for i, rep in enumerate(self.nested_reps):\n",
        "                    metrics = metrics_evaluator.compute_node_metrics(i, y=rep)\n",
        "                    node_metrics.append(np.array([metrics[k] for k in metrics_keys]))\n",
        "                node_metrics = np.array(node_metrics)\n",
        "\n",
        "                for i in range(D):\n",
        "                    for j in range(D):\n",
        "                        if i == j:\n",
        "                            tensor[i, j, :] = node_metrics[j]\n",
        "                        else:\n",
        "                            weight = np.clip(abs(self.chosen_Gmat[i, j]), 0, 1)\n",
        "                            tensor[i, j, :] = weight * node_metrics[j]\n",
        "\n",
        "                if normalize:\n",
        "                    tensor = (tensor - tensor.min()) / (tensor.max() - tensor.min() + 1e-12)\n",
        "\n",
        "                if verbose:\n",
        "                    print(\"Fuzzy Metric Tensor shape:\", tensor.shape)\n",
        "\n",
        "                return tensor\n",
        "\n",
        "            def plot_fuzzy_metric_tensor_heatmaps(self, fuzzy_tensor=None, metrics_keys=None):\n",
        "                if metrics_keys is None:\n",
        "                    metrics_keys = self.MK\n",
        "                if fuzzy_tensor is None:\n",
        "                    fuzzy_tensor = self.compute_fuzzy_metric_tensor(normalize=True)\n",
        "\n",
        "                D = self.D_graph\n",
        "                num_metrics = len(metrics_keys)\n",
        "\n",
        "                fig, axes = plt.subplots(1, num_metrics, figsize=(4 * num_metrics, 4))\n",
        "                if num_metrics == 1: axes = [axes]\n",
        "\n",
        "                im = None\n",
        "                for k, key in enumerate(metrics_keys):\n",
        "                    data = fuzzy_tensor[:, :, k]\n",
        "                    im = axes[k].imshow(data, cmap='viridis', vmin=0, vmax=1)\n",
        "                    for i in range(D):\n",
        "                        for j in range(D):\n",
        "                            axes[k].text(j, i, f\"{data[i, j]:.2f}\", ha='center', va='center', color='white', fontsize=9)\n",
        "                    axes[k].set_xticks(range(D))\n",
        "                    axes[k].set_yticks(range(D))\n",
        "                    axes[k].set_xticklabels([f'Node {j}' for j in range(D)])\n",
        "                    axes[k].set_yticklabels([f'Node {i}' for i in range(D)])\n",
        "                    axes[k].set_title(f'FMT - {key}')\n",
        "\n",
        "                fig.colorbar(im, ax=axes, orientation='vertical', fraction=0.025, pad=0.04, label='Normalized Metric Value')\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "            def compute_fmt_with_elite_bounds(self, top_k=21):\n",
        "                metrics_keys = self.MK\n",
        "                D = self.D_graph\n",
        "                num_metrics = len(metrics_keys)\n",
        "                tensor_bounds = np.zeros((D, D, num_metrics, 2))\n",
        "\n",
        "                metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "\n",
        "                for i in range(D):\n",
        "                    base = self.nested_reps[i]\n",
        "                    reps = np.clip(base + np.random.normal(0, 0.05, (top_k, len(base))), 0, 1)\n",
        "\n",
        "                    metrics_matrix = np.zeros((top_k, num_metrics))\n",
        "                    for idx, rep in enumerate(reps):\n",
        "                        m = metrics_evaluator.compute_node_metrics(i, y=rep)\n",
        "                        metrics_matrix[idx, :] = [m[k] for k in metrics_keys]\n",
        "\n",
        "                    lower_i = metrics_matrix.min(axis=0)\n",
        "                    upper_i = metrics_matrix.max(axis=0)\n",
        "\n",
        "                    for j in range(D):\n",
        "                        tensor_bounds[i, j, :, 0] = lower_i\n",
        "                        tensor_bounds[i, j, :, 1] = upper_i\n",
        "\n",
        "                return tensor_bounds\n",
        "\n",
        "            def plot_fmt_with_bounds(self, fmt_tensor_bounds):\n",
        "                D = self.D_graph\n",
        "                metrics_keys = self.MK\n",
        "                M_actual = len(metrics_keys)\n",
        "\n",
        "                mean_vals = (fmt_tensor_bounds[:, :, :, 0] + fmt_tensor_bounds[:, :, :, 1]) / 2\n",
        "                mean_vals = mean_vals.mean(axis=1)  # mean across targets\n",
        "                mean_vals = mean_vals.mean(axis=0, keepdims=True)  # mean across nodes\n",
        "\n",
        "                if hasattr(self, 'best_alpha') and hasattr(self, 'best_w_contrib'):\n",
        "                    mean_weight = (self.best_alpha * self.best_w_contrib).mean()\n",
        "                    mean_vals = mean_vals * mean_weight\n",
        "\n",
        "                fig, ax = plt.subplots(figsize=(1.2 * M_actual + 4, 2))\n",
        "                im = ax.imshow(mean_vals, cmap='viridis', aspect='auto')\n",
        "\n",
        "                vmin, vmax = mean_vals.min(), mean_vals.max()\n",
        "                for i in range(mean_vals.shape[0]):\n",
        "                    for k in range(M_actual):\n",
        "                        val = mean_vals[i, k]\n",
        "                        color = 'white' if val < (vmin + 0.5 * (vmax - vmin)) else 'black'\n",
        "                        ax.text(k, i, f\"{val:.2f}\", ha='center', va='center', color=color, fontsize=8)\n",
        "\n",
        "                ax.set_xticks(range(M_actual))\n",
        "                ax.set_xticklabels(metrics_keys[:M_actual], rotation=45, ha='right')\n",
        "                ax.set_yticks([0])\n",
        "                ax.set_yticklabels(['Mean across nodes'])\n",
        "                ax.set_title(\"Weighted FMT with Bounds\")\n",
        "                fig.colorbar(im, ax=ax, label='Weighted Mean Metric Value')\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "            def plot_node_score_contribution(self, metrics_keys=None):\n",
        "                if metrics_keys is None:\n",
        "                    metrics_keys = self.MK\n",
        "                D = self.D_graph\n",
        "                node_contributions = np.array(self.node_score_contributions)\n",
        "\n",
        "                if hasattr(self, 'weighted_fmt'):\n",
        "                    fuzzy_tensor = np.array(self.weighted_fmt)\n",
        "                else:\n",
        "                    fuzzy_tensor = self.compute_fuzzy_metric_tensor(normalize=True)\n",
        "\n",
        "                fuzzy_tensor_norm = (fuzzy_tensor - fuzzy_tensor.min()) / (fuzzy_tensor.max() - fuzzy_tensor.min() + 1e-12)\n",
        "                fmt_matrix = fuzzy_tensor_norm.sum(axis=2)\n",
        "                np.fill_diagonal(fmt_matrix, 0)\n",
        "\n",
        "                raw_matrix = np.zeros((D, D))\n",
        "                np.fill_diagonal(raw_matrix, node_contributions)\n",
        "\n",
        "                total_matrix = raw_matrix + fmt_matrix\n",
        "\n",
        "                fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "                matrices = [raw_matrix, fmt_matrix, total_matrix]\n",
        "                titles = [\"Raw Node Contribution\", \"Normalized FMT Contribution\", \"Total Contribution\"]\n",
        "\n",
        "                im = None\n",
        "                for ax, mat, title in zip(axes, matrices, titles):\n",
        "                    im = ax.imshow(mat, cmap='viridis', vmin=0, vmax=1)\n",
        "                    for i in range(D):\n",
        "                        for j in range(D):\n",
        "                            ax.text(j, i, f\"{mat[i, j]:.2f}\", ha='center', va='center', color='white', fontsize=8)\n",
        "                    ax.set_title(title)\n",
        "                    ax.set_xticks(range(D))\n",
        "                    ax.set_xticklabels([f\"Node {i + 1}\" for i in range(D)])\n",
        "                    ax.set_yticks(range(D))\n",
        "                    ax.set_yticklabels([f\"Node {i + 1}\" for i in range(D)])\n",
        "\n",
        "                fig.colorbar(im, ax=axes, orientation='vertical', fraction=0.025, pad=0.04, label='Contribution Value')\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "            def plot_fmt_with_run_metrics(self, metrics_keys=None):\n",
        "                if metrics_keys is None:\n",
        "                    metrics_keys = self.MK\n",
        "                D = self.D_graph\n",
        "                M_actual = len(metrics_keys)\n",
        "\n",
        "                if not hasattr(self, 'capped_node_metrics'):\n",
        "                    raise ValueError(\"No node metrics available. Run run_outer() first.\")\n",
        "\n",
        "                weighted_fmt = np.zeros((D, D, M_actual))\n",
        "                for i in range(D):\n",
        "                    for j in range(D):\n",
        "                        for k, key in enumerate(metrics_keys):\n",
        "                            val = self.capped_node_metrics[j][key]\n",
        "                            if i != j:\n",
        "                                val *= np.clip(abs(self.chosen_Gmat[i, j]), 0, 1)\n",
        "                            weighted_fmt[i, j, k] = val\n",
        "\n",
        "                for i in range(D):\n",
        "                    for k in range(M_actual):\n",
        "                        if not METRIC_TARGET[i][k]:\n",
        "                            weighted_fmt[i, :, k] = 0.0\n",
        "\n",
        "                mean_vals = weighted_fmt.mean(axis=1)\n",
        "\n",
        "                fig, ax = plt.subplots(figsize=(1.2 * M_actual + 4, 0.35 * D + 4))\n",
        "                im = ax.imshow(mean_vals, cmap='viridis', aspect='auto')\n",
        "\n",
        "                vmin, vmax = mean_vals.min(), mean_vals.max()\n",
        "                for i in range(D):\n",
        "                    for k in range(M_actual):\n",
        "                        val = mean_vals[i, k]\n",
        "                        color = 'white' if val < (vmin + 0.5 * (vmax - vmin)) else 'black'\n",
        "                        ax.text(k, i, f\"{val:.2f}\", ha='center', va='center', color=color, fontsize=8)\n",
        "\n",
        "                ax.set_xticks(range(M_actual))\n",
        "                ax.set_xticklabels(metrics_keys[:M_actual], rotation=45, ha='right')\n",
        "                ax.set_yticks(range(D))\n",
        "                ax.set_yticklabels([f\"Node {i}\" for i in range(D)])\n",
        "                ax.set_title(\"Weighted FMT Metrics (Actual Run Output)\")\n",
        "                fig.colorbar(im, ax=ax, label='Weighted Metric Value')\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "            def collect_fmt_datapoints(self):\n",
        "                self.fmt_datapoints = {k: [] for k in self.MK}\n",
        "                for node_idx in range(self.D_graph):\n",
        "                    if len(self.PLMS[node_idx]) == 0:\n",
        "                        continue\n",
        "                    for weighted_input_val, metric_vals in self.PLMS[node_idx]:\n",
        "                        for m, key in enumerate(self.MK):\n",
        "                            if m < len(metric_vals):\n",
        "                                self.fmt_datapoints[key].append((weighted_input_val, metric_vals[m]))\n",
        "\n",
        "            def collect_metric_traces_per_node(self):\n",
        "                self.metric_traces_per_node = [{} for _ in range(self.D_graph)]\n",
        "                for node_idx in range(self.D_graph):\n",
        "                    self.metric_traces_per_node[node_idx] = {k: [] for k in self.MK}\n",
        "                    if len(self.PLMS[node_idx]) == 0:\n",
        "                        continue\n",
        "                    for weighted_input_val, metric_vals in self.PLMS[node_idx]:\n",
        "                        for m, key in enumerate(self.MK):\n",
        "                            if m < len(metric_vals):\n",
        "                                self.metric_traces_per_node[node_idx][key].append((weighted_input_val, metric_vals[m]))\n",
        "\n",
        "            def plot_fmt_per_datapoint(self, top_k=21, span=0.3, grid_size=100):\n",
        "                if not hasattr(self, 'fmt_datapoints'):\n",
        "                    self.collect_fmt_datapoints()\n",
        "\n",
        "                for key, formula in zip(self.MK, METRIC_FORMULAS):\n",
        "                    if key not in self.fmt_datapoints or len(self.fmt_datapoints[key]) == 0:\n",
        "                        continue\n",
        "\n",
        "                    data = np.array(self.fmt_datapoints[key])\n",
        "                    x_data, y_data = data[:, 0], data[:, 1]\n",
        "\n",
        "                    x_curve = np.linspace(x_data.min() - span, x_data.max() + span, grid_size)\n",
        "                    y_curve = np.array([formula(x) for x in x_curve])\n",
        "\n",
        "                    plt.figure(figsize=(6, 4))\n",
        "                    plt.scatter(x_data, y_data, alpha=0.6, label=\"FMT datapoints\")\n",
        "                    plt.plot(x_curve, y_curve, 'r', lw=2, label=\"Metric equation\")\n",
        "                    plt.xlabel(\"Weighted Input\")\n",
        "                    plt.ylabel(f\"{key} (FMT)\")\n",
        "                    plt.title(f\"FMT per datapoint - Metric: {key}\")\n",
        "                    plt.legend()\n",
        "                    plt.grid(alpha=0.3)\n",
        "                    plt.tight_layout()\n",
        "                    plt.show()\n",
        "\n",
        "            def plot_metric_equations_per_node(self, grid_size=100, span=0.3):\n",
        "                if not hasattr(self, 'metric_traces_per_node'):\n",
        "                    self.collect_metric_traces_per_node()\n",
        "\n",
        "                for node_idx in range(self.D_graph):\n",
        "                    node_traces = self.metric_traces_per_node[node_idx]\n",
        "                    if all(len(v) == 0 for v in node_traces.values()):\n",
        "                        continue\n",
        "\n",
        "                    plt.figure(figsize=(6, 4))\n",
        "                    for key, formula in zip(self.MK, METRIC_FORMULAS):\n",
        "                        if key not in node_traces or len(node_traces[key]) == 0:\n",
        "                            continue\n",
        "                        data = np.array(node_traces[key])\n",
        "                        x_data, y_data = data[:, 0], data[:, 1]\n",
        "\n",
        "                        x_curve = np.linspace(x_data.min() - span, x_data.max() + span, grid_size)\n",
        "                        y_curve = np.array([formula(x) for x in x_curve])\n",
        "\n",
        "                        plt.scatter(x_data, y_data, alpha=0.6, label=f\"{key} datapoints\")\n",
        "                        plt.plot(x_curve, y_curve, 'r', lw=2, label=f\"{key} equation\")\n",
        "\n",
        "                    plt.xlabel(\"Weighted Input\")\n",
        "                    plt.ylabel(\"Metric Value\")\n",
        "                    plt.title(f\"Node {node_idx} - Metric Equations\")\n",
        "                    plt.legend()\n",
        "                    plt.grid(alpha=0.3)\n",
        "                    plt.tight_layout()\n",
        "                    plt.show()\n",
        "\n",
        "\n",
        "        # =============================================================================\n",
        "        # MAIN EXECUTION BLOCK\n",
        "        # =============================================================================\n",
        "\n",
        "        if __name__ == \"__main__\":\n",
        "            # Ensure necessary globals exist before running; otherwise this block is illustrative\n",
        "            try:\n",
        "                optimizer = Fuzzy_Hierarchical_Multiplex(\n",
        "                    candidate_dims, D_graph,\n",
        "                    synthetic_targets,\n",
        "                    gamma_interlayer=0,\n",
        "                    causal_flag=False\n",
        "                )\n",
        "\n",
        "                # Run Optimization\n",
        "                metrics_list = optimizer.run()\n",
        "\n",
        "                # Visualizations\n",
        "            # optimizer.plot_pointwise_minmax_elite()\n",
        "                #optimizer.plot_nested_activations()\n",
        "\n",
        "                # Compute FMT with elite bounds\n",
        "                #fmt_elite_bounds = optimizer.compute_fmt_with_elite_bounds(top_k=top_k + 10)\n",
        "\n",
        "                # Plot as heatmaps\n",
        "                #optimizer.plot_fmt_with_run_metrics()\n",
        "\n",
        "                # Compute fuzzy multiplex tensor\n",
        "                #fmt_tensor = optimizer.compute_fuzzy_metric_tensor(normalize=False)\n",
        "                #optimizer.plot_fuzzy_metric_tensor_heatmaps(fmt_tensor)\n",
        "\n",
        "                # Plot Contributions & Graph\n",
        "                #optimizer.plot_node_score_contribution()\n",
        "                optimizer.plot_outer_fuzzy_graph()\n",
        "\n",
        "                # Interactions\n",
        "            # tensor = optimizer.print_interactions()\n",
        "                #print(\"Tensor shape:\", tensor.shape, '\\n', tensor)\n",
        "\n",
        "                # Datapoints & Equations\n",
        "                #optimizer.collect_fmt_datapoints()\n",
        "                #optimizer.plot_fmt_per_datapoint()\n",
        "                #optimizer.collect_metric_traces_per_node()\n",
        "            # optimizer.plot_metric_equations_per_node()\n",
        "\n",
        "                # DSM Tracking Demo\n",
        "                dsm_tracker = DSM_Tracker(optimizer)\n",
        "\n",
        "                # Run extra DSM update\n",
        "                optimizer.run_outer()\n",
        "                dsm_tracker.update_dsms()\n",
        "\n",
        "                # Retrieve matrices\n",
        "                primary, residual = dsm_tracker.get_matrices()\n",
        "                #print(\"Primary DSM:\\n\", primary)\n",
        "                #rint(\"Residual DSM:\\n\", residual)\n",
        "\n",
        "            except NameError as e:\n",
        "                print(f\"Error: Missing external dependency definition. \\n{e}\")\n",
        "                print(\"Please ensure D_graph, DATA_MATRIX, METRIC_KEYS, etc. are defined.\")\n",
        "\n",
        "       # primary, residual = dsm_tracker.get_matrices()\n",
        "\n",
        "        import numpy as np\n",
        "        import pandas as pd\n",
        "        import networkx as nx\n",
        "\n",
        "        # Reproducibility\n",
        "        np.random.seed(2025)\n",
        "\n",
        "        # =====================================================\n",
        "        # 1. THE UNIVERSAL SIMULATOR CLASS\n",
        "        # =====================================================\n",
        "        class UniversalSystemSimulator:\n",
        "            \"\"\"\n",
        "            A domain-agnostic simulator that executes a Dependency Structure Matrix (DSM).\n",
        "            It handles topological sorting, cycle detection (Strongly Connected Components),\n",
        "            and feedback loop execution (Rework).\n",
        "            \"\"\"\n",
        "\n",
        "            def __init__(self, touchpoints, metric_map, initial_state=None):\n",
        "                self.TOUCHPOINTS = touchpoints\n",
        "                self.METRIC_MAP = metric_map\n",
        "                self.D = len(touchpoints)\n",
        "                # Default state container\n",
        "                self.state = initial_state or {}\n",
        "                self.node_registry = {}\n",
        "\n",
        "            def register_node_logic(self, node_name, logic_fn):\n",
        "                \"\"\"Register a function to run when a specific node is visited.\"\"\"\n",
        "                self.node_registry[node_name] = logic_fn\n",
        "\n",
        "            def run_node(self, node_name, is_rework=False):\n",
        "                \"\"\"Execute the logic for a specific node.\"\"\"\n",
        "                if node_name in self.node_registry:\n",
        "                    # logic_fn(current_state, is_rework_flag)\n",
        "                    return self.node_registry[node_name](self.state, is_rework)\n",
        "                return {\"status\": \"processed_default\"}\n",
        "\n",
        "            def _calculate_kpis(self, log_df):\n",
        "                \"\"\"\n",
        "                Maps specific node variables to Universal KPIs (Risk, Asset, Cost)\n",
        "                using the provided METRIC_MAP.\n",
        "                \"\"\"\n",
        "                # 1. Identify which variables belong to which category based on METRIC_MAP\n",
        "                risk_vars = []\n",
        "                asset_vars = []\n",
        "                reg_vars = []\n",
        "\n",
        "                # Reverse map for easy lookup\n",
        "                # METRIC_MAP format: Node -> [VarName, Category]\n",
        "                for node, (var_name, category) in self.METRIC_MAP.items():\n",
        "                    if category == \"Loss_Ratio_Prevention\":\n",
        "                        risk_vars.append(var_name)\n",
        "                    elif category == \"Capital_Efficiency\":\n",
        "                        asset_vars.append(var_name)\n",
        "                    elif category == \"Regulatory_Alpha\":\n",
        "                        reg_vars.append(var_name)\n",
        "                    elif category == \"Clinical_Safety\":\n",
        "                        # Safety acts as an inverse risk modifier in this model\n",
        "                        risk_vars.append(var_name)\n",
        "\n",
        "                # 2. Extract values from Log\n",
        "                # We take the *last* recorded value for each variable\n",
        "                final_state = log_df.drop_duplicates(subset=['Variable'], keep='last').set_index('Variable')['Value']\n",
        "\n",
        "                # 3. Calculate Aggregates\n",
        "                # Normalization: Assume higher Asset is good, Lower Risk is good.\n",
        "\n",
        "                # Calculate Total Asset Value (Sum of Capital Efficiency items)\n",
        "                total_asset = final_state[final_state.index.isin(asset_vars)].sum()\n",
        "                if total_asset == 0: total_asset = 1000.0 # Fallback\n",
        "\n",
        "                # Calculate Avg Risk Index (Lower is better).\n",
        "                # Note: Safety metrics need inversion if they are 0-1 scores where 1 is good.\n",
        "                risk_values = final_state[final_state.index.isin(risk_vars)]\n",
        "                avg_risk = risk_values.mean() if not risk_values.empty else 0.5\n",
        "\n",
        "                # OPEX/Cost is derived from the \"rework\" intensity in the log\n",
        "                # More rework steps = Higher Opex\n",
        "                rework_steps = len(log_df[log_df['Is_Rework'] == True])\n",
        "                base_opex = total_asset * 0.05 # 5% base opex\n",
        "                final_opex = base_opex + (rework_steps * (base_opex * 0.1))\n",
        "\n",
        "                return {\n",
        "                    \"avg_risk_index\": round(avg_risk, 4),\n",
        "                    \"total_asset_index\": round(total_asset, 2),\n",
        "                    \"avg_opex_cost\": round(final_opex, 2)\n",
        "                }\n",
        "\n",
        "            def evaluate_dsm(self, dsm, label=\"Simulation\"):\n",
        "                \"\"\"Main execution engine.\"\"\"\n",
        "                # 1. Build Graph from DSM\n",
        "                G = nx.DiGraph()\n",
        "                for i in range(self.D):\n",
        "                    for j in range(self.D):\n",
        "                        if dsm[i, j] > 0:\n",
        "                            G.add_edge(self.TOUCHPOINTS[i], self.TOUCHPOINTS[j])\n",
        "\n",
        "                # 2. Condense Cycles (SCCs)\n",
        "                sccs = list(nx.strongly_connected_components(G))\n",
        "                comp_map = {node: idx for idx, comp in enumerate(sccs) for node in comp}\n",
        "\n",
        "                CG = nx.DiGraph()\n",
        "                CG.add_nodes_from(range(len(sccs)))\n",
        "                for u, v in G.edges():\n",
        "                    if comp_map[u] != comp_map[v]:\n",
        "                        CG.add_edge(comp_map[u], comp_map[v])\n",
        "\n",
        "                # 3. Topological Sort (Execution Order)\n",
        "                try:\n",
        "                    order = list(nx.topological_generations(CG))\n",
        "                except:\n",
        "                    # Fallback if condensation graph somehow has cycles (shouldn't happen)\n",
        "                    order = [list(CG.nodes())]\n",
        "\n",
        "                # 4. Execute\n",
        "                log = []\n",
        "                steps = 0\n",
        "                cycle_blocks = 0\n",
        "\n",
        "                # Fresh state for this run\n",
        "                self.state = {}\n",
        "\n",
        "                for level in order:\n",
        "                    steps += 1\n",
        "                    for c_idx in level:\n",
        "                        block = list(sccs[c_idx])\n",
        "                        # Check if this block is a feedback loop (cycle)\n",
        "                        is_cycle = len(block) > 1 or (len(block)==1 and dsm[self.TOUCHPOINTS.index(block[0]), self.TOUCHPOINTS.index(block[0])] > 0)\n",
        "\n",
        "                        if is_cycle:\n",
        "                            cycle_blocks += 1\n",
        "\n",
        "                        # If cycle, we run: Initial Pass -> Feedback Calculation -> Rework Pass\n",
        "                        passes = 2 if is_cycle else 1\n",
        "\n",
        "                        for pass_id in range(passes):\n",
        "                            is_rework = (pass_id == 1)\n",
        "\n",
        "                            # Sort nodes within block deterministically for consistency\n",
        "                            block.sort()\n",
        "\n",
        "                            for node in block:\n",
        "                                out = self.run_node(node, is_rework=is_rework)\n",
        "                                # Update global state\n",
        "                                self.state.update(out)\n",
        "                                # Log\n",
        "                                for k, v in out.items():\n",
        "                                    log.append({\n",
        "                                        \"Step\": steps,\n",
        "                                        \"Touchpoint\": node,\n",
        "                                        \"Variable\": k,\n",
        "                                        \"Value\": v,\n",
        "                                        \"Is_Rework\": is_rework\n",
        "                                    })\n",
        "\n",
        "                df = pd.DataFrame(log)\n",
        "                kpis = self._calculate_kpis(df)\n",
        "\n",
        "                return {\n",
        "                    \"label\": label,\n",
        "                    \"steps\": steps,\n",
        "                    \"cycle_blocks\": cycle_blocks,\n",
        "                    **kpis,\n",
        "                    \"execution_log\": df,\n",
        "                    \"cycles\": [list(c) for c in sccs if len(c) > 1]\n",
        "                }\n",
        "\n",
        "\n",
        "        # =====================================================\n",
        "        # 2. DATA & LOGIC DEFINITIONS (Domain Specific)\n",
        "        # =====================================================\n",
        "\n",
        "        # Combined Generator Logic\n",
        "        GENERATOR_MAP = get_generators()\n",
        "\n",
        "        def register_hospital_logic(sim):\n",
        "            \"\"\"\n",
        "            Injects the specific Hospital/Insurance logic into the universal simulator.\n",
        "            Crucially, this handles the 'Rework' logic where coupled systems\n",
        "            improve quality/safety but increase cost.\n",
        "            \"\"\"\n",
        "\n",
        "            def make_handler(name, gen_func):\n",
        "                def handler(state, is_rework):\n",
        "                    # 1. Generate Base Data\n",
        "                    data = gen_func()\n",
        "\n",
        "                    # 2. Apply Feedback/Rework Logic\n",
        "                    if is_rework:\n",
        "                        # In a coupled loop, rework usually means:\n",
        "                        # - Better Safety/Compliance (Risk goes down, Scores go up)\n",
        "                        # - Higher Cost/Value (Assets are reinforced)\n",
        "                        for k, v in data.items():\n",
        "                            if \"score\" in k or \"ratio\" in k or \"level\" in k:\n",
        "                                # Improve quality metrics by 10%\n",
        "                                data[k] = min(v * 1.1, 1.0)\n",
        "                            elif \"val\" in k or \"cost\" in k or \"limit\" in k:\n",
        "                                # Costs/Values increase by 15% due to redesign\n",
        "                                data[k] = v * 1.15\n",
        "                            elif \"pml\" in k or \"incident\" in k:\n",
        "                                # Risk decreases by 20%\n",
        "                                data[k] = v * 0.8\n",
        "\n",
        "                    return data\n",
        "                return handler\n",
        "\n",
        "            for node in TOUCHPOINTS:\n",
        "                if node in GENERATOR_MAP:\n",
        "                    sim.register_node_logic(node, make_handler(node, GENERATOR_MAP[node]))\n",
        "\n",
        "\n",
        "        # =====================================================\n",
        "        # 3. SETUP DSMs (Linear vs Coupled)\n",
        "        # =====================================================\n",
        "        D_GRAPH = len(TOUCHPOINTS)\n",
        "\n",
        "        # A. Linear (Waterfall) - No Feedback\n",
        "        DSM_WATERFALL = np.eye(D_GRAPH, k=1) # Simple chain 0->1->2...\n",
        "\n",
        "        # B. Complex (The \"Gordian Knot\")\n",
        "        DSM_COMPLEX = np.zeros((D_GRAPH, D_GRAPH))\n",
        "\n",
        "        # Define Mappings for DSM indices\n",
        "        idx = {name: i for i, name in enumerate(TOUCHPOINTS)}\n",
        "\n",
        "        # Base Flow (similar to waterfall)\n",
        "        for i in range(D_GRAPH - 1):\n",
        "            DSM_COMPLEX[i, i+1] = 1\n",
        "\n",
        "        # Add The Loops Defined in Prompt\n",
        "        # 1. Strategy <-> Risk\n",
        "        DSM_COMPLEX[idx[\"Clinical_Strategy\"], idx[\"Risk_Underwriting\"]] = 1\n",
        "        DSM_COMPLEX[idx[\"Risk_Underwriting\"], idx[\"Clinical_Strategy\"]] = 1\n",
        "\n",
        "        # 2. Med Planning -> Infection -> Med Planning (Feedback)\n",
        "        DSM_COMPLEX[idx[\"Medical_Planning\"], idx[\"Infection_Control\"]] = 1\n",
        "        DSM_COMPLEX[idx[\"Infection_Control\"], idx[\"Medical_Planning\"]] = 1\n",
        "\n",
        "        # 3. Equipment <-> Structural <-> Reinsurance (3-way loop)\n",
        "        DSM_COMPLEX[idx[\"Equipment_Assets\"], idx[\"Structural_Integrity\"]] = 1\n",
        "        DSM_COMPLEX[idx[\"Structural_Integrity\"], idx[\"Reinsurance_Layer\"]] = 1\n",
        "        DSM_COMPLEX[idx[\"Reinsurance_Layer\"], idx[\"Equipment_Assets\"]] = 1\n",
        "\n",
        "        # 4. Cyber <-> Policy\n",
        "        DSM_COMPLEX[idx[\"ICT_Cyber_Security\"], idx[\"Operational_Policy\"]] = 1\n",
        "        DSM_COMPLEX[idx[\"Operational_Policy\"], idx[\"ICT_Cyber_Security\"]] = 1\n",
        "\n",
        "        # 5. Regulatory dependencies (Receiver of many, sender of approval)\n",
        "        DSM_COMPLEX[idx[\"Regulatory_Compliance\"], idx[\"Clinical_Strategy\"]] = 1\n",
        "\n",
        "        # =====================================================\n",
        "        # 4. EXECUTION\n",
        "        # =====================================================\n",
        "\n",
        "        # Init Simulator\n",
        "        sim = UniversalSystemSimulator(TOUCHPOINTS, METRIC_MAP)\n",
        "        register_hospital_logic(sim)\n",
        "\n",
        "        # Run Simulations\n",
        "        res_waterfall = sim.evaluate_dsm(DSM_COMPLEX, \"Waterfall (Linear)\")\n",
        "        res_coupled   = sim.evaluate_dsm(primary, \"Coupled (Iterative)\")\n",
        "\n",
        "        # =====================================================\n",
        "        # 5. REPORTING\n",
        "        # =====================================================\n",
        "\n",
        "        print(\"\\n===== UNIVERSAL SYSTEM SIMULATION REPORT: HOSPITAL & INSURANCE =====\")\n",
        "        print(f\"Scenario: {'Waterfall':<20} vs {'Coupled (Interdependent)':<20}\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        # KPI Comparison Table\n",
        "        kpis = [\"steps\", \"cycle_blocks\", \"avg_risk_index\", \"total_asset_index\", \"avg_opex_cost\"]\n",
        "        labels = {\n",
        "            \"steps\": \"Process Batches\",\n",
        "            \"cycle_blocks\": \"Detected Cycles\",\n",
        "            \"avg_risk_index\": \"Avg Risk Score (Lower=Better)\",\n",
        "            \"total_asset_index\": \"Total Asset Value ($M)\",\n",
        "            \"avg_opex_cost\": \"Operational Cost (Opex)\"\n",
        "        }\n",
        "\n",
        "        print(f\"{'KPI Indicator':<30} | {'WATERFALL':<15} | {'COUPLED':<15} | {'DELTA'}\")\n",
        "        print(\"-\" * 75)\n",
        "\n",
        "        for k in kpis:\n",
        "            val1 = res_waterfall[k]\n",
        "            val2 = res_coupled[k]\n",
        "            delta = val2 - val1\n",
        "            print(f\"{labels[k]:<30} | {val1:<15} | {val2:<15} | {delta:+.2f}\")\n",
        "\n",
        "        print(\"=\"*75)\n",
        "\n",
        "        print(\"\\n[ANALYSIS] Impact of Coupling:\")\n",
        "        print(f\"1. ASSET VALUE: The Coupled system produced a higher asset value (+{res_coupled['total_asset_index'] - res_waterfall['total_asset_index']:.2f}).\")\n",
        "        print(\"   -> Reason: Feedback loops allowed 'Rework' to optimize equipment and digital twin fidelity.\")\n",
        "        print(f\"2. OPEX COST: The Coupled system cost significantly more (+{res_coupled['avg_opex_cost'] - res_waterfall['avg_opex_cost']:.2f}).\")\n",
        "        print(\"   -> Reason: Iterative cycles (Design <-> Insurance) require operational overhead.\")\n",
        "        print(f\"3. RISK: Risk was reduced in the Coupled system ({res_coupled['avg_risk_index']} vs {res_waterfall['avg_risk_index']}).\")\n",
        "        print(\"   -> Reason: The 'Cyber <-> Policy' and 'Clinical <-> Risk' loops allowed mitigation strategies to take effect.\")\n",
        "\n",
        "        print(\"\\n[DETECTED FEEDBACK CYCLES]\")\n",
        "        for i, cycle in enumerate(res_coupled['cycles']):\n",
        "            print(f\"Cycle {i+1}: {' <-> '.join(cycle)}\")\n",
        "\n",
        "        print(\"\\n[SAMPLE LOG - Last 5 Operations in Coupled System]\")\n",
        "        print(res_coupled['execution_log'][['Step', 'Touchpoint', 'Variable', 'Value', 'Is_Rework']].tail(5).to_string(index=False))\n"
      ],
      "metadata": {
        "id": "uvgYhXoaVrxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA <--> METRICS <--> TP/DSM <--> GENERIC_SIMULATOR"
      ],
      "metadata": {
        "id": "9j9b5wHYe8I9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K5K8mxseV53s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "u_yrXF9qZW2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pIMnjU_Bc2XG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}