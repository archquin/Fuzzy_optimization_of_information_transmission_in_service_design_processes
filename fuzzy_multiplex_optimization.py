# -*- coding: utf-8 -*-
"""Fuzzy_Multiplex_Optimization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WpMMMlJml6wndDeo5Wbgsm-gO9bLn7IJ

# Fuzzy multiplex Optimization

- Inputs
    - Synthetic data targets
    - A fuzzy multiplex
- Model Components and Function
    - Inner graph/FCM-like multilayer
    - ACOR (Ant Colony Optimization for continuous values) for the inner layer
    - Soft target influence for each inner FCM to target data based on outer layer
    - Interdimensional per outer-graph node layers
    - Mutual information selection and targeted evolution of the inner multilayer
    - Outer graph network layer
    - ACOR for the outer layer based on inner layer outputs
    - Update rule of the outer layer based on objectives
    - Update rule for the inner layer based on soft output targets of the outer layer
- Outputs
    - Inner FCM/graph structures tune for processes with fuzzy intervals
    - Activations for the inner per outer graph nodes and targets
    - Outer graph structure tune to objectives
- Utility
    - What if scenarios simulation
    - Case study testing and hypotheses testing
    - Service robustness testing
    - Metric based service optimization
    - Trait based service optimization
    - Predictive analysis suggestions



**Notes**
Few words about the model and the following script; this model is a bidirectional co-evolutionary soft targeting hierarchical fuzzy multiplex optimizer. Following are three distinct cases that illustrated how the same concept can be utilized for different occasions in a simple and elegant manner and a few words about the model.

The optimal state of the model is achieved when it converges to the objectives, yet does not overfit inner data patterns. To achieve that soft argets are derived from the outer layer and are in place constraining the inner layer activations to optimal patterns, but not optimal fit, i.e overfit. Addionally, the model optimizes objectives simultaneously-dynamically in the inner layer and then then at the outer, based on external criteria. Lastly, during the optimization process the inner layer structures act indipendendly and evolve dynamically with the outer nodes, yet another fuzzy or logical constrain is imposed; the evolution of each inner FCM is evaluated against its peers on a local level and is penalized or rewarded based on its information merit.

Finally, the model creates multiple good instances of the targeted data; at the processes-outer layer level an activation, i.e. the best approximation, is selected. Furthermore, other top inner layer imitations are used to define a fuzzy interval IVFS (internal valued fuzzy set) for each outer node instance corresponding to each inner activation; that can be acheived at an arbitrary level and upon implementation it creates soft-target activations.

## Case 1

### - Synthetic-data trait optimization

#### Here the assumption is that the synthetic targets mimic traits, and traits are optimized using generic objectives
"""

# ---------------- CONFIG ----------------
import numpy as np
import matplotlib.pyplot as plt
import networkx as nx

# ---------------- SETTINGS ----------------
VERBOSE = True
candidate_dims = [4,4,4,4,4,4,4,4,4,4,] # multiple inner FCMs per node
D_graph = 6
inner_archive_size = 80
inner_offspring = 40
outer_archive_size = 40
outer_offspring = 40
inner_iters_per_outer = 5
outer_generations = 100
graph_strength = 1
IT_coefficient = 1
beta_relIT = 1
print_every = 20
inner_learning = 0.1
causal_flag = False
gamma_inter = 0.33
seed = 42
top_k = 19
np.random.seed(seed)

# ---------------- SYNTHETIC TARGETS ----------------
def generate_synthetic_targets(D_graph, candidate_dims, seed=42):
    np.random.seed(seed)
    targets = []
    for i in range(D_graph):
        node_targets = {}
        for dim in candidate_dims:
            y = 0.2 + 0.6 * np.random.rand(dim)
            y += np.linspace(0, 0.2, dim)
            y = np.clip(y, 0, 1)
            node_targets[dim] = y
        targets.append(node_targets)
    return targets

synthetic_targets = generate_synthetic_targets(D_graph, candidate_dims, seed=seed)

# ---------------- OBJECTIVES ----------------
class ObjectiveFunctions:
    @staticmethod
    def evaluate_inner_real(genome, D_fcm, metrics, alpha_w=1.0, alpha_t=1.0, alpha_u=1.0):
        """
        genome: inner FCM genome (x + W)
        D_fcm: dimension of this FCM
        metrics: dict with keys 'wait', 'throughput', 'util', optionally 'W_max','T_max','U_max'
        """
        x = genome[:D_fcm].copy()
        W = genome[D_fcm:].reshape(D_fcm,D_fcm).copy()
        np.fill_diagonal(W,0)
        y = ObjectiveFunctions.fcm_propagate(x, W)

        # extract metrics
        w_i = metrics['wait']
        t_i = metrics['throughput']
        u_i = metrics['util']
        W_max = metrics.get('W_max', 1.0)
        T_max = metrics.get('T_max', 1.0)
        U_max = metrics.get('U_max', 1.0)

        # objective: minimize waiting, maximize throughput, penalize over-utilization
        cost = alpha_w*(w_i/W_max) - alpha_t*(t_i/T_max) + alpha_u*(u_i/U_max)
        score = -cost  # higher score = better
        return score, y, W
    @staticmethod
    def sigmoid(v, lam=1.0):
        return 1.0 / (1.0 + np.exp(-lam*v))

    @staticmethod
    def fcm_propagate(x, W, ext=None, steps=30):
        y = x.copy()
        for _ in range(steps):
            inp = W.dot(y) + (ext if ext is not None else 0) + x
            y = ObjectiveFunctions.sigmoid(inp)
        return y

    @staticmethod
    def behavioral_update(W, y, alpha=0.6, lr=0.05, decay=0.01, causal_mask=None, eps=1e-6):
        C = y.copy()
        D = np.abs(y - np.mean(y))
        S = alpha*C + (1-alpha)*D
        RC = (y - np.mean(y)) / (np.std(y)+eps)
        RI = (S - np.mean(S)) / (np.std(S)+eps)
        delta = lr*np.outer(RC,RI) - decay*W
        np.fill_diagonal(delta,0)
        W_new = W + delta
        if causal_mask is not None:
            W_new = np.sign(causal_mask)*np.abs(W_new)
        np.clip(W_new,-1.0,1.0)
        np.fill_diagonal(W_new,0)
        return W_new

    @staticmethod
    def evaluate_nested_genome(genome, D_fcm, target=None):
        x = genome[:D_fcm].copy()
        W = genome[D_fcm:].reshape(D_fcm,D_fcm).copy()
        np.fill_diagonal(W,0)
        y = ObjectiveFunctions.fcm_propagate(x,W)
        if target is not None:
            if len(target) < len(y):
                target = np.pad(target, (0,len(y)-len(target)), mode='constant')
            elif len(target) > len(y):
                target = target[:len(y)]
            score = -np.linalg.norm(y-target)
        else:
            score = np.sum(y)
        return score, y, W

    @staticmethod
    def evaluate_graph(genome, nested_reps, D_graph, graph_strength, beta=0.5):
        Gmat = genome.reshape(D_graph,D_graph).copy()
        np.fill_diagonal(Gmat,0)
        max_dim = max(len(y) for y in nested_reps)
        padded = np.array([np.pad(y,(0,max_dim-len(y)),"constant") for y in nested_reps])
        modified = padded + graph_strength*Gmat.dot(padded)
        IT_node = np.sum(modified**2, axis=1)
        total_IT = np.sum(IT_node)+1e-12
        rel_IT = IT_node/total_IT
        FCM_signal = np.mean(IT_node)
        penalty = np.var(rel_IT)
        return FCM_signal - beta*penalty, penalty

# ---------------- INTER-LAYER CLASS ----------------
class InterLayer:
    def __init__(self, D_graph, max_inner_dim, inter_dim=len(candidate_dims), edge_threshold=0.02, seed=42):
        np.random.seed(seed)
        self.D_graph = D_graph
        self.inter_dim = inter_dim
        self.edge_threshold = edge_threshold
        self.max_input = 2 * max_inner_dim
        self.weights = {}
        for i in range(D_graph):
            for j in range(D_graph):
                if i == j: continue
                self.weights[(i,j)] = np.random.uniform(-0.6,0.6,(inter_dim, self.max_input))
        self.bias = {k: np.random.uniform(-0.3,0.3,inter_dim) for k in self.weights.keys()}

    def compute_edge_activation(self, i, j, nested_reps):
        concat = np.concatenate([nested_reps[i], nested_reps[j]])
        if len(concat) < self.max_input:
            concat = np.pad(concat,(0,self.max_input-len(concat)),"constant")
        else:
            concat = concat[:self.max_input]
        W = self.weights[(i,j)]
        b = self.bias[(i,j)]
        v = W.dot(concat)+b
        return 1/(1+np.exp(-v))

    def build_inter_activations(self, Gmat, nested_reps):
        acts = {}
        for i in range(self.D_graph):
            for j in range(self.D_graph):
                if i==j: continue
                if abs(Gmat[i,j])>self.edge_threshold:
                    acts[(i,j)] = self.compute_edge_activation(i,j,nested_reps)
        return acts

    @staticmethod
    def pairwise_squared_corr(acts):
        if len(acts)<2: return 0.0
        A = np.stack(list(acts.values()))
        A_cent = A - A.mean(axis=1, keepdims=True)
        stds = np.sqrt((A_cent**2).sum(axis=1)/(A.shape[1]-1)+1e-12)
        cov = A_cent.dot(A_cent.T)/(A.shape[1]-1)
        denom = np.outer(stds,stds)+1e-12
        corr = cov/denom
        np.fill_diagonal(corr,0)
        return (corr**2).sum()

    def mi_for_graph(self, Gmat, nested_reps):
        acts = self.build_inter_activations(Gmat,nested_reps)
        if len(acts)==0: return 0.0
        return float(self.pairwise_squared_corr(acts))

    def plot_inter_activations(self, Gmat, nested_reps):
        acts = self.build_inter_activations(Gmat,nested_reps)
        plt.figure(figsize=(8,4))
        for idx,(edge,act) in enumerate(acts.items()):
            plt.subplot(1,len(acts),idx+1)
            plt.bar(range(len(act)),act,color=plt.cm.viridis(act))
            plt.ylim(0,1)
            plt.title(f"{edge[0]}->{edge[1]}")
        plt.tight_layout()
        plt.show()
# ---------------- UNIFIED ACOR WITH INTER-LAYER MI ----------------
# ---------------- UNIFIED ACOR WITH INTER-LAYER MI ----------------
class UnifiedACORMultiplex:
    def __init__(self, candidate_dims, D_graph, inner_archive_size, inner_offspring,
                 outer_archive_size, outer_offspring, graph_strength, IT_coeff,
                 synthetic_targets, gamma_inter, inner_learning ,causal_flag=True, seed=42,
                 wait_time=None, throughput=None, utilization=None,
                 max_wait=1.0, max_throughput=1.0, max_util=1.0):

        self.wait_time = wait_time
        self.throughput = throughput
        self.utilization = utilization
        self.max_wait = max_wait
        self.max_throughput = max_throughput
        self.max_util = max_util
        self.inner_learning=inner_learning
        np.random.seed(seed)
        self.candidate_dims = candidate_dims
        self.D_graph = D_graph
        self.inner_archive_size = inner_archive_size
        self.inner_offspring = inner_offspring
        self.outer_archive_size = outer_archive_size
        self.outer_offspring = outer_offspring
        self.graph_strength = graph_strength
        self.IT_coeff = IT_coeff
        self.synthetic_targets = synthetic_targets
        self.causal_flag = causal_flag

        # ---- Inner archives ----
        self.inner_archives = []
        self.causal_masks = []
        for node in range(D_graph):
            node_archives = {}
            node_masks = {}
            for dim in candidate_dims:
                mask = np.sign(np.random.uniform(-1,1,(dim,dim)))
                np.fill_diagonal(mask,0)
                node_masks[dim] = mask
                members = []
                for _ in range(inner_archive_size):
                    x = np.random.rand(dim)
                    W = np.random.uniform(-0.6,0.6,(dim,dim))
                    np.fill_diagonal(W,0)
                    members.append(np.concatenate([x,W.flatten()]))
                node_archives[dim] = members
            self.inner_archives.append(node_archives)
            self.causal_masks.append(node_masks)

        # ---- Outer archive ----
        self.outer_archive = []
        for _ in range(outer_archive_size):
            Gm = np.random.uniform(-0.9,0.9,(D_graph,D_graph))
            np.fill_diagonal(Gm,0)
            self.outer_archive.append(Gm.flatten())

        # ---- Nested representations and history ----
        self.nested_reps = [np.zeros(max(candidate_dims)) for _ in range(D_graph)]
        self.best_dim_per_node = [candidate_dims[0] for _ in range(D_graph)]
        self.chosen_Gmat = np.zeros((D_graph,D_graph))
        self.inner_history = [[] for _ in range(D_graph)]
        self.outer_history = []
        self.outer_relIT_history = []

        # ---- Inter-layer ----
        self.inter_layer = InterLayer(D_graph, max_inner_dim=max(candidate_dims))
        self.gamma_inter = gamma_inter


    # ---------------- ACOR UTILS ----------------
    @staticmethod
    def acor_weights(m):
        ranks = np.arange(1,m+1)
        denom = m*(m+1)/2
        w = (m+1-ranks)/denom
        return w/w.sum()

    def sample_archive(self, archive):
        m = len(archive)
        w = self.acor_weights(m)
        idx = np.random.choice(m,p=w)
        center = archive[idx]
        arr = np.stack(archive,axis=0)
        std = np.std(arr,axis=0)
        sigma = self.IT_coeff*np.where(std<1e-6, self.IT_coeff,std)
        return np.random.normal(center,sigma)

    def enforce_bounds(self, gen, D_fcm=None, is_graph=False):
        g = np.clip(gen,-1,1)
        if is_graph:
            G = g.reshape(self.D_graph,self.D_graph)
            np.fill_diagonal(G,0)
            return G.flatten()
        elif D_fcm is not None:
            W = g[D_fcm:].reshape(D_fcm,D_fcm)
            np.fill_diagonal(W,0)
            g[D_fcm:] = W.flatten()
            return g
        return g

    # ---------------- INNER ADAPTATION ----------------
    ################### LR
    def run_inner(self, archive, mask, D_fcm, target, node_idx, adapt_steps=10, outer_influence=None, lr=inner_learning):
        """
        Inner FCM adaptation for a single node.
        """
        evals = [ObjectiveFunctions.evaluate_nested_genome(g, D_fcm, target)[0] for g in archive]
        best_idx = int(np.argmax(evals))
        best_genome = archive[best_idx].copy()
        x = best_genome[:D_fcm].copy()
        W = best_genome[D_fcm:].reshape(D_fcm, D_fcm).copy()

        for _ in range(adapt_steps):
            y = ObjectiveFunctions.fcm_propagate(x, W)
            W = ObjectiveFunctions.behavioral_update(W, y, causal_mask=mask if self.causal_flag else None)

            # --- Soft target from inter-layer ---
            if outer_influence is None and self.wait_time is not None:
                soft_target = np.array([
                    self.wait_time[node_idx][D_fcm] / self.max_wait,
                    self.throughput[node_idx][D_fcm] / self.max_throughput,
                    self.utilization[node_idx][D_fcm] / self.max_util
                ])
            else:
                soft_target = outer_influence[0]  # keep previous behavior if passed

                delta = lr * np.outer((soft_target - y), y)
                np.fill_diagonal(delta, 0)
                W += delta
                W = np.clip(W, -1, 1)

        adapted = np.concatenate([x, W.flatten()])
        # metrics for this node/dim should come from your dataset
        dim_idx = self.candidate_dims.index(D_fcm)
        metrics = {
        'wait': self.wait_time[node_idx][dim_idx],
        'throughput': self.throughput[node_idx][dim_idx],
        'util': self.utilization[node_idx][dim_idx],
        'W_max': self.max_wait,
        'T_max': self.max_throughput,
        'U_max': self.max_util
            }

        score, y_final, W_final = ObjectiveFunctions.evaluate_inner_real(adapted, D_fcm, metrics)
        archive[best_idx] = adapted
        return adapted, y_final, score, W_final

    # ---------------- OUTER EVALUATION ----------------
    def run_outer(self, nested_reps, beta=1.0):
        m = len(self.outer_archive)
        offspring = []
        for _ in range(self.outer_offspring):
            s = self.sample_archive(self.outer_archive)
            s = self.enforce_bounds(s,is_graph=True)
            s += np.random.normal(0,0.02,s.shape)
            s = self.enforce_bounds(s,is_graph=True)
            offspring.append(s)
        combined = self.outer_archive + offspring
        evals = []
        relITs = []

        for g_idx, g in enumerate(combined):
            # base FCM score
            score, relIT = ObjectiveFunctions.evaluate_graph(g, nested_reps, self.D_graph, self.graph_strength, beta=beta)

            # inter-layer MI penalty
            if hasattr(self,"inter_layer"):
                Gmat = g.reshape(self.D_graph,self.D_graph)
                score -= self.gamma_inter * self.inter_layer.mi_for_graph(Gmat, nested_reps)

            # --- incorporate dataset metrics per node ---
            # --- incorporate dataset metrics per node ---
            metric_score = 0
            if self.wait_time is not None:
                for i in range(self.D_graph):
                    dim = self.best_dim_per_node[i]           # chosen dimension for node i
                    dim_idx = self.candidate_dims.index(dim)  # index in candidate_dims
                    w = self.wait_time[i][dim_idx] / self.max_wait
                    t = self.throughput[i][dim_idx] / self.max_throughput
                    u = self.utilization[i][dim_idx] / self.max_util
                    metric_score += t - w - u  # accumulate across nodes
                score += 0.1 * metric_score   # weight this influence

            evals.append(score)
            relITs.append(relIT)

        # select top outer candidates
        sorted_idx = np.argsort(-np.array(evals))
        self.outer_archive = [combined[i] for i in sorted_idx[:m]]
        best_idx = int(np.argmax(evals))
        return combined[best_idx], evals[best_idx], relITs[best_idx]

    # ---------------- INTER-LAYER SOFT TARGETS ----------------
    def compute_node_soft_targets(self, alpha=0.0):#########3
        """
        Compute per-node soft targets blending FCM outputs and actual target activations.
        alpha = weight for FCM outputs; (1-alpha) = weight for true activations
        """
        soft_targets = []
        for i in range(self.D_graph):
            candidate_outputs = []
            for dim in self.candidate_dims:
                archive = self.inner_archives[i][dim]
                for genome in archive:
                    _, y, _ = ObjectiveFunctions.evaluate_nested_genome(genome, dim)
                    candidate_outputs.append(np.pad(y, (0, max(self.candidate_dims)-len(y))))
            candidate_outputs = np.stack(candidate_outputs)
            y_fcm_mean = candidate_outputs.mean(axis=0)

            # Get true activation (soft target)
            best_dim = self.best_dim_per_node[i]
            y_true = self.synthetic_targets[i][best_dim]
            if len(y_true) < len(y_fcm_mean):
                y_true = np.pad(y_true, (0,len(y_fcm_mean)-len(y_true)))
            elif len(y_true) > len(y_fcm_mean):
                y_true = y_true[:len(y_fcm_mean)]

            # Blend FCM and actual target
            soft_target = alpha*y_fcm_mean + (1-alpha)*y_true
            soft_targets.append(soft_target)
        return soft_targets


    # ---------------- MAIN RUN ----------------
    def run(self, outer_generations=60, inner_iters=10, print_every=10, perturb=0.05, beta_relIT=1.0):
        self.l2_before = []
        self.l2_after = []

        for i in range(self.D_graph):
            dim = self.candidate_dims[0]
            archive = self.inner_archives[i][dim]
            target = self.synthetic_targets[i][dim]
            _, y_init, _ = ObjectiveFunctions.evaluate_nested_genome(archive[0], dim, target)
            self.l2_before.append(np.linalg.norm(y_init - target))

        for gen in range(outer_generations):
            # --- Compute per-node soft targets ---
            node_soft_targets = self.compute_node_soft_targets()

            nested_traits = []
            new_reps = []
            for i in range(self.D_graph):
                dim_results = {}
                for dim in self.candidate_dims:
                    archive = self.inner_archives[i][dim]
                    mask = self.causal_masks[i][dim]
                    target = self.synthetic_targets[i][dim]
                    adapted, y_best, trait_val, W_best = self.run_inner(
                        archive, mask, dim, target, node_idx=i,
                        adapt_steps=inner_iters,
                        outer_influence=[node_soft_targets[i]]  # pass soft target
                    )
                    score = -np.linalg.norm(y_best - target)
                    dim_results[dim] = {"score": score, "genome": adapted, "y": y_best, "trait": trait_val, "W": W_best}

                best_dim = max(dim_results,key=lambda d: dim_results[d]["score"])
                self.best_dim_per_node[i] = best_dim
                chosen = dim_results[best_dim]
                new_reps.append(chosen["y"])
                nested_traits.append(chosen["trait"])
                self.inner_history[i].append(chosen["trait"])

            self.nested_reps = new_reps
            best_graph_genome, best_obj, best_relIT = self.run_outer(self.nested_reps,beta=beta_relIT)
            self.chosen_Gmat = best_graph_genome.reshape(self.D_graph,self.D_graph)
            np.fill_diagonal(self.chosen_Gmat,0)
            self.outer_history.append(best_obj)
            self.outer_relIT_history.append(best_relIT)

            if (gen+1)%print_every==0 or gen==0:
                print(f"\nGen {gen+1} | Outer Obj {best_obj:.4f} | Rel IT {best_relIT:.4f}")
                for i,val in enumerate(nested_traits):
                    print(f"Node {i+1} | Dim {self.best_dim_per_node[i]} | Trait {val:.4f}")

        for i in range(self.D_graph):
            y_final = self.nested_reps[i]
            dim = self.best_dim_per_node[i]
            target = self.synthetic_targets[i][dim]
            if len(target) < len(y_final):
                target = np.pad(target,(0,len(y_final)-len(target)),"constant")
            elif len(target) > len(y_final):
                target = target[:len(y_final)]
            self.l2_after.append(np.linalg.norm(y_final - target))

    def collect_pointwise_minmax_elite(self, node_idx, dim, top_k=10):
        """
        Compute pointwise min/max only from the top-k tuned kernels.
        This removes outliers and produces a clean interval.
        """
        archive = self.inner_archives[node_idx][dim]

        # Evaluate all kernels
        scores = []
        outputs = []
        for genome in archive:
            score, y, _ = ObjectiveFunctions.evaluate_nested_genome(genome, dim)
            scores.append(score)
            outputs.append(y)

        scores = np.array(scores)
        outputs = np.array([np.array(y, dtype=np.float64) for y in outputs])

               # --- Pointwise top-K selection (replaces genome-level top-K) ---

        # ---------------------------
        # ALIGN TO OPTIMAL FCM OUTPUT
        # ---------------------------

        # Get the soft-computed optimal FCM activation for this node/dimension
        Y_opt = self.nested_reps[node_idx]
        # the length must match dim; pad if needed
        maxdim = max(self.candidate_dims)
        Y_opt_padded = np.pad(Y_opt, (0, maxdim - len(Y_opt)))

        # Pad all outputs to identical dimension
        Y_all = np.array([
            np.pad(y, (0, maxdim - len(y))) for y in outputs
        ])

        # Compute distance to optimal FCM curve
        # L1 distance works best for shape alignment
        dist = np.sum(np.abs(Y_all - Y_opt_padded), axis=1)

        # Pick top-K closest kernels
        elite_idx = np.argsort(dist)[:top_k]
        elite_padded = np.array(Y_all[elite_idx], dtype=np.float64)

        # Build clean fuzzy interval around the optimal curve
        y_min = elite_padded.min(axis=0)
        y_max = elite_padded.max(axis=0)

        return y_min, y_max


    # ---------------- PLOTTING ----------------
    def plot_nested_activations(self):
        plt.figure(figsize=(12,3))
        for i,rep in enumerate(self.nested_reps):
            plt.subplot(1,self.D_graph,i+1)
            plt.bar(range(len(rep)),rep,color=plt.cm.plasma(rep))
            plt.ylim(0,1)
            plt.title(f"Node {i+1} Activations")
        plt.tight_layout()
        plt.show()

    def plot_outer_fuzzy_graph(self):
        G = nx.DiGraph()
        for i in range(self.D_graph): G.add_node(i)
        for i in range(self.D_graph):
            for j in range(self.D_graph):
                if i!=j and abs(self.chosen_Gmat[i,j])>0.02:
                    G.add_edge(i,j,weight=self.chosen_Gmat[i,j])
        node_sizes = [self.best_dim_per_node[i]*200 for i in range(self.D_graph)]
        edge_colors = ['green' if d['weight']>0 else 'red' for _,_,d in G.edges(data=True)]
        edge_widths = [abs(d['weight'])*3 for _,_,d in G.edges(data=True)]
        pos = nx.circular_layout(G)
        plt.figure(figsize=(6,6))
        nx.draw(G,pos,node_size=node_sizes,node_color='skyblue',edge_color=edge_colors,width=edge_widths,arrows=True,with_labels=True)
        plt.title("Outer Fuzzy Multiplex Graph")
        plt.show()

    def plot_nested_vs_target(self):
        plt.figure(figsize=(12,4))
        for i in range(self.D_graph):
            best_dim = self.best_dim_per_node[i]
            y_actual = self.nested_reps[i]
            y_target = self.synthetic_targets[i][best_dim]
            if len(y_target) < len(y_actual):
                y_target = np.pad(y_target,(0,len(y_actual)-len(y_target)),"constant")
            elif len(y_target) > len(y_actual):
                y_target = y_target[:len(y_actual)]
            plt.subplot(1,self.D_graph,i+1)
            plt.plot(range(len(y_actual)),y_actual,'o-',label='FCM Output')
            plt.plot(range(len(y_target)),y_target,'x--',label='Target')
            plt.ylim(0,1.1)
            plt.title(f"Node {i+1} | Dim {best_dim}")
            if i==0: plt.legend()
        plt.tight_layout()
        plt.show()
    def plot_pointwise_minmax_elite(self, top_k=10):
        plt.figure(figsize=(14, 3))

        for i in range(self.D_graph):
            dim = self.best_dim_per_node[i]

            # Clean elite interval
            y_min, y_max = self.collect_pointwise_minmax_elite(i, dim, top_k=top_k)

            # Chosen winner line (your best estimated FCM output)
            y_sel = self.nested_reps[i]

            # True FCM activation (or ground-truth / synthetic target)
            y_true = self.synthetic_targets[i][dim]
            if len(y_true) < len(y_sel):
                y_true = np.pad(y_true,(0,len(y_sel)-len(y_true)),"constant")
            elif len(y_true) > len(y_sel):
                y_true = y_true[:len(y_sel)]

            x = np.arange(len(y_min))

            plt.subplot(1, self.D_graph, i + 1)

            # Shaded elite interval
            plt.fill_between(x, y_min, y_max, color='skyblue', alpha=0.4, label='Elite Interval')

            # Winner line (estimated activation)
            plt.plot(x, y_sel, 'k-', lw=2, label='Estimated Activation')

            # True activation line
            plt.plot(x, y_true, 'r--', lw=2, label='True Activation')

            plt.ylim(0, 1.05)
            plt.title(f"Node {i+1} | Dim {dim}")
            if i == 0:
                plt.legend()

        plt.tight_layout()
        plt.show()



    def print_l2_summary(self):
        print("\nL2 Distances to Target per Node:")
        for i,(b,a) in enumerate(zip(self.l2_before,self.l2_after)):
            print(f"Node {i+1}: Before={b:.4f} | After={a:.4f} | Improvement={b-a:.4f}")


# ---------------- RUN EXAMPLE ----------------
if __name__ == "__main__":
    # example dummy datasets
    wait_data = np.random.rand(D_graph, max(candidate_dims))
    throughput_data = np.random.rand(D_graph, max(candidate_dims))
    util_data = np.random.rand(D_graph, max(candidate_dims))

    optimizer = UnifiedACORMultiplex(
    candidate_dims=candidate_dims,
    D_graph=D_graph,
    inner_archive_size=inner_archive_size,
    inner_offspring=inner_offspring,
    outer_archive_size=outer_archive_size,
    outer_offspring=outer_offspring,
    graph_strength=graph_strength,
    IT_coeff=IT_coefficient,
    synthetic_targets=synthetic_targets,
    gamma_inter=gamma_inter,
    inner_learning=inner_learning,
    causal_flag=causal_flag,
    seed=seed,
    wait_time=wait_data,
    throughput=throughput_data,
    utilization=util_data,
)

    optimizer.run(
        outer_generations=outer_generations,
        inner_iters=inner_iters_per_outer,
        print_every=print_every,
        perturb=0.0,
        beta_relIT=beta_relIT,
    )
    optimizer.plot_pointwise_minmax_elite(top_k=top_k)
    optimizer.plot_nested_activations()
    optimizer.plot_outer_fuzzy_graph()
    optimizer.plot_nested_vs_target()
    optimizer.print_l2_summary()
    optimizer.inter_layer.plot_inter_activations(optimizer.chosen_Gmat, optimizer.nested_reps)

"""## Case 2

### - Synthetic-data trait-metric optimization

#### Here the assumption is that the synthetic targets mimic traits, but metrics are optimized using generic objectives along the given triats. To achieve that, the objectives now revolve around the output y genome.
"""

# ---------------- CONFIG ----------------
import numpy as np
import matplotlib.pyplot as plt
import networkx as nx
# ---------------- SETTINGS ----------------
VERBOSE = True
candidate_dims = [4,4,4,4,4,4,4,4,4,4,4,4,] # multiple inner FCMs per node
D_graph = 6
inner_archive_size = 80
inner_offspring = 40
outer_archive_size = 40
outer_offspring = 40
inner_iters_per_outer = 24
outer_generations = 120
graph_strength = 1
IT_coefficient = 1
beta_relIT = 1
print_every = 10
inner_learning = 0.5
causal_flag = False
gamma_inter = 0.25
seed = 42
top_k = 21
np.random.seed(seed)

# ---------------- SYNTHETIC TARGETS ----------------
def generate_synthetic_targets(D_graph, candidate_dims, seed=42):
    np.random.seed(seed)
    targets = []
    for i in range(D_graph):
        node_targets = {}
        for dim in candidate_dims:
            y = 0.2 + 0.6 * np.random.rand(dim)
            y += np.linspace(0, 0.2, dim)
            y = np.clip(y, 0, 1)
            node_targets[dim] = y
        targets.append(node_targets)
    return targets

synthetic_targets = generate_synthetic_targets(D_graph, candidate_dims, seed=seed)

# ---------------- OBJECTIVES ----------------
class ObjectiveFunctions:
    @staticmethod
    def evaluate_nested_genome(genome, D_fcm, metrics=None, target=None,
                           alpha_w=1.0, alpha_t=1.0, alpha_u=1.0):
        x = genome[:D_fcm].copy()
        W = genome[D_fcm:].reshape(D_fcm,D_fcm).copy()
        np.fill_diagonal(W,0)
        y = ObjectiveFunctions.fcm_propagate(x,W)

        if target is not None:
            if len(target) < len(y):
                target = np.pad(target, (0,len(y)-len(target)), mode='constant')
            elif len(target) > len(y):
                target = target[:len(y)]
            score = -np.linalg.norm(y-target)

        elif isinstance(metrics, dict):
            w_i = metrics['wait']
            t_i = metrics['throughput']
            u_i = metrics['util']
            W_max = metrics.get('W_max',1.0)
            T_max = metrics.get('T_max',1.0)
            U_max = metrics.get('U_max',1.0)

            # trait-sensitive cost
            cost = alpha_w*(w_i/W_max) - alpha_t*(t_i/T_max) + alpha_u*(u_i/U_max)
            score = -cost

        else:
            # Trait-aware fallback: each trait contributes separately
            # You can also weigh traits differently here if you want
            score = np.mean(y)  # per-trait contribution
        return score, y, W



    @staticmethod
    def sigmoid(v, lam=1.5):
        return 1.0 / (1.0 + np.exp(-lam*v))

    @staticmethod
    def fcm_propagate(x, W, ext=None, steps=30):
        y = x.copy()
        for _ in range(steps):
            inp = W.dot(y) + (ext if ext is not None else 0) + x
            y = ObjectiveFunctions.sigmoid(inp)
        return y

    @staticmethod
    def behavioral_update(W, y, alpha=0.6, lr=0.05, decay=0.01, causal_mask=None, eps=1e-6):
        C = y.copy()
        D = np.abs(y - np.mean(y))
        S = alpha*C + (1-alpha)*D
        RC = (y - np.mean(y)) / (np.std(y)+eps)
        RI = (S - np.mean(S)) / (np.std(S)+eps)
        delta = lr*np.outer(RC,RI) - decay*W
        np.fill_diagonal(delta,0)
        W_new = W + delta
        if causal_mask is not None:
            W_new = np.sign(causal_mask)*np.abs(W_new)
        np.clip(W_new,-1.0,1.0)
        np.fill_diagonal(W_new,0)
        return W_new

    @staticmethod
    def evaluatenested_genome(genome, D_fcm, target=None):
        x = genome[:D_fcm].copy()
        W = genome[D_fcm:].reshape(D_fcm,D_fcm).copy()
        np.fill_diagonal(W,0)
        y = ObjectiveFunctions.fcm_propagate(x,W)
        if target is not None:
            if len(target) < len(y):
                target = np.pad(target, (0,len(y)-len(target)), mode='constant')
            elif len(target) > len(y):
                target = target[:len(y)]
            score = -np.linalg.norm(y-target)
        else:
            score = np.sum(y)
        return score, y, W

    @staticmethod
    def compute_dynamic_metrics(y):
        # simple example: make metrics a function of activations
        wait = 1.0 - y.mean()          # want lower wait if activation is high
        throughput = y.mean()           # want higher throughput if activation is high
        utilization = y.var()           # maybe penalize high variance
        return wait, throughput, utilization

    @staticmethod
    def evaluate_graph(genome, nested_reps, D_graph, graph_strength, beta=0.5):
        Gmat = genome.reshape(D_graph,D_graph).copy()
        np.fill_diagonal(Gmat,0)
        max_dim = max(len(y) for y in nested_reps)
        padded = np.array([np.pad(y,(0,max_dim-len(y)),"constant") for y in nested_reps])
        modified = padded + graph_strength*Gmat.dot(padded)
        IT_node = np.sum(modified**2, axis=1)
        total_IT = np.sum(IT_node)+1e-12
        rel_IT = IT_node/total_IT
        FCM_signal = np.mean(IT_node)
        penalty = np.var(rel_IT)
        return FCM_signal - beta*penalty, penalty

# ---------------- INTER-LAYER CLASS ----------------
class InterLayer:
    def __init__(self, D_graph, max_inner_dim, inter_dim=len(candidate_dims), edge_threshold=0.02, seed=42):
        np.random.seed(seed)
        self.D_graph = D_graph
        self.inter_dim = inter_dim
        self.edge_threshold = edge_threshold
        self.max_input = 2 * max_inner_dim
        self.weights = {}
        for i in range(D_graph):
            for j in range(D_graph):
                if i == j: continue
                self.weights[(i,j)] = np.random.uniform(-0.6,0.6,(inter_dim, self.max_input))
        self.bias = {k: np.random.uniform(-0.3,0.3,inter_dim) for k in self.weights.keys()}

    def compute_edge_activation(self, i, j, nested_reps):
        concat = np.concatenate([nested_reps[i], nested_reps[j]])
        if len(concat) < self.max_input:
            concat = np.pad(concat,(0,self.max_input-len(concat)),"constant")
        else:
            concat = concat[:self.max_input]
        W = self.weights[(i,j)]
        b = self.bias[(i,j)]
        v = W.dot(concat)+b
        return 1/(1+np.exp(-v))

    def build_inter_activations(self, Gmat, nested_reps):
        acts = {}
        for i in range(self.D_graph):
            for j in range(self.D_graph):
                if i==j: continue
                if abs(Gmat[i,j])>self.edge_threshold:
                    acts[(i,j)] = self.compute_edge_activation(i,j,nested_reps)
        return acts

    @staticmethod
    def pairwise_squared_corr(acts):
        if len(acts)<2: return 0.0
        A = np.stack(list(acts.values()))
        A_cent = A - A.mean(axis=1, keepdims=True)
        stds = np.sqrt((A_cent**2).sum(axis=1)/(A.shape[1]-1)+1e-12)
        cov = A_cent.dot(A_cent.T)/(A.shape[1]-1)
        denom = np.outer(stds,stds)+1e-12
        corr = cov/denom
        np.fill_diagonal(corr,0)
        return (corr**2).sum()

    def mi_for_graph(self, Gmat, nested_reps):
        acts = self.build_inter_activations(Gmat,nested_reps)
        if len(acts)==0: return 0.0
        return float(self.pairwise_squared_corr(acts))

    def plot_inter_activations(self, Gmat, nested_reps):
        acts = self.build_inter_activations(Gmat,nested_reps)
        plt.figure(figsize=(8,4))
        for idx,(edge,act) in enumerate(acts.items()):
            plt.subplot(1,len(acts),idx+1)
            plt.bar(range(len(act)),act,color=plt.cm.viridis(act))
            plt.ylim(0,1)
            plt.title(f"{edge[0]}->{edge[1]}")
        plt.tight_layout()
        plt.show()
# ---------------- UNIFIED ACOR WITH INTER-LAYER MI ----------------
# ---------------- UNIFIED ACOR WITH INTER-LAYER MI ----------------
class UnifiedACORMultiplex:
    def __init__(self, candidate_dims, D_graph, inner_archive_size, inner_offspring,
                 outer_archive_size, outer_offspring, graph_strength, IT_coeff,
                 synthetic_targets, gamma_inter, inner_learning ,causal_flag=True, seed=42,
                 wait_time=None, throughput=None, utilization=None,
                 max_wait=1.0, max_throughput=1.0, max_util=1.0):

        self.wait_time = wait_time
        self.throughput = throughput
        self.utilization = utilization
        self.max_wait = max_wait
        self.max_throughput = max_throughput
        self.max_util = max_util
        self.inner_learning=inner_learning
        np.random.seed(seed)
        self.candidate_dims = candidate_dims
        self.D_graph = D_graph
        self.inner_archive_size = inner_archive_size
        self.inner_offspring = inner_offspring
        self.outer_archive_size = outer_archive_size
        self.outer_offspring = outer_offspring
        self.graph_strength = graph_strength
        self.IT_coeff = IT_coeff
        self.synthetic_targets = synthetic_targets
        self.causal_flag = causal_flag

        # ---- Inner archives ----
        self.inner_archives = []
        self.causal_masks = []
        for node in range(D_graph):
            node_archives = {}
            node_masks = {}
            for dim in candidate_dims:
                mask = np.sign(np.random.uniform(-1,1,(dim,dim)))
                np.fill_diagonal(mask,0)
                node_masks[dim] = mask
                members = []
                for _ in range(inner_archive_size):
                    x = np.random.rand(dim)
                    W = np.random.uniform(-0.6,0.6,(dim,dim))
                    np.fill_diagonal(W,0)
                    members.append(np.concatenate([x,W.flatten()]))
                node_archives[dim] = members
            self.inner_archives.append(node_archives)
            self.causal_masks.append(node_masks)

        # ---- Outer archive ----
        self.outer_archive = []
        for _ in range(outer_archive_size):
            Gm = np.random.uniform(-0.9,0.9,(D_graph,D_graph))
            np.fill_diagonal(Gm,0)
            self.outer_archive.append(Gm.flatten())

        # ---- Nested representations and history ----
        self.nested_reps = [np.zeros(max(candidate_dims)) for _ in range(D_graph)]
        self.best_dim_per_node = [candidate_dims[0] for _ in range(D_graph)]
        self.chosen_Gmat = np.zeros((D_graph,D_graph))
        self.inner_history = [[] for _ in range(D_graph)]
        self.outer_history = []
        self.outer_relIT_history = []

        # ---- Inter-layer ----
        self.inter_layer = InterLayer(D_graph, max_inner_dim=max(candidate_dims))
        self.gamma_inter = gamma_inter


    # ---------------- ACOR UTILS ----------------
    @staticmethod
    def acor_weights(m):
        ranks = np.arange(1,m+1)
        denom = m*(m+1)/2
        w = (m+1-ranks)/denom
        return w/w.sum()

    def sample_archive(self, archive):
        m = len(archive)
        w = self.acor_weights(m)
        idx = np.random.choice(m,p=w)
        center = archive[idx]
        arr = np.stack(archive,axis=0)
        std = np.std(arr,axis=0)
        sigma = self.IT_coeff*np.where(std<1e-6, self.IT_coeff,std)
        return np.random.normal(center,sigma)

    def enforce_bounds(self, gen, D_fcm=None, is_graph=False):
        g = np.clip(gen,-1,1)
        if is_graph:
            G = g.reshape(self.D_graph,self.D_graph)
            np.fill_diagonal(G,0)
            return G.flatten()
        elif D_fcm is not None:
            W = g[D_fcm:].reshape(D_fcm,D_fcm)
            np.fill_diagonal(W,0)
            g[D_fcm:] = W.flatten()
            return g
        return g

    # ---------------- INNER ADAPTATION ----------------
    ################### LR
    def run_inner(self, archive, mask, D_fcm, target, node_idx, adapt_steps=10, outer_influence=None, lr=None):
        """
        Inner FCM adaptation for a single node.
        Returns adapted genome, best FCM output, trait score, final weights, and dynamic metrics.
        """
        if lr is None: lr = self.inner_learning
        # Evaluate initial archive
        evals = [ObjectiveFunctions.evaluate_nested_genome(g, D_fcm, target)[0] for g in archive]
        best_idx = int(np.argmax(evals))
        best_genome = archive[best_idx].copy()
        x = best_genome[:D_fcm].copy()
        W = best_genome[D_fcm:].reshape(D_fcm,D_fcm).copy()

        for _ in range(adapt_steps):
            y = ObjectiveFunctions.fcm_propagate(x, W)
            W = ObjectiveFunctions.behavioral_update(W, y, causal_mask=mask if self.causal_flag else None)

            # Soft target influence from inter-layer
            if outer_influence is not None:
                delta = lr * np.outer((outer_influence[0] - y), y)
                np.fill_diagonal(delta, 0)
                W += delta
                W = np.clip(W, -1, 1)

        # Compute dynamic metrics
        w = 1.0 - y.mean()
        t = y.mean()
        u = y.var()
        metrics = {"wait": w, "throughput": t, "util": u}

        # Evaluate final genome
        adapted = np.concatenate([x, W.flatten()])
        score, y_final, W_final = ObjectiveFunctions.evaluate_nested_genome(adapted, D_fcm, metrics)

        archive[best_idx] = adapted
        return adapted, y_final, score, W_final, metrics


    # ---------------- OUTER EVALUATION ----------------
    def run_outer(self, nested_reps, beta=1.0):
        m = len(self.outer_archive)
        offspring = []

        # --- generate offspring ---
        for _ in range(self.outer_offspring):
            s = self.sample_archive(self.outer_archive)
            s = self.enforce_bounds(s, is_graph=True)
            s += np.random.normal(0, 0.02, s.shape)
            s = self.enforce_bounds(s, is_graph=True)
            offspring.append(s)

        combined = self.outer_archive + offspring
        evals = []
        relITs = []

        for g_idx, g in enumerate(combined):
            # Evaluate FCM graph score
            score, relIT = ObjectiveFunctions.evaluate_graph(
                g, nested_reps, self.D_graph, self.graph_strength, beta=beta
            )

            # Inter-layer MI penalty
            if hasattr(self, "inter_layer"):
                Gmat = g.reshape(self.D_graph, self.D_graph)
                score -= self.gamma_inter * self.inter_layer.mi_for_graph(Gmat, nested_reps)

            # Incorporate dataset metrics per node
            metric_score = 0
            if self.wait_time is not None:
                for i in range(self.D_graph):
                    dim = self.best_dim_per_node[i]           # chosen dimension for node i
                    dim_idx = self.candidate_dims.index(dim)  # index in candidate_dims
                    w = self.wait_time[i][dim_idx] / self.max_wait
                    t = self.throughput[i][dim_idx] / self.max_throughput
                    u = self.utilization[i][dim_idx] / self.max_util
                    metric_score += t - w - u
                score += 0.1 * metric_score  # weight influence

            evals.append(score)
            relITs.append(relIT)

        # --- select top outer candidates ---
        sorted_idx = np.argsort(-np.array(evals))  # descending
        top_idx = sorted_idx[:m].tolist()          # convert to Python ints
        self.outer_archive = [combined[i] for i in top_idx]

        best_idx = int(np.argmax(evals))
        return combined[best_idx], evals[best_idx], relITs[best_idx]



    # ---------------- INTER-LAYER SOFT TARGETS ----------------
    def compute_node_soft_targets(self, alpha=0.0):#########3
        """
        Compute per-node soft targets blending FCM outputs and actual target activations.
        alpha = weight for FCM outputs; (1-alpha) = weight for true activations
        """
        soft_targets = []
        for i in range(self.D_graph):
            candidate_outputs = []
            for dim in self.candidate_dims:
                archive = self.inner_archives[i][dim]
                for genome in archive:
                    _, y, _ = ObjectiveFunctions.evaluate_nested_genome(genome, dim)
                    candidate_outputs.append(np.pad(y, (0, max(self.candidate_dims)-len(y))))
            candidate_outputs = np.stack(candidate_outputs)
            y_fcm_mean = candidate_outputs.mean(axis=0)

            # Get true activation (soft target)
            best_dim = self.best_dim_per_node[i]
            y_true = self.synthetic_targets[i][best_dim]
            if len(y_true) < len(y_fcm_mean):
                y_true = np.pad(y_true, (0,len(y_fcm_mean)-len(y_true)))
            elif len(y_true) > len(y_fcm_mean):
                y_true = y_true[:len(y_fcm_mean)]

            # Blend FCM and actual target
            soft_target = alpha*y_fcm_mean + (1-alpha)*y_true
            soft_targets.append(soft_target)
        return soft_targets


    # ---------------- MAIN RUN ----------------
    def run(self, outer_generations=60, inner_iters=10, print_every=10, perturb=0.05, beta_relIT=1.0):
        self.l2_before, self.l2_after = [], []

        # Initial L2 distance
        for i in range(self.D_graph):
            dim = self.candidate_dims[0]
            archive = self.inner_archives[i][dim]
            _, y_init, _ = ObjectiveFunctions.evaluate_nested_genome(archive[0], dim, self.synthetic_targets[i][dim])
            self.l2_before.append(np.linalg.norm(y_init - self.synthetic_targets[i][dim]))

        for gen in range(outer_generations):
            node_soft_targets = self.compute_node_soft_targets()
            nested_traits, new_reps, self.nested_metrics = [], [], []

            for i in range(self.D_graph):
                dim_results = {}
                for dim in self.candidate_dims:
                    archive = self.inner_archives[i][dim]
                    mask = self.causal_masks[i][dim]
                    target = self.synthetic_targets[i][dim]

                    adapted, y_best, trait_val, W_best, metrics = self.run_inner(
                        archive, mask, dim, target, node_idx=i,
                        adapt_steps=inner_iters,
                        outer_influence=[node_soft_targets[i]]
                    )

                    dim_results[dim] = {
                        "score": -np.linalg.norm(y_best - target),
                        "genome": adapted,
                        "y": y_best,
                        "trait": trait_val,
                        "W": W_best,
                        "metrics": metrics
                    }

                best_dim = max(dim_results, key=lambda d: dim_results[d]["score"])
                self.best_dim_per_node[i] = best_dim
                chosen = dim_results[best_dim]
                new_reps.append(chosen["y"])
                nested_traits.append(chosen["trait"])
                self.nested_metrics.append(chosen["metrics"])
                self.inner_history[i].append(chosen["trait"])

            self.nested_reps = new_reps
            best_graph_genome, best_obj, best_relIT = self.run_outer(self.nested_reps, beta=beta_relIT)
            self.chosen_Gmat = best_graph_genome.reshape(self.D_graph,self.D_graph)
            np.fill_diagonal(self.chosen_Gmat,0)
            self.outer_history.append(best_obj)
            self.outer_relIT_history.append(best_relIT)

            if (gen+1)%print_every==0 or gen==0:
                print(f"\nGen {gen+1} | Outer Obj {best_obj:.4f} | Rel IT {best_relIT:.4f}")
                for i,val in enumerate(nested_traits):
                    metrics = self.nested_metrics[i]
                    print(
                        f"Node {i+1} | Dim {self.best_dim_per_node[i]} | Trait {val:.4f} "
                        f"| Wait {metrics['wait']:.3f} | Throughput {metrics['throughput']:.3f} | Util {metrics['util']:.3f}"
                    )

        # Final L2 distance
        for i in range(self.D_graph):
            y_final = self.nested_reps[i]
            dim = self.best_dim_per_node[i]
            target = self.synthetic_targets[i][dim]
            if len(target) < len(y_final):
                target = np.pad(target,(0,len(y_final)-len(target)),"constant")
            elif len(target) > len(y_final):
                target = target[:len(y_final)]
            self.l2_after.append(np.linalg.norm(y_final - target))


    def collect_pointwise_minmax_elite(self, node_idx, dim, top_k=10):
        """
        Compute pointwise min/max only from the top-k tuned kernels.
        This removes outliers and produces a clean interval.
        """
        archive = self.inner_archives[node_idx][dim]

        # Evaluate all kernels
        scores = []
        outputs = []
        for genome in archive:
            score, y, _ = ObjectiveFunctions.evaluate_nested_genome(genome, dim)
            scores.append(score)
            outputs.append(y)

        scores = np.array(scores)
        outputs = np.array([np.array(y, dtype=np.float64) for y in outputs])

               # --- Pointwise top-K selection (replaces genome-level top-K) ---

        # ---------------------------
        # ALIGN TO OPTIMAL FCM OUTPUT
        # ---------------------------

        # Get the soft-computed optimal FCM activation for this node/dimension
        Y_opt = self.nested_reps[node_idx]
        # the length must match dim; pad if needed
        maxdim = max(self.candidate_dims)
        Y_opt_padded = np.pad(Y_opt, (0, maxdim - len(Y_opt)))

        # Pad all outputs to identical dimension
        Y_all = np.array([
            np.pad(y, (0, maxdim - len(y))) for y in outputs
        ])

        # Compute distance to optimal FCM curve
        # L1 distance works best for shape alignment
        dist = np.sum(np.abs(Y_all - Y_opt_padded), axis=1)

        # Pick top-K closest kernels
        elite_idx = np.argsort(dist)[:top_k]
        elite_padded = np.array(Y_all[elite_idx], dtype=np.float64)

        # Build clean fuzzy interval around the optimal curve
        y_min = elite_padded.min(axis=0)
        y_max = elite_padded.max(axis=0)

        return y_min, y_max


    # ---------------- PLOTTING ----------------
    def plot_nested_activations(self):
        plt.figure(figsize=(12,3))
        for i,rep in enumerate(self.nested_reps):
            plt.subplot(1,self.D_graph,i+1)
            plt.bar(range(len(rep)),rep,color=plt.cm.plasma(rep))
            plt.ylim(0,1)
            plt.title(f"Node {i+1} Activations")
        plt.tight_layout()
        plt.show()

    def plot_outer_fuzzy_graph(self):
        G = nx.DiGraph()
        for i in range(self.D_graph): G.add_node(i)
        for i in range(self.D_graph):
            for j in range(self.D_graph):
                if i!=j and abs(self.chosen_Gmat[i,j])>0.02:
                    G.add_edge(i,j,weight=self.chosen_Gmat[i,j])
        node_sizes = [self.best_dim_per_node[i]*200 for i in range(self.D_graph)]
        edge_colors = ['green' if d['weight']>0 else 'red' for _,_,d in G.edges(data=True)]
        edge_widths = [abs(d['weight'])*3 for _,_,d in G.edges(data=True)]
        pos = nx.circular_layout(G)
        plt.figure(figsize=(6,6))
        nx.draw(G,pos,node_size=node_sizes,node_color='skyblue',edge_color=edge_colors,width=edge_widths,arrows=True,with_labels=True)
        plt.title("Outer Fuzzy Multiplex Graph")
        plt.show()

    def plot_nested_vs_target(self):
        plt.figure(figsize=(12,4))
        for i in range(self.D_graph):
            best_dim = self.best_dim_per_node[i]
            y_actual = self.nested_reps[i]
            y_target = self.synthetic_targets[i][best_dim]
            if len(y_target) < len(y_actual):
                y_target = np.pad(y_target,(0,len(y_actual)-len(y_target)),"constant")
            elif len(y_target) > len(y_actual):
                y_target = y_target[:len(y_actual)]
            plt.subplot(1,self.D_graph,i+1)
            plt.plot(range(len(y_actual)),y_actual,'o-',label='FCM Output')
            plt.plot(range(len(y_target)),y_target,'x--',label='Target')
            plt.ylim(0,1.1)
            plt.title(f"Node {i+1} | Dim {best_dim}")
            if i==0: plt.legend()
        plt.tight_layout()
        plt.show()
    def plot_pointwise_minmax_elite(self, top_k=10):
        plt.figure(figsize=(14, 3))

        for i in range(self.D_graph):
            dim = self.best_dim_per_node[i]

            # Clean elite interval
            y_min, y_max = self.collect_pointwise_minmax_elite(i, dim, top_k=top_k)

            # Chosen winner line (your best estimated FCM output)
            y_sel = self.nested_reps[i]

            # True FCM activation (or ground-truth / synthetic target)
            y_true = self.synthetic_targets[i][dim]
            if len(y_true) < len(y_sel):
                y_true = np.pad(y_true,(0,len(y_sel)-len(y_true)),"constant")
            elif len(y_true) > len(y_sel):
                y_true = y_true[:len(y_sel)]

            x = np.arange(len(y_min))

            plt.subplot(1, self.D_graph, i + 1)

            # Shaded elite interval
            plt.fill_between(x, y_min, y_max, color='skyblue', alpha=0.4, label='Elite Interval')

            # Winner line (estimated activation)
            plt.plot(x, y_sel, 'k-', lw=2, label='Estimated Activation')

            # True activation line
            plt.plot(x, y_true, 'r--', lw=2, label='True Activation')

            plt.ylim(0, 1.05)
            plt.title(f"Node {i+1} | Dim {dim}")
            if i == 0:
                plt.legend()

        plt.tight_layout()
        plt.show()



    def print_l2_summary(self):
        print("\nL2 Distances to Target per Node:")
        for i,(b,a) in enumerate(zip(self.l2_before,self.l2_after)):
            print(f"Node {i+1}: Before={b:.4f} | After={a:.4f} | Improvement={b-a:.4f}")

# ---------------- RUN EXAMPLE ----------------
if __name__ == "__main__":
    # example dummy datasets
    wait_data = np.random.rand(D_graph, max(candidate_dims))
    throughput_data = np.random.rand(D_graph, max(candidate_dims))
    util_data = np.random.rand(D_graph, max(candidate_dims))

    optimizer = UnifiedACORMultiplex(
    candidate_dims=candidate_dims,
    D_graph=D_graph,
    inner_archive_size=inner_archive_size,
    inner_offspring=inner_offspring,
    outer_archive_size=outer_archive_size,
    outer_offspring=outer_offspring,
    graph_strength=graph_strength,
    IT_coeff=IT_coefficient,
    synthetic_targets=synthetic_targets,
    gamma_inter=gamma_inter,
    inner_learning=inner_learning,
    causal_flag=causal_flag,
    seed=seed,
    wait_time=wait_data,
    throughput=throughput_data,
    utilization=util_data,
)

    optimizer.run(
        outer_generations=outer_generations,
        inner_iters=inner_iters_per_outer,
        print_every=print_every,
        perturb=0.0,
        beta_relIT=beta_relIT,
    )
    optimizer.plot_pointwise_minmax_elite(top_k=top_k)
    optimizer.plot_nested_activations()
    optimizer.plot_outer_fuzzy_graph()
    optimizer.plot_nested_vs_target()
    optimizer.print_l2_summary()
    optimizer.inter_layer.plot_inter_activations(optimizer.chosen_Gmat, optimizer.nested_reps)

"""## Case 3

### - Synthetic-data metric optimization

#### Here the assumption is that the synthetic targets mimic metrics, and now metrics are optimized using generic, but to optimize the model now the objectives revolve only around the output y genome. That is synthetic target values for each process
"""

# ---------------- CONFIG ----------------
import numpy as np
import matplotlib.pyplot as plt
import networkx as nx

# ---------------- SETTINGS ----------------
VERBOSE = True
candidate_dims = [3,3,3,3,3,3,3,3,3,3,3,3] # multiple inner FCMs per node
D_graph = 6
inner_archive_size = 80
inner_offspring = 40
outer_archive_size = 40
outer_offspring = 40
inner_iters_per_outer = 20
outer_generations = 60
graph_strength = 1
IT_coefficient = 1
beta_relIT = 1
print_every = 10
inner_learning = 0.75
causal_flag = False
gamma_inter = 0.25
seed = 42
top_k = 19
np.random.seed(seed)

# ---------------- SYNTHETIC TARGETS ----------------
def generate_synthetic_targets(D_graph, candidate_dims, seed=42):
    np.random.seed(seed)
    targets = []
    for i in range(D_graph):
        node_targets = {}
        for dim in candidate_dims:
            y = 0.2 + 0.6 * np.random.rand(dim)
            y += np.linspace(0, 0.2, dim)
            y = np.clip(y, 0, 1)
            node_targets[dim] = y
        targets.append(node_targets)
    return targets

synthetic_targets = generate_synthetic_targets(D_graph, candidate_dims, seed=seed)

# ---------------- OBJECTIVES ----------------
class ObjectiveFunctions:
    @staticmethod
    def evaluate_nested_genome(genome, D_fcm,
                               metrics=None,          # computed metrics can be passed
                               target=None,           # activation vector target (old style)
                               metric_target=None,    # dict {"wait":..., "throughput":..., "util":...}
                               alpha_w=1.0, alpha_t=1.0, alpha_u=1.0,
                               lambda_target=1.0,     # weight for activation L2
                               lambda_metrics=0.0     # weight for metric-L2
                              ):
        """
        Behavior:
          - If metric_target is provided and lambda_metrics>0 -> compute metric-L2 loss.
          - If target is provided and lambda_target>0 -> compute activation-L2 loss.
          - Final score = -(lambda_target * ||y-target||_2 + lambda_metrics * ||m - metric_target||_2)
          - If neither target nor metric_target provided, fall back to trait-sensitive cost if metrics dict given.
        """
        x = genome[:D_fcm].copy()
        W = genome[D_fcm:].reshape(D_fcm,D_fcm).copy()
        np.fill_diagonal(W,0)
        y = ObjectiveFunctions.fcm_propagate(x,W)

        # prepare losses
        act_loss = 0.0
        metric_loss = 0.0

        if target is not None and lambda_target > 0.0:
            if len(target) < len(y):
                target = np.pad(target, (0,len(y)-len(target)), mode='constant')
            elif len(target) > len(y):
                target = target[:len(y)]
            act_loss = np.linalg.norm(y - target)

        # compute metrics from y if needed
        if (metric_target is not None and lambda_metrics > 0.0) or (isinstance(metrics, dict) and metric_target is None):
            # compute dynamic metrics m from y
            w_i, t_i, u_i = ObjectiveFunctions.compute_dynamic_metrics(y)
            m = {"wait": w_i, "throughput": t_i, "util": u_i}
        else:
            m = metrics if isinstance(metrics, dict) else None

        if metric_target is not None and lambda_metrics > 0.0:
            # flatten metric vectors to compute L2: keep order [wait,throughput,util]
            mt = np.array([metric_target["wait"], metric_target["throughput"], metric_target["util"]])
            mv = np.array([m["wait"], m["throughput"], m["util"]])
            metric_loss = np.linalg.norm(mv - mt)

        # If neither metric_target nor activation target used, fallback to earlier trait-sensitive cost
        if lambda_target==0.0 and lambda_metrics==0.0:
            if isinstance(metrics, dict):
                w_i = metrics['wait']; t_i = metrics['throughput']; u_i = metrics['util']
                W_max = metrics.get('W_max',1.0); T_max = metrics.get('T_max',1.0); U_max = metrics.get('U_max',1.0)
                cost = alpha_w*(w_i/W_max) - alpha_t*(t_i/T_max) + alpha_u*(u_i/U_max)
                score = -cost
                return score, y, W
            else:
                score = np.mean(y)
                return score, y, W

        # combined score (negated losses)
        total_loss = lambda_target * act_loss + lambda_metrics * metric_loss
        score = - total_loss
        return score, y, W



    @staticmethod
    def sigmoid(v, lam=1.5):
        return 1.0 / (1.0 + np.exp(-lam*v))

    @staticmethod
    def fcm_propagate(x, W, ext=None, steps=30):
        y = x.copy()
        for _ in range(steps):
            inp = W.dot(y) + (ext if ext is not None else 0) + x
            y = ObjectiveFunctions.sigmoid(inp)
        return y

    @staticmethod
    def behavioral_update(W, y, alpha=0.6, lr=0.05, decay=0.01, causal_mask=None, eps=1e-6):
        C = y.copy()
        D = np.abs(y - np.mean(y))
        S = alpha*C + (1-alpha)*D
        RC = (y - np.mean(y)) / (np.std(y)+eps)
        RI = (S - np.mean(S)) / (np.std(S)+eps)
        delta = lr*np.outer(RC,RI) - decay*W
        np.fill_diagonal(delta,0)
        W_new = W + delta
        if causal_mask is not None:
            W_new = np.sign(causal_mask)*np.abs(W_new)
        np.clip(W_new,-1.0,1.0)
        np.fill_diagonal(W_new,0)
        return W_new

    @staticmethod
    def evaluatenested_genome(genome, D_fcm, target=None):
        x = genome[:D_fcm].copy()
        W = genome[D_fcm:].reshape(D_fcm,D_fcm).copy()
        np.fill_diagonal(W,0)
        y = ObjectiveFunctions.fcm_propagate(x,W)
        if target is not None:
            if len(target) < len(y):
                target = np.pad(target, (0,len(y)-len(target)), mode='constant')
            elif len(target) > len(y):
                target = target[:len(y)]
            score = -np.linalg.norm(y-target)
        else:
            score = np.sum(y)
        return score, y, W

    @staticmethod
    def compute_dynamic_metrics(y):
        # Make sure y has at least 3 elements
        wait = y[0] if len(y) > 0 else 0.0
        throughput = y[1] if len(y) > 1 else 0.0
        utilization = y[2] if len(y) > 2 else 0.0
        return wait, throughput, utilization


    @staticmethod
    def evaluate_graph(genome, nested_reps, D_graph, graph_strength, beta=0.5):
        Gmat = genome.reshape(D_graph,D_graph).copy()
        np.fill_diagonal(Gmat,0)
        max_dim = max(len(y) for y in nested_reps)
        padded = np.array([np.pad(y,(0,max_dim-len(y)),"constant") for y in nested_reps])
        modified = padded + graph_strength*Gmat.dot(padded)
        IT_node = np.sum(modified**2, axis=1)
        total_IT = np.sum(IT_node)+1e-12
        rel_IT = IT_node/total_IT
        FCM_signal = np.mean(IT_node)
        penalty = np.var(rel_IT)
        return FCM_signal - beta*penalty, penalty

# ---------------- INTER-LAYER CLASS ----------------
class InterLayer:
    def __init__(self, D_graph, max_inner_dim, inter_dim=len(candidate_dims), edge_threshold=0.02, seed=42):
        np.random.seed(seed)
        self.D_graph = D_graph
        self.inter_dim = inter_dim
        self.edge_threshold = edge_threshold
        self.max_input = 2 * max_inner_dim
        self.weights = {}
        for i in range(D_graph):
            for j in range(D_graph):
                if i == j: continue
                self.weights[(i,j)] = np.random.uniform(-0.6,0.6,(inter_dim, self.max_input))
        self.bias = {k: np.random.uniform(-0.3,0.3,inter_dim) for k in self.weights.keys()}

    def compute_edge_activation(self, i, j, nested_reps):
        concat = np.concatenate([nested_reps[i], nested_reps[j]])
        if len(concat) < self.max_input:
            concat = np.pad(concat,(0,self.max_input-len(concat)),"constant")
        else:
            concat = concat[:self.max_input]
        W = self.weights[(i,j)]
        b = self.bias[(i,j)]
        v = W.dot(concat)+b
        return 1/(1+np.exp(-v))

    def build_inter_activations(self, Gmat, nested_reps):
        acts = {}
        for i in range(self.D_graph):
            for j in range(self.D_graph):
                if i==j: continue
                if abs(Gmat[i,j])>self.edge_threshold:
                    acts[(i,j)] = self.compute_edge_activation(i,j,nested_reps)
        return acts

    @staticmethod
    def pairwise_squared_corr(acts):
        if len(acts)<2: return 0.0
        A = np.stack(list(acts.values()))
        A_cent = A - A.mean(axis=1, keepdims=True)
        stds = np.sqrt((A_cent**2).sum(axis=1)/(A.shape[1]-1)+1e-12)
        cov = A_cent.dot(A_cent.T)/(A.shape[1]-1)
        denom = np.outer(stds,stds)+1e-12
        corr = cov/denom
        np.fill_diagonal(corr,0)
        return (corr**2).sum()

    def mi_for_graph(self, Gmat, nested_reps):
        acts = self.build_inter_activations(Gmat,nested_reps)
        if len(acts)==0: return 0.0
        return float(self.pairwise_squared_corr(acts))

    def plot_inter_activations(self, Gmat, nested_reps):
        acts = self.build_inter_activations(Gmat,nested_reps)
        plt.figure(figsize=(8,4))
        for idx,(edge,act) in enumerate(acts.items()):
            plt.subplot(1,len(acts),idx+1)
            plt.bar(range(len(act)),act,color=plt.cm.viridis(act))
            plt.ylim(0,1)
            plt.title(f"{edge[0]}->{edge[1]}")
        plt.tight_layout()
        plt.show()
# ---------------- UNIFIED ACOR WITH INTER-LAYER MI ----------------
# ---------------- UNIFIED ACOR WITH INTER-LAYER MI ----------------
class UnifiedACORMultiplex:
    def __init__(self, candidate_dims, D_graph, inner_archive_size, inner_offspring,
                 outer_archive_size, outer_offspring, graph_strength, IT_coeff,
                 synthetic_targets, gamma_inter, inner_learning ,causal_flag=True, seed=42,
                 wait_time=None, throughput=None, utilization=None,
                 max_wait=1.0, max_throughput=1.0, max_util=1.0):

        self.wait_time = wait_time
        self.throughput = throughput
        self.utilization = utilization
        self.max_wait = max_wait
        self.max_throughput = max_throughput
        self.max_util = max_util
        self.inner_learning=inner_learning
        np.random.seed(seed)
        self.candidate_dims = candidate_dims
        self.D_graph = D_graph
        self.inner_archive_size = inner_archive_size
        self.inner_offspring = inner_offspring
        self.outer_archive_size = outer_archive_size
        self.outer_offspring = outer_offspring
        self.graph_strength = graph_strength
        self.IT_coeff = IT_coeff
        self.synthetic_targets = synthetic_targets
        self.causal_flag = causal_flag

        # ---- Inner archives ----
        self.inner_archives = []
        self.causal_masks = []
        for node in range(D_graph):
            node_archives = {}
            node_masks = {}
            for dim in candidate_dims:
                mask = np.sign(np.random.uniform(-1,1,(dim,dim)))
                np.fill_diagonal(mask,0)
                node_masks[dim] = mask
                members = []
                for _ in range(inner_archive_size):
                    x = np.random.rand(dim)
                    W = np.random.uniform(-0.6,0.6,(dim,dim))
                    np.fill_diagonal(W,0)
                    members.append(np.concatenate([x,W.flatten()]))
                node_archives[dim] = members
            self.inner_archives.append(node_archives)
            self.causal_masks.append(node_masks)

        # ---- Outer archive ----
        self.outer_archive = []
        for _ in range(outer_archive_size):
            Gm = np.random.uniform(-0.9,0.9,(D_graph,D_graph))
            np.fill_diagonal(Gm,0)
            self.outer_archive.append(Gm.flatten())

        # ---- Nested representations and history ----
        self.nested_reps = [np.zeros(max(candidate_dims)) for _ in range(D_graph)]
        self.best_dim_per_node = [candidate_dims[0] for _ in range(D_graph)]
        self.chosen_Gmat = np.zeros((D_graph,D_graph))
        self.inner_history = [[] for _ in range(D_graph)]
        self.outer_history = []
        self.outer_relIT_history = []

        # ---- Inter-layer ----
        self.inter_layer = InterLayer(D_graph, max_inner_dim=max(candidate_dims))
        self.gamma_inter = gamma_inter


    # ---------------- ACOR UTILS ----------------
    @staticmethod
    def acor_weights(m):
        ranks = np.arange(1,m+1)
        denom = m*(m+1)/2
        w = (m+1-ranks)/denom
        return w/w.sum()

    def sample_archive(self, archive):
        m = len(archive)
        w = self.acor_weights(m)
        idx = np.random.choice(m,p=w)
        center = archive[idx]
        arr = np.stack(archive,axis=0)
        std = np.std(arr,axis=0)
        sigma = self.IT_coeff*np.where(std<1e-6, self.IT_coeff,std)
        return np.random.normal(center,sigma)

    def enforce_bounds(self, gen, D_fcm=None, is_graph=False):
        g = np.clip(gen,-1,1)
        if is_graph:
            G = g.reshape(self.D_graph,self.D_graph)
            np.fill_diagonal(G,0)
            return G.flatten()
        elif D_fcm is not None:
            W = g[D_fcm:].reshape(D_fcm,D_fcm)
            np.fill_diagonal(W,0)
            g[D_fcm:] = W.flatten()
            return g
        return g

    # ---------------- INNER ADAPTATION ----------------
    ################### LR
    # ---------------- INNER ADAPTATION ----------------
    def run_inner(self, archive, mask, D_fcm, target, node_idx, adapt_steps=10,
                  outer_influence=None, lr=None,
                  use_metric_target=False, metric_target=None,
                  lambda_target=1.0, lambda_metrics=0.0):
        """
        Inner FCM adaptation for a single node.
        Returns adapted genome, best FCM output, trait score, final weights, and dynamic metrics.
        """
        if lr is None: lr = self.inner_learning
        # Evaluate initial archive
        evals = [ObjectiveFunctions.evaluate_nested_genome(g, D_fcm, target)[0] for g in archive]
        best_idx = int(np.argmax(evals))
        best_genome = archive[best_idx].copy()
        x = best_genome[:D_fcm].copy()
        W = best_genome[D_fcm:].reshape(D_fcm,D_fcm).copy()

        for _ in range(adapt_steps):
            y = ObjectiveFunctions.fcm_propagate(x, W)
            W = ObjectiveFunctions.behavioral_update(W, y, causal_mask=mask if self.causal_flag else None)

            # Soft target influence from inter-layer
            if outer_influence is not None:
                delta = lr * np.outer((outer_influence[0] - y), y)
                np.fill_diagonal(delta, 0)
                W += delta
                W = np.clip(W, -1, 1)

        # Compute dynamic metrics
        w = y[0] if len(y) > 0 else 0.0
        t = y[1] if len(y) > 1 else 0.0
        u = y[2] if len(y) > 2 else 0.0
        metrics = {"wait": w, "throughput": t, "util": u}

        adapted = np.concatenate([x, W.flatten()])

        # Evaluate genome with either activation or metric target
        score, y_final, W_final = ObjectiveFunctions.evaluate_nested_genome(
            adapted, D_fcm,
            metrics=metrics if use_metric_target else None,
            metric_target=metric_target if use_metric_target else None,
            target=None if use_metric_target else target,
            lambda_target=lambda_target,
            lambda_metrics=lambda_metrics
        )

        # Replace best in archive
        archive[best_idx] = adapted
        return adapted, y_final, score, W_final, metrics


    # ---------------- OUTER EVALUATION ----------------
    def run_outer(self, nested_reps, beta=1.0):
        m = len(self.outer_archive)
        offspring = []

        # --- generate offspring ---
        for _ in range(self.outer_offspring):
            s = self.sample_archive(self.outer_archive)
            s = self.enforce_bounds(s, is_graph=True)
            s += np.random.normal(0, 0.02, s.shape)
            s = self.enforce_bounds(s, is_graph=True)
            offspring.append(s)

        combined = self.outer_archive + offspring
        evals = []
        relITs = []

        for g_idx, g in enumerate(combined):
            # Evaluate FCM graph score
            score, relIT = ObjectiveFunctions.evaluate_graph(
                g, nested_reps, self.D_graph, self.graph_strength, beta=beta
            )

            # Inter-layer MI penalty
            if hasattr(self, "inter_layer"):
                Gmat = g.reshape(self.D_graph, self.D_graph)
                score -= self.gamma_inter * self.inter_layer.mi_for_graph(Gmat, nested_reps)

            # Incorporate dataset metrics per node
            metric_score = 0
            if self.wait_time is not None:
                for i in range(self.D_graph):
                    dim = self.best_dim_per_node[i]           # chosen dimension for node i
                    dim_idx = self.candidate_dims.index(dim)  # index in candidate_dims
                    w = self.wait_time[i][dim_idx] / self.max_wait
                    t = self.throughput[i][dim_idx] / self.max_throughput
                    u = self.utilization[i][dim_idx] / self.max_util
                    metric_score += t - w - u
                score += 0.1 * metric_score  # weight influence

            evals.append(score)
            relITs.append(relIT)

        # --- select top outer candidates ---
        sorted_idx = np.argsort(-np.array(evals))  # descending
        top_idx = sorted_idx[:m].tolist()          # convert to Python ints
        self.outer_archive = [combined[i] for i in top_idx]

        best_idx = int(np.argmax(evals))
        return combined[best_idx], evals[best_idx], relITs[best_idx]



    # ---------------- INTER-LAYER SOFT TARGETS ----------------
    def compute_node_soft_targets(self, alpha=0.0):#########3
        """
        Compute per-node soft targets blending FCM outputs and actual target activations.
        alpha = weight for FCM outputs; (1-alpha) = weight for true activations
        """
        soft_targets = []
        for i in range(self.D_graph):
            candidate_outputs = []
            for dim in self.candidate_dims:
                archive = self.inner_archives[i][dim]
                for genome in archive:
                    _, y, _ = ObjectiveFunctions.evaluate_nested_genome(genome, dim)
                    candidate_outputs.append(np.pad(y, (0, max(self.candidate_dims)-len(y))))
            candidate_outputs = np.stack(candidate_outputs)
            y_fcm_mean = candidate_outputs.mean(axis=0)

            # Get true activation (soft target)
            best_dim = self.best_dim_per_node[i]
            y_true = self.synthetic_targets[i][best_dim]
            if len(y_true) < len(y_fcm_mean):
                y_true = np.pad(y_true, (0,len(y_fcm_mean)-len(y_true)))
            elif len(y_true) > len(y_fcm_mean):
                y_true = y_true[:len(y_fcm_mean)]

            # Blend FCM and actual target
            soft_target = alpha*y_fcm_mean + (1-alpha)*y_true
            soft_targets.append(soft_target)
        return soft_targets


    # ---------------- MAIN RUN ----------------
    def run(self, outer_generations=60, inner_iters=10, print_every=10, perturb=0.05, beta_relIT=1.0):
        self.l2_before, self.l2_after = [], []

        # Initial L2 distance
        for i in range(self.D_graph):
            dim = self.candidate_dims[0]
            archive = self.inner_archives[i][dim]
            _, y_init, _ = ObjectiveFunctions.evaluate_nested_genome(archive[0], dim, self.synthetic_targets[i][dim])
            self.l2_before.append(np.linalg.norm(y_init - self.synthetic_targets[i][dim]))

        for gen in range(outer_generations):
            node_soft_targets = self.compute_node_soft_targets()
            nested_traits, new_reps, self.nested_metrics = [], [], []

            for i in range(self.D_graph):
                dim_results = {}
                for dim in self.candidate_dims:
                    archive = self.inner_archives[i][dim]
                    mask = self.causal_masks[i][dim]
                    target = self.synthetic_targets[i][dim]

                    adapted, y_best, trait_val, W_best, metrics = self.run_inner(
                        archive, mask, dim, target, node_idx=i,
                        adapt_steps=inner_iters,
                        outer_influence=[node_soft_targets[i]]
                    )

                    dim_results[dim] = {
                        "score": -np.linalg.norm(y_best - target),
                        "genome": adapted,
                        "y": y_best,
                        "trait": trait_val,
                        "W": W_best,
                        "metrics": metrics
                    }

                best_dim = max(dim_results, key=lambda d: dim_results[d]["score"])
                self.best_dim_per_node[i] = best_dim
                chosen = dim_results[best_dim]
                new_reps.append(chosen["y"])
                nested_traits.append(chosen["trait"])
                self.nested_metrics.append(chosen["metrics"])
                self.inner_history[i].append(chosen["trait"])

            self.nested_reps = new_reps
            best_graph_genome, best_obj, best_relIT = self.run_outer(self.nested_reps, beta=beta_relIT)
            self.chosen_Gmat = best_graph_genome.reshape(self.D_graph,self.D_graph)
            np.fill_diagonal(self.chosen_Gmat,0)
            self.outer_history.append(best_obj)
            self.outer_relIT_history.append(best_relIT)

            if (gen+1)%print_every==0 or gen==0:
                print(f"\nGen {gen+1} | Outer Obj {best_obj:.4f} | Rel IT {best_relIT:.4f}")
                for i,val in enumerate(nested_traits):
                    metrics = self.nested_metrics[i]
                    print(
                        f"Node {i+1} | Dim {self.best_dim_per_node[i]} | Trait {val:.4f} "
                        f"| Wait {metrics['wait']:.3f} | Throughput {metrics['throughput']:.3f} | Util {metrics['util']:.3f}"
                    )

        # Final L2 distance
        for i in range(self.D_graph):
            y_final = self.nested_reps[i]
            dim = self.best_dim_per_node[i]
            target = self.synthetic_targets[i][dim]
            if len(target) < len(y_final):
                target = np.pad(target,(0,len(y_final)-len(target)),"constant")
            elif len(target) > len(y_final):
                target = target[:len(y_final)]
            self.l2_after.append(np.linalg.norm(y_final - target))


    def collect_pointwise_minmax_elite(self, node_idx, dim, top_k=10):
        """
        Compute pointwise min/max only from the top-k tuned kernels.
        This removes outliers and produces a clean interval.
        """
        archive = self.inner_archives[node_idx][dim]

        # Evaluate all kernels
        scores = []
        outputs = []
        for genome in archive:
            score, y, _ = ObjectiveFunctions.evaluate_nested_genome(genome, dim)
            scores.append(score)
            outputs.append(y)

        scores = np.array(scores)
        outputs = np.array([np.array(y, dtype=np.float64) for y in outputs])

               # --- Pointwise top-K selection (replaces genome-level top-K) ---

        # ---------------------------
        # ALIGN TO OPTIMAL FCM OUTPUT
        # ---------------------------

        # Get the soft-computed optimal FCM activation for this node/dimension
        Y_opt = self.nested_reps[node_idx]
        # the length must match dim; pad if needed
        maxdim = max(self.candidate_dims)
        Y_opt_padded = np.pad(Y_opt, (0, maxdim - len(Y_opt)))

        # Pad all outputs to identical dimension
        Y_all = np.array([
            np.pad(y, (0, maxdim - len(y))) for y in outputs
        ])

        # Compute distance to optimal FCM curve
        # L1 distance works best for shape alignment
        dist = np.sum(np.abs(Y_all - Y_opt_padded), axis=1)

        # Pick top-K closest kernels
        elite_idx = np.argsort(dist)[:top_k]
        elite_padded = np.array(Y_all[elite_idx], dtype=np.float64)

        # Build clean fuzzy interval around the optimal curve
        y_min = elite_padded.min(axis=0)
        y_max = elite_padded.max(axis=0)

        return y_min, y_max


    # ---------------- PLOTTING ----------------
    def plot_nested_activations(self):
        plt.figure(figsize=(12,3))
        for i,rep in enumerate(self.nested_reps):
            plt.subplot(1,self.D_graph,i+1)
            plt.bar(range(len(rep)),rep,color=plt.cm.plasma(rep))
            plt.ylim(0,1)
            plt.title(f"Node {i+1} Activations")
        plt.tight_layout()
        plt.show()

    def plot_outer_fuzzy_graph(self):
        G = nx.DiGraph()
        for i in range(self.D_graph): G.add_node(i)
        for i in range(self.D_graph):
            for j in range(self.D_graph):
                if i!=j and abs(self.chosen_Gmat[i,j])>0.02:
                    G.add_edge(i,j,weight=self.chosen_Gmat[i,j])
        node_sizes = [self.best_dim_per_node[i]*200 for i in range(self.D_graph)]
        edge_colors = ['green' if d['weight']>0 else 'red' for _,_,d in G.edges(data=True)]
        edge_widths = [abs(d['weight'])*3 for _,_,d in G.edges(data=True)]
        pos = nx.circular_layout(G)
        plt.figure(figsize=(6,6))
        nx.draw(G,pos,node_size=node_sizes,node_color='skyblue',edge_color=edge_colors,width=edge_widths,arrows=True,with_labels=True)
        plt.title("Outer Fuzzy Multiplex Graph")
        plt.show()

    def plot_nested_vs_target(self):
        plt.figure(figsize=(12,4))
        for i in range(self.D_graph):
            best_dim = self.best_dim_per_node[i]
            y_actual = self.nested_reps[i]
            y_target = self.synthetic_targets[i][best_dim]
            if len(y_target) < len(y_actual):
                y_target = np.pad(y_target,(0,len(y_actual)-len(y_target)),"constant")
            elif len(y_target) > len(y_actual):
                y_target = y_target[:len(y_actual)]
            plt.subplot(1,self.D_graph,i+1)
            plt.plot(range(len(y_actual)),y_actual,'o-',label='FCM Output')
            plt.plot(range(len(y_target)),y_target,'x--',label='Target')
            plt.ylim(0,1.1)
            plt.title(f"Node {i+1} | Dim {best_dim}")
            if i==0: plt.legend()
        plt.tight_layout()
        plt.show()
    def plot_pointwise_minmax_elite(self, top_k=10):
        plt.figure(figsize=(14, 3))

        for i in range(self.D_graph):
            dim = self.best_dim_per_node[i]

            # Clean elite interval
            y_min, y_max = self.collect_pointwise_minmax_elite(i, dim, top_k=top_k)

            # Chosen winner line (your best estimated FCM output)
            y_sel = self.nested_reps[i]

            # True FCM activation (or ground-truth / synthetic target)
            y_true = self.synthetic_targets[i][dim]
            if len(y_true) < len(y_sel):
                y_true = np.pad(y_true,(0,len(y_sel)-len(y_true)),"constant")
            elif len(y_true) > len(y_sel):
                y_true = y_true[:len(y_sel)]

            x = np.arange(len(y_min))

            plt.subplot(1, self.D_graph, i + 1)

            # Shaded elite interval
            plt.fill_between(x, y_min, y_max, color='skyblue', alpha=0.4, label='Elite Interval')

            # Winner line (estimated activation)
            plt.plot(x, y_sel, 'k-', lw=2, label='Estimated Activation')

            # True activation line
            plt.plot(x, y_true, 'r--', lw=2, label='True Activation')

            plt.ylim(0, 1.05)
            plt.title(f"Node {i+1} | Dim {dim}")
            if i == 0:
                plt.legend()

        plt.tight_layout()
        plt.show()



    def print_l2_summary(self):
        print("\nL2 Distances to Target per Node:")
        for i,(b,a) in enumerate(zip(self.l2_before,self.l2_after)):
            print(f"Node {i+1}: Before={b:.4f} | After={a:.4f} | Improvement={b-a:.4f}")

# ---------------- RUN EXAMPLE ----------------
if __name__ == "__main__":
    # example dummy datasets
    wait_data = np.random.rand(D_graph, max(candidate_dims))
    throughput_data = np.random.rand(D_graph, max(candidate_dims))
    util_data = np.random.rand(D_graph, max(candidate_dims))

    optimizer = UnifiedACORMultiplex(
    candidate_dims=candidate_dims,
    D_graph=D_graph,
    inner_archive_size=inner_archive_size,
    inner_offspring=inner_offspring,
    outer_archive_size=outer_archive_size,
    outer_offspring=outer_offspring,
    graph_strength=graph_strength,
    IT_coeff=IT_coefficient,
    synthetic_targets=synthetic_targets,
    gamma_inter=gamma_inter,
    inner_learning=inner_learning,
    causal_flag=causal_flag,
    seed=seed,
    wait_time=wait_data,
    throughput=throughput_data,
    utilization=util_data,
)

    optimizer.run(
        outer_generations=outer_generations,
        inner_iters=inner_iters_per_outer,
        print_every=print_every,
        perturb=0.0,
        beta_relIT=beta_relIT,
    )
    optimizer.plot_pointwise_minmax_elite(top_k=top_k)
    optimizer.plot_nested_activations()
    optimizer.plot_outer_fuzzy_graph()
    optimizer.plot_nested_vs_target()
    optimizer.print_l2_summary()
    optimizer.inter_layer.plot_inter_activations(optimizer.chosen_Gmat, optimizer.nested_reps)

