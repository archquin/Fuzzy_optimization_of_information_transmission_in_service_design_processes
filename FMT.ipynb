{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "Ug91TCNW2AuU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "# ---------------- Synthetic dataset ----------------\n",
        "np.random.seed()\n",
        "N = 10000\n",
        "\n",
        "route_planning = pd.DataFrame({\n",
        "    'origin_x': np.random.uniform(0, 100, N),\n",
        "    'origin_y': np.random.uniform(0, 100, N),\n",
        "    'dest_x': np.random.uniform(0, 100, N),\n",
        "    'dest_y': np.random.uniform(0, 100, N),\n",
        "    'traffic_density': np.random.uniform(0, 1, N),\n",
        "    'road_type': np.random.choice([1, 2, 3], N),\n",
        "})\n",
        "route_planning['distance'] = np.sqrt((route_planning['dest_x'] - route_planning['origin_x'])**2 +\n",
        "                                     (route_planning['dest_y'] - route_planning['origin_y'])**2)\n",
        "speed_base = {1:50, 2:40, 3:30}\n",
        "route_planning['speed'] = route_planning['road_type'].map(speed_base) * np.random.uniform(0.8,1.2,N)\n",
        "route_planning['travel_time'] = (route_planning['distance']/route_planning['speed'])*60*\\\n",
        "                                (1+route_planning['traffic_density']*np.random.uniform(0.1,0.5,N))\n",
        "\n",
        "vehicle_assignment = pd.DataFrame({\n",
        "    'vehicle_capacity': np.random.randint(50,200,N),\n",
        "    'battery_level': np.random.uniform(0.3,1.0,N),\n",
        "\n",
        "    'delivery_size': np.random.randint(5,50,N),\n",
        "    'vehicle_type': np.random.choice([1,2],N),\n",
        "    'speed_factor': np.random.uniform(0.9,1.1,N),\n",
        "})\n",
        "vehicle_assignment['assigned_speed'] = route_planning['speed']*vehicle_assignment['speed_factor']\n",
        "vehicle_assignment['load_utilization'] = vehicle_assignment['delivery_size']/vehicle_assignment['vehicle_capacity']\n",
        "\n",
        "time_scheduling = pd.DataFrame({\n",
        "    'requested_time': np.random.randint(8,20,N),\n",
        "    'delivery_priority': np.random.randint(1,5,N),\n",
        "    'customer_patience': np.random.uniform(0,1,N),\n",
        "})\n",
        "time_scheduling['delay_probability'] = np.clip(\n",
        "    (route_planning['travel_time']/60)*(1+vehicle_assignment['load_utilization']*0.5)*np.random.uniform(0.8,1.2,N),\n",
        "    0,1\n",
        ")\n",
        "\n",
        "dynamic_rerouting = pd.DataFrame({\n",
        "    'current_x': np.random.uniform(0,100,N),\n",
        "    'current_y': np.random.uniform(0,100,N),\n",
        "    'traffic_updates': np.random.uniform(0,1,N),\n",
        "    'new_delivery_requests': np.random.randint(0,3,N),\n",
        "    'vehicle_status': np.random.choice([0,1],N),\n",
        "    'weather': np.random.choice([0,1],N),\n",
        "})\n",
        "dynamic_rerouting['congestion_score'] = dynamic_rerouting['traffic_updates'] + \\\n",
        "                                       dynamic_rerouting['new_delivery_requests']*0.5 + \\\n",
        "                                       dynamic_rerouting['weather']*0.5 + \\\n",
        "                                       (route_planning['travel_time']/route_planning['travel_time'].max())*0.5\n",
        "\n",
        "# ---------------- Combine and normalize ----------------\n",
        "datasets = [route_planning, vehicle_assignment, time_scheduling, dynamic_rerouting]\n",
        "dataset_dims = [df.shape[1] for df in datasets]\n",
        "max_dim = max(dataset_dims)\n",
        "\n",
        "padded_data = []\n",
        "for df in datasets:\n",
        "    arr = df.values\n",
        "    if arr.shape[1] < max_dim:\n",
        "        arr = np.hstack([arr, np.zeros((arr.shape[0], max_dim - arr.shape[1]))])\n",
        "    padded_data.append(arr)\n",
        "\n",
        "DATA_MATRIX = np.hstack(padded_data)\n",
        "DATA_MATRIX = (DATA_MATRIX - DATA_MATRIX.min(axis=0)) / (np.ptp(DATA_MATRIX, axis=0)+1e-8)\n",
        "def generate_targets(DATA_MATRIX, datasets, candidate_dims, D_graph):\n",
        "    \"\"\"\n",
        "    Generate targets using meaningful metrics for each node.\n",
        "    Each node aggregates features from multiple datasets.\n",
        "    \"\"\"\n",
        "    targets = []\n",
        "    N = DATA_MATRIX.shape[0]\n",
        "\n",
        "    for node_idx in range(D_graph):\n",
        "        # pick a random row for each dataset\n",
        "        route_idx = node_idx % len(datasets[0])\n",
        "        vehicle_idx = node_idx % len(datasets[1])\n",
        "        time_idx = node_idx % len(datasets[2])\n",
        "        dynamic_idx = node_idx % len(datasets[3])\n",
        "\n",
        "        # Construct node vector (can use weighted metrics)\n",
        "        node_vector = np.hstack([\n",
        "            datasets[0].iloc[route_idx][['distance','travel_time','speed','traffic_density']].values,\n",
        "            datasets[1].iloc[vehicle_idx][['load_utilization','assigned_speed','vehicle_capacity']].values,\n",
        "            datasets[2].iloc[time_idx][['delay_probability','delivery_priority']].values,\n",
        "            datasets[3].iloc[dynamic_idx][['congestion_score','vehicle_status','weather']].values\n",
        "        ])\n",
        "\n",
        "        # Normalize node vector\n",
        "        node_vector = (node_vector - node_vector.min()) / (np.ptp(node_vector)+1e-8)\n",
        "\n",
        "        # Sample candidate dimensions\n",
        "        node_targets = {}\n",
        "        for dim in candidate_dims:\n",
        "            if len(node_vector) >= dim:\n",
        "                sampled = node_vector[:dim]\n",
        "            else:\n",
        "                sampled = np.pad(node_vector, (0, dim - len(node_vector)), constant_values=0.5)\n",
        "            node_targets[dim] = sampled\n",
        "        targets.append(node_targets)\n",
        "\n",
        "    return targets\n",
        "\n",
        "candidate_dims = [len(synthetic_targets[i][list(synthetic_targets[i].keys())[0]])\n",
        "                  for i in range(D_graph)]\n",
        "#[6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,]\n",
        "D_graph = 4\n",
        "datasets = [route_planning, vehicle_assignment, time_scheduling, dynamic_rerouting]\n",
        "\n",
        "synthetic_targets = generate_targets(DATA_MATRIX, datasets, candidate_dims, D_graph)\n",
        "\n",
        "\n",
        "# ---------------- Targets ----------------\n",
        "#candidate_dims = [6,6,6,6,6,6,6]#,6,6,6,6,6,6,6,6,6,6,6,6,]\n",
        "#D_graph = 4\n",
        "inner_archive_size = 120\n",
        "inner_offspring = 80\n",
        "outer_archive_size = 80\n",
        "outer_offspring = 80\n",
        "inner_iters_per_outer = 15\n",
        "outer_generations = 500\n",
        "outer_cost_limit = 1350\n",
        "inner_learning = 0.9\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "#candidate_dims = [3] * 50\n",
        "D_graph = 4\n",
        "\n",
        "inner_archive_size = 80\n",
        "inner_offspring = 40\n",
        "outer_archive_size = 40\n",
        "outer_offspring = 40\n",
        "inner_iters_per_outer = 50\n",
        "outer_generations = 1500\n",
        "outer_cost_limit = 10000\n",
        "inner_learning = 0.0\n",
        "gamma_interlayer = 0\n",
        "top_k = 2630\n",
        "np.random.seed()\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MetricsEvaluator:\n",
        "    def __init__(self, data_matrix, D_graph, metric_keys=['wait','throughput','util','patience']):\n",
        "        \"\"\"\n",
        "        data_matrix: normalized data\n",
        "        D_graph: number of nodes\n",
        "        metric_keys: metrics per node\n",
        "        \"\"\"\n",
        "        self.data_matrix = data_matrix\n",
        "        self.D_graph = D_graph\n",
        "        self.metric_keys = metric_keys\n",
        "        self.num_features = data_matrix.shape[1]\n",
        "\n",
        "        # Initialize learnable parameters per node and per metric\n",
        "        # Each node has a weight vector (num_features) and bias per metric\n",
        "        self.params = {}\n",
        "        for node_idx in range(D_graph):\n",
        "            self.params[node_idx] = {}\n",
        "            for key in metric_keys:\n",
        "                self.params[node_idx][key] = {\n",
        "                    'weights': np.random.uniform(-1,1,self.num_features),\n",
        "                    'bias': np.random.uniform(0,1)\n",
        "                }\n",
        "\n",
        "    def compute_node_metrics(self, node_idx, y=None):\n",
        "        \"\"\"\n",
        "        Compute metrics using linear combination of features + bias per metric.\n",
        "        y: optional auxiliary vector to modulate metrics\n",
        "        \"\"\"\n",
        "        row = self.data_matrix[node_idx % self.data_matrix.shape[0]]\n",
        "        if y is None:\n",
        "            y = np.zeros(3)\n",
        "        else:\n",
        "            y = np.array(y[:3]) if len(y)>=3 else np.pad(y,(0,3-len(y)),constant_values=0.5)\n",
        "\n",
        "        metrics = {}\n",
        "        for key in self.metric_keys:\n",
        "            w = self.params[node_idx][key]['weights']\n",
        "            b = self.params[node_idx][key]['bias']\n",
        "            # Linear combination of features, plus optional small modulation from y\n",
        "            metrics[key] = float(np.dot(row, w) + b + 0.1*np.sum(y))\n",
        "\n",
        "        # Combine into score\n",
        "        metrics['score'] = -metrics['wait'] + metrics['throughput'] + metrics['util'] + metrics['patience']\n",
        "        return metrics\n",
        "\n",
        "    def compute_all_metrics(self):\n",
        "        all_metrics = []\n",
        "        for node_idx in range(self.D_graph):\n",
        "            all_metrics.append(self.compute_node_metrics(node_idx))\n",
        "        return all_metrics\n",
        "\n",
        "\n",
        "# ---------------- INTER-LAYER MUTUAL INFORMATION ----------------\n",
        "class InterLayer:\n",
        "    def __init__(self, D_graph, max_inner_dim, inter_dim=None, edge_threshold=0.02, gamma=1.0, seed=42):\n",
        "        np.random.seed(seed)\n",
        "        self.D_graph = D_graph\n",
        "        self.max_input = 2*max_inner_dim\n",
        "        self.edge_threshold = edge_threshold\n",
        "        self.gamma = gamma\n",
        "        self.inter_dim = inter_dim if inter_dim is not None else max_inner_dim\n",
        "        self.weights = {(i,j): np.random.uniform(-0.6,0.6,(self.inter_dim,self.max_input))\n",
        "                        for i in range(D_graph) for j in range(D_graph) if i!=j}\n",
        "        self.bias = {(i,j): np.random.uniform(-0.3,0.3,self.inter_dim)\n",
        "                     for i in range(D_graph) for j in range(D_graph) if i!=j}\n",
        "\n",
        "    def compute_edge_activation(self, i, j, nested_reps):\n",
        "        concat = np.concatenate([nested_reps[i], nested_reps[j]])\n",
        "        concat = np.pad(concat, (0, max(0,self.max_input-len(concat))))[:self.max_input]\n",
        "        v = self.weights[(i,j)].dot(concat) + self.bias[(i,j)]\n",
        "        return 1/(1+np.exp(-v))\n",
        "\n",
        "    def build_activations(self, Gmat, nested_reps):\n",
        "        acts = {}\n",
        "        for i in range(self.D_graph):\n",
        "            for j in range(self.D_graph):\n",
        "                if i==j: continue\n",
        "                if abs(Gmat[i,j])>self.edge_threshold:\n",
        "                    acts[(i,j)] = self.compute_edge_activation(i,j,nested_reps)\n",
        "        return acts\n",
        "\n",
        "    @staticmethod\n",
        "    def pairwise_squared_corr(acts):\n",
        "        if len(acts)<2: return 0.0\n",
        "        A = np.stack(list(acts.values()))\n",
        "        A_centered = A - A.mean(axis=1,keepdims=True)\n",
        "        stds = np.sqrt(np.sum(A_centered**2,axis=1)/(A.shape[1]-1) + 1e-12)\n",
        "        cov = A_centered @ A_centered.T / (A.shape[1]-1)\n",
        "        corr = cov / (np.outer(stds,stds)+1e-12)\n",
        "        np.fill_diagonal(corr,0)\n",
        "        return float((corr**2).sum())\n",
        "\n",
        "    def mi_for_graph(self, Gmat, nested_reps):\n",
        "        acts = self.build_activations(Gmat,nested_reps)\n",
        "        if not acts: return 0.0\n",
        "        return self.gamma * self.pairwise_squared_corr(acts)\n",
        "\n",
        "# ---------------- UNIFIED ACOR MULTIPLEX ----------------\n",
        "class FMT:\n",
        "    def __init__(self, candidate_dims, D_graph, inner_archive_size, inner_offspring,\n",
        "                 outer_archive_size, outer_offspring, synthetic_targets, inner_learning,\n",
        "                 gamma_interlayer=1.0, causal_flag=True):\n",
        "        self.candidate_dims = candidate_dims\n",
        "        self.D_graph = D_graph\n",
        "        self.inner_archive_size = inner_archive_size\n",
        "        self.inner_offspring = inner_offspring\n",
        "        self.outer_archive_size = outer_archive_size\n",
        "        self.outer_offspring = outer_offspring\n",
        "        self.synthetic_targets = synthetic_targets\n",
        "        self.inner_learning = inner_learning\n",
        "        self.causal_flag = causal_flag\n",
        "\n",
        "        self.nested_reps = [np.zeros(max(candidate_dims)) for _ in range(D_graph)]\n",
        "        self.best_dim_per_node = [candidate_dims[0] for _ in range(D_graph)]\n",
        "        self.inter_layer = InterLayer(D_graph, max_inner_dim=max(candidate_dims), gamma=gamma_interlayer)\n",
        "        self.chosen_Gmat = np.random.uniform(-0.5,0.5,(D_graph,D_graph))\n",
        "        np.fill_diagonal(self.chosen_Gmat,0)\n",
        "        self.l2_before, self.l2_after = [], []\n",
        "\n",
        "    # ---------- INNER LOOP (FCM) ----------\n",
        "    def run_inner(self, node_idx, target, D_fcm,\n",
        "              steps=100, lr_x=0.00001, lr_y=0.00001, lr_W=0.00001):\n",
        "\n",
        "        # two activation channels\n",
        "        x = target.copy()\n",
        "        y = target.copy()\n",
        "\n",
        "        W = np.random.uniform(-0.6, 0.6, (D_fcm, D_fcm))\n",
        "        np.fill_diagonal(W, 0)\n",
        "\n",
        "        self.l2_before.append(np.linalg.norm(self.nested_reps[node_idx] - target))\n",
        "\n",
        "        for _ in range(steps):\n",
        "\n",
        "            z = W.dot(x)\n",
        "\n",
        "            Theta_grad_z = 2*z - target\n",
        "\n",
        "            # u = x, v = y\n",
        "            Theta_grad_x = Theta_grad_z @ W + (y + 1)\n",
        "            Theta_grad_y = (x + 1)\n",
        "\n",
        "            Theta_grad_W = np.outer(Theta_grad_z, x)\n",
        "\n",
        "            # updates\n",
        "            x -= lr_x * np.clip(Theta_grad_x, -0.05, 0.05)\n",
        "            y -= lr_y * np.clip(Theta_grad_y, -0.05, 0.05)\n",
        "\n",
        "            x = np.clip(x, 0, 1)\n",
        "            y = np.clip(y, 0, 1)\n",
        "\n",
        "            W -= lr_W * np.clip(Theta_grad_W, -0.01, 0.01)\n",
        "            np.fill_diagonal(W, 0)\n",
        "            W = np.clip(W, -1, 1)\n",
        "\n",
        "        self.nested_reps[node_idx] = x\n",
        "\n",
        "        self.l2_after.append(np.linalg.norm(x - target))\n",
        "        mi_score = self.inter_layer.mi_for_graph(self.chosen_Gmat, self.nested_reps)\n",
        "\n",
        "        return x, y, W, mi_score\n",
        "\n",
        "    # ---------- OUTER LOOP ----------\n",
        "    def run_outer(self, outer_cost_limit=1000):\n",
        "        metrics_evaluator = MetricsEvaluator(DATA_MATRIX, self.D_graph)\n",
        "        node_metrics_list = []\n",
        "        raw_scores = []\n",
        "\n",
        "        # --- Compute node metrics per node ---\n",
        "        for i, y in enumerate(self.nested_reps):\n",
        "            metrics = metrics_evaluator.compute_node_metrics(i, y=y)\n",
        "            node_metrics_list.append(metrics)\n",
        "            raw_scores.append(metrics['score'])\n",
        "\n",
        "        raw_scores = np.array(raw_scores)\n",
        "        total_raw = raw_scores.sum()\n",
        "\n",
        "        # --- Apply cap to raw metrics ---\n",
        "        capped_total_raw = total_raw\n",
        "        if total_raw > outer_cost_limit:\n",
        "            scale_factor = outer_cost_limit / total_raw\n",
        "            for metrics in node_metrics_list:\n",
        "                for key in ['wait', 'throughput', 'util', 'patience', 'score']:\n",
        "                    metrics[key] *= scale_factor\n",
        "            raw_scores *= scale_factor\n",
        "            capped_total_raw = outer_cost_limit\n",
        "\n",
        "        # --- Compute Fuzzy Metric Tensor contribution ---\n",
        "        fuzzy_tensor = self.compute_fuzzy_metric_tensor(normalize=True)\n",
        "        D = self.D_graph\n",
        "\n",
        "        # Only consider off-diagonal entries for inter-node interactions\n",
        "        off_diag_mask = np.ones((D, D), dtype=bool)\n",
        "        np.fill_diagonal(off_diag_mask, 0)\n",
        "        fuzzy_score_offdiag = fuzzy_tensor[off_diag_mask].sum()\n",
        "\n",
        "        # --- Compute per-node contribution ---\n",
        "        node_contributions = np.zeros(D)\n",
        "        for i in range(D):\n",
        "            # Contribution from own metrics\n",
        "            own_score = raw_scores[i]\n",
        "\n",
        "            # Contribution from FMT interactions (row i -> others)\n",
        "            fmt_contrib = fuzzy_tensor[i, :, :].sum() - fuzzy_tensor[i, i, :].sum()  # exclude self\n",
        "            node_contributions[i] = own_score + self.inter_layer.gamma * fmt_contrib\n",
        "\n",
        "        # --- Combine total score ---\n",
        "        combined_score = node_contributions.sum()\n",
        "\n",
        "        # --- Store for plotting / further analysis ---\n",
        "        self.capped_node_metrics = node_metrics_list\n",
        "        self.node_score_contributions = node_contributions\n",
        "\n",
        "        return node_metrics_list, combined_score, node_contributions\n",
        "\n",
        "\n",
        "\n",
        "    # ---------- FULL RUN ----------\n",
        "    def run(self, outer_generations=outer_generations):\n",
        "      best_score = -np.inf\n",
        "      best_state = {}\n",
        "\n",
        "      for gen in range(outer_generations):\n",
        "          mi_scores = []\n",
        "\n",
        "          # --- Inner loop per node ---\n",
        "          for node_idx in range(self.D_graph):\n",
        "              dim = self.best_dim_per_node[node_idx]\n",
        "              target = self.synthetic_targets[node_idx][dim]\n",
        "              _, _, _, mi_score = self.run_inner(node_idx, target, dim)\n",
        "              mi_scores.append(mi_score)\n",
        "\n",
        "          # --- Outer loop ---\n",
        "          metrics_list, capped_score, node_contributions = self.run_outer()\n",
        "\n",
        "          # --- Keep best score and full state ---\n",
        "          if capped_score > best_score:\n",
        "              best_score = capped_score\n",
        "              best_state = {\n",
        "                  \"nested_reps\": [np.copy(rep) for rep in self.nested_reps],\n",
        "                  \"capped_node_metrics\": metrics_list,\n",
        "                  \"node_score_contributions\": np.copy(node_contributions),\n",
        "                  \"l2_before\": self.l2_before.copy(),\n",
        "                  \"l2_after\": self.l2_after.copy(),\n",
        "                  \"chosen_Gmat\": np.copy(self.chosen_Gmat),\n",
        "                  \"inter_layer\": self.inter_layer  # already holds weights/bias\n",
        "              }\n",
        "\n",
        "      # --- Restore best state ---\n",
        "      self.nested_reps = best_state[\"nested_reps\"]\n",
        "      self.capped_node_metrics = best_state[\"capped_node_metrics\"]\n",
        "      self.node_score_contributions = best_state[\"node_score_contributions\"]\n",
        "      self.l2_before = best_state[\"l2_before\"]\n",
        "      self.l2_after = best_state[\"l2_after\"]\n",
        "      self.chosen_Gmat = best_state[\"chosen_Gmat\"]\n",
        "      self.inter_layer = best_state[\"inter_layer\"]\n",
        "\n",
        "      print(f\"\\nBest Outer Score Achieved: {best_score:.4f}\")\n",
        "      return self.capped_node_metrics\n",
        "\n",
        "\n",
        "\n",
        "    # ---------- VISUALIZATIONS ----------\n",
        "    def plot_pointwise_minmax_elite(self, top_k=21):\n",
        "        plt.figure(figsize=(14,3))\n",
        "        for i in range(self.D_graph):\n",
        "            base = self.nested_reps[i]\n",
        "            reps = np.clip(base + np.random.normal(0,0.05,(top_k,len(base))),0,1)\n",
        "            y_min, y_max = reps.min(axis=0), reps.max(axis=0)\n",
        "            y_sel = base\n",
        "            y_true = self.synthetic_targets[i][self.best_dim_per_node[i]]\n",
        "            if len(y_true)<len(y_sel):\n",
        "                y_true = np.pad(y_true,(0,len(y_sel)-len(y_true)),\"constant\")\n",
        "            else:\n",
        "                y_true = y_true[:len(y_sel)]\n",
        "\n",
        "            plt.subplot(1,self.D_graph,i+1)\n",
        "            plt.fill_between(range(len(y_min)),y_min,y_max,color='skyblue',alpha=0.4,label='Elite Interval')\n",
        "            plt.plot(y_sel,'k-',lw=2,label='Estimated')\n",
        "            plt.plot(y_true,'r--',lw=2,label='True')\n",
        "            plt.ylim(0,1.05)\n",
        "            plt.title(f\"Node {i+1}\")\n",
        "            if i==0: plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def plot_nested_activations(self):\n",
        "        plt.figure(figsize=(12,3))\n",
        "        for i,rep in enumerate(self.nested_reps):\n",
        "            plt.subplot(1,self.D_graph,i+1)\n",
        "            plt.bar(range(len(rep)),rep,color=plt.cm.plasma(rep))\n",
        "            plt.ylim(0,1)\n",
        "            plt.title(f\"Node {i+1}\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def plot_outer_fuzzy_graph(self):\n",
        "        G = nx.DiGraph()\n",
        "        for i in range(self.D_graph): G.add_node(i)\n",
        "        for i in range(self.D_graph):\n",
        "            for j in range(self.D_graph):\n",
        "                if i!=j and abs(self.chosen_Gmat[i,j])>0.02:\n",
        "                    G.add_edge(i,j,weight=self.chosen_Gmat[i,j])\n",
        "        node_sizes = [self.best_dim_per_node[i]*200 for i in range(self.D_graph)]\n",
        "        edge_colors = ['green' if d['weight']>0 else 'red' for _,_,d in G.edges(data=True)]\n",
        "        edge_widths = [abs(d['weight'])*3 for _,_,d in G.edges(data=True)]\n",
        "        pos = nx.circular_layout(G)\n",
        "        plt.figure(figsize=(6,6))\n",
        "        nx.draw(G,pos,node_size=node_sizes,node_color='skyblue',\n",
        "                edge_color=edge_colors,width=edge_widths,arrows=True,with_labels=True)\n",
        "        plt.title(\"Outer Fuzzy Multiplex Graph\")\n",
        "        plt.show()\n",
        "# ---------------- INTERACTIONS INSPECTOR ----------------\n",
        "\n",
        "    def print_interactions(self, return_tensor=True, verbose=True):\n",
        "            D_graph = self.D_graph\n",
        "            inter_dim = self.inter_layer.inter_dim\n",
        "            inter_tensor = np.zeros((D_graph, D_graph, inter_dim))\n",
        "\n",
        "            acts = self.inter_layer.build_activations(self.chosen_Gmat, self.nested_reps)\n",
        "            if not acts:\n",
        "                if verbose:\n",
        "                    print(\"No active edges above threshold.\")\n",
        "                return inter_tensor if return_tensor else None\n",
        "\n",
        "            for (i, j), vec in acts.items():\n",
        "                inter_tensor[i, j, :] = vec\n",
        "                if verbose:\n",
        "                    act_str = \", \".join([f\"{v:.3f}\" for v in vec])\n",
        "                    print(f\"Node {i} -> Node {j}: [{act_str}]\")\n",
        "            return inter_tensor if return_tensor else None\n",
        "\n",
        "        # Move these outside of print_interactions (class-level)\n",
        "    def print_l2_summary(self):\n",
        "            print(\"\\nL2 Distances to Target per Node:\")\n",
        "            for idx, (before, after) in enumerate(zip(self.l2_before, self.l2_after)):\n",
        "                print(f\"Node {idx}: Before={before:.4f}, After={after:.4f}\")\n",
        "\n",
        "    def compute_fuzzy_metric_tensor(self, normalize=True, verbose=False):\n",
        "            metrics_keys = ['wait', 'throughput', 'util', 'patience']\n",
        "            D = self.D_graph\n",
        "            num_metrics = len(metrics_keys)\n",
        "            tensor = np.zeros((D, D, num_metrics))\n",
        "\n",
        "            metrics_evaluator = MetricsEvaluator(DATA_MATRIX, self.D_graph)\n",
        "\n",
        "            node_metrics = []\n",
        "            for i, rep in enumerate(self.nested_reps):\n",
        "                metrics = metrics_evaluator.compute_node_metrics(i, y=rep)\n",
        "                node_metrics.append(np.array([metrics[k] for k in metrics_keys]))\n",
        "            node_metrics = np.array(node_metrics)  # (D, num_metrics)\n",
        "\n",
        "            for i in range(D):\n",
        "                for j in range(D):\n",
        "                    if i == j:\n",
        "                        tensor[i, j, :] = node_metrics[j]\n",
        "                    else:\n",
        "                        weight = np.clip(abs(self.chosen_Gmat[i, j]), 0, 1)\n",
        "                        tensor[i, j, :] = weight * node_metrics[j]\n",
        "\n",
        "            if normalize:\n",
        "                tensor = (tensor - tensor.min()) / (tensor.max() - tensor.min() + 1e-12)\n",
        "\n",
        "            if verbose:\n",
        "                print(\"Fuzzy Metric Tensor shape:\", tensor.shape)\n",
        "\n",
        "            return tensor\n",
        "\n",
        "    def compute_fuzzy_metric_tensor(self, normalize=True, verbose=False):\n",
        "            \"\"\"\n",
        "            Computes a Fuzzy Metric Tensor (D_graph x D_graph x num_metrics)\n",
        "            using current nested reps and node metrics.\n",
        "            Each slice [i,j,:] represents metrics of node j (optionally weighted by Gmat[i,j])\n",
        "            \"\"\"\n",
        "            metrics_keys = ['wait', 'throughput', 'util', 'patience']\n",
        "            D = self.D_graph\n",
        "            num_metrics = len(metrics_keys)\n",
        "            tensor = np.zeros((D, D, num_metrics))\n",
        "\n",
        "            metrics_evaluator = MetricsEvaluator(DATA_MATRIX,self.D_graph)\n",
        "\n",
        "            node_metrics = []\n",
        "            for i, rep in enumerate(self.nested_reps):\n",
        "                metrics = metrics_evaluator.compute_node_metrics(i, y=rep)\n",
        "                node_metrics.append(np.array([metrics[k] for k in metrics_keys]))\n",
        "            node_metrics = np.array(node_metrics)  # (D, num_metrics)\n",
        "\n",
        "            for i in range(D):\n",
        "                for j in range(D):\n",
        "                    if i==j:\n",
        "                        tensor[i,j,:] = node_metrics[j]\n",
        "                    else:\n",
        "                        weight = np.clip(abs(self.chosen_Gmat[i,j]), 0, 1)\n",
        "                        tensor[i,j,:] = weight * node_metrics[j]\n",
        "\n",
        "            if normalize:\n",
        "                tensor = (tensor - tensor.min()) / (tensor.max() - tensor.min() + 1e-12)\n",
        "\n",
        "            if verbose:\n",
        "                print(\"Fuzzy Metric Tensor shape:\", tensor.shape)\n",
        "\n",
        "            return tensor\n",
        "    def plot_fuzzy_metric_tensor_heatmaps(self, fuzzy_tensor=None, metrics_keys=['wait','throughput','util','patience']):\n",
        "        \"\"\"\n",
        "        Plot a heatmap panel for each metric in the FMT.\n",
        "        Rows: source node i\n",
        "        Columns: target node j\n",
        "        \"\"\"\n",
        "        if fuzzy_tensor is None:\n",
        "            fuzzy_tensor = self.compute_fuzzy_metric_tensor(normalize=True)\n",
        "\n",
        "        D = self.D_graph\n",
        "        num_metrics = len(metrics_keys)\n",
        "\n",
        "        fig, axes = plt.subplots(1, num_metrics, figsize=(4*num_metrics,4))\n",
        "        if num_metrics == 1: axes = [axes]\n",
        "\n",
        "        for k, key in enumerate(metrics_keys):\n",
        "            data = fuzzy_tensor[:,:,k]\n",
        "            im = axes[k].imshow(data, cmap='viridis', vmin=0, vmax=1)\n",
        "            for i in range(D):\n",
        "                for j in range(D):\n",
        "                    axes[k].text(j,i,f\"{data[i,j]:.2f}\",ha='center',va='center',color='white',fontsize=9)\n",
        "            axes[k].set_xticks(range(D))\n",
        "            axes[k].set_yticks(range(D))\n",
        "            axes[k].set_xticklabels([f'Node {j}' for j in range(D)])\n",
        "            axes[k].set_yticklabels([f'Node {i}' for i in range(D)])\n",
        "            axes[k].set_title(f'FMT - {key}')\n",
        "\n",
        "        fig.colorbar(im, ax=axes, orientation='vertical', fraction=0.025, pad=0.04, label='Normalized Metric Value')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def compute_fmt_with_elite_bounds(self, top_k=21):\n",
        "        \"\"\"\n",
        "        Computes FMT bounds using pointwise min/max across elite solutions.\n",
        "        Returns tensor shape (D,D,num_metrics,2) [lower, upper].\n",
        "        \"\"\"\n",
        "        metrics_keys = ['wait','throughput','util','patience']\n",
        "        D = self.D_graph\n",
        "        num_metrics = len(metrics_keys)\n",
        "        tensor_bounds = np.zeros((D,D,num_metrics,2))\n",
        "\n",
        "        metrics_evaluator = MetricsEvaluator(DATA_MATRIX, self.D_graph)\n",
        "\n",
        "        for i in range(D):\n",
        "            # Generate top_k perturbations around current nested_rep (like in plot_pointwise_minmax_elite)\n",
        "            base = self.nested_reps[i]\n",
        "            reps = np.clip(base + np.random.normal(0,0.05,(top_k,len(base))),0,1)\n",
        "\n",
        "            # Compute node metrics for each perturbed solution\n",
        "            metrics_matrix = np.zeros((top_k, num_metrics))\n",
        "            for idx, rep in enumerate(reps):\n",
        "                m = metrics_evaluator.compute_node_metrics(i, y=rep)\n",
        "                metrics_matrix[idx,:] = [m[k] for k in metrics_keys]\n",
        "\n",
        "            # Compute pointwise min/max across elite solutions\n",
        "            lower_i = metrics_matrix.min(axis=0)\n",
        "            upper_i = metrics_matrix.max(axis=0)\n",
        "\n",
        "            # Fill bounds tensor for all source nodes (i->j)\n",
        "            for j in range(D):\n",
        "                tensor_bounds[i,j,:,0] = lower_i\n",
        "                tensor_bounds[i,j,:,1] = upper_i\n",
        "\n",
        "        return tensor_bounds\n",
        "\n",
        "\n",
        "    def plot_fmt_with_bounds(self, fmt_tensor_bounds):\n",
        "        D = self.D_graph\n",
        "        metrics_keys = ['wait','throughput','util','patience']\n",
        "        num_metrics = len(metrics_keys)\n",
        "\n",
        "        fig, axes = plt.subplots(1, num_metrics, figsize=(4*num_metrics,4))\n",
        "        if num_metrics == 1: axes = [axes]\n",
        "\n",
        "        for k, key in enumerate(metrics_keys):\n",
        "            lower = fmt_tensor_bounds[:,:,k,0]\n",
        "            upper = fmt_tensor_bounds[:,:,k,1]\n",
        "            mean_vals = (lower+upper)/2\n",
        "            range_vals = upper-lower\n",
        "            max_range = range_vals.max() if range_vals.max()>0 else 1.0\n",
        "            alphas = 0.2 + 0.8 * range_vals/max_range\n",
        "\n",
        "            im = axes[k].imshow(mean_vals, cmap='viridis', vmin=0, vmax=mean_vals.max())\n",
        "            for i in range(D):\n",
        "                for j in range(D):\n",
        "                    alpha_val = np.clip(1-alphas[i,j],0,1)\n",
        "                    rect = plt.Rectangle((j-0.5,i-0.5),1,1,color='white',alpha=alpha_val)\n",
        "                    axes[k].add_patch(rect)\n",
        "                    axes[k].text(j,i,f\"{lower[i,j]:.1f}\\n{upper[i,j]:.1f}\",ha='center',va='center',fontsize=8)\n",
        "            axes[k].set_title(f'FMT Bounds - {key}')\n",
        "            axes[k].set_xticks(range(D))\n",
        "            axes[k].set_yticks(range(D))\n",
        "            axes[k].set_xticklabels([f'Node {j}' for j in range(D)])\n",
        "            axes[k].set_yticklabels([f'Node {i}' for i in range(D)])\n",
        "\n",
        "        fig.colorbar(im, ax=axes, orientation='vertical', fraction=0.025, pad=0.04, label='Mean Metric Value')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    def plot_node_score_contribution(self, metrics_keys=['wait','throughput','util','patience']):\n",
        "        \"\"\"\n",
        "        Plot per-node total score contribution in the SAME STYLE as the FMT plots:\n",
        "            - uses imshow\n",
        "            - one panel for: raw, FMT, and total stacked\n",
        "            - diagonal masked\n",
        "            - annotated cells\n",
        "            - node contribution highlighted like your FMT code\n",
        "        \"\"\"\n",
        "        # ---------------------------------------------------------------------\n",
        "        # 1. Collect node contributions from run_outer()\n",
        "        # ---------------------------------------------------------------------\n",
        "        _, _, node_contributions = self.run_outer()\n",
        "        node_contributions = np.array(node_contributions)\n",
        "        D = len(node_contributions)\n",
        "\n",
        "        # ---------------------------------------------------------------------\n",
        "        # 2. Recompute FMT influence (same style as your FMT plots)\n",
        "        # ---------------------------------------------------------------------\n",
        "        fuzzy_tensor = self.compute_fuzzy_metric_tensor(normalize=True)\n",
        "        total_tensor = fuzzy_tensor.sum(axis=2)           # sum over metrics\n",
        "        fmt_tensor = total_tensor.copy()\n",
        "        np.fill_diagonal(fmt_tensor, 0)                   # mask diagonal\n",
        "\n",
        "        fmt_per_node = fmt_tensor.sum(axis=1)             # row sum\n",
        "        raw_per_node = node_contributions - fmt_per_node  # everything else\n",
        "\n",
        "        # Construct matrices for plotting (D×D)\n",
        "        raw_matrix = np.zeros((D, D))\n",
        "        fmt_matrix = fmt_tensor\n",
        "        total_matrix = raw_matrix + fmt_matrix            # raw only on diagonal? no → distribute raw as row diag\n",
        "        np.fill_diagonal(raw_matrix, raw_per_node)\n",
        "        total_matrix = raw_matrix + fmt_matrix\n",
        "\n",
        "        # ---------------------------------------------------------------------\n",
        "        # 3. Plot - 3 subplots in SAME STYLE as FMT panels\n",
        "        # ---------------------------------------------------------------------\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "        matrices = [raw_matrix, fmt_matrix, total_matrix]\n",
        "        titles = [\"Raw Node Contribution\", \"FMT Interaction Contribution\", \"Total Contribution\"]\n",
        "\n",
        "        for ax, mat, title in zip(axes, matrices, titles):\n",
        "\n",
        "            im = ax.imshow(mat, cmap='viridis', vmin=np.min(mat), vmax=np.max(mat))\n",
        "\n",
        "            # annotate values\n",
        "            for i in range(D):\n",
        "                for j in range(D):\n",
        "                    val = mat[i, j]\n",
        "                    ax.text(j, i, f\"{val:.2f}\", ha='center',\n",
        "                            va='center', color='white', fontsize=8)\n",
        "\n",
        "            ax.set_title(title)\n",
        "            ax.set_xticks(range(D))\n",
        "            ax.set_xticklabels([f\"Node {i+1}\" for i in range(D)])\n",
        "            ax.set_yticks(range(D))\n",
        "            ax.set_yticklabels([f\"Node {i+1}\" for i in range(D)])\n",
        "\n",
        "        fig.colorbar(im, ax=axes, orientation='vertical', fraction=0.025, pad=0.04,\n",
        "                    label='Contribution Value')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# ---------------- USAGE ----------------\n",
        "if __name__ == \"__main__\":\n",
        "    optimizer = FMT(\n",
        "        candidate_dims, D_graph,\n",
        "        inner_archive_size, inner_offspring,\n",
        "        outer_archive_size, outer_offspring,\n",
        "        synthetic_targets,\n",
        "        inner_learning, gamma_interlayer=gamma_interlayer,\n",
        "        causal_flag=False\n",
        "    )\n",
        "    metrics_list = optimizer.run()\n",
        "    optimizer.plot_pointwise_minmax_elite()\n",
        "    optimizer.plot_nested_activations()\n",
        "    # Compute FMT with elite bounds\n",
        "    fmt_elite_bounds = optimizer.compute_fmt_with_elite_bounds(top_k=top_k)\n",
        "\n",
        "# Plot as heatmaps\n",
        "    optimizer.plot_fmt_with_bounds(fmt_elite_bounds)\n",
        "\n",
        "    # Compute fuzzy multiplex tensor\n",
        "    fmt_tensor = optimizer.compute_fuzzy_metric_tensor(normalize=True)\n",
        "    optimizer.plot_fuzzy_metric_tensor_heatmaps(fmt_tensor)\n",
        "\n",
        "    # Compute FMT with bounds (minimax elite intervals)\n",
        "    optimizer.plot_node_score_contribution()\n",
        "    optimizer.plot_outer_fuzzy_graph()\n",
        "  #  optimizer.print_interactions()\n",
        "    tensor = optimizer.print_interactions()\n",
        "\n",
        "    print(\"Tensor shape:\", tensor.shape,'\\n',tensor)\n",
        "    import networkx as nx\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    from mpl_toolkits.mplot3d import Axes3D\n",
        "D_graph = len(optimizer.nested_reps)\n",
        "tensor = optimizer.print_interactions(return_tensor=True, verbose=False)\n",
        "\n",
        "# ---------------- Outer nodes (hubs) ----------------\n",
        "G_outer = nx.DiGraph()\n",
        "for i in range(D_graph):\n",
        "    G_outer.add_node(i)\n",
        "for i in range(D_graph):\n",
        "    for j in range(D_graph):\n",
        "        if i != j and np.any(tensor[i,j,:] != 0):\n",
        "            # Shift to signed weights: 0.5 -> 0, <0.5 negative, >0.5 positive\n",
        "            mean_weight = 2 * (np.mean(tensor[i,j,:]) - 0.5)\n",
        "            G_outer.add_edge(i, j, weight=mean_weight)\n",
        "\n",
        "# Outer spring layout\n",
        "pos_outer_2d = nx.circular_layout(G_outer, scale=5)\n",
        "pos_outer = np.array([[x, y, 0] for x, y in pos_outer_2d.values()])\n",
        "\n",
        "fig = plt.figure(figsize=(10,8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Plot outer nodes\n",
        "for i in range(D_graph):\n",
        "    ax.scatter(*pos_outer[i], s=300, color='skyblue')\n",
        "    ax.text(*pos_outer[i], f'Node {i}', color='black')\n",
        "\n",
        "# Plot outer edges with positive/negative colors\n",
        "\n",
        "for i, j, data in G_outer.edges(data=True):\n",
        "    x_vals = [pos_outer[i,0], pos_outer[j,0]]\n",
        "    y_vals = [pos_outer[i,1], pos_outer[j,1]]\n",
        "    z_vals = [pos_outer[i,2], pos_outer[j,2]]\n",
        "\n",
        "    # Positive = bright green, Negative = bright red\n",
        "    color = 'green' if data['weight'] > 0 else 'red'\n",
        "    linewidth = 2 + 4*abs(data['weight'])  # scale width by magnitude\n",
        "    ax.plot(x_vals, y_vals, z_vals, color=color, linewidth=linewidth)\n",
        "# ---------------- Inner FCMs (small circular around hub) ----------------\n",
        "for i, rep in enumerate(optimizer.nested_reps):\n",
        "    dims = len(rep)\n",
        "    angle = np.linspace(0, 2*np.pi, dims, endpoint=False)\n",
        "    radius = 0.8  # small circle\n",
        "    xs = pos_outer[i,0] + radius * np.cos(angle)\n",
        "    ys = pos_outer[i,1] + radius * np.sin(angle)\n",
        "    zs = pos_outer[i,2] + rep  # activation as height\n",
        "\n",
        "    # Plot inner nodes\n",
        "    ax.scatter(xs, ys, zs, c=rep, cmap='plasma', s=50)\n",
        "\n",
        "    # Connect inner nodes in circle\n",
        "    for k in range(dims):\n",
        "        ax.plot([xs[k], xs[(k+1)%dims]], [ys[k], ys[(k+1)%dims]], [zs[k], zs[(k+1)%dims]], color='gray', alpha=0.5)\n",
        "\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Activation')\n",
        "ax.set_title('Outer Nodes with Inner FCMs (Signed correlations)')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WGiyqUiV2IBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dvkz5KJ72L8a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}