{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Replace Bellow cells 1 by 1 with exp"
      ],
      "metadata": {
        "id": "16DS9q3X--lL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from typing import Dict, List, Callable\n",
        "\n",
        "# Reset random seed for repeatability\n",
        "np.random.seed(2025)\n",
        "\n",
        "# ======================================================\n",
        "# 1. CORE REGISTRIES (The 4-Node Service Journey)\n",
        "# ======================================================\n",
        "# The farmer's journey is mapped as a service blueprint with 4 distinct stages.\n",
        "TOUCHPOINTS = [\n",
        "    \"Resource_Acquisition\",  # Buying stocks, finding land\n",
        "    \"Cultivation_Process\",   # Crops, supplements, land usage\n",
        "    \"Harvest_Operations\",    # Yield processing, spoilage management\n",
        "    \"Market_Distribution\"    # Getting a seller, logistics\n",
        "]\n",
        "\n",
        "METRIC_KEYS = [\n",
        "    \"Resource_Efficiency\",   # Metric for Acquisition (Cost/Quality balance)\n",
        "    \"Growth_Rate\",           # Metric for Cultivation (Speed/Health)\n",
        "    \"Yield_Volume\",          # Metric for Harvest (Tonnage/Spoilage)\n",
        "    \"Market_Penetration\"     # Metric for Distribution (Price/Speed)\n",
        "]\n",
        "# Features representing the Service Design inputs\n",
        "FEATURE_KEYS = [\n",
        "    'land_arability_index', 'seed_stock_cost',     # Acquisition\n",
        "    'soil_nitrogen_level', 'supplement_ph_bal',    # Cultivation\n",
        "    'crop_yield_tonnage', 'spoilage_rate',         # Harvest\n",
        "    'market_spot_price', 'logistics_overhead'      # Distribution\n",
        "]\n",
        "\n",
        "# ======================================================\n",
        "# 2. MAPPING & TARGET MATRICES\n",
        "# ======================================================\n",
        "# Mapping specific features to the service stages\n",
        "metric_feature_map = {\n",
        "    \"Resource_Acquisition\": ['land_arability_index', 'seed_stock_cost'],\n",
        "    \"Cultivation_Process\":  ['soil_nitrogen_level', 'supplement_ph_bal'],\n",
        "    \"Harvest_Operations\":   ['crop_yield_tonnage', 'spoilage_rate'],\n",
        "    \"Market_Distribution\":  ['market_spot_price', 'logistics_overhead']\n",
        "}\n",
        "\n",
        "# The Target Influence Matrix (4x4)\n",
        "# Defines which stage targets affect others (1 = influence, 0 = independent)\n",
        "METRIC_TARGET = np.array([\n",
        "    [1, 1, 0, 1],  # Acquisition impacts Cultivation & Distribution (capital)\n",
        "    [0, 1, 1, 0],  # Cultivation impacts Harvest directly\n",
        "    [0, 0, 1, 1],  # Harvest impacts Distribution\n",
        "    [0, 0, 0, 1]   # Distribution is the final sink\n",
        "])\n",
        "\n",
        "# Service Transformation Formulas (Non-linear service transfer functions)\n",
        "METRIC_FORMULAS = [\n",
        "    lambda x: np.log(x + 10),      # Diminishing returns on Land\n",
        "    lambda x: np.tanh(x) * 3.5,    # Saturation point for Supplements\n",
        "    lambda x: np.exp(x * 0.1),     # Exponential growth potential in Harvest\n",
        "    lambda x: 1.0 / (x + 0.5)      # Inverse elasticity in Distribution\n",
        "]\n",
        "\n",
        "# Design Structure Matrix (DSM) - 4x4\n",
        "# Represents the physical flow of goods/service\n",
        "DSM = np.array([\n",
        "    [0, 1, 0, 0],  # Acquisition -> Cultivation\n",
        "    [0, 0, 1, 0],  # Cultivation -> Harvest\n",
        "    [0, 0.4, 0, 1],# Feedback loop from Harvest to Cultivation (seeds), flow to Market\n",
        "    [0, 0, 0, 0]   # Market\n",
        "])\n",
        "\n",
        "# Node Dimensionality (The \"Hubs\")\n",
        "# Acquisition/Market are lower dim (4), Cultivation/Harvest are high complexity hubs (16)\n",
        "candidate_dims = [[4], [16], [16], [4]]\n",
        "D_graph = len(candidate_dims) # 4 Nodes\n",
        "\n",
        "# ======================================================\n",
        "# 3. GENERATOR ENGINE (Agricultural Physics)\n",
        "# ======================================================\n",
        "GENERATOR_MAP = {\n",
        "    \"Resource_Acquisition\": lambda: {\n",
        "        \"land_arability_index\": np.random.beta(7, 2) * 100, # 0-100 score\n",
        "        \"seed_stock_cost\": np.random.normal(5000, 1200)     # Currency\n",
        "    },\n",
        "    \"Cultivation_Process\": lambda: {\n",
        "        \"soil_nitrogen_level\": np.random.uniform(20, 80),   # ppm\n",
        "        \"supplement_ph_bal\": np.random.normal(6.5, 0.4)     # pH level\n",
        "    },\n",
        "    \"Harvest_Operations\": lambda: {\n",
        "        \"crop_yield_tonnage\": np.random.gamma(5, 2.0),      # Tons\n",
        "        \"spoilage_rate\": np.random.exponential(0.05)        # % lost\n",
        "    },\n",
        "    \"Market_Distribution\": lambda: {\n",
        "        \"market_spot_price\": np.random.uniform(1.2, 3.5),   # Price per unit\n",
        "        \"logistics_overhead\": np.random.poisson(150)        # Cost index\n",
        "    }\n",
        "}\n",
        "\n",
        "# ======================================================\n",
        "# 4. DATA PROCESSING & ASSEMBLY\n",
        "# ======================================================\n",
        "num_samples = 150 # Increased sample size for better training\n",
        "feature_list = []\n",
        "\n",
        "for tp in TOUCHPOINTS:\n",
        "    samples = [GENERATOR_MAP[tp]() for _ in range(num_samples)]\n",
        "    df = pd.DataFrame(samples)\n",
        "    feature_list.append(df)\n",
        "\n",
        "DATA_MATRIX_RAW = pd.concat(feature_list, axis=1).to_numpy()\n",
        "\n",
        "# Normalization (Min-Max scaling for optimization stability)\n",
        "DATA_MATRIX = (DATA_MATRIX_RAW - DATA_MATRIX_RAW.min(axis=0)) / (np.ptp(DATA_MATRIX_RAW, axis=0) + 1e-8)\n",
        "# Dependency Graph Initialization\n",
        "G = nx.DiGraph(METRIC_TARGET)\n",
        "mapping = {i: name for i, name in enumerate(TOUCHPOINTS)}\n",
        "nx.relabel_nodes(G, mapping, copy=False)\n",
        "\n",
        "print(f\"Service Matrix Initialized: {DATA_MATRIX.shape}\")\n",
        "print(f\"Service Nodes: {list(G.nodes)}\")\n",
        "\n",
        "# ======================================================\n",
        "# 5. HIGH-FIDELITY SYNTHETIC TARGET GENERATOR\n",
        "# ======================================================\n",
        "def generate_targets(DATA_MATRIX, candidate_dims):\n",
        "    dims_flat = [i[0] for i in candidate_dims]\n",
        "    num_nodes = len(dims_flat)\n",
        "    targets = []\n",
        "    current_col = 0\n",
        "\n",
        "    # Total features available\n",
        "    total_feats = DATA_MATRIX.shape[1]\n",
        "\n",
        "    for node_idx in range(num_nodes):\n",
        "        dim = dims_flat[node_idx]\n",
        "\n",
        "        # Row selection: Modular index to ensure we use all data\n",
        "        row_idx = node_idx % DATA_MATRIX.shape[0]\n",
        "        row = DATA_MATRIX[row_idx]\n",
        "\n",
        "        # Slicing the contiguous block for target signature\n",
        "        # We wrap around columns if the dimension requirement exceeds remaining features\n",
        "        start = current_col % total_feats\n",
        "        end = start + dim\n",
        "\n",
        "        if end > total_feats:\n",
        "            # Wrap around logic\n",
        "            part1 = row[start:]\n",
        "            part2 = row[:end - total_feats]\n",
        "            block = np.concatenate([part1, part2])\n",
        "        else:\n",
        "            block = row[start:end]\n",
        "\n",
        "        # HUB LOGIC: If still smaller (e.g., dim=16 but features=8), pad reflectively\n",
        "        if len(block) < dim:\n",
        "            pad_needed = dim - len(block)\n",
        "            block = np.pad(block, (0, pad_needed), mode='reflect')\n",
        "        elif len(block) > dim:\n",
        "            block = block[:dim]\n",
        "\n",
        "        targets.append({\n",
        "            'node_id': node_idx,\n",
        "            'touchpoint': TOUCHPOINTS[node_idx],\n",
        "            'dim_required': dim,\n",
        "            'target': np.round(block, 4)\n",
        "        })\n",
        "\n",
        "        # Shift column pointer for next node to create variance\n",
        "        current_col = (current_col + 2) % total_feats\n",
        "\n",
        "    return targets\n",
        "\n",
        "# Execute generation\n",
        "synthetic_targets = generate_targets(DATA_MATRIX, candidate_dims)\n",
        "\n",
        "# ======================================================\n",
        "# 6. DIAGNOSTIC REPORT\n",
        "# ======================================================\n",
        "print(f\"\\n--- Farmer Cultivation Service Optimization (4-Node) ---\")\n",
        "print(f\"Total Service Features Analyzed: {DATA_MATRIX.shape[1]}\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"{'ID':<3} | {'Service Touchpoint':<25} | {'Type':<12} | {'Target Signature (Head)'}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for t in synthetic_targets:\n",
        "    # Identify \"Hubs\" (Complex nodes) vs \"Spokes\" (Simple nodes)\n",
        "    node_type = \"[COMPLEX]\" if t['dim_required'] >= 16 else \"[SIMPLE]\"\n",
        "\n",
        "    # Format the signature for display\n",
        "    sig_preview = str(t['target'][:4]).replace('\\n', '') + \"...\"\n",
        "\n",
        "    print(f\"{t['node_id']:02d}  | {t['touchpoint']:<25} | {node_type:<12} | {sig_preview}\")\n",
        "\n",
        "print(\"-\" * 80)"
      ],
      "metadata": {
        "id": "TyiLuN-7VTz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ======================================================\n",
        "# 6. OPTIMIZER (Branch & Bound)\n",
        "# ======================================================\n",
        "class BranchBoundOptimizer:\n",
        "    def __init__(self, tol=1e-3, max_depth=20, minimize=True, value_range=(0.0, 10.0)):\n",
        "        self.tol = tol\n",
        "        self.max_depth = max_depth\n",
        "        self.minimize = minimize\n",
        "        self.value_range = value_range\n",
        "\n",
        "    def optimize(self, features, y=None, metric_mask=None):\n",
        "        y = np.zeros(3) if y is None else np.array(y[:3])\n",
        "        base = np.mean(list(features.values())) + np.mean(y)\n",
        "\n",
        "        a0, b0 = self.value_range\n",
        "        a0 += base\n",
        "        b0 += base\n",
        "\n",
        "        work = [(a0, b0, 0)]\n",
        "        best_x = None\n",
        "        best_score = np.inf if self.minimize else -np.inf\n",
        "\n",
        "        def better(s1, s2):\n",
        "            return s1 < s2 if self.minimize else s1 > s2\n",
        "\n",
        "        while work:\n",
        "            a, b, depth = work.pop()\n",
        "            mid = 0.5 * (a + b)\n",
        "\n",
        "            # Apply Formula if mask is active\n",
        "            mv = [f(mid) if m else 0.0 for f, m in zip(METRIC_FORMULAS, metric_mask)]\n",
        "            score = sum(mv)\n",
        "\n",
        "            if best_x is None or better(score, best_score):\n",
        "                best_x = mid\n",
        "                best_score = score\n",
        "\n",
        "            if depth >= self.max_depth or (b - a) < self.tol:\n",
        "                continue\n",
        "\n",
        "            work.append((a, mid, depth + 1))\n",
        "            work.append((mid, b, depth + 1))\n",
        "\n",
        "        return best_x\n",
        "\n",
        "# ======================================================\n",
        "# 8. EVALUATOR\n",
        "# ======================================================\n",
        "class MetricsEvaluator:\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_matrix,\n",
        "        metric_formulas=METRIC_FORMULAS,\n",
        "        metric_feature_map=metric_feature_map,\n",
        "        feature_keys=FEATURE_KEYS,\n",
        "        feature_target=None,\n",
        "        metric_target=None,\n",
        "        tol=1e-3,\n",
        "        max_depth=20,\n",
        "        minimize=False,\n",
        "        value_range=(0.0, 5.0)\n",
        "    ):\n",
        "        self.data_matrix = data_matrix\n",
        "        self.metric_formulas = metric_formulas\n",
        "        self.metric_feature_map = metric_feature_map\n",
        "        self.feature_keys = feature_keys\n",
        "\n",
        "        # Defaults\n",
        "        self.feature_target = feature_target or [[1]*len(feature_keys) for _ in range(data_matrix.shape[0])]\n",
        "        self.metric_target = metric_target or METRIC_TARGET\n",
        "        self.num_nodes = data_matrix.shape[0]\n",
        "\n",
        "        self.optimizer = BranchBoundOptimizer(\n",
        "            tol=tol,\n",
        "            max_depth=max_depth,\n",
        "            minimize=minimize,\n",
        "            value_range=value_range\n",
        "        )\n",
        "\n",
        "    def extract_features(self, node_idx):\n",
        "        if node_idx >= len(self.data_matrix): return {}\n",
        "        row = self.data_matrix[node_idx]\n",
        "        mask = self.feature_target[node_idx]\n",
        "        features = {k: v for k, v, m in zip(self.feature_keys, row, mask) if m}\n",
        "        return features\n",
        "\n",
        "    def compute_node_metrics(self, node_idx, y=None):\n",
        "        features = self.extract_features(node_idx)\n",
        "        metric_mask = self.metric_target[node_idx] if node_idx < len(self.metric_target) else [0]*len(METRIC_KEYS)\n",
        "        metric_values = {}\n",
        "\n",
        "        for key, formula, mask in zip(METRIC_KEYS, self.metric_formulas, metric_mask):\n",
        "            if mask and key in self.metric_feature_map:\n",
        "                relevant_keys = self.metric_feature_map[key]\n",
        "                relevant_features = [features[f] for f in relevant_keys if f in features]\n",
        "                x = np.mean(relevant_features) if relevant_features else 0.0\n",
        "\n",
        "                # Optimize\n",
        "                opt_value = self.optimizer.optimize(features={key: x}, y=y, metric_mask=[1])\n",
        "                metric_values[key] = formula(opt_value)\n",
        "            else:\n",
        "                metric_values[key] = 0.0\n",
        "\n",
        "        metric_values['score'] = sum(metric_values.values())\n",
        "        return metric_values\n",
        "# ======================================================\n",
        "# 8. EXECUTION\n",
        "# ======================================================\n",
        "\n",
        "# Generate Mock Data (15 Nodes x 25 Features)\n",
        "# ======================================================\n",
        "# 9. DIMENSIONALITY CONFIGURATION (15 Nodes)\n",
        "# ======================================================\n",
        "\n",
        "# Dimensions assigned based on node complexity and coupling depth.\n",
        "# \"Hub\" nodes and Financial nodes get higher dimensions.\n"
      ],
      "metadata": {
        "id": "Q0fGU91EVabm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rCIcB9UMVjhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Replace above cells with Expert knowledge__\n",
        "# Service Design Generator Simulation"
      ],
      "metadata": {
        "id": "G5m6U-HipINy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "# Target Mask (which metrics apply to which node)\n",
        "\n",
        "def top_k_masked_probs(weights, k):\n",
        "    \"\"\"\n",
        "    Keep only top-k weights, zero out the rest, renormalize.\n",
        "    \"\"\"\n",
        "    if k >= len(weights):\n",
        "        return weights / (weights.sum() + 1e-12)\n",
        "\n",
        "    idx = np.argpartition(weights, -k)[-k:]\n",
        "    mask = np.zeros_like(weights)\n",
        "    mask[idx] = weights[idx]\n",
        "\n",
        "    s = mask.sum()\n",
        "    if s > 0:\n",
        "        mask /= s\n",
        "    return mask\n",
        "def get_all_paths(G, C, node_types, start=0, end=None):\n",
        "    \"\"\"\n",
        "    Return all simple paths that respect coupling constraints.\n",
        "    \"\"\"\n",
        "    if end is None:\n",
        "        end = G.shape[0] - 1\n",
        "\n",
        "    D = G.shape[0]\n",
        "    paths = []\n",
        "\n",
        "    def coupling_ok(i, j):\n",
        "        label = C[i, j]\n",
        "\n",
        "        # Default direct coupling\n",
        "        if \"->\" in label and \"->C\" not in label:\n",
        "            src, dst = label.split(\"->\")\n",
        "            return src == node_types[i] and dst == node_types[j]\n",
        "\n",
        "        # Escalation case (B->B->C)\n",
        "        if label == \"B->B->C\":\n",
        "            return node_types[i] == \"B\" and node_types[j] == \"B\"\n",
        "\n",
        "        return False\n",
        "\n",
        "    def dfs(node, path, visited):\n",
        "        if node == end:\n",
        "            paths.append(path.copy())\n",
        "            return\n",
        "\n",
        "        for nxt in range(D):\n",
        "            if G[node, nxt] > 0 and nxt not in visited:\n",
        "                if not coupling_ok(node, nxt):\n",
        "                    continue\n",
        "\n",
        "                visited.add(nxt)\n",
        "                dfs(nxt, path + [nxt], visited)\n",
        "                visited.remove(nxt)\n",
        "\n",
        "    dfs(start, [start], {start})\n",
        "    return paths\n",
        "\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# EXTERNAL DEPENDENCIES & CONFIGURATION\n",
        "# =============================================================================\n",
        "# These variables are referenced in the original code but not defined.\n",
        "# Assumed to be present in the execution environment.\n",
        "# -----------------------------------------------------------------------------\n",
        "# D_graph = ...\n",
        "# DATA_MATRIX = ...\n",
        "# METRIC_KEYS = [...]\n",
        "# METRIC_TARGET = [...]\n",
        "# METRIC_FORMULAS = [...]\n",
        "# METRIC_INVERSES = {...}\n",
        "# synthetic_targets = ...\n",
        "# MetricsEvaluator = ... (Class)\n",
        "# -----------------------------------------------------------------------------\n",
        "# =============================================================================\n",
        "# NEW: DETERMINISTIC DFS & PATH EVALUATOR\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "def evaluate_fixed_paths(paths, node_metrics, beta=0.3):\n",
        "    \"\"\"\n",
        "    Replaces collect_and_select_best_walks.\n",
        "    Scores the specific paths found by DFS.\n",
        "    \"\"\"\n",
        "    walks = []\n",
        "    best = None\n",
        "\n",
        "    for path in paths:\n",
        "        cost = 0.0\n",
        "        quality = 0.0\n",
        "\n",
        "        # Calculate Path Metrics\n",
        "        for node_idx in path:\n",
        "            m = node_metrics[node_idx]\n",
        "            cost += m.get(\"cost\", 0.0)\n",
        "            quality += m.get(\"quality\", 0.0)\n",
        "\n",
        "        score = quality - 1.0 * cost # using default lambda_cost=1.0\n",
        "\n",
        "        # DFS paths are unique, so count is always 1\n",
        "        count = 1\n",
        "        adjusted_score = score / (1 + beta * count)\n",
        "\n",
        "        record = {\n",
        "            \"path\": path,\n",
        "            \"score\": score,\n",
        "            \"adjusted_score\": adjusted_score,\n",
        "            \"cost\": cost,\n",
        "            \"quality\": quality,\n",
        "            \"count\": count\n",
        "        }\n",
        "        walks.append(record)\n",
        "\n",
        "        if best is None or adjusted_score > best[\"adjusted_score\"]:\n",
        "            best = record\n",
        "\n",
        "    return walks, best\n",
        "\n",
        "def dsm_walk(G, node_metrics, start=0, max_steps=50, lambda_cost=1.0):\n",
        "    D = G.shape[0]\n",
        "    target_node = D - 1  # Define Nlast\n",
        "\n",
        "    # Force start at 0 if not provided\n",
        "    current = start\n",
        "    path = [current]\n",
        "    visited = {current}\n",
        "\n",
        "    cost = 0.0\n",
        "    quality = 0.0\n",
        "\n",
        "    # Metrics for the start node\n",
        "    m_start = node_metrics[current]\n",
        "    cost += m_start.get(\"cost\", 0.0)\n",
        "    quality += m_start.get(\"quality\", 0.0)\n",
        "\n",
        "    reached_target = False\n",
        "\n",
        "    for _ in range(max_steps):\n",
        "        # If we reached the last node, stop successfully\n",
        "        if current == target_node:\n",
        "            reached_target = True\n",
        "            break\n",
        "\n",
        "        weights = np.zeros(D)\n",
        "\n",
        "        for j in range(D):\n",
        "            # Allow visiting the target even if visited (though unlikely to revisit in DAG)\n",
        "            # But generally prevent cycles\n",
        "            if j == 0:\n",
        "                continue\n",
        "            if j in visited:\n",
        "                continue\n",
        "\n",
        "            q = node_metrics[j].get(\"quality\", 0.0)\n",
        "            c = node_metrics[j].get(\"cost\", 0.0)\n",
        "\n",
        "            # Standard probability weight\n",
        "            weights[j] = G[current, j] * (q / (1 + c))\n",
        "\n",
        "        if weights.sum() == 0:\n",
        "            break\n",
        "\n",
        "        weights /= weights.sum()\n",
        "        nxt = np.random.choice(D, p=weights)\n",
        "\n",
        "        m = node_metrics[nxt]\n",
        "        cost += m.get(\"cost\", 0.0)\n",
        "        quality += m.get(\"quality\", 0.0)\n",
        "\n",
        "        path.append(nxt)\n",
        "        visited.add(nxt)\n",
        "        current = nxt\n",
        "\n",
        "    # Recalculate score based on success\n",
        "    score = quality - lambda_cost * cost\n",
        "\n",
        "    # Heavy penalty if the walk did not reach Nlast\n",
        "    if not reached_target:\n",
        "        score = -1e9\n",
        "\n",
        "    return path, score, cost, quality\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "def collect_and_select_best_walks(G, node_metrics, n_walks=300, beta=0.3):\n",
        "    walks = []\n",
        "    best = None\n",
        "    path_counts = defaultdict(int)\n",
        "    D = G.shape[0]\n",
        "\n",
        "    for _ in range(n_walks):\n",
        "        # Force start at 0\n",
        "        start = 0\n",
        "        path, score, cost, quality = dsm_walk(\n",
        "            G, node_metrics, start\n",
        "        )\n",
        "\n",
        "        # Only record if it successfully reached the last node\n",
        "        # (Score is -1e9 if it failed, per updated dsm_walk)\n",
        "        if path[-1] != (D - 1):\n",
        "            continue\n",
        "\n",
        "        key = tuple(path)\n",
        "        path_counts[key] += 1\n",
        "\n",
        "        # novelty penalty\n",
        "        adjusted_score = score / (1 + beta * path_counts[key])\n",
        "\n",
        "        record = {\n",
        "            \"path\": path,\n",
        "            \"score\": score,\n",
        "            \"adjusted_score\": adjusted_score,\n",
        "            \"cost\": cost,\n",
        "            \"quality\": quality,\n",
        "            \"count\": path_counts[key]\n",
        "        }\n",
        "        walks.append(record)\n",
        "\n",
        "        if best is None or adjusted_score > best[\"adjusted_score\"]:\n",
        "            best = record\n",
        "\n",
        "    return walks, best\n",
        "\n",
        "\n",
        "# Configuration\n",
        "#candidate_dims = [[2], [2], [2], [2], [2], [2], [2], [2], [1]]\n",
        "outer_generations = 1\n",
        "inner_learning = 0.1\n",
        "gamma_interlayer = 1\n",
        "top_k = 21\n",
        "\n",
        "# Initialize random state\n",
        "np.random.seed()\n",
        "seed = None  # Placeholder as per original logic\n",
        "\n",
        "# Placeholder for Data Matrix generation (from original snippet)\n",
        "# new_DATA_MATRIX = np.random.rand(D_graph, DATA_MATRIX.shape[1])\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# HELPER CLASSES\n",
        "# =============================================================================\n",
        "class DSM_Tracker:\n",
        "    \"\"\"\n",
        "    Tracks a DSM (Design Structure Matrix) layer and its residual.\n",
        "    \"\"\"\n",
        "    def __init__(self, multiplex_layer):\n",
        "        self.layer = multiplex_layer\n",
        "        self.primary_dsm = None\n",
        "        self.residual_dsm = None\n",
        "        self.update_dsms()\n",
        "\n",
        "    def update_dsms(self):\n",
        "        if self.primary_dsm is None:\n",
        "            # First time: store current DSM as reference\n",
        "            self.primary_dsm = self.layer.chosen_Gmat.copy()\n",
        "            self.residual_dsm = np.zeros_like(self.primary_dsm)\n",
        "        else:\n",
        "            # Update residual: current DSM minus primary\n",
        "            current = self.layer.chosen_Gmat\n",
        "            self.residual_dsm = current - self.primary_dsm\n",
        "\n",
        "    def get_matrices(self):\n",
        "        return self.primary_dsm, self.residual_dsm\n",
        "\n",
        "    def print_matrices(self):\n",
        "        print(\"\\n--- Primary DSM ---\")\n",
        "        print(self.primary_dsm)\n",
        "        print(\"\\n--- Residual DSM ---\")\n",
        "        print(self.residual_dsm)\n",
        "\n",
        "\n",
        "class DSM_Layer_Decomposer:\n",
        "    \"\"\"\n",
        "    Manages the additive decomposition of DSM layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, baseline_matrix, mode='additive'):\n",
        "        self.baseline_matrix = baseline_matrix.copy()\n",
        "        self.current_total = baseline_matrix.copy()\n",
        "        self.mode = mode\n",
        "        self.layers = []\n",
        "        self.residuals = []\n",
        "\n",
        "    def add_snapshot(self, new_total_matrix):\n",
        "        \"\"\"\n",
        "        Calculates the DELTA (change) between the new state and the previous state,\n",
        "        stores that delta as a layer.\n",
        "        \"\"\"\n",
        "        delta = new_total_matrix - self.current_total\n",
        "        self.layers.append(delta.copy())\n",
        "\n",
        "        # Update current tracker\n",
        "        self.current_total = new_total_matrix.copy()\n",
        "\n",
        "        # Calculate residual (Difference from the baseline)\n",
        "        residual = self.current_total - self.baseline_matrix\n",
        "        self.residuals.append(residual)\n",
        "\n",
        "        layer_id = len(self.layers) - 1\n",
        "        print(f\"\\n=== DSM LAYER {layer_id} CAPTURED ===\")\n",
        "        print(f\"Layer Contribution (Delta):\\n{np.round(delta, 3)}\")\n",
        "\n",
        "        return delta\n",
        "\n",
        "    def get_reconstruction(self):\n",
        "        return np.sum(self.layers, axis=0)\n",
        "\n",
        "\n",
        "class SVM:\n",
        "    \"\"\"\n",
        "    Metric-inverse multi-output SVM with epsilon-insensitive loss.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, output_dim=None, metric_keys=None, lr=0.001, epsilon=0.1):\n",
        "        self.input_dim = input_dim\n",
        "        self.metric_keys = metric_keys\n",
        "\n",
        "        if metric_keys is not None:\n",
        "            self.output_dim = len(metric_keys)\n",
        "        elif output_dim is not None:\n",
        "            self.output_dim = output_dim\n",
        "        else:\n",
        "            self.output_dim = 1\n",
        "\n",
        "        self.lr = lr\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        # Lazy initialization\n",
        "        self.X_train = None\n",
        "        self.y_train = None\n",
        "        self.alpha = None\n",
        "        self.b = None\n",
        "\n",
        "    def train_step(self, X, y_true):\n",
        "        X = np.array(X)\n",
        "        y_true = np.array(y_true)\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        if self.X_train is None:\n",
        "            self.X_train = X.copy()\n",
        "            self.y_train = y_true.copy()\n",
        "            self.alpha = np.zeros((X.shape[1], self.output_dim))\n",
        "            self.b = np.zeros(self.output_dim)\n",
        "\n",
        "        # Linear kernel\n",
        "        y_pred = X.dot(self.alpha) + self.b\n",
        "\n",
        "        # Epsilon-insensitive loss\n",
        "        diff = y_pred - y_true\n",
        "        mask = np.abs(diff) > self.epsilon\n",
        "        diff *= mask\n",
        "\n",
        "        grad_alpha = X.T.dot(diff) / n_samples\n",
        "        grad_b = diff.mean(axis=0)\n",
        "\n",
        "        self.alpha -= self.lr * grad_alpha\n",
        "        self.b -= self.lr * grad_b\n",
        "\n",
        "        loss = np.mean(np.maximum(0, np.abs(y_pred - y_true) - self.epsilon))\n",
        "        return loss\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.array(X)\n",
        "        y_pred = X.dot(self.alpha) + self.b\n",
        "\n",
        "        if self.metric_keys is None:\n",
        "            return y_pred\n",
        "\n",
        "        # Apply metric inverses\n",
        "        y_transformed = np.zeros_like(y_pred)\n",
        "        for i, key in enumerate(self.metric_keys):\n",
        "            inverse_fn = METRIC_INVERSES[key]\n",
        "            # Handle potential list return from inverse_fn\n",
        "            val_func = lambda y: inverse_fn(y)[0] if isinstance(inverse_fn(y), list) else inverse_fn(y)\n",
        "            y_transformed[:, i] = np.array([val_func(y) for y in y_pred[:, i]])\n",
        "\n",
        "        return y_transformed\n",
        "\n",
        "\n",
        "class InterLayer:\n",
        "    def __init__(self, D_graph, max_inner_dim, inter_dim=None, edge_threshold=0.02, gamma=1.0, seed=42):\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "\n",
        "        self.D_graph = D_graph\n",
        "        self.edge_threshold = edge_threshold\n",
        "        self.gamma = gamma\n",
        "\n",
        "        # Handle list vs int inputs for dimensions\n",
        "        m_dim = max_inner_dim[0] if isinstance(max_inner_dim, list) else max_inner_dim\n",
        "\n",
        "        if inter_dim is not None:\n",
        "            self.inter_dim = inter_dim[0] if isinstance(inter_dim, list) else inter_dim\n",
        "        else:\n",
        "            self.inter_dim = m_dim\n",
        "\n",
        "        self.max_input = 2 * m_dim\n",
        "\n",
        "        # Initialize weights\n",
        "        self.weights = {}\n",
        "        self.bias = {}\n",
        "        for i in range(D_graph):\n",
        "            for j in range(D_graph):\n",
        "                if i != j:\n",
        "                    w_init = np.random.uniform(-0.1, 0.1, (self.inter_dim, self.max_input))\n",
        "                    self.weights[(i, j)] = w_init\n",
        "                    self.bias[(i, j)] = np.zeros(self.inter_dim)\n",
        "\n",
        "    def compute_edge_activation(self, i, j, nested_reps):\n",
        "        concat = np.concatenate([nested_reps[i], nested_reps[j]])\n",
        "        # Pad or truncation\n",
        "        if len(concat) < self.max_input:\n",
        "            concat = np.pad(concat, (0, self.max_input - len(concat)))\n",
        "        else:\n",
        "            concat = concat[:self.max_input]\n",
        "\n",
        "        # Normalize\n",
        "        concat = (concat - np.mean(concat)) / (np.std(concat) + 1e-12)\n",
        "\n",
        "        # Activation\n",
        "        v = self.weights[(i, j)].dot(concat) + self.bias[(i, j)]\n",
        "        input_strength = np.clip(np.mean(np.abs(concat)), 0, 1)\n",
        "        v = v * input_strength\n",
        "\n",
        "        return 1 / (1 + np.exp(-v))\n",
        "\n",
        "    def build_activations(self, Gmat, nested_reps):\n",
        "        acts = {}\n",
        "        for i in range(self.D_graph):\n",
        "            for j in range(self.D_graph):\n",
        "                if i == j:\n",
        "                    continue\n",
        "                if abs(Gmat[i, j]) > self.edge_threshold:\n",
        "                    acts[(i, j)] = self.compute_edge_activation(i, j, nested_reps)\n",
        "        return acts\n",
        "\n",
        "    @staticmethod\n",
        "    def pairwise_squared_corr(acts):\n",
        "        if len(acts) < 2:\n",
        "            return 0.0\n",
        "        A = np.stack(list(acts.values()))\n",
        "        A_centered = A - A.mean(axis=1, keepdims=True)\n",
        "        stds = np.sqrt(np.sum(A_centered**2, axis=1) / (A.shape[1]-1) + 1e-12)\n",
        "        cov = A_centered @ A_centered.T / (A.shape[1]-1)\n",
        "        corr = cov / (np.outer(stds, stds) + 1e-12)\n",
        "        np.fill_diagonal(corr, 0)\n",
        "        return float((corr**2).sum())\n",
        "\n",
        "    def mi_for_graph(self, Gmat, nested_reps):\n",
        "        acts = self.build_activations(Gmat, nested_reps)\n",
        "        if not acts:\n",
        "            return 0.0\n",
        "        return self.gamma * self.pairwise_squared_corr(acts)\n",
        "\n",
        "def build_dsm_from_walks(D, paths):\n",
        "    \"\"\"\n",
        "    Constructs a DSM where entry [i,j] is the probability\n",
        "    that a successful process moves from i to j.\n",
        "    \"\"\"\n",
        "    flow = np.zeros((D, D))\n",
        "\n",
        "    # Count transitions\n",
        "    for path in paths:\n",
        "        for k in range(len(path) - 1):\n",
        "            u, v = path[k], path[k+1]\n",
        "            flow[u, v] += 1\n",
        "\n",
        "    # Normalize by the total number of successful walks.\n",
        "    # This prevents 'saturation'â€”if an edge is rarely used, it stays small.\n",
        "    n_paths = len(paths)\n",
        "    if n_paths > 0:\n",
        "        flow = flow / n_paths\n",
        "\n",
        "    np.fill_diagonal(flow, 0.0)\n",
        "    return flow\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7Z_mRbD4VpM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    D_GRAPH = D_graph\n",
        "except:\n",
        "    D_graph=D_GRAPH\n",
        "\n",
        "\n",
        "def build_coupling_weight(label, m_i, m_j):\n",
        "    \"\"\"\n",
        "    Interpretation: Pascal's Law P = F / A\n",
        "    Force (F) = Potential delta in metrics (Improvement)\n",
        "    Area (A) = Complexity/Friction of the destination node\n",
        "    Valve = Orifice efficiency based on coupling type\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. THE NODAL VECTORS\n",
        "    v_i = np.array(list(m_i.values())) if isinstance(m_i, dict) else np.array(m_i)\n",
        "    v_j = np.array(list(m_j.values())) if isinstance(m_j, dict) else np.array(m_j)\n",
        "\n",
        "    # 2. THE DRIVING FORCE (F)\n",
        "    # The pressure only builds if the destination offers more \"potential\"\n",
        "    # F = || max(0, v_j - v_i) ||\n",
        "    force = np.linalg.norm(np.maximum(v_j - v_i, 0)) + 1e-9\n",
        "\n",
        "    # 3. THE SURFACE AREA (A)\n",
        "    # The 'wider' the destination node (more complex/costly),\n",
        "    # the more the force is distributed, lowering the pressure.\n",
        "    surface_area = np.mean(v_j) + 0.1\n",
        "\n",
        "    # 4. THE VALVE ORIFICE (Efficiency)\n",
        "    # A-type couplings are 'Wide Nozzles' (High pressure translation)\n",
        "    # B-type couplings are 'Constricted Nozzles' (Damped flow)\n",
        "    label_str = str(label) if label is not None else \"\"\n",
        "    if \"A\" in label_str:\n",
        "        motivator = force # High flow efficiency\n",
        "    elif \"B\" in label_str:\n",
        "        motivator =  1.0  # Damped efficiency\n",
        "    else:\n",
        "        motivator = surface_area# Standard atmospheric pressure\n",
        "\n",
        "    # 5. THE SYSTEM PRESSURE (P)\n",
        "    # (i) Power driver\n",
        "    # (iii) Force driver\n",
        "    # (ii) Pressure\n",
        "    pressure = motivator * (force / surface_area)\n",
        "\n",
        "    return max(pressure, 0.0001)\n",
        "import numpy as np\n",
        "\n",
        "def infer_node_types(node_metrics, candidate_dims=candidate_dims):\n",
        "    \"\"\"\n",
        "    Generic Tier Inference:\n",
        "    - Lowest Dim  -> A (Edge)\n",
        "    - Highest Dim -> C (Core)\n",
        "    - All Others  -> B (Hub/Median)\n",
        "\n",
        "    This adapts to any number of unique dimensions.\n",
        "    \"\"\"\n",
        "    # 1. Flatten dimensions\n",
        "    dims_flat = [d[0] for d in candidate_dims]\n",
        "\n",
        "    # 2. Identify Boundaries\n",
        "    min_dim = min(dims_flat)\n",
        "    max_dim = max(dims_flat)\n",
        "\n",
        "    # 3. Assign Types based on Relative Position\n",
        "    node_types = []\n",
        "    for d in dims_flat:\n",
        "        if d == min_dim:\n",
        "            # The smallest nodes are always Edges\n",
        "            node_types.append(\"A\")\n",
        "        elif d == max_dim:\n",
        "            # The largest nodes are always Core\n",
        "            node_types.append(\"A\")\n",
        "        else:\n",
        "            # Anything in the middle is a Hub\n",
        "            node_types.append(\"A\")\n",
        "\n",
        "    # Diagnostic\n",
        "    unique_tiers = sorted(list(set(dims_flat)))\n",
        "    print(f\"Generic Architecture Inference:\")\n",
        "    print(f\" - Edge (A): {min_dim}D\")\n",
        "    print(f\" - Hubs (B): {max_dim}D\")\n",
        "    print(f\" - Core (C): {[u for u in unique_tiers if u != min_dim and u != max_dim]}D\")\n",
        "\n",
        "    return node_types\n",
        "\n",
        "def build_coupling_matrix(node_types):\n",
        "    \"\"\"\n",
        "    Generic Behavioral Matrix Builder.\n",
        "    Uses a 'Transition Rulebook' to enforce flow constraints.\n",
        "    \"\"\"\n",
        "    D = len(node_types)\n",
        "    C = np.full((D, D), \"VOID\", dtype=object)\n",
        "\n",
        "    # DEFINING THE FLOW LOGIC (The \"Constitution\")\n",
        "    # Format: { Source_Type: { Allowed_Dest_Type: Label } }\n",
        "    # Any transition not listed here becomes \"ABSORBED\"\n",
        "\n",
        "\n",
        "\n",
        "    RULES = {\n",
        "        \"C\": {\n",
        "            \"B\": \"C->B\"      # Core feeds Hub\n",
        "        },\n",
        "        \"B\": {\n",
        "            \"B\": \"B->B\",     # Hub processes (Self-Loop)\n",
        "            \"A\": \"B->A\"      # Hub ejects to Edge\n",
        "        },\n",
        "        \"A\": {\n",
        "            \"A\": \"A->A\",     # Edge roams (Local routing)\n",
        "            \"C\": \"A->C\"      # Edge returns to Core (Recycle)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    for i in range(D):\n",
        "        src = node_types[i]\n",
        "\n",
        "        for j in range(D):\n",
        "            dst = node_types[j]\n",
        "\n",
        "            # 1. Check if Source Type has rules\n",
        "            if src in RULES:\n",
        "                allowed_moves = RULES[src]\n",
        "\n",
        "                # 2. Check specific destination logic\n",
        "                if dst in allowed_moves:\n",
        "                    # Special check for Self-Loops if needed\n",
        "                    # (In this logic, A->A and B->B are allowed, C->C is not in the dict)\n",
        "\n",
        "                    # Apply Label\n",
        "                    C[i, j] = allowed_moves[dst]\n",
        "\n",
        "                    # Refinement: Ensure B->B is strictly i==j (Self-Loop only)\n",
        "                    # If B goes to OTHER B, that is blocked to prevent \"Hub Hopping\"\n",
        "                    if src == \"B\" and dst == \"B\" and i != j:\n",
        "                        C[i, j] = \"ABSORBED\"\n",
        "\n",
        "                else:\n",
        "                    C[i, j] = \"ABSORBED\"\n",
        "            else:\n",
        "                C[i, j] = \"ABSORBED\"\n",
        "\n",
        "    return C\n",
        "\n",
        "def build_dsm_from_walks(D, paths):\n",
        "    \"\"\"\n",
        "    Constructs a DSM (Design Structure Matrix) from walk paths.\n",
        "    Entry [i,j] is the probability that a process moves from i to j.\n",
        "    \"\"\"\n",
        "    flow = np.zeros((D, D))\n",
        "    for path in paths:\n",
        "        for k in range(len(path) - 1):\n",
        "            u, v = path[k], path[k+1]\n",
        "            flow[u, v] += 1\n",
        "\n",
        "    n_paths = len(paths)\n",
        "    if n_paths > 0:\n",
        "        flow = flow / n_paths\n",
        "\n",
        "    np.fill_diagonal(flow, 0.0)\n",
        "    return flow\n",
        "\n",
        "# =============================================================================\n",
        "# 2. THE METRIC-DRIVEN RANDOM WALKER (Consolidated)\n",
        "# =============================================================================\n",
        "\n",
        "class CouplingState:\n",
        "    \"\"\"\n",
        "    Automaton enforcing the Hub Saturation Loop.\n",
        "    Sequence: ... -> B (Accumulate) -> B (Saturate) -> A (Release) ...\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # \"NORMAL\": Free flow\n",
        "        # \"SATURATED\": Hub has self-looped and must eject\n",
        "        self.state = \"NORMAL\"\n",
        "\n",
        "    def update(self, coupling_label):\n",
        "        \"\"\"\n",
        "        Updates internal pressure state based on the move just made.\n",
        "        \"\"\"\n",
        "        # Trigger: We detect the Hub Self-Loop (Generic \"B->B\")\n",
        "        if coupling_label == \"B->B\":\n",
        "            self.state = \"SATURATED\"\n",
        "        else:\n",
        "            # Any other move (entering B, or leaving B->A) relieves pressure\n",
        "            self.state = \"NORMAL\"\n",
        "\n",
        "    def allows(self, next_node_type):\n",
        "        \"\"\"\n",
        "        Validates the NEXT intended move based on current pressure.\n",
        "        \"\"\"\n",
        "        # CRITICAL LOGIC: If Hub is Saturated, it MUST eject to Edge (A).\n",
        "        # It cannot hold pressure (B) and cannot backflow to Core (C).\n",
        "        if self.state == \"SATURATED\":\n",
        "            return next_node_type == \"A\"\n",
        "\n",
        "        return True\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MetricDrivenRandomWalk:\n",
        "    def __init__(self, coupling_matrix, node_metrics, metric_targets=None, max_steps=50, top_k=2, temperature=1.0):\n",
        "        \"\"\"\n",
        "        Unified Walker Class.\n",
        "        :param coupling_matrix: DxD matrix of transition labels.\n",
        "        :param node_metrics: List of dicts/vectors for node values.\n",
        "        :param metric_targets: (Optional) Binary mask or Target Matrix.\n",
        "        :param max_steps: Maximum path length.\n",
        "        :param top_k: Branching factor constraint.\n",
        "        :param temperature: Control randomness (Higher = more random, Lower = more deterministic).\n",
        "        \"\"\"\n",
        "        self.C = coupling_matrix\n",
        "        self.node_metrics = node_metrics\n",
        "        self.node_types = infer_node_types(node_metrics)\n",
        "\n",
        "        self.D = coupling_matrix.shape[0]\n",
        "        self.max_steps = max_steps\n",
        "        self.top_k = top_k\n",
        "        self.temperature = temperature\n",
        "\n",
        "        # Handle Mask: If None, allow all connections (ones)\n",
        "        if metric_targets is not None:\n",
        "            self.mask = np.array(metric_targets)\n",
        "        else:\n",
        "            self.mask = np.ones((self.D, self.D))\n",
        "\n",
        "    def run(self, start=0):\n",
        "        path = [start]\n",
        "        current = start\n",
        "        target_node = self.D - 1\n",
        "\n",
        "        # Initialize Automaton for this specific run\n",
        "        automaton = CouplingState()\n",
        "\n",
        "        for _ in range(self.max_steps):\n",
        "            if current == target_node:\n",
        "                break\n",
        "\n",
        "            # 1. Calculate Raw Weights (Ohmic Conductance)\n",
        "            weights = np.zeros(self.D)\n",
        "            for j in range(self.D):\n",
        "                if current == j: continue\n",
        "\n",
        "                # A. Structural Mask Check (User provided mask)\n",
        "                if self.mask[current, j] == 0:\n",
        "                    continue\n",
        "\n",
        "                # B. Automaton Rule Check\n",
        "                if not automaton.allows(self.node_types[j]):\n",
        "                    continue\n",
        "\n",
        "                # C. Compute Physics-based Weight\n",
        "                base_w = build_coupling_weight(self.C[current, j], self.node_metrics[current], self.node_metrics[j])\n",
        "\n",
        "                # D. Directional Bias (Gravity)\n",
        "                gravity = 1.5 if j > current else 0.4\n",
        "\n",
        "                # E. Loop Friction (Novelty check)\n",
        "                friction = 0.1 if j in path else 1.0\n",
        "\n",
        "                weights[j] = base_w * gravity * friction\n",
        "\n",
        "            # 2. Check for Dead End\n",
        "            if weights.sum() == 0:\n",
        "                break\n",
        "\n",
        "            # 3. Apply Temperature (Softmax-ish scaling) for True Randomness\n",
        "            # Normalize first to avoid overflow in exp\n",
        "            weights = weights / (weights.max() + 1e-12)\n",
        "            exp_weights = np.exp(weights / self.temperature)\n",
        "\n",
        "            # Zero out the ones that were originally zero\n",
        "            exp_weights[weights == 0] = 0\n",
        "\n",
        "            # 4. Top-K Sparsity & Normalization\n",
        "            probs = top_k_masked_probs(exp_weights, self.top_k)\n",
        "\n",
        "            # 5. Stochastic Jump\n",
        "            nxt = np.random.choice(self.D, p=probs)\n",
        "\n",
        "            # Update state\n",
        "            automaton.update(self.C[current, nxt])\n",
        "            path.append(nxt)\n",
        "            current = nxt\n",
        "\n",
        "        return path\n",
        "from collections import deque\n",
        "all_results = []\n",
        "\n",
        "        #return True\n",
        "# --- Inner Loop (FCM & Learning) ---\n",
        "INNER_FCM_STEPS = 1000       # Iterations per node simulation\n",
        "INNER_LR_X = 1.0             # Learning rate for State X\n",
        "INNER_LR_Y = 0.01           # Learning rate for State Y\n",
        "INNER_LR_W = 1.0             # Learning rate for Weights\n",
        "INNER_SVM_LR = 0.01          # SVM Learning Rate\n",
        "INNER_GAMMA = 1.0            # Inter-layer neural connection strength\n",
        "\n",
        "# --- Random Walk & Pathfinding ---\n",
        "WALK_BETA = 0.3              # Novelty penalty (dampens repeated paths)\n",
        "WALK_LAMBDA_COST = 1.0       # Penalty weight for cost metrics\n",
        "\n",
        "# --- Outer Loop (Topology Optimization) ---\n",
        "OUTER_GENERATIONS = 1        # Iterations per Layer\n",
        "OUTER_COST_LIMIT = 1000      # Normalization ceiling for scores\n",
        "INTER_EDGE_THRESH = 0.02     # Min DSM weight to trigger neural link\n",
        "\n",
        "for ijk in range(D_GRAPH):\n",
        "    for jik in range(2,12):\n",
        "        for kij in [0.3,0.45,0.6,0.75,0.9]:\n",
        "            print(50*'_',ijk,50*'-',jik,50*'=')\n",
        "            #===============================================================================\n",
        "            OUTER_N_SIMS = 1000          # More simulations to find the \"Hidden Gem\" paths\n",
        "            WALK_MAX_STEPS = 100          # Let the walker explore complex relationships deeply\n",
        "            DSM_TARGET_EDGES = jik        # Allow HIGHER density (Complexity is allowed!)\n",
        "            OUTER_DSM_LAYERS = 1         # Balanced hierarchy (Structure -> Systems -> Skin)\n",
        "            DSM_ADDITIVE_RATE = kij   # Low Learning Rate: Learn slowly, don't panic.\n",
        "            DSM_FEEDBACK_STR = 0.05      # Weak Feedback: Listen to problems, but don't obsess.\n",
        "            WALK_TOP_K = 2               # Soft Sparsity: Consider more options per step.\n",
        "            DSM_FEEDBACK_FILTER = 0.1    # Only react to major issues.\n",
        "            DSM_PRUNE_THRESH = 0.02      # Keep subtle connections.\n",
        "            DSM_INIT_RANGE = 0.2         # Start with a blanker slate.\n",
        "            STARTING_POINT = ijk           # START AT SITE ANALYSIS (Respect the Land).\n",
        "            #=============================================================================\n",
        "            class Fuzzy_Hierarchical_Multiplex:\n",
        "                def __init__(self, candidate_dims, D_graph,\n",
        "                            synthetic_targets,\n",
        "                            gamma_interlayer=1.0, causal_flag=False,\n",
        "                            metrics=METRIC_KEYS, metric_mask=METRIC_TARGET):\n",
        "\n",
        "                    self.candidate_dims = candidate_dims\n",
        "                    self.D_graph = D_graph\n",
        "                    self.synthetic_targets = synthetic_targets\n",
        "                    self.causal_flag = causal_flag\n",
        "                    self.best_dim_per_node = [len(t)-1 for t in synthetic_targets]\n",
        "                    self.MM = metric_mask\n",
        "                    self.MK = metrics\n",
        "                    self.MKI = metrics + ['score']\n",
        "\n",
        "                    self.PLM = [[] for _ in range(self.D_graph)]\n",
        "                    self.PLMS = [[] for _ in range(self.D_graph)]\n",
        "                    self.nested_reps = [np.zeros(c[0]) for c in candidate_dims]\n",
        "\n",
        "                    # Inter-layer setup\n",
        "                    self.inter_layer = InterLayer(D_graph, max_inner_dim=max(candidate_dims), gamma=gamma_interlayer)\n",
        "                    self.chosen_Gmat = np.random.uniform(0.0, 0.3, (D_graph, D_graph))\n",
        "                    np.fill_diagonal(self.chosen_Gmat, 0)\n",
        "\n",
        "                    self.l2_before, self.l2_after = [], []\n",
        "                    self.max_target_len = max(len(t['target']) for t in synthetic_targets)\n",
        "                    self.svm_lr = 0.01\n",
        "\n",
        "                    self.metric_traces = {k: [] for k in metrics}\n",
        "                    self.metric_traces_per_node = [{} for _ in range(self.D_graph)]\n",
        "\n",
        "                    # DSM optimization hyperparameters\n",
        "                    self.dsm_lr = 0.1\n",
        "                    self.dsm_l1 = 0.02\n",
        "                    self.dsm_clip = 1.0\n",
        "                    self.dsm_history = []\n",
        "                    self.dsm_cost_weight = 0.05\n",
        "\n",
        "                def print_dsm_basic(self):\n",
        "                    D = self.D_graph\n",
        "                    print(\"\\n=== DESIGN STRUCTURE MATRIX (DSM) : Gmat ===\")\n",
        "                    header = \"     \" + \" \".join([f\"N{j:>4}\" for j in range(D)])\n",
        "                    print(header)\n",
        "                    for i in range(D):\n",
        "                        row = \"N{:>2} | \".format(i)\n",
        "                        for j in range(D):\n",
        "                            row += f\"{self.chosen_Gmat[i, j]:>5.2f} \"\n",
        "                        print(row)\n",
        "\n",
        "                # ---------- INNER LOOP (FCM) ----------\n",
        "                def run_inner(self, node_idx, target, D_fcm,\n",
        "                            steps=INNER_FCM_STEPS, lr_x=INNER_LR_X, lr_y=INNER_LR_Y, lr_W=INNER_LR_W,\n",
        "                            decorrelate_metrics=False):\n",
        "\n",
        "                    # --- Initialize activations ---\n",
        "                    x = np.random.uniform(-0.6, 0.6, D_fcm)\n",
        "                    y = np.random.uniform(-0.1, 0.1, D_fcm)\n",
        "\n",
        "                    # L2 tracking\n",
        "                    self.l2_before.append(np.linalg.norm(self.nested_reps[node_idx][:len(target)] - target))\n",
        "\n",
        "                    # --- FCM updates ---\n",
        "                    W = np.random.uniform(-0.6, 0.6, (D_fcm, D_fcm))\n",
        "                    np.fill_diagonal(W, 0)\n",
        "\n",
        "                    for _ in range(steps):\n",
        "                        z = y.dot(W) + x\n",
        "                        Theta_grad_z = z - target\n",
        "                        Theta_grad_x = Theta_grad_z\n",
        "                        Theta_grad_y = Theta_grad_z.dot(W.T)\n",
        "                        Theta_grad_W = np.outer(y, Theta_grad_z)\n",
        "\n",
        "                        x -= lr_x * np.clip(Theta_grad_x, -0.05, 0.05)\n",
        "                        y -= lr_y * np.clip(Theta_grad_y, -0.05, 0.05)\n",
        "                        W -= lr_W * np.clip(Theta_grad_W, -0.01, 0.01)\n",
        "\n",
        "                        x = np.clip(x, 0, 1)\n",
        "                        y = np.clip(y, 0, 1)\n",
        "                        np.fill_diagonal(W, 0)\n",
        "                        W = np.clip(W, -1, 1)\n",
        "\n",
        "                    # --- Update nested representation ---\n",
        "                    self.nested_reps[node_idx][:len(x)] = x\n",
        "                    self.l2_after.append(np.linalg.norm(x - target))\n",
        "\n",
        "                    # --- Extract node features ---\n",
        "                    # Assuming MetricsEvaluator is a global or imported class\n",
        "                    metrics_evaluator = MetricsEvaluator(data_matrix=DATA_MATRIX)\n",
        "                    features = metrics_evaluator.extract_features(node_idx)\n",
        "                    feat_vals = np.array(list(features.values()))\n",
        "\n",
        "                    # --- Compute metrics scaled by activations + features ---\n",
        "                    metric_mask = METRIC_TARGET[node_idx]\n",
        "                    metric_values = {}\n",
        "\n",
        "                    for key, formula, mask in zip(METRIC_KEYS, METRIC_FORMULAS, metric_mask):\n",
        "                        if mask:\n",
        "                            weighted_input = np.mean(feat_vals)\n",
        "                            # Outer scale check\n",
        "                            outer_scale = getattr(self, 'best_node_weights', {}).get(node_idx, 1.0)\n",
        "                            if isinstance(outer_scale, (list, np.ndarray)):\n",
        "                                # fallback if it was stored incorrectly in previous context\n",
        "                                outer_scale = 1.0\n",
        "\n",
        "                            weighted_input *= outer_scale\n",
        "                            metric_val = formula(weighted_input)\n",
        "                            metric_values[key] = metric_val\n",
        "\n",
        "                            # STORE DATAPOINT\n",
        "                            self.metric_traces[key].append((weighted_input, metric_val))\n",
        "                        else:\n",
        "                            metric_values[key] = 0.0\n",
        "\n",
        "                    # --- Total score ---\n",
        "                    metric_values['score'] = sum(metric_values.values())\n",
        "\n",
        "                    # --- Build SVM Training Data ---\n",
        "                    metric_output_vals = np.array(\n",
        "                        [v for k, v in metric_values.items() if k not in ['score', 'x', 'feat_vals']]\n",
        "                    )\n",
        "\n",
        "                    # Lazy init per-node SVM\n",
        "                    if not hasattr(self, \"node_svms\"):\n",
        "                        self.node_svms = {}\n",
        "\n",
        "                    if node_idx not in self.node_svms:\n",
        "                        self.node_svms[node_idx] = SVM(\n",
        "                            input_dim=len(self.MK),\n",
        "                            output_dim=self.candidate_dims[node_idx][0],\n",
        "                            lr=self.svm_lr\n",
        "                        )\n",
        "\n",
        "                    svm = self.node_svms[node_idx]\n",
        "\n",
        "                    # Build SVM Input/Output\n",
        "                    x_in_full = np.zeros(len(self.MK))\n",
        "                    x_in_full[:len(metric_output_vals)] = metric_output_vals\n",
        "                    x_in = x_in_full.reshape(1, -1)\n",
        "\n",
        "                    y_out_full = np.zeros(self.candidate_dims[node_idx][0])\n",
        "                    y_out_full[:len(x)] = x\n",
        "                    y_out = y_out_full.reshape(1, -1)\n",
        "\n",
        "                    # Train SVM\n",
        "                    _ = svm.train_step(x_in, y_out)\n",
        "\n",
        "                    # --- Store PLMS trace ---\n",
        "                    self.PLMS[node_idx].append((float(weighted_input), metric_output_vals))\n",
        "\n",
        "                    if len(self.PLMS[node_idx]) % 100 == 0:\n",
        "                        print(f\"Node {node_idx}, samples learned:\", len(self.PLMS[node_idx]))\n",
        "\n",
        "                    # --- Compute inter-layer MI ---\n",
        "                    mi_score = self.inter_layer.mi_for_graph(self.chosen_Gmat, self.nested_reps)\n",
        "\n",
        "                    return x, y, W, mi_score, metric_values\n",
        "\n",
        "                # ---------- OUTER LOOP (Topology Optimization) ----------\n",
        "                def run_outer(self, outer_cost_limit=OUTER_COST_LIMIT, alpha=0.0, additive_rate=DSM_ADDITIVE_RATE, n_simulations=OUTER_N_SIMS):\n",
        "                    \"\"\"\n",
        "                    CORRECTED LOGIC (Distributed Flow):\n",
        "                    1. Uses Random Walk to find high-probability metric flows starting from ANY node.\n",
        "                    2. Injects FEEDBACK LOOPS to create Coupled Blocks.\n",
        "                    3. Prunes weak edges to prevent 'Total Chaos'.\n",
        "\n",
        "                    *Update:* Removed 'Start at 0' handicap. Now samples flows from all subsystems.\n",
        "                    \"\"\"\n",
        "                    node_metrics_list = self.capped_node_metrics\n",
        "                    D = self.D_graph\n",
        "\n",
        "                    # =========================================================\n",
        "                    # 1. METRIC SCORING\n",
        "                    # =========================================================\n",
        "                    raw_scores = np.array([m['score'] for m in node_metrics_list])\n",
        "                    total_raw = raw_scores.sum()\n",
        "                    if total_raw > outer_cost_limit:\n",
        "                        scale_factor = outer_cost_limit / total_raw\n",
        "                        for metrics in node_metrics_list:\n",
        "                            for key in self.MKI:\n",
        "                                metrics[key] *= scale_factor\n",
        "                        raw_scores *= scale_factor\n",
        "\n",
        "                    fuzzy_tensor = self.compute_fuzzy_metric_tensor(normalize=False)\n",
        "                    self.weighted_fmt = fuzzy_tensor.copy()\n",
        "\n",
        "                    # Calculate Contributions\n",
        "                    node_contributions = np.zeros(D)\n",
        "                    for i in range(D):\n",
        "                        own_score = raw_scores[i]\n",
        "                        fmt_contrib = fuzzy_tensor[i, :, :].sum() - fuzzy_tensor[i, i, :].sum()\n",
        "                        node_contributions[i] = own_score + self.inter_layer.gamma * fmt_contrib\n",
        "                    self.node_score_contributions = node_contributions\n",
        "                    self.correlation_penalty = 0.0\n",
        "\n",
        "                    # =========================================================\n",
        "                    # 2. PROBABILISTIC PATHFINDING (Distributed Random Walk)\n",
        "                    # =========================================================\n",
        "\n",
        "                    node_metrics = self.capped_node_metrics\n",
        "                    node_types = infer_node_types(node_metrics)\n",
        "                    C_matrix = build_coupling_matrix(node_types)\n",
        "                    print(node_types)\n",
        "                    # Initialize Walker\n",
        "                    # Inside run_outer method:\n",
        "\n",
        "    # ... (Previous metric scoring logic) ...\n",
        "\n",
        "                    # Pass the mask (METRIC_TARGET) explicitly\n",
        "                    # Assuming 'self.MM' holds the METRIC_TARGET data structure you provided\n",
        "                    walker = MetricDrivenRandomWalk(\n",
        "                        C_matrix,\n",
        "                        node_metrics,\n",
        "                        metric_targets=self.MM, # <--- PASS THE MASK HERE\n",
        "                        max_steps=WALK_MAX_STEPS,\n",
        "                        top_k=WALK_TOP_K\n",
        "                    )\n",
        "\n",
        "    # ... (Rest of pathfinding logic) ...\n",
        "\n",
        "                    # --- VALIDATOR (RELAXED) ---\n",
        "                                    # 1. Define the Tiers of Validation\n",
        "                    def is_path_valid(path):\n",
        "                        # 1. Topology Checks\n",
        "                        if len(path) < 2: return False        # A path must go somewhere\n",
        "                        if path[-1] != (D - 1): return False  # Must still converge to Project Completion\n",
        "                        if len(set(path)) != len(path): return False # No self-cycles\n",
        "\n",
        "                        # 2. Coupling Constraints\n",
        "                        state_machine = CouplingState()\n",
        "                        for k in range(len(path) - 1):\n",
        "                            u, v = path[k], path[k+1]\n",
        "                            type_u, type_v = node_types[u], node_types[v]\n",
        "\n",
        "                            if not state_machine.allows(type_v): return False\n",
        "\n",
        "                            coupling = \"B->B->C\" if (type_u == \"B\" and type_v == \"B\") else f\"{type_u}->{type_v}\"\n",
        "                            state_machine.update(coupling)\n",
        "                        return True\n",
        "\n",
        "\n",
        "                    # --- DISTRIBUTED SAMPLING ---\n",
        "                    valid_paths = []\n",
        "                    print(f\" [Optimizer] Sampling {n_simulations} distributed paths (Any Start -> End)...\")\n",
        "\n",
        "                    # Potential start nodes: 0 to D-2 (Any node except the final Sink node)\n",
        "                    possible_starts = list(range(D - 1))\n",
        "\n",
        "                    for _ in range(n_simulations):\n",
        "                        # Randomly select a starting subsystem to ensure \"All Processes Included\"\n",
        "\n",
        "                        path = walker.run(start=STARTING_POINT)\n",
        "\n",
        "                        if is_path_valid(path):\n",
        "                            valid_paths.append(path)\n",
        "\n",
        "                    # FALLBACK\n",
        "                    if len(valid_paths) == 0:\n",
        "                        print(\" [WARNING] Strict constraints failed. Synthesizing default backbone.\")\n",
        "                        valid_paths.append(list(range(D)))\n",
        "\n",
        "                    # Deduplicate\n",
        "                    unique_paths = sorted(list(set(tuple(p) for p in valid_paths)), key=lambda x: len(x), reverse=True)\n",
        "                    valid_paths = [list(p) for p in unique_paths]\n",
        "\n",
        "                    # Deduplicate\n",
        "                    unique_paths = sorted(list(set(tuple(p) for p in valid_paths)), key=lambda x: len(x), reverse=True)\n",
        "                    valid_paths = [list(p) for p in unique_paths]\n",
        "\n",
        "                    # =========================================================\n",
        "                    # 3. LAYER CONSTRUCTION & FEEDBACK INJECTION\n",
        "                    # =========================================================\n",
        "\n",
        "                    G_layer = build_dsm_from_walks(D, valid_paths)\n",
        "\n",
        "                    # Feedback Loops\n",
        "                    feedback_strength = DSM_FEEDBACK_STR\n",
        "                    G_feedback = G_layer.T * feedback_strength\n",
        "\n",
        "                    # Filter feedback\n",
        "                    G_feedback[G_layer < DSM_FEEDBACK_FILTER] = 0.0\n",
        "\n",
        "                    # Combine\n",
        "                    G_layer_final = G_layer + G_feedback\n",
        "\n",
        "                    # =========================================================\n",
        "                    # 4. UPDATE, AMPLIFY & PRUNE\n",
        "                    # =========================================================\n",
        "\n",
        "                    self.chosen_Gmat = self.chosen_Gmat + (additive_rate * G_layer_final)\n",
        "\n",
        "                    if np.max(self.chosen_Gmat) > 0:\n",
        "                        self.chosen_Gmat /= np.max(self.chosen_Gmat)\n",
        "\n",
        "                    # Top-K Pruning\n",
        "                    TARGET_EDGES = DSM_TARGET_EDGES\n",
        "                    flat = self.chosen_Gmat.ravel()\n",
        "                    if len(flat) > TARGET_EDGES:\n",
        "                        threshold = np.partition(flat, -TARGET_EDGES)[-TARGET_EDGES]\n",
        "                        self.chosen_Gmat[self.chosen_Gmat < threshold] = 0.0\n",
        "\n",
        "                    # Noise Pruning\n",
        "                    self.chosen_Gmat[self.chosen_Gmat < DSM_PRUNE_THRESH] = 0.0\n",
        "\n",
        "                    density = np.count_nonzero(self.chosen_Gmat)\n",
        "                    print(f\" [Optimizer] Matrix Updated. Density: {density} edges. Max Val: {np.max(self.chosen_Gmat):.2f}\")\n",
        "\n",
        "                    self.walks, self.best_walk = collect_and_select_best_walks(\n",
        "                        self.chosen_Gmat,\n",
        "                        self.capped_node_metrics,\n",
        "                        beta=WALK_BETA\n",
        "                    )\n",
        "\n",
        "                    self.print_dsm_basic()\n",
        "\n",
        "                    if not hasattr(self, \"_node_contributions_history\"):\n",
        "                        self._node_contributions_history = []\n",
        "                    self._node_contributions_history.append(node_contributions.copy())\n",
        "\n",
        "                    return node_metrics_list, 0.0, node_contributions\n",
        "\n",
        "                def run(self, outer_generations=OUTER_GENERATIONS, num_dsm_layers=OUTER_DSM_LAYERS):\n",
        "                    best_score = -np.inf\n",
        "\n",
        "                    # 1. FIX INITIALIZATION:\n",
        "                    # Use the random initial state as the baseline.\n",
        "                    # This ensures Layer 0 captures the \"Jump\" from noise to structure.\n",
        "                    baseline = self.chosen_Gmat.copy()\n",
        "                    dsm_decomposer = DSM_Layer_Decomposer(baseline, mode='additive')\n",
        "                    dsm_decomposer.current_total = baseline.copy()\n",
        "\n",
        "                    print(f\"Starting Optimization: {num_dsm_layers} Layers x {outer_generations} Gens\")\n",
        "\n",
        "                    # Define the \"Building Blocks\" for the 3 layers (based on your 12-15 node stack)\n",
        "                    # Layer 0: Structure (Nodes 0-5), Layer 1: Systems (Nodes 6-10), Layer 2: Skin/Ops (Nodes 11-14)\n",
        "                    nodes_per_layer = np.array_split(range(self.D_graph), num_dsm_layers)\n",
        "\n",
        "                    for layer_idx in range(num_dsm_layers):\n",
        "                        print(f\"\\n>>> COMPILING LAYER {layer_idx + 1}: {['STRUCTURE', 'SYSTEMS', 'SKIN'][layer_idx]} <<<\")\n",
        "\n",
        "                        # Determine the nodes active in this specific layer\n",
        "                        active_nodes = nodes_per_layer[layer_idx]\n",
        "\n",
        "                        for gen in range(outer_generations):\n",
        "                            # 1. Inner Loop (Targeting active nodes for this layer)\n",
        "                            node_metrics_list = []\n",
        "                            for node_idx in range(self.D_graph):\n",
        "                                full_target = self.synthetic_targets[node_idx]['target']\n",
        "                                D_fcm = self.candidate_dims[node_idx][0]\n",
        "                                target = full_target[:D_fcm]\n",
        "\n",
        "                                # We simulate everything, but the \"Learning\" is focused on the active layer\n",
        "                                _, _, _, _, metrics = self.run_inner(node_idx, target, D_fcm)\n",
        "                                node_metrics_list.append(metrics)\n",
        "\n",
        "                            self.capped_node_metrics = node_metrics_list\n",
        "\n",
        "                            # 2. Outer Loop (Topology Optimization)\n",
        "                            # We pass the layer_idx to run_outer if you want to adjust the WALK_TOP_K\n",
        "                            # or additive_rate per layer (e.g., higher for structure, lower for skin)\n",
        "                            _, capped_score, _ = self.run_outer()\n",
        "\n",
        "                            best_score = max(best_score, capped_score)\n",
        "                            print(f\" [Gen {gen+1}] Score: {capped_score:.4f}\", end='\\r')\n",
        "\n",
        "                        print(\"\")\n",
        "\n",
        "                        # 3. SNAPSHOT: The Decomposer captures the \"Delta\" for this layer\n",
        "                        # This is where the MUX/DEMUX logic is voucher-ed.\n",
        "                        dsm_decomposer.add_snapshot(self.chosen_Gmat)\n",
        "\n",
        "                    self.dsm_layers = dsm_decomposer.layers\n",
        "                    print(\"\\nOptimization Complete. All 3 Layers Compiled.\")\n",
        "                    return best_score\n",
        "\n",
        "                # ---------- VISUALIZATIONS & ANALYSIS ----------\n",
        "\n",
        "                def plot_pointwise_minmax_elite(self, top_k=21):\n",
        "                    plt.figure(figsize=(14, 3))\n",
        "                    for i in range(self.D_graph):\n",
        "                        dim_i = self.candidate_dims[i][0]\n",
        "                        base = self.nested_reps[i][:dim_i]\n",
        "                        reps = np.clip(base + np.random.normal(0, 0.05, (top_k, len(base))), 0, 1)\n",
        "                        y_min, y_max = reps.min(axis=0), reps.max(axis=0)\n",
        "                        y_sel = base\n",
        "\n",
        "                        y_true = self.synthetic_targets[i]['target'][:len(y_sel)]\n",
        "                        if len(y_true) < len(y_sel):\n",
        "                            y_true = np.pad(y_true, (0, len(y_sel) - len(y_true)), \"constant\")\n",
        "                        else:\n",
        "                            y_true = y_true[:len(y_sel)]\n",
        "\n",
        "                        plt.subplot(1, self.D_graph, i + 1)\n",
        "                        plt.fill_between(range(len(y_min)), y_min, y_max, color='skyblue', alpha=0.4, label='Elite Interval')\n",
        "                        plt.plot(y_sel, 'k-', lw=2, label='Estimated')\n",
        "                        plt.plot(y_true, 'r--', lw=2, label='True')\n",
        "                        plt.ylim(0, 1.05)\n",
        "                        plt.title(f\"Node {i + 1}\")\n",
        "                        if i == 0: plt.legend()\n",
        "                    plt.tight_layout()\n",
        "                    plt.show()\n",
        "\n",
        "                def plot_nested_activations(self):\n",
        "                    plt.figure(figsize=(12, 3))\n",
        "                    for i, rep in enumerate(self.nested_reps):\n",
        "                        dim_i = self.candidate_dims[i][0]\n",
        "                        rep_i = rep[:dim_i]\n",
        "                        plt.subplot(1, self.D_graph, i + 1)\n",
        "                        plt.bar(range(len(rep_i)), rep_i, color=plt.cm.plasma(rep_i))\n",
        "                        plt.ylim(0, 1)\n",
        "                        plt.title(f\"Node {i + 1}\")\n",
        "                    plt.tight_layout()\n",
        "                    plt.show()\n",
        "\n",
        "                def plot_outer_fuzzy_graph(self):\n",
        "                    G = nx.DiGraph()\n",
        "                    for i in range(self.D_graph): G.add_node(i)\n",
        "                    for i in range(self.D_graph):\n",
        "                        for j in range(self.D_graph):\n",
        "                            if i != j and abs(self.chosen_Gmat[i, j]) > 0.02:\n",
        "                                G.add_edge(i, j, weight=self.chosen_Gmat[i, j])\n",
        "\n",
        "                    node_sizes = [self.best_dim_per_node[i] * 200 for i in range(self.D_graph)]\n",
        "                    edge_colors = ['green' if d['weight'] > 0 else 'red' for _, _, d in G.edges(data=True)]\n",
        "                    edge_widths = [abs(d['weight']) * 3 for _, _, d in G.edges(data=True)]\n",
        "\n",
        "                    pos = nx.spring_layout(G)\n",
        "                    plt.figure(figsize=(6, 6))\n",
        "                    nx.draw(G, pos, node_size=node_sizes, node_color='skyblue',\n",
        "                            edge_color=edge_colors, width=edge_widths, arrows=True, with_labels=True)\n",
        "                    plt.title(\"Outer Fuzzy Multiplex Graph\")\n",
        "                    plt.show()\n",
        "\n",
        "                def print_interactions(self, return_tensor=True, verbose=True):\n",
        "                    D_graph = self.D_graph\n",
        "                    inter_dim = self.inter_layer.inter_dim\n",
        "                    inter_tensor = np.zeros((D_graph, D_graph, inter_dim))\n",
        "\n",
        "                    acts = self.inter_layer.build_activations(self.chosen_Gmat, self.nested_reps)\n",
        "                    if not acts:\n",
        "                        if verbose:\n",
        "                            print(\"No active edges above threshold.\")\n",
        "                        return inter_tensor if return_tensor else None\n",
        "\n",
        "                    for (i, j), vec in acts.items():\n",
        "                        inter_tensor[i, j, :] = vec\n",
        "                        if verbose:\n",
        "                            act_str = \", \".join([f\"{v:.3f}\" for v in vec])\n",
        "                            print(f\"Node {i} -> Node {j}: [{act_str}]\")\n",
        "                    return inter_tensor if return_tensor else None\n",
        "\n",
        "                def compute_fuzzy_metric_tensor(self, normalize=True, verbose=False):\n",
        "                    metrics_keys = self.MK\n",
        "                    D = self.D_graph\n",
        "                    num_metrics = len(metrics_keys)\n",
        "                    tensor = np.zeros((D, D, num_metrics))\n",
        "\n",
        "                    metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "\n",
        "                    node_metrics = []\n",
        "                    for i, rep in enumerate(self.nested_reps):\n",
        "                        metrics = metrics_evaluator.compute_node_metrics(i, y=rep)\n",
        "                        node_metrics.append(np.array([metrics[k] for k in metrics_keys]))\n",
        "                    node_metrics = np.array(node_metrics)\n",
        "\n",
        "                    for i in range(D):\n",
        "                        for j in range(D):\n",
        "                            if i == j:\n",
        "                                tensor[i, j, :] = node_metrics[j]\n",
        "                            else:\n",
        "                                weight = np.clip(abs(self.chosen_Gmat[i, j]), 0, 1)\n",
        "                                tensor[i, j, :] = weight * node_metrics[j]\n",
        "\n",
        "                    if normalize:\n",
        "                        tensor = (tensor - tensor.min()) / (tensor.max() - tensor.min() + 1e-12)\n",
        "\n",
        "                    if verbose:\n",
        "                        print(\"Fuzzy Metric Tensor shape:\", tensor.shape)\n",
        "\n",
        "                    return tensor\n",
        "\n",
        "                def plot_fuzzy_metric_tensor_heatmaps(self, fuzzy_tensor=None, metrics_keys=None):\n",
        "                    if metrics_keys is None:\n",
        "                        metrics_keys = self.MK\n",
        "                    if fuzzy_tensor is None:\n",
        "                        fuzzy_tensor = self.compute_fuzzy_metric_tensor(normalize=True)\n",
        "\n",
        "                    D = self.D_graph\n",
        "                    num_metrics = len(metrics_keys)\n",
        "\n",
        "                    fig, axes = plt.subplots(1, num_metrics, figsize=(4 * num_metrics, 4))\n",
        "                    if num_metrics == 1: axes = [axes]\n",
        "\n",
        "                    im = None\n",
        "                    for k, key in enumerate(metrics_keys):\n",
        "                        data = fuzzy_tensor[:, :, k]\n",
        "                        im = axes[k].imshow(data, cmap='viridis', vmin=0, vmax=1)\n",
        "                        for i in range(D):\n",
        "                            for j in range(D):\n",
        "                                axes[k].text(j, i, f\"{data[i, j]:.2f}\", ha='center', va='center', color='white', fontsize=9)\n",
        "                        axes[k].set_xticks(range(D))\n",
        "                        axes[k].set_yticks(range(D))\n",
        "                        axes[k].set_xticklabels([f'Node {j}' for j in range(D)])\n",
        "                        axes[k].set_yticklabels([f'Node {i}' for i in range(D)])\n",
        "                        axes[k].set_title(f'FMT - {key}')\n",
        "\n",
        "                    fig.colorbar(im, ax=axes, orientation='vertical', fraction=0.025, pad=0.04, label='Normalized Metric Value')\n",
        "                    plt.tight_layout()\n",
        "                    plt.show()\n",
        "\n",
        "                def compute_fmt_with_elite_bounds(self, top_k=21):\n",
        "                    metrics_keys = self.MK\n",
        "                    D = self.D_graph\n",
        "                    num_metrics = len(metrics_keys)\n",
        "                    tensor_bounds = np.zeros((D, D, num_metrics, 2))\n",
        "\n",
        "                    metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "\n",
        "                    for i in range(D):\n",
        "                        base = self.nested_reps[i]\n",
        "                        reps = np.clip(base + np.random.normal(0, 0.05, (top_k, len(base))), 0, 1)\n",
        "\n",
        "                        metrics_matrix = np.zeros((top_k, num_metrics))\n",
        "                        for idx, rep in enumerate(reps):\n",
        "                            m = metrics_evaluator.compute_node_metrics(i, y=rep)\n",
        "                            metrics_matrix[idx, :] = [m[k] for k in metrics_keys]\n",
        "\n",
        "                        lower_i = metrics_matrix.min(axis=0)\n",
        "                        upper_i = metrics_matrix.max(axis=0)\n",
        "\n",
        "                        for j in range(D):\n",
        "                            tensor_bounds[i, j, :, 0] = lower_i\n",
        "                            tensor_bounds[i, j, :, 1] = upper_i\n",
        "\n",
        "                    return tensor_bounds\n",
        "\n",
        "                def plot_fmt_with_bounds(self, fmt_tensor_bounds):\n",
        "                    D = self.D_graph\n",
        "                    metrics_keys = self.MK\n",
        "                    M_actual = len(metrics_keys)\n",
        "\n",
        "                    mean_vals = (fmt_tensor_bounds[:, :, :, 0] + fmt_tensor_bounds[:, :, :, 1]) / 2\n",
        "                    mean_vals = mean_vals.mean(axis=1)  # mean across targets\n",
        "                    mean_vals = mean_vals.mean(axis=0, keepdims=True)  # mean across nodes\n",
        "\n",
        "                    if hasattr(self, 'best_alpha') and hasattr(self, 'best_w_contrib'):\n",
        "                        mean_weight = (self.best_alpha * self.best_w_contrib).mean()\n",
        "                        mean_vals = mean_vals * mean_weight\n",
        "\n",
        "                    fig, ax = plt.subplots(figsize=(1.2 * M_actual + 4, 2))\n",
        "                    im = ax.imshow(mean_vals, cmap='viridis', aspect='auto')\n",
        "\n",
        "                    vmin, vmax = mean_vals.min(), mean_vals.max()\n",
        "                    for i in range(mean_vals.shape[0]):\n",
        "                        for k in range(M_actual):\n",
        "                            val = mean_vals[i, k]\n",
        "                            color = 'white' if val < (vmin + 0.5 * (vmax - vmin)) else 'black'\n",
        "                            ax.text(k, i, f\"{val:.2f}\", ha='center', va='center', color=color, fontsize=8)\n",
        "\n",
        "                    ax.set_xticks(range(M_actual))\n",
        "                    ax.set_xticklabels(metrics_keys[:M_actual], rotation=45, ha='right')\n",
        "                    ax.set_yticks([0])\n",
        "                    ax.set_yticklabels(['Mean across nodes'])\n",
        "                    ax.set_title(\"Weighted FMT with Bounds\")\n",
        "                    fig.colorbar(im, ax=ax, label='Weighted Mean Metric Value')\n",
        "                    plt.tight_layout()\n",
        "                    plt.show()\n",
        "\n",
        "                def plot_node_score_contribution(self, metrics_keys=None):\n",
        "                    if metrics_keys is None:\n",
        "                        metrics_keys = self.MK\n",
        "                    D = self.D_graph\n",
        "                    node_contributions = np.array(self.node_score_contributions)\n",
        "\n",
        "                    if hasattr(self, 'weighted_fmt'):\n",
        "                        fuzzy_tensor = np.array(self.weighted_fmt)\n",
        "                    else:\n",
        "                        fuzzy_tensor = self.compute_fuzzy_metric_tensor(normalize=True)\n",
        "\n",
        "                    fuzzy_tensor_norm = (fuzzy_tensor - fuzzy_tensor.min()) / (fuzzy_tensor.max() - fuzzy_tensor.min() + 1e-12)\n",
        "                    fmt_matrix = fuzzy_tensor_norm.sum(axis=2)\n",
        "                    np.fill_diagonal(fmt_matrix, 0)\n",
        "\n",
        "                    raw_matrix = np.zeros((D, D))\n",
        "                    np.fill_diagonal(raw_matrix, node_contributions)\n",
        "\n",
        "                    total_matrix = raw_matrix + fmt_matrix\n",
        "\n",
        "                    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "                    matrices = [raw_matrix, fmt_matrix, total_matrix]\n",
        "                    titles = [\"Raw Node Contribution\", \"Normalized FMT Contribution\", \"Total Contribution\"]\n",
        "\n",
        "                    im = None\n",
        "                    for ax, mat, title in zip(axes, matrices, titles):\n",
        "                        im = ax.imshow(mat, cmap='viridis', vmin=0, vmax=1)\n",
        "                        for i in range(D):\n",
        "                            for j in range(D):\n",
        "                                ax.text(j, i, f\"{mat[i, j]:.2f}\", ha='center', va='center', color='white', fontsize=8)\n",
        "                        ax.set_title(title)\n",
        "                        ax.set_xticks(range(D))\n",
        "                        ax.set_xticklabels([f\"Node {i + 1}\" for i in range(D)])\n",
        "                        ax.set_yticks(range(D))\n",
        "                        ax.set_yticklabels([f\"Node {i + 1}\" for i in range(D)])\n",
        "\n",
        "                    fig.colorbar(im, ax=axes, orientation='vertical', fraction=0.025, pad=0.04, label='Contribution Value')\n",
        "                    plt.tight_layout()\n",
        "                    plt.show()\n",
        "\n",
        "                def plot_fmt_with_run_metrics(self, metrics_keys=None):\n",
        "                    if metrics_keys is None:\n",
        "                        metrics_keys = self.MK\n",
        "                    D = self.D_graph\n",
        "                    M_actual = len(metrics_keys)\n",
        "\n",
        "                    if not hasattr(self, 'capped_node_metrics'):\n",
        "                        raise ValueError(\"No node metrics available. Run run_outer() first.\")\n",
        "\n",
        "                    weighted_fmt = np.zeros((D, D, M_actual))\n",
        "                    for i in range(D):\n",
        "                        for j in range(D):\n",
        "                            for k, key in enumerate(metrics_keys):\n",
        "                                val = self.capped_node_metrics[j][key]\n",
        "                                if i != j:\n",
        "                                    val *= np.clip(abs(self.chosen_Gmat[i, j]), 0, 1)\n",
        "                                weighted_fmt[i, j, k] = val\n",
        "\n",
        "                    for i in range(D):\n",
        "                        for k in range(M_actual):\n",
        "                            if not METRIC_TARGET[i][k]:\n",
        "                                weighted_fmt[i, :, k] = 0.0\n",
        "\n",
        "                    mean_vals = weighted_fmt.mean(axis=1)\n",
        "\n",
        "                    fig, ax = plt.subplots(figsize=(1.2 * M_actual + 4, 0.35 * D + 4))\n",
        "                    im = ax.imshow(mean_vals, cmap='viridis', aspect='auto')\n",
        "\n",
        "                    vmin, vmax = mean_vals.min(), mean_vals.max()\n",
        "                    for i in range(D):\n",
        "                        for k in range(M_actual):\n",
        "                            val = mean_vals[i, k]\n",
        "                            color = 'white' if val < (vmin + 0.5 * (vmax - vmin)) else 'black'\n",
        "                            ax.text(k, i, f\"{val:.2f}\", ha='center', va='center', color=color, fontsize=8)\n",
        "\n",
        "                    ax.set_xticks(range(M_actual))\n",
        "                    ax.set_xticklabels(metrics_keys[:M_actual], rotation=45, ha='right')\n",
        "                    ax.set_yticks(range(D))\n",
        "                    ax.set_yticklabels([f\"Node {i}\" for i in range(D)])\n",
        "                    ax.set_title(\"Weighted FMT Metrics (Actual Run Output)\")\n",
        "                    fig.colorbar(im, ax=ax, label='Weighted Metric Value')\n",
        "                    plt.tight_layout()\n",
        "                    plt.show()\n",
        "\n",
        "                def collect_fmt_datapoints(self):\n",
        "                    self.fmt_datapoints = {k: [] for k in self.MK}\n",
        "                    for node_idx in range(self.D_graph):\n",
        "                        if len(self.PLMS[node_idx]) == 0:\n",
        "                            continue\n",
        "                        for weighted_input_val, metric_vals in self.PLMS[node_idx]:\n",
        "                            for m, key in enumerate(self.MK):\n",
        "                                if m < len(metric_vals):\n",
        "                                    self.fmt_datapoints[key].append((weighted_input_val, metric_vals[m]))\n",
        "\n",
        "                def collect_metric_traces_per_node(self):\n",
        "                    self.metric_traces_per_node = [{} for _ in range(self.D_graph)]\n",
        "                    for node_idx in range(self.D_graph):\n",
        "                        self.metric_traces_per_node[node_idx] = {k: [] for k in self.MK}\n",
        "                        if len(self.PLMS[node_idx]) == 0:\n",
        "                            continue\n",
        "                        for weighted_input_val, metric_vals in self.PLMS[node_idx]:\n",
        "                            for m, key in enumerate(self.MK):\n",
        "                                if m < len(metric_vals):\n",
        "                                    self.metric_traces_per_node[node_idx][key].append((weighted_input_val, metric_vals[m]))\n",
        "\n",
        "                def plot_fmt_per_datapoint(self, top_k=21, span=0.3, grid_size=100):\n",
        "                    if not hasattr(self, 'fmt_datapoints'):\n",
        "                        self.collect_fmt_datapoints()\n",
        "\n",
        "                    for key, formula in zip(self.MK, METRIC_FORMULAS):\n",
        "                        if key not in self.fmt_datapoints or len(self.fmt_datapoints[key]) == 0:\n",
        "                            continue\n",
        "\n",
        "                        data = np.array(self.fmt_datapoints[key])\n",
        "                        x_data, y_data = data[:, 0], data[:, 1]\n",
        "\n",
        "                        x_curve = np.linspace(x_data.min() - span, x_data.max() + span, grid_size)\n",
        "                        y_curve = np.array([formula(x) for x in x_curve])\n",
        "\n",
        "                        plt.figure(figsize=(6, 4))\n",
        "                        plt.scatter(x_data, y_data, alpha=0.6, label=\"FMT datapoints\")\n",
        "                        plt.plot(x_curve, y_curve, 'r', lw=2, label=\"Metric equation\")\n",
        "                        plt.xlabel(\"Weighted Input\")\n",
        "                        plt.ylabel(f\"{key} (FMT)\")\n",
        "                        plt.title(f\"FMT per datapoint - Metric: {key}\")\n",
        "                        plt.legend()\n",
        "                        plt.grid(alpha=0.3)\n",
        "                        plt.tight_layout()\n",
        "                        plt.show()\n",
        "\n",
        "                def plot_metric_equations_per_node(self, grid_size=100, span=0.3):\n",
        "                    if not hasattr(self, 'metric_traces_per_node'):\n",
        "                        self.collect_metric_traces_per_node()\n",
        "\n",
        "                    for node_idx in range(self.D_graph):\n",
        "                        node_traces = self.metric_traces_per_node[node_idx]\n",
        "                        if all(len(v) == 0 for v in node_traces.values()):\n",
        "                            continue\n",
        "\n",
        "                        plt.figure(figsize=(6, 4))\n",
        "                        for key, formula in zip(self.MK, METRIC_FORMULAS):\n",
        "                            if key not in node_traces or len(node_traces[key]) == 0:\n",
        "                                continue\n",
        "                            data = np.array(node_traces[key])\n",
        "                            x_data, y_data = data[:, 0], data[:, 1]\n",
        "\n",
        "                            x_curve = np.linspace(x_data.min() - span, x_data.max() + span, grid_size)\n",
        "                            y_curve = np.array([formula(x) for x in x_curve])\n",
        "\n",
        "                            plt.scatter(x_data, y_data, alpha=0.6, label=f\"{key} datapoints\")\n",
        "                            plt.plot(x_curve, y_curve, 'r', lw=2, label=f\"{key} equation\")\n",
        "\n",
        "                        plt.xlabel(\"Weighted Input\")\n",
        "                        plt.ylabel(\"Metric Value\")\n",
        "                        plt.title(f\"Node {node_idx} - Metric Equations\")\n",
        "                        plt.legend()\n",
        "                        plt.grid(alpha=0.3)\n",
        "                        plt.tight_layout()\n",
        "                        plt.show()\n",
        "\n",
        "\n",
        "            # =============================================================================\n",
        "            # MAIN EXECUTION BLOCK\n",
        "            # =============================================================================\n",
        "\n",
        "            if __name__ == \"__main__\":\n",
        "                # Ensure necessary globals exist before running; otherwise this block is illustrative\n",
        "                try:\n",
        "                    optimizer = Fuzzy_Hierarchical_Multiplex(\n",
        "                        candidate_dims, D_graph,\n",
        "                        synthetic_targets,\n",
        "                        gamma_interlayer=0,\n",
        "                        causal_flag=False\n",
        "                    )\n",
        "\n",
        "                    # Run Optimization\n",
        "                    metrics_list = optimizer.run()\n",
        "\n",
        "                    # Visualizations\n",
        "                # optimizer.plot_pointwise_minmax_elite()\n",
        "                    #optimizer.plot_nested_activations()\n",
        "\n",
        "                    # Compute FMT with elite bounds\n",
        "                    #fmt_elite_bounds = optimizer.compute_fmt_with_elite_bounds(top_k=top_k + 10)\n",
        "\n",
        "                    # Plot as heatmaps\n",
        "                    #optimizer.plot_fmt_with_run_metrics()\n",
        "\n",
        "                    # Compute fuzzy multiplex tensor\n",
        "                    #fmt_tensor = optimizer.compute_fuzzy_metric_tensor(normalize=False)\n",
        "                    #optimizer.plot_fuzzy_metric_tensor_heatmaps(fmt_tensor)\n",
        "\n",
        "                    # Plot Contributions & Graph\n",
        "                    #optimizer.plot_node_score_contribution()\n",
        "                    optimizer.plot_outer_fuzzy_graph()\n",
        "\n",
        "                    # Interactions\n",
        "                # tensor = optimizer.print_interactions()\n",
        "                    #print(\"Tensor shape:\", tensor.shape, '\\n', tensor)\n",
        "\n",
        "                    # Datapoints & Equations\n",
        "                    #optimizer.collect_fmt_datapoints()\n",
        "                    #optimizer.plot_fmt_per_datapoint()\n",
        "                    #optimizer.collect_metric_traces_per_node()\n",
        "                # optimizer.plot_metric_equations_per_node()\n",
        "\n",
        "                    # DSM Tracking Demo\n",
        "                    dsm_tracker = DSM_Tracker(optimizer)\n",
        "\n",
        "                    # Run extra DSM update\n",
        "                    optimizer.run_outer()\n",
        "                    dsm_tracker.update_dsms()\n",
        "\n",
        "                    # Retrieve matrices\n",
        "                    primary, residual = dsm_tracker.get_matrices()\n",
        "                    #print(\"Primary DSM:\\n\", primary)\n",
        "                    #rint(\"Residual DSM:\\n\", residual)\n",
        "\n",
        "                except NameError as e:\n",
        "                    print(f\"Error: Missing external dependency definition. \\n{e}\")\n",
        "                    print(\"Please ensure D_graph, DATA_MATRIX, METRIC_KEYS, etc. are defined.\")\n",
        "\n",
        "        # primary, residual = dsm_tracker.get_matrices()\n",
        "\n",
        "            import networkx as nx\n",
        "            import numpy as np\n",
        "\n",
        "            import networkx as nx\n",
        "            import numpy as np\n",
        "\n",
        "            class TopologicalEvaluator:\n",
        "                def __init__(self, dsm, node_metrics, node_types=None):\n",
        "                    \"\"\"\n",
        "                    :param dsm: The adjacency matrix (np.array)\n",
        "                    :param node_metrics: List of dicts containing 'cost', 'quality', etc.\n",
        "                    :param node_types: List of 'A' or 'B' types (inferred if None)\n",
        "                    \"\"\"\n",
        "                    self.dsm = np.array(dsm)\n",
        "                    self.metrics = node_metrics\n",
        "                    self.node_types = node_types if node_types else self._infer_types()\n",
        "                    self.G = self._build_graph()\n",
        "\n",
        "                def _infer_types(self):\n",
        "                    return [\"A\" if m.get('quality', 0) >= m.get('cost', 0) else \"B\" for m in self.metrics]\n",
        "\n",
        "                def _build_graph(self):\n",
        "                    G = nx.DiGraph()\n",
        "                    for i in range(len(self.dsm)):\n",
        "                        for j in range(len(self.dsm)):\n",
        "                            if self.dsm[i, j] > 0:\n",
        "                                G.add_edge(i, j, weight=self.dsm[i, j])\n",
        "                    return G\n",
        "\n",
        "                def calculate_system_pressure(self):\n",
        "                    \"\"\"Calculates total pressure based on P = F/A logic across the topology.\"\"\"\n",
        "                    total_p = 0\n",
        "                    edge_data = []\n",
        "\n",
        "                    for u, v in self.G.edges():\n",
        "                        # Force (Potential delta)\n",
        "                        v_u = np.array(list(self.metrics[u].values()))\n",
        "                        v_v = np.array(list(self.metrics[v].values()))\n",
        "                        force = np.linalg.norm(np.maximum(v_v - v_u, 0)) + 1e-9\n",
        "\n",
        "                        # Area (Friction of destination)\n",
        "                        area = np.mean(v_v) + 0.1\n",
        "\n",
        "                        # Pressure\n",
        "                        p = self.dsm[u, v] * (force / area)\n",
        "                        total_p += p\n",
        "                        edge_data.append({'link': (u, v), 'pressure': p})\n",
        "\n",
        "                    return total_p, edge_data\n",
        "\n",
        "                def evaluate_topology(self):\n",
        "                    # 1. Component Analysis\n",
        "                    sccs = list(nx.strongly_connected_components(self.G))\n",
        "                    cycles = [list(c) for c in sccs if len(c) > 1]\n",
        "\n",
        "                    # 2. Hierarchical Depth\n",
        "                    condensed = nx.condensation(self.G)\n",
        "                    layers = list(nx.topological_generations(condensed))\n",
        "\n",
        "                    # 3. System Pressure\n",
        "                    total_p, edge_pressures = self.calculate_system_pressure()\n",
        "\n",
        "                    # 4. NODE METRIC FITNESS (New Implementation)\n",
        "                    # Calculate how much leverage nodes have based on connectivity\n",
        "                    try:\n",
        "                        centrality = nx.degree_centrality(self.G)\n",
        "                    except:\n",
        "                        centrality = {i: 0 for i in range(len(self.metrics))}\n",
        "\n",
        "                    metric_score = 0\n",
        "                    for i, m in enumerate(self.metrics):\n",
        "                        # Net utility of the node\n",
        "                        val = m.get('quality', 0) - m.get('cost', 0)\n",
        "                        # Boost score if the node is topologically significant\n",
        "                        metric_score += val * (1 + centrality.get(i, 0))\n",
        "\n",
        "                    # 5. Final Fitness Calculation\n",
        "                    # Combines Structural Depth (Stability) - Pressure (Friction) + Metric Performance (Value)\n",
        "                    fitness = (len(layers) * 5) - (total_p * 0.25) + (metric_score * 10)\n",
        "\n",
        "                    return {\n",
        "                        \"hierarchy_depth\": len(layers),\n",
        "                        \"cycle_count\": len(cycles),\n",
        "                        \"system_pressure\": round(total_p, 4),\n",
        "                        \"net_metric_value\": round(metric_score, 4),\n",
        "                        \"fitness_score\": round(fitness, 2),\n",
        "                        \"bottlenecks\": sorted(edge_pressures, key=lambda x: x['pressure'], reverse=True)[:3]\n",
        "                    }\n",
        "\n",
        "            import numpy as np\n",
        "\n",
        "            # 1. GENERATE ACTUAL STATE DATA\n",
        "            # We must turn the generator functions into a snapshot of data\n",
        "            generators = GENERATOR_MAP\n",
        "            current_state = {node: gen() for node, gen in generators.items()}\n",
        "\n",
        "            # 2. TRANSFORM STATE INTO EVALUATOR-FRIENDLY METRICS\n",
        "            # The evaluator looks for 'quality' and 'cost'. We map your architectural KPIs:\n",
        "            # Quality = performance-based metrics (safety, aesthetics, completeness)\n",
        "            # Cost = resource-based metrics (budget, tonnage, violations)\n",
        "            evaluator_metrics = []\n",
        "            METRIC_MAP = metric_feature_map\n",
        "            node_list = list(METRIC_MAP.keys())\n",
        "\n",
        "            for node in node_list:\n",
        "                node_data = current_state.get(node, {})\n",
        "                feat_key = METRIC_MAP[node][0]\n",
        "                category = METRIC_MAP[node][1]\n",
        "\n",
        "                val = node_data.get(feat_key, 0.5)\n",
        "\n",
        "                # Logic: If it's a 'Prevention' or 'Alpha' category, it's Quality.\n",
        "                # If it's 'Efficiency' or 'Total Cost', it relates to Cost.\n",
        "                if category in [\"Regulatory_Alpha\", \"Clinical_Safety\", \"Loss_Ratio_Prevention\"]:\n",
        "                    evaluator_metrics.append({\"quality\": val, \"cost\": 0.1}) # Low baseline cost\n",
        "                else:\n",
        "                    evaluator_metrics.append({\"quality\": 0.5, \"cost\": val / 1000.0}) # Scale budget to 0-1 range\n",
        "\n",
        "            # 3. RUN EVALUATOR\n",
        "            # Ensure 'primary' (your DSM) is indexed in the same order as node_list\n",
        "            evaluator = TopologicalEvaluator(primary, evaluator_metrics)\n",
        "            report = evaluator.evaluate_topology()\n",
        "\n",
        "\n",
        "            # 4. OUTPUT RESULTS\n",
        "                    # Unify all analysis and print logic into a single command\n",
        "            import numpy as np\n",
        "\n",
        "            def run_unified_audit(primary_matrix, target_dsm, evaluator_metrics, node_list):\n",
        "                \"\"\"\n",
        "                Unified Topological Audit comparing the 'Primary' system against the 'Target DSM'.\n",
        "                \"\"\"\n",
        "                # 1. Setup Evaluators for both matrices\n",
        "                # We are comparing Primary (The baseline/current state) vs DSM (The target/proposed state)\n",
        "                eval_primary = TopologicalEvaluator(primary_matrix, evaluator_metrics)\n",
        "                eval_target = TopologicalEvaluator(target_dsm, evaluator_metrics)\n",
        "\n",
        "                # 2. Process Reports\n",
        "                res_p = eval_primary.evaluate_topology()\n",
        "                res_t = eval_target.evaluate_topology()\n",
        "\n",
        "                # 3. UNIFIED OUTPUT TABLE\n",
        "                print(f\"\\n{' TOPOLOGICAL SYSTEM AUDIT: PRIMARY VS. TARGET DSM ':=^75}\")\n",
        "                print(f\"{'KPI Indicator':<32} | {'PRIMARY':<15} | {'TARGET DSM':<15} | {'DELTA':<10}\")\n",
        "                print(\"-\" * 75)\n",
        "\n",
        "                metrics_map = [\n",
        "                    (\"Fitness Score (0-100)\", res_p['fitness_score'], res_t['fitness_score']),\n",
        "                    (\"System Pressure (Pa)\", res_p['system_pressure'], res_t['system_pressure']),\n",
        "                    (\"Sequential Hierarchy Depth\", res_p['hierarchy_depth'], res_t['hierarchy_depth']),\n",
        "                    (\"Coupled Feedback Cycles\", res_p['cycle_count'], res_t['cycle_count'])\n",
        "                ]\n",
        "\n",
        "                for label, p_val, t_val in metrics_map:\n",
        "                    print(f\"{label:<32} | {p_val:<15.2f} | {t_val:<15.2f} | {t_val - p_val:+.2f}\")\n",
        "\n",
        "                print(\"=\" * 75)\n",
        "\n",
        "                # 4. PRIMARY BOTTLENECK\n",
        "                if res_p['bottlenecks']:\n",
        "                    p_top = res_p['bottlenecks'][0]\n",
        "                    p_src, p_dst = p_top['link']\n",
        "                    print(f\"PRIMARY BOTTLENECK: {node_list[p_src]} â†’ {node_list[p_dst]}\")\n",
        "                    print(f\"Friction Magnitude: {p_top['pressure']:.4f}\")\n",
        "\n",
        "                print(\"-\" * 75)\n",
        "\n",
        "                # 5. TARGET DSM BOTTLENECK\n",
        "                if res_t['bottlenecks']:\n",
        "                    t_top = res_t['bottlenecks'][0]\n",
        "                    t_src, t_dst = t_top['link']\n",
        "                    print(f\"TARGET DSM BOTTLENECK: {node_list[t_src]} â†’ {node_list[t_dst]}\")\n",
        "                    print(f\"Friction Magnitude: {t_top['pressure']:.4f}\")\n",
        "\n",
        "            # --- EXECUTION ---\n",
        "            # This compares the Primary matrix against the target DSM directly.\n",
        "            run_unified_audit(primary, DSM, evaluator_metrics, node_list)\n",
        "            import numpy as np\n",
        "            import pandas as pd\n",
        "            import networkx as nx\n",
        "\n",
        "            import numpy as np\n",
        "            import pandas as pd\n",
        "            import networkx as nx\n",
        "\n",
        "            class GenericServiceSimulator:\n",
        "                def __init__(self, touchpoints, generator_map):\n",
        "                    self.TOUCHPOINTS = touchpoints\n",
        "                    self.GENERATOR_MAP = generator_map\n",
        "\n",
        "                def evaluate_dsm(self, dsm_matrix, label=\"Generic Run\"):\n",
        "                    # [Existing logic for Graph building, SCCs, and Simulation...]\n",
        "                    D = dsm_matrix.shape[0]\n",
        "                    G = nx.DiGraph()\n",
        "                    for i in range(D):\n",
        "                        G.add_node(self.TOUCHPOINTS[i])\n",
        "                        for j in range(D):\n",
        "                            if dsm_matrix[i, j] > 0:\n",
        "                                G.add_edge(self.TOUCHPOINTS[i], self.TOUCHPOINTS[j])\n",
        "\n",
        "                    # Identify Components\n",
        "                    sccs = list(nx.strongly_connected_components(G))\n",
        "                    node_to_module = {}\n",
        "                    module_details = {}\n",
        "                    detected_feedback_loops = []\n",
        "\n",
        "                    for idx, comp in enumerate(sccs):\n",
        "                        comp_list = list(comp)\n",
        "                        comp_list.sort(key=lambda x: self.TOUCHPOINTS.index(x))\n",
        "\n",
        "                        for node in comp_list:\n",
        "                            node_to_module[node] = idx\n",
        "\n",
        "                        is_coupled = len(comp_list) > 1\n",
        "                        cluster_name = \" + \".join(comp_list) if is_coupled else comp_list[0]\n",
        "\n",
        "                        # Detect Loops\n",
        "                        if is_coupled:\n",
        "                            for u in comp_list:\n",
        "                                for v in comp_list:\n",
        "                                    if G.has_edge(u, v):\n",
        "                                        if self.TOUCHPOINTS.index(u) > self.TOUCHPOINTS.index(v):\n",
        "                                            detected_feedback_loops.append({\n",
        "                                                \"cluster\": cluster_name,\n",
        "                                                \"description\": f\"Feedback: {u} -> {v}\"\n",
        "                                            })\n",
        "\n",
        "                        module_details[idx] = {\n",
        "                            \"name\": cluster_name,\n",
        "                            \"type\": \"Coupled Cluster\" if is_coupled else \"Linear Unit\",\n",
        "                            \"members\": comp_list,\n",
        "                            \"size\": len(comp_list)\n",
        "                        }\n",
        "\n",
        "                    # Leveling\n",
        "                    MG = nx.DiGraph()\n",
        "                    MG.add_nodes_from(range(len(sccs)))\n",
        "                    for u, v in G.edges():\n",
        "                        mu, mv = node_to_module[u], node_to_module[v]\n",
        "                        if mu != mv:\n",
        "                            MG.add_edge(mu, mv)\n",
        "\n",
        "                    try:\n",
        "                        levels = list(nx.topological_generations(MG))\n",
        "                    except nx.NetworkXUnfeasible:\n",
        "                        levels = [list(MG.nodes())]\n",
        "\n",
        "                    # Execution\n",
        "                    execution_log = []\n",
        "                    process_time_steps = 0\n",
        "                    total_rework_risk = 0.0\n",
        "\n",
        "                    for level_idx, modules_in_level in enumerate(levels, 1):\n",
        "                        process_time_steps += 1\n",
        "                        for mod_idx in modules_in_level:\n",
        "                            details = module_details[mod_idx]\n",
        "\n",
        "                            if details[\"size\"] > 1:\n",
        "                                total_rework_risk += (details[\"size\"] ** 1.5) * 0.15\n",
        "\n",
        "                            for tp in details[\"members\"]:\n",
        "                                data = self.GENERATOR_MAP[tp]()\n",
        "                                for k, v in data.items():\n",
        "                                    execution_log.append({\n",
        "                                        \"Run Label\": label,\n",
        "                                        \"Time Step\": level_idx,\n",
        "                                        \"Cluster Name\": details[\"name\"],\n",
        "                                        \"Node\": tp,\n",
        "                                        \"Metric\": k,\n",
        "                                        \"Value\": v\n",
        "                                    })\n",
        "\n",
        "                    results = {\n",
        "                        \"label\": label,\n",
        "                        \"total_nodes\": D,\n",
        "                        \"total_time_steps\": process_time_steps,\n",
        "                        \"rework_risk_score\": round(total_rework_risk, 2),\n",
        "                        \"feedback_loops\": detected_feedback_loops,\n",
        "                        \"structure\": module_details,\n",
        "                        \"execution_df\": pd.DataFrame(execution_log)\n",
        "                    }\n",
        "\n",
        "                    # *** NEW: Calculate Fitness Immediately ***\n",
        "                    results[\"fitness_analysis\"] = self.calculate_design_fitness(results)\n",
        "                    return results\n",
        "\n",
        "                def calculate_design_fitness(self, results):\n",
        "                    \"\"\"\n",
        "                    Derives a fitness score, classifies the design archetype,\n",
        "                    and stores critical performance metrics.\n",
        "                    \"\"\"\n",
        "                    N = results['total_nodes']\n",
        "                    steps = results['total_time_steps']\n",
        "                    risk = results['rework_risk_score']\n",
        "\n",
        "                    # 1. PARALLELISM SCORE (Efficiency)\n",
        "                    # Avoid division by zero\n",
        "                    safe_steps = steps if steps > 0 else 1\n",
        "                    parallelism_ratio = N / safe_steps\n",
        "\n",
        "                    # Cap generic parallelism bonuses at 20 points\n",
        "                    speed_score = min(parallelism_ratio * 10, 20)\n",
        "\n",
        "                    # 2. STABILITY SCORE (Safety)\n",
        "                    # Higher risk reduces score drastically\n",
        "                    # Perfect score is 10. Risk of 5 reduces it to ~1.6\n",
        "                    stability_score = 10 / (1 + (risk * 0.5))\n",
        "\n",
        "                    # 3. COMPOSITE FITNESS\n",
        "                    # Balanced mix: Speed (40%) and Stability (60%)\n",
        "                    raw_fitness = (speed_score * 0.4) + (stability_score * 0.6)\n",
        "\n",
        "                    # 4. DESIGN IMPLICATION (Archetype Classification)\n",
        "                    implication = \"Unknown\"\n",
        "                    if parallelism_ratio >= 1.5 and risk > 1.0:\n",
        "                        implication = \"Agile / High Velocity\" # Fast but risky\n",
        "                    elif parallelism_ratio < 1.2 and risk < 0.5:\n",
        "                        implication = \"Waterfall / Bureaucratic\" # Stable but slow\n",
        "                    elif parallelism_ratio >= 1.5 and risk < 0.5:\n",
        "                        implication = \"Ideal Architecture\" # Fast & Stable (Rare)\n",
        "                    elif parallelism_ratio < 1.2 and risk > 1.0:\n",
        "                        implication = \"Chaotic / Bottlenecked\" # Slow & Risky (Worst case)\n",
        "                    else:\n",
        "                        implication = \"Hybrid / Moderate\"\n",
        "\n",
        "                    # 5. CONSTRUCT RESULT DICTIONARY\n",
        "                    return {\n",
        "                        # --- The Requested Fields ---\n",
        "                        \"design_archetype\": implication, # The classification\n",
        "                        \"total_steps\": steps,            # Raw duration\n",
        "                        \"rework_risk\": round(risk, 4),   # Raw risk\n",
        "\n",
        "                        # --- Calculation Details ---\n",
        "                        \"parallelism_ratio\": round(parallelism_ratio, 2),\n",
        "                        \"speed_points\": round(speed_score, 2),\n",
        "                        \"stability_points\": round(stability_score, 2),\n",
        "                        \"composite_score\": round(raw_fitness, 2)\n",
        "                    }\n",
        "\n",
        "            # ==========================================\n",
        "            # UPDATED PRINT HELPER\n",
        "            # ==========================================\n",
        "            def print_fitness_comparison(res_b, res_a):\n",
        "                print(f\"\\n{'='*80}\")\n",
        "                print(f\" DESIGN FITNESS COMPARISON\")\n",
        "                print(f\"{'='*80}\")\n",
        "\n",
        "                # Headers\n",
        "                print(f\"{'Metric':<30} | {res_a['label']:<22} | {res_b['label']:<22}\")\n",
        "                print(f\"{'-'*30} | {'-'*22} | {'-'*22}\")\n",
        "\n",
        "                fit_a = res_a['fitness_analysis']\n",
        "                fit_b = res_b['fitness_analysis']\n",
        "\n",
        "                # Core Stats from the new dictionary structure\n",
        "                print(f\"{'Design Archetype':<30} | {fit_a['design_archetype']:<22} | {fit_b['design_archetype']:<22}\")\n",
        "                print(f\"{'Total Steps':<30} | {fit_a['total_steps']:<22} | {fit_b['total_steps']:<22}\")\n",
        "                print(f\"{'Rework Risk':<30} | {fit_a['rework_risk']:<22} | {fit_b['rework_risk']:<22}\")\n",
        "                print(f\"{'-'*30} | {'-'*22} | {'-'*22}\")\n",
        "                print(f\"{'Parallelism Ratio':<30} | {fit_a['parallelism_ratio']:<22} | {fit_b['parallelism_ratio']:<22}\")\n",
        "                print(f\"{'Composite Fitness (0-10)':<30} | {fit_a['composite_score']:<22} | {fit_b['composite_score']:<22}\")\n",
        "                print(f\"{'='*80}\")\n",
        "\n",
        "            # ==========================================\n",
        "            # 1. DEFINE GENERIC INPUTS\n",
        "            # ==========================================\n",
        "\n",
        "            # Generic Touchpoints (Nodes A through E)\n",
        "            generic_touchpoints = TOUCHPOINTS\n",
        "            # Generic Generator Map (Abstract Work)\n",
        "            # - Node A: Input Source (High variance inputs)\n",
        "            # - Node B/C: Processing (Efficiency metrics)\n",
        "            # - Node D/E: Output/Control (Quality metrics)\n",
        "            generic_generator_map = GENERATOR_MAP\n",
        "\n",
        "            # ==========================================\n",
        "            # 2. DEFINE GENERIC ARCHITECTURES (DSMs)\n",
        "            # ==========================================\n",
        "            N = D_graph\n",
        "\n",
        "            # A. SERIAL PROCESS (0->1->2->3->4)\n",
        "            serial_dsm = np.zeros((N, N))\n",
        "            for i in range(N-1):\n",
        "                serial_dsm[i, i+1] = 1\n",
        "\n",
        "            # B. INTEGRATED PROCESS (Coupled Core)\n",
        "            # Node B (1), C (2), D (3) form a feedback loop\n",
        "            integrated_dsm = serial_dsm.copy()\n",
        "            integrated_dsm[3, 1] = 1 # D feeds back to B\n",
        "\n",
        "            # ==========================================\n",
        "            # 3. RUN SIMULATION\n",
        "            # ==========================================\n",
        "\n",
        "            sim = GenericServiceSimulator(generic_touchpoints, generic_generator_map)\n",
        "\n",
        "            # Run Serial\n",
        "            res_serial = sim.evaluate_dsm(DSM, label=\"Serial Flow\")\n",
        "\n",
        "            # Run Integrated\n",
        "            res_integrated = sim.evaluate_dsm(primary, label=\"Integrated Core\")\n",
        "\n",
        "            # ==========================================\n",
        "            # 4. VIEW RESULTS\n",
        "            # ==========================================\n",
        "\n",
        "            print(f\"--- COMPARISON (Nodes: {len(generic_touchpoints)}) ---\")\n",
        "            print(f\"Serial Duration:     {res_serial['total_time_steps']} steps (Risk: {res_serial['rework_risk_score']})\")\n",
        "            print(f\"Integrated Duration: {res_integrated['total_time_steps']} steps (Risk: {res_integrated['rework_risk_score']})\")\n",
        "\n",
        "            print(\"\\n--- DETAILED EXECUTION LOG (INTEGRATED) ---\")\n",
        "            df = res_integrated['execution_df']\n",
        "            # Show how Node B, C, D ran in the same Time Step\n",
        "            target_nodes = TOUCHPOINTS[1:4]\n",
        "\n",
        "            results = sim.evaluate_dsm(primary, label=\"Feedback Analysis\")\n",
        "\n",
        "            print(f\"Filtering for: {target_nodes}\")\n",
        "            print(df[df['Node'].isin(target_nodes)][['Time Step', 'Node', 'Metric', 'Value']].head(6))\n",
        "            print(\"--- STRUCTURAL ANALYSIS ---\")\n",
        "            print(f\"Rework Risk Score: {results['rework_risk_score']}\")\n",
        "\n",
        "            print(\"\\n[1] IDENTIFIED MODULES/CLUSTERS:\")\n",
        "            for idx, details in results['structure'].items():\n",
        "                print(f\"  Module {idx}: {details['name']} ({details['type']})\")\n",
        "\n",
        "            print(\"\\n[2] DETECTED FEEDBACK LOOPS:\")\n",
        "            if results['feedback_loops']:\n",
        "                for loop in results['feedback_loops']:\n",
        "                    print(f\"  Warning: {loop['description']}\")\n",
        "                    print(f\"           (Found inside cluster: '{loop['cluster']}')\")\n",
        "            else:\n",
        "                print(\"  None detected.\")\n",
        "\n",
        "            print(\"\\n[3] EXECUTION SNAPSHOT (Coupled Block):\")\n",
        "            df = results['execution_df']\n",
        "            # Filter for the coupled cluster name dynamically\n",
        "            coupled_cluster_name = [d['name'] for d in results['structure'].values() if d['type'] == 'Coupled Cluster']\n",
        "            if coupled_cluster_name:\n",
        "                print(df[df['Cluster Name'] == coupled_cluster_name[0]].head(3).to_string(index=False))\n",
        "\n",
        "\n",
        "\n",
        "            res_serial = sim.evaluate_dsm(DSM, label=\"Linear Baseline\")\n",
        "            res_integrated = sim.evaluate_dsm(primary, label=\"Integrated Loop\")\n",
        "\n",
        "            # --- Compare Fitness ---\n",
        "            print_fitness_comparison(res_serial, res_integrated)\n",
        "\n",
        "            print(f\"\\n[Detailed Implication for Integrated Run]\")\n",
        "            print(f\"Why this score? Because we found {len(res_integrated['feedback_loops'])} feedback loops.\")\n",
        "            if res_integrated['fitness_analysis']['composite_score'] < res_serial['fitness_analysis']['composite_score']:\n",
        "                print(\">> CONCLUSION: The gain in speed did not justify the cost of coordination risk.\")\n",
        "            else:\n",
        "                print(\">> CONCLUSION: The design is robust enough to handle the complexity.\")\n",
        "\n",
        "\n",
        "            all_results.append(sim.evaluate_dsm(primary, label=\"Integrated (Core)\"))\n",
        "\n"
      ],
      "metadata": {
        "id": "uvgYhXoaVrxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DATA <--> METRICS <--> TP/DSM <--> GENERIC_SIMULATOR"
      ],
      "metadata": {
        "id": "9j9b5wHYe8I9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class ParetoAnalyzer:\n",
        "    @staticmethod\n",
        "    def identify_pareto_efficient(costs):\n",
        "        \"\"\"\n",
        "        Finds the Pareto-efficient points in a cloud of data.\n",
        "        :param costs: (n_points, 2) array where columns are (Steps, Risk).\n",
        "                      Assumes LOWER is better for both.\n",
        "        :return: Boolean mask of efficient points.\n",
        "        \"\"\"\n",
        "        is_efficient = np.ones(costs.shape[0], dtype=bool)\n",
        "        for i, c in enumerate(costs):\n",
        "            if is_efficient[i]:\n",
        "                # Keep points where no other point is strictly better\n",
        "                # A point j dominates i if:\n",
        "                # step[j] <= step[i] AND risk[j] <= risk[i]\n",
        "                # AND at least one is strictly smaller.\n",
        "                is_efficient[is_efficient] = np.any(costs[is_efficient] < c, axis=1)  | \\\n",
        "                                             np.any(costs[is_efficient] == c, axis=1)\n",
        "                is_efficient[i] = True  # Keep self\n",
        "        return is_efficient\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_tradeoff_landscape(results_list):\n",
        "        \"\"\"\n",
        "        Visualizes Steps vs Risk with the Pareto Front highlighted.\n",
        "        \"\"\"\n",
        "        # 1. Extract Data\n",
        "        data = []\n",
        "        labels = []\n",
        "        archetypes = []\n",
        "\n",
        "        for res in results_list:\n",
        "            fit = res['fitness_analysis']\n",
        "            # We want to MINIMIZE Steps and MINIMIZE Risk\n",
        "            data.append([fit['total_steps'], fit['rework_risk']])\n",
        "            labels.append(res['label'])\n",
        "            archetypes.append(fit['design_archetype'])\n",
        "\n",
        "        points = np.array(data)\n",
        "\n",
        "        # 2. Calculate Frontier\n",
        "        # Simple sorting method for 2D Pareto:\n",
        "        # Sort by X (Steps). The Pareto front are points where Y (Risk) is lower than all previous Ys.\n",
        "        sorted_indices = np.argsort(points[:, 0])\n",
        "        sorted_points = points[sorted_indices]\n",
        "\n",
        "        pareto_front = []\n",
        "        current_min_risk = float('inf')\n",
        "\n",
        "        pareto_indices = []\n",
        "\n",
        "        for idx in sorted_indices:\n",
        "            step, risk = points[idx]\n",
        "            if risk < current_min_risk:\n",
        "                pareto_front.append([step, risk])\n",
        "                pareto_indices.append(idx)\n",
        "                current_min_risk = risk\n",
        "\n",
        "        pareto_front = np.array(pareto_front)\n",
        "\n",
        "        # 3. Plotting\n",
        "        plt.figure(figsize=(12, 7))\n",
        "\n",
        "        # Color map for archetypes\n",
        "        unique_archs = list(set(archetypes))\n",
        "        colors = plt.cm.get_cmap('tab10', len(unique_archs))\n",
        "        color_map = {arch: colors(i) for i, arch in enumerate(unique_archs)}\n",
        "\n",
        "        # Scatter all points\n",
        "        for i, point in enumerate(points):\n",
        "            plt.scatter(point[0], point[1], c=[color_map[archetypes[i]]],\n",
        "                        s=100, alpha=0.7, edgecolors='gray')\n",
        "\n",
        "            # Annotate interesting points (Pareto or outliers)\n",
        "            if i in pareto_indices:\n",
        "                plt.text(point[0], point[1]+0.05, labels[i], fontsize=9, fontweight='bold')\n",
        "\n",
        "        # Draw Pareto Line\n",
        "        if len(pareto_front) > 0:\n",
        "            plt.plot(pareto_front[:, 0], pareto_front[:, 1], 'r--', linewidth=2, label='Pareto Frontier', alpha=0.6)\n",
        "            plt.fill_between(pareto_front[:, 0], pareto_front[:, 1], points[:, 1].max(), color='red', alpha=0.05)\n",
        "\n",
        "        # Legend creation\n",
        "        from matplotlib.lines import Line2D\n",
        "        legend_elements = [Line2D([0], [0], color='r', linestyle='--', label='Pareto Frontier')]\n",
        "        for arch, col in color_map.items():\n",
        "            legend_elements.append(Line2D([0], [0], marker='o', color='w', markerfacecolor=col, label=arch, markersize=10))\n",
        "\n",
        "        plt.title(\"Design Space Trade-offs: Efficiency vs Stability\", fontsize=14)\n",
        "        plt.xlabel(\"Total Time Steps (Lower is Faster)\", fontsize=12)\n",
        "        plt.ylabel(\"Rework Risk Score (Lower is Safer)\", fontsize=12)\n",
        "        plt.legend(handles=legend_elements)\n",
        "        plt.grid(True, linestyle=':', alpha=0.6)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # 4. Print Table\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\" PARETO EFFICIENT DESIGNS (The 'Sweet Spots')\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"{'Design Label':<20} | {'Steps':<8} | {'Risk':<8} | {'Archetype'}\")\n",
        "        print(f\"{'-'*20} | {'-'*8} | {'-'*8} | {'-'*15}\")\n",
        "\n",
        "        for idx in pareto_indices:\n",
        "            fit = results_list[idx]['fitness_analysis']\n",
        "            print(f\"{results_list[idx]['label']:<20} | {fit['total_steps']:<8} | {fit['rework_risk']:<8} | {fit['design_archetype']}\")\n",
        "        print(f\"{'='*60}\")\n",
        "ParetoAnalyzer.plot_tradeoff_landscape(all_results)\n"
      ],
      "metadata": {
        "id": "u_yrXF9qZW2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pIMnjU_Bc2XG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y1ef4-QkQ1Sp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ms_KNqwDV3y6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}