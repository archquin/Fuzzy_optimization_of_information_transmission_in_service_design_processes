# -*- coding: utf-8 -*-
"""Service_optimization_with_fuzzy_multiplex.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Jz6NbTiJ5W00lTDfof9j3eLxOnroXUe6

# Abstract
This time more realistic and sophisticated data is generated for our optimizer; in addition to the problem of basic per process metrics a global cost constrain is introduced. Furthermore, synthetic data that represents 4 major problems is generated, normalized, and used as inputs. A fuzzy multiplex optimizer is then used after some adaptations to make it more flexible and fast to find optimal outputs.

Lastly, about this fuzzy mutiplex structure the idea is to model services using the multiplex structure; the objective is to create a two layered structure, having an inner layer that emulates the process provides local inputs that contribute partially to the service; as for the outer layer it can be optimized globally and re adjust partial contributions, using metaheuristics, optimally.

## - Synthetic Data

Our synthetic data now is more complicated, 4 major problems are addressed, which are route planning, vehicle assignment, time scheduling, and dynamic rerouting. Then the data is normalized, padded, and used as target; a casual mask can be applied on the optimizer to retain meta-data relationships about the information, e.g. the speed base of a vehicle. Each process is optimized for utility, wait time, and throughput; these generic metrics are global and provide an accurate representation of the service demands
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import networkx as nx

# ---------------- Synthetic dataset ----------------
np.random.seed(42)
N = 1000

route_planning = pd.DataFrame({
    'origin_x': np.random.uniform(0, 100, N),
    'origin_y': np.random.uniform(0, 100, N),
    'dest_x': np.random.uniform(0, 100, N),
    'dest_y': np.random.uniform(0, 100, N),
    'traffic_density': np.random.uniform(0, 1, N),
    'road_type': np.random.choice([1, 2, 3], N),
})
route_planning['distance'] = np.sqrt((route_planning['dest_x'] - route_planning['origin_x'])**2 +
                                     (route_planning['dest_y'] - route_planning['origin_y'])**2)
speed_base = {1:50, 2:40, 3:30}
route_planning['speed'] = route_planning['road_type'].map(speed_base) * np.random.uniform(0.8,1.2,N)
route_planning['travel_time'] = (route_planning['distance']/route_planning['speed'])*60*\
                                (1+route_planning['traffic_density']*np.random.uniform(0.1,0.5,N))

vehicle_assignment = pd.DataFrame({
    'vehicle_capacity': np.random.randint(50,200,N),
    'battery_level': np.random.uniform(0.3,1.0,N),
    'delivery_size': np.random.randint(5,50,N),
    'vehicle_type': np.random.choice([1,2],N),
    'speed_factor': np.random.uniform(0.9,1.1,N),
})
vehicle_assignment['assigned_speed'] = route_planning['speed']*vehicle_assignment['speed_factor']
vehicle_assignment['load_utilization'] = vehicle_assignment['delivery_size']/vehicle_assignment['vehicle_capacity']

time_scheduling = pd.DataFrame({
    'requested_time': np.random.randint(8,20,N),
    'delivery_priority': np.random.randint(1,5,N),
    'customer_patience': np.random.uniform(0,1,N),
})
time_scheduling['delay_probability'] = np.clip(
    (route_planning['travel_time']/60)*(1+vehicle_assignment['load_utilization']*0.5)*np.random.uniform(0.8,1.2,N),
    0,1
)

dynamic_rerouting = pd.DataFrame({
    'current_x': np.random.uniform(0,100,N),
    'current_y': np.random.uniform(0,100,N),
    'traffic_updates': np.random.uniform(0,1,N),
    'new_delivery_requests': np.random.randint(0,3,N),
    'vehicle_status': np.random.choice([0,1],N),
    'weather': np.random.choice([0,1],N),
})
dynamic_rerouting['congestion_score'] = dynamic_rerouting['traffic_updates'] + \
                                       dynamic_rerouting['new_delivery_requests']*0.5 + \
                                       dynamic_rerouting['weather']*0.5 + \
                                       (route_planning['travel_time']/route_planning['travel_time'].max())*0.5

# ---------------- Combine and normalize ----------------
datasets = [route_planning, vehicle_assignment, time_scheduling, dynamic_rerouting]
dataset_dims = [df.shape[1] for df in datasets]
max_dim = max(dataset_dims)

padded_data = []
for df in datasets:
    arr = df.values
    if arr.shape[1] < max_dim:
        arr = np.hstack([arr, np.zeros((arr.shape[0], max_dim - arr.shape[1]))])
    padded_data.append(arr)

DATA_MATRIX = np.hstack(padded_data)
DATA_MATRIX = (DATA_MATRIX - DATA_MATRIX.min(axis=0)) / (np.ptp(DATA_MATRIX, axis=0)+1e-8)

def generate_targets(DATA_MATRIX, candidate_dims, D_graph):
    targets = []
    for node_idx in range(D_graph):
        row = DATA_MATRIX[node_idx % len(DATA_MATRIX)]
        node_targets = {}
        for dim in candidate_dims:
            if len(row) >= dim:
                sampled = row[:dim]
            else:
                sampled = np.pad(row, (0, dim - len(row)), constant_values=0.5)
            node_targets[dim] = sampled
        targets.append(node_targets)
    return targets
candidate_dims = [6,6,6,6,6,6]#,6,6,6,6,6,6,6,6,6,6,6,6,]
D_graph = 4
synthetic_targets = generate_targets(DATA_MATRIX, candidate_dims, D_graph)

"""# - Constrained fuzzy multiplex optimizer for a given cost

Here, the mechanics of the previous optimizer are still utilized with some aadjustments. Now instead of using a storage for the solutions only the genome is updated, inner-outer, which provides the answer to the optimization problem. Nonetheless, it is more unstable yet it can evolve the genome as well as the metrics better. Finally, the same co-ecolution dynamics as well as layers and co-adaptation mechanics are implemented for the generated data; information transmission is still included but is now implicit. That is to say, the relations among service processes that are plotted.

In this example there are at most six variables contained in any given task, i.e. the input processess targets, hence the candidate inner dimensions of the input processes are six. As it happens, many candidate dimensions are used to evolve the system because it works more efficiently; given the choice to use more or less depth one misses the targets eventually unless the candidate dimensions are correct. Note that depth corelates with the learning rate as well as the outer generations. Additionally, the inter layer was not really used at this point but it will be later. Moreover, the outer graph dimension is 4; there are four porcesses defining the service hence the number
"""

# ---------------- Targets ----------------#
#candidate_dims = [6,6,6,6,6,6,6]#,6,6,6,6,6,6,6,6,6,6,6,6,]
#D_graph = 4
inner_archive_size = 120
inner_offspring = 80
outer_archive_size = 80
outer_offspring = 80
inner_iters_per_outer = 5
outer_generations = 250
outer_cost_limit = 250
inner_learning = 0.6
seed = 42
np.random.seed(seed)

# ---------------- Metrics Evaluator ----------------
class MetricsEvaluator:
    def __init__(self, data_matrix):
        self.data_matrix = data_matrix
        self.num_features = data_matrix.shape[1]
        self.W0 = 10.0
        self.T0 = 100.0
        self.U0 = 0.5

    def compute_node_metrics(self, node_idx, y=None):
        row = self.data_matrix[node_idx % self.data_matrix.shape[0]]
        k = self.num_features
        base = k // 3 if k >= 3 else 1
        wait_cols = list(range(0, base))
        thr_cols = list(range(base, 2*base))
        util_cols = list(range(2*base, k))
        wait_signal = np.mean(row[wait_cols])
        throughput_signal = np.mean(row[thr_cols])
        util_signal = np.mean(row[util_cols])
        if y is None: y = np.array([0.5,0.5,0.5])
        else: y = np.array(y[:3]) if len(y)>=3 else np.pad(y,(0,3-len(y)),constant_values=0.5)
        wait = self.W0*(1+1.2*wait_signal +0.8*y[0])
        throughput = self.T0*(1+1.1*throughput_signal +0.6*y[1]-0.4*wait_signal)
        util = self.U0 + 0.8*util_signal + 0.6*y[2]
        wait = float(np.clip(wait,0,100))
        throughput = float(np.clip(throughput,0,150))
        util = float(np.clip(util,0,1))
        score = -wait + throughput + util
        return {'wait': wait, 'throughput': throughput, 'util': util, 'score': score}

metrics_evaluator = MetricsEvaluator(DATA_MATRIX)

# ---------------- InterLayer ----------------
class InterLayer:
    def __init__(self,D_graph,max_inner_dim,inter_dim=len(candidate_dims),seed=42):
        np.random.seed(seed)
        self.D_graph=D_graph
        self.inter_dim=inter_dim
        self.max_input=2*max_inner_dim
        self.weights={}
        self.bias={}
        for i in range(D_graph):
            for j in range(D_graph):
                if i==j: continue
                self.weights[(i,j)] = np.random.uniform(-0.6,0.6,(inter_dim,self.max_input))
                self.bias[(i,j)] = np.random.uniform(-0.3,0.3,inter_dim)

# ---------------- Multiplex Optimizer ----------------
class UnifiedACORMultiplex:
    def __init__(self, candidate_dims,D_graph,inner_archive_size,inner_offspring,
                 outer_archive_size,outer_offspring,synthetic_targets,inner_learning,causal_flag=True):
        self.candidate_dims=candidate_dims
        self.D_graph=D_graph
        self.inner_archive_size=inner_archive_size
        self.inner_offspring=inner_offspring
        self.outer_archive_size=outer_archive_size
        self.outer_offspring=outer_offspring
        self.synthetic_targets=synthetic_targets
        self.inner_learning=inner_learning
        self.causal_flag=causal_flag
        self.nested_reps=[np.zeros(max(candidate_dims)) for _ in range(D_graph)]
        self.best_dim_per_node=[candidate_dims[0] for _ in range(D_graph)]
        self.inter_layer=InterLayer(D_graph,max_inner_dim=max(candidate_dims))
        self.chosen_Gmat=np.random.uniform(-0.5,0.5,(D_graph,D_graph))
        np.fill_diagonal(self.chosen_Gmat,0)
        self.l2_before=[]
        self.l2_after=[]

    @staticmethod
    def fcm_propagate(x,W,steps=30):
        y=x.copy()
        for _ in range(steps):
            y=1/(1+np.exp(- (W.dot(y)+x)))
        return y

    @staticmethod
    def behavioral_update(W,y,alpha=0.6,lr=0.1,decay=0.01,causal_mask=None,eps=1e-6):
        C=y.copy()
        D=np.abs(y-np.mean(y))
        S=alpha*C+(1-alpha)*D
        RC=(y-np.mean(y))/(np.std(y)+eps)
        RI=(S-np.mean(S))/(np.std(S)+eps)
        delta=lr*np.outer(RC,RI)-decay*W
        np.fill_diagonal(delta,0)
        W_new=W+delta
        if causal_mask is not None:
            W_new=np.sign(causal_mask)*np.abs(W_new)
        np.clip(W_new,-1,1)
        np.fill_diagonal(W_new,0)
        return W_new

    def run_inner(self, node_idx, target, D_fcm):
        self.l2_before.append(np.linalg.norm(self.nested_reps[node_idx] - target))
        x = target.copy()
        W = np.random.uniform(-0.6, 0.6, (D_fcm, D_fcm))
        np.fill_diagonal(W, 0)

      #  print(f"\nNode {node_idx} Inner Loop (Learning rate = {self.inner_learning})")
        for it in range(inner_iters_per_outer):
            y = self.fcm_propagate(x, W)
            W = self.behavioral_update(W, y)
            x += self.inner_learning * (target - y)
            x = np.clip(x, 0, 1)
            l2 = np.linalg.norm(y - target)
         #   print(f" Iter {it+1:02d}: L2 distance to target = {l2:.4f}")

        self.nested_reps[node_idx] = y
        self.l2_after.append(np.linalg.norm(y - target))
        return x, W, y


    def run_outer(self):
        node_metrics_list=[]
        raw_scores=[]
        for i,y in enumerate(self.nested_reps):
            metrics=metrics_evaluator.compute_node_metrics(i,y=y)
            node_metrics_list.append(metrics)
            raw_scores.append(metrics['score'])
        raw_scores=np.array(raw_scores)
        total_raw=raw_scores.sum()
        if total_raw>outer_cost_limit:
            scale_factor=outer_cost_limit/total_raw
            scaled_scores=raw_scores*scale_factor
            for i,s in enumerate(scaled_scores):
                node_metrics_list[i]['score']=s
            total_capped=scaled_scores.sum()
        else:
            total_capped=total_raw
        return node_metrics_list,total_capped

    def run(self,outer_generations=outer_generations):
        for gen in range(outer_generations):
            for node_idx in range(self.D_graph):
                dim=self.best_dim_per_node[node_idx]
                target=self.synthetic_targets[node_idx][dim]
                self.run_inner(node_idx,target,dim)
            metrics_list,capped_score=self.run_outer()
            if gen%30==0:
                print(f"\n--- Generation {gen} Metrics ---")
                for i,m in enumerate(metrics_list):
                    metric_str=" | ".join([f"{k}: {v:.2f}" for k,v in m.items()])
                    print(f"Node {i} | {metric_str}")
                print(f"Outer Score (global capped): {capped_score:.3f}")

    # ---------------- PLOTTING ----------------
    def plot_nested_activations(self):
        plt.figure(figsize=(12,3))
        for i,rep in enumerate(self.nested_reps):
            plt.subplot(1,self.D_graph,i+1)
            plt.bar(range(len(rep)),rep,color=plt.cm.plasma(rep))
            plt.ylim(0,1)
            plt.title(f"Node {i+1} Activations")
        plt.tight_layout()
        plt.show()

    def plot_outer_fuzzy_graph(self):
        G=nx.DiGraph()
        for i in range(self.D_graph): G.add_node(i)
        for i in range(self.D_graph):
            for j in range(self.D_graph):
                if i!=j and abs(self.chosen_Gmat[i,j])>0.02:
                    G.add_edge(i,j,weight=self.chosen_Gmat[i,j])
        node_sizes=[self.best_dim_per_node[i]*200 for i in range(self.D_graph)]
        edge_colors=['green' if d['weight']>0 else 'red' for _,_,d in G.edges(data=True)]
        edge_widths=[abs(d['weight'])*3 for _,_,d in G.edges(data=True)]
        pos=nx.circular_layout(G)
        plt.figure(figsize=(6,6))
        nx.draw(G,pos,node_size=node_sizes,node_color='skyblue',edge_color=edge_colors,
                width=edge_widths,arrows=True,with_labels=True)
        plt.title("Outer Fuzzy Multiplex Graph")
        plt.show()

    def plot_nested_vs_target(self):
        plt.figure(figsize=(12,4))
        for i in range(self.D_graph):
            best_dim=self.best_dim_per_node[i]
            y_actual=self.nested_reps[i]
            y_target=self.synthetic_targets[i][best_dim]
            if len(y_target)<len(y_actual):
                y_target=np.pad(y_target,(0,len(y_actual)-len(y_target)),"constant")
            elif len(y_target)>len(y_actual):
                y_target=y_target[:len(y_actual)]
            plt.subplot(1,self.D_graph,i+1)
            plt.plot(range(len(y_actual)),y_actual,'o-',label='FCM Output')
            plt.plot(range(len(y_target)),y_target,'x--',label='Target')
            plt.ylim(0,1.1)
            plt.title(f"Node {i+1} | Dim {best_dim}")
            if i==0: plt.legend()
        plt.tight_layout()
        plt.show()

    def collect_pointwise_minmax_elite(self,node_idx,dim,top_k=10):
        # simulate top_k elite samples (here using nested_reps with small noise)
        reps=[]
        base=self.nested_reps[node_idx]
        for _ in range(top_k):
            reps.append(np.clip(base + np.random.normal(0,0.05,len(base)),0,1))
        reps=np.array(reps)
        return reps.min(axis=0), reps.max(axis=0)

    def plot_pointwise_minmax_elite(self,top_k=21):
        plt.figure(figsize=(14,3))
        for i in range(self.D_graph):
            dim=self.best_dim_per_node[i]
            y_min,y_max=self.collect_pointwise_minmax_elite(i,dim,top_k)
            y_sel=self.nested_reps[i]
            y_true=self.synthetic_targets[i][dim]
            if len(y_true)<len(y_sel):
                y_true=np.pad(y_true,(0,len(y_sel)-len(y_true)),"constant")
            elif len(y_true)>len(y_sel):
                y_true=y_true[:len(y_sel)]
            x=np.arange(len(y_min))
            plt.subplot(1,self.D_graph,i+1)
            plt.fill_between(x,y_min,y_max,color='skyblue',alpha=0.4,label='Elite Interval')
            plt.plot(x,y_sel,'k-',lw=2,label='Estimated Activation')
            plt.plot(x,y_true,'r--',lw=2,label='True Activation')
            plt.ylim(0,1.05)
            plt.title(f"Node {i+1} | Dim {dim}")
            if i==0: plt.legend()
        plt.tight_layout()
        plt.show()

    def print_l2_summary(self):
        print("\nL2 Distances to Target per Node:")
        for i,(b,a) in enumerate(zip(self.l2_before,self.l2_after)):
            print(f"Node {i+1}: Before={b:.4f} | After={a:.4f} | Improvement={b-a:.4f}")


# ---------------- RUN ----------------
if __name__=="__main__":
    optimizer=UnifiedACORMultiplex(candidate_dims,D_graph,
                                    inner_archive_size,inner_offspring,
                                    outer_archive_size,outer_offspring,
                                    synthetic_targets,
                                    inner_learning,causal_flag=False)
    optimizer.run()
    optimizer.plot_pointwise_minmax_elite(top_k=21)
    optimizer.plot_nested_activations()
    optimizer.plot_outer_fuzzy_graph()
    optimizer.plot_nested_vs_target()
    optimizer.print_l2_summary()

"""**Some comments**

Notice that now the program runs fast, and that there are multiple node before - after situations, likely due to co adaptation of many inner structures. Additionally there are activations for each process as well as a general service relation outline. This run is considered a good one as the targets are close to the data inputs. Moreover the cost constrain should define the outer graph relationships. Eventually, metrics are shown for each step of the optimization, and if one were to show the plots the targets would not necessarily match the inputs.


$$ One \ can \ assume \ that \ the \ service \ is \ now \ likely \ Optimized \ or \ that \ it \ soon \ will \ be $$

# - Service optimizer, uncostrained problem VS FCM baseline

As the next step, it is noted that few minor changes are made to increase the programs potential. This time, the issue at hand is to compare an unconstrained metaheuristic suggestion with an FCM baseline. Furthermore, an FCM baseline is also established which assumes that the four best node candidate solutions are the optimal service design processes; from that a score is calculated and some comparisons are drawn between the fuzzy multiplex optimizer and the FCM baseline.

### Here is the fuzzy multiplex optimizer with inter layer
"""

# ---------------- Targets ----------------
#candidate_dims = [6,6,6,6,6,6,6]#,6,6,6,6,6,6,6,6,6,6,6,6,]
#D_graph = 4
inner_archive_size = 120
inner_offspring = 80
outer_archive_size = 80
outer_offspring = 80
inner_iters_per_outer = 15
outer_generations = 500
outer_cost_limit = 1350
inner_learning = 0.9
seed = 42
np.random.seed(seed)

# ---------------- Metrics Evaluator ----------------
class MetricsEvaluator:
    def __init__(self, data_matrix):
        self.data_matrix = data_matrix
        self.num_features = data_matrix.shape[1]
        self.W0 = 10.0
        self.T0 = 100.0
        self.U0 = 0.5

    def compute_node_metrics(self, node_idx, y=None):
        row = self.data_matrix[node_idx % self.data_matrix.shape[0]]
        k = self.num_features
        base = k // 3 if k >= 3 else 1
        wait_cols = list(range(0, base))
        thr_cols = list(range(base, 2*base))
        util_cols = list(range(2*base, k))
        wait_signal = np.mean(row[wait_cols])
        throughput_signal = np.mean(row[thr_cols])
        util_signal = np.mean(row[util_cols])
        if y is None: y = np.array([0.5,0.5,0.5])
        else: y = np.array(y[:3]) if len(y)>=3 else np.pad(y,(0,3-len(y)),constant_values=0.5)
        wait = self.W0*(1+1.2*wait_signal +0.8*y[0])
        throughput = self.T0*(1+1.1*throughput_signal +0.6*y[1]-0.4*wait_signal)
        util = self.U0 + 0.8*util_signal + 0.6*y[2]
        wait = float(np.clip(wait,0,100))
        throughput = float(np.clip(throughput,0,150))
        util = float(np.clip(util,0,1))
        score = -wait + throughput + util
        return {'wait': wait, 'throughput': throughput, 'util': util, 'score': score}

metrics_evaluator = MetricsEvaluator(DATA_MATRIX)
# ---------------- InterLayer (FULL) ----------------
class InterLayer:
    def __init__(self, D_graph, max_inner_dim, edge_threshold=0.02, seed=42):
        np.random.seed(seed)
        self.D_graph = D_graph
        self.max_input = 2*max_inner_dim
        self.weights = {}
        self.bias = {}
        for i in range(D_graph):
            for j in range(D_graph):
                if i==j: continue
                self.weights[(i,j)] = np.random.uniform(-0.6,0.6,(max_inner_dim, self.max_input))
                self.bias[(i,j)] = np.random.uniform(-0.3,0.3,max_inner_dim)
        self.edge_threshold = edge_threshold

    def compute_edge_activation(self, i,j,nested_reps):
        concat = np.concatenate([nested_reps[i], nested_reps[j]])
        if len(concat)<self.max_input:
            concat = np.pad(concat,(0,self.max_input-len(concat)))
        else:
            concat = concat[:self.max_input]
        v = self.weights[(i,j)].dot(concat) + self.bias[(i,j)]
        return 1/(1+np.exp(-v))

    def build_inter_activations(self,Gmat,nested_reps):
        acts = {}
        for i in range(self.D_graph):
            for j in range(self.D_graph):
                if i==j: continue
                if abs(Gmat[i,j])>self.edge_threshold:
                    acts[(i,j)] = self.compute_edge_activation(i,j,nested_reps)
        return acts

    @staticmethod
    def pairwise_squared_corr(acts):
        if len(acts)<2: return 0.0
        A = np.stack(list(acts.values()))
        A_cent = A - A.mean(axis=1, keepdims=True)
        stds = np.sqrt((A_cent**2).sum(axis=1)/(A.shape[1]-1)+1e-12)
        cov = A_cent.dot(A_cent.T)/(A.shape[1]-1)
        denom = np.outer(stds,stds)+1e-12
        corr = cov/denom
        np.fill_diagonal(corr,0)
        return (corr**2).sum()

    def mi_for_graph(self,Gmat,nested_reps):
        acts = self.build_inter_activations(Gmat,nested_reps)
        if len(acts)==0: return 0.0
        return float(self.pairwise_squared_corr(acts))


# ---------------- UnifiedACORMultiplex ----------------
class UnifiedACORMultiplex:
    def __init__(self, candidate_dims,D_graph,inner_archive_size,inner_offspring,
                 outer_archive_size,outer_offspring,synthetic_targets,inner_learning,causal_flag=True):
        self.candidate_dims=candidate_dims
        self.D_graph=D_graph
        self.inner_archive_size=inner_archive_size
        self.inner_offspring=inner_offspring
        self.outer_archive_size=outer_archive_size
        self.outer_offspring=outer_offspring
        self.synthetic_targets=synthetic_targets
        self.inner_learning=inner_learning
        self.causal_flag=causal_flag

        self.nested_reps=[np.zeros(max(candidate_dims)) for _ in range(D_graph)]
        self.best_dim_per_node=[candidate_dims[0] for _ in range(D_graph)]
        self.inter_layer=InterLayer(D_graph,max_inner_dim=max(candidate_dims))
        self.chosen_Gmat=np.random.uniform(-0.5,0.5,(D_graph,D_graph))
        np.fill_diagonal(self.chosen_Gmat,0)
        self.l2_before=[]
        self.l2_after=[]

    @staticmethod
    def fcm_propagate(x,W,steps=30):
        y=x.copy()
        for _ in range(steps):
            y=1/(1+np.exp(- (W.dot(y)+x)))
        return y

    @staticmethod
    def behavioral_update(W,y,alpha=0.6,lr=0.1,decay=0.01,causal_mask=None,eps=1e-6):
        C=y.copy()
        D=np.abs(y-np.mean(y))
        S=alpha*C+(1-alpha)*D
        RC=(y-np.mean(y))/(np.std(y)+eps)
        RI=(S-np.mean(S))/(np.std(S)+eps)
        delta=lr*np.outer(RC,RI)-decay*W
        np.fill_diagonal(delta,0)
        W_new=W+delta
        if causal_mask is not None:
            W_new=np.sign(causal_mask)*np.abs(W_new)
        np.clip(W_new,-1,1)
        np.fill_diagonal(W_new,0)
        return W_new

    def run_inner(self, node_idx, target, D_fcm):
        self.l2_before.append(np.linalg.norm(self.nested_reps[node_idx] - target))
        x = target.copy()
        W = np.random.uniform(-0.6, 0.6, (D_fcm, D_fcm))
        np.fill_diagonal(W, 0)

        for it in range(inner_iters_per_outer):
            y = self.fcm_propagate(x, W)
            W = self.behavioral_update(W, y)
            x += self.inner_learning * (target - y)
            x = np.clip(x, 0, 1)

        self.nested_reps[node_idx] = y
        self.l2_after.append(np.linalg.norm(y - target))

        # --- INTER-LAYER MI COMPUTATION ---
        mi_score = self.inter_layer.mi_for_graph(self.chosen_Gmat, self.nested_reps)
        # Optionally use mi_score to adjust x or W
        # print(f"Node {node_idx} MI score: {mi_score:.4f}")

        return x, W, y, mi_score

    def run_outer(self):
        node_metrics_list=[]
        raw_scores=[]
        for i,y in enumerate(self.nested_reps):
            metrics=metrics_evaluator.compute_node_metrics(i,y=y)
            node_metrics_list.append(metrics)
            raw_scores.append(metrics['score'])
        raw_scores=np.array(raw_scores)
        total_raw=raw_scores.sum()
        if total_raw>outer_cost_limit:
            scale_factor=outer_cost_limit/total_raw
            scaled_scores=raw_scores*scale_factor
            for i,s in enumerate(scaled_scores):
                node_metrics_list[i]['score']=s
            total_capped=scaled_scores.sum()
        else:
            total_capped=total_raw
        return node_metrics_list,total_capped

    def run(self, outer_generations=outer_generations):
        final_metrics_list = None  # store last generation metrics
        for gen in range(outer_generations):
            mi_scores=[]
            for node_idx in range(self.D_graph):
                dim = self.best_dim_per_node[node_idx]
                target = self.synthetic_targets[node_idx][dim]
                _, _, _, mi_score = self.run_inner(node_idx, target, dim)
                mi_scores.append(mi_score)
            metrics_list, capped_score = self.run_outer()

            print(f"\n--- Generation {gen} Metrics ---")
            for i, m in enumerate(metrics_list):
                metric_str = " | ".join([f"{k}: {v:.2f}" for k, v in m.items()])
                print(f"Node {i} | {metric_str}")
            print(f"Outer Score (global capped): {capped_score:.3f}")

            final_metrics_list = metrics_list  # save last generation metrics

        return final_metrics_list  # <-- return metrics for comparison

           # print(f"Inter-layer MI (sum over edges): {sum
    # ---------------- PLOTTING ----------------
    def plot_nested_activations(self):
        plt.figure(figsize=(12,3))
        for i,rep in enumerate(self.nested_reps):
            plt.subplot(1,self.D_graph,i+1)
            plt.bar(range(len(rep)),rep,color=plt.cm.plasma(rep))
            plt.ylim(0,1)
            plt.title(f"Node {i+1} Activations")
        plt.tight_layout()
        plt.show()

    def plot_outer_fuzzy_graph(self):
        G=nx.DiGraph()
        for i in range(self.D_graph): G.add_node(i)
        for i in range(self.D_graph):
            for j in range(self.D_graph):
                if i!=j and abs(self.chosen_Gmat[i,j])>0.02:
                    G.add_edge(i,j,weight=self.chosen_Gmat[i,j])
        node_sizes=[self.best_dim_per_node[i]*200 for i in range(self.D_graph)]
        edge_colors=['green' if d['weight']>0 else 'red' for _,_,d in G.edges(data=True)]
        edge_widths=[abs(d['weight'])*3 for _,_,d in G.edges(data=True)]
        pos=nx.circular_layout(G)
        plt.figure(figsize=(6,6))
        nx.draw(G,pos,node_size=node_sizes,node_color='skyblue',edge_color=edge_colors,
                width=edge_widths,arrows=True,with_labels=True)
        plt.title("Outer Fuzzy Multiplex Graph")
        plt.show()

    def plot_nested_vs_target(self):
        plt.figure(figsize=(12,4))
        for i in range(self.D_graph):
            best_dim=self.best_dim_per_node[i]
            y_actual=self.nested_reps[i]
            y_target=self.synthetic_targets[i][best_dim]
            if len(y_target)<len(y_actual):
                y_target=np.pad(y_target,(0,len(y_actual)-len(y_target)),"constant")
            elif len(y_target)>len(y_actual):
                y_target=y_target[:len(y_actual)]
            plt.subplot(1,self.D_graph,i+1)
            plt.plot(range(len(y_actual)),y_actual,'o-',label='FCM Output')
            plt.plot(range(len(y_target)),y_target,'x--',label='Target')
            plt.ylim(0,1.1)
            plt.title(f"Node {i+1} | Dim {best_dim}")
            if i==0: plt.legend()
        plt.tight_layout()
        plt.show()

    def collect_pointwise_minmax_elite(self,node_idx,dim,top_k=10):
        # simulate top_k elite samples (here using nested_reps with small noise)
        reps=[]
        base=self.nested_reps[node_idx]
        for _ in range(top_k):
            reps.append(np.clip(base + np.random.normal(0,0.05,len(base)),0,1))
        reps=np.array(reps)
        return reps.min(axis=0), reps.max(axis=0)

    def plot_pointwise_minmax_elite(self,top_k=21):
        plt.figure(figsize=(14,3))
        for i in range(self.D_graph):
            dim=self.best_dim_per_node[i]
            y_min,y_max=self.collect_pointwise_minmax_elite(i,dim,top_k)
            y_sel=self.nested_reps[i]
            y_true=self.synthetic_targets[i][dim]
            if len(y_true)<len(y_sel):
                y_true=np.pad(y_true,(0,len(y_sel)-len(y_true)),"constant")
            elif len(y_true)>len(y_sel):
                y_true=y_true[:len(y_sel)]
            x=np.arange(len(y_min))
            plt.subplot(1,self.D_graph,i+1)
            plt.fill_between(x,y_min,y_max,color='skyblue',alpha=0.4,label='Elite Interval')
            plt.plot(x,y_sel,'k-',lw=2,label='Estimated Activation')
            plt.plot(x,y_true,'r--',lw=2,label='True Activation')
            plt.ylim(0,1.05)
            plt.title(f"Node {i+1} | Dim {dim}")
            if i==0: plt.legend()
        plt.tight_layout()
        plt.show()

    def print_l2_summary(self):
        print("\nL2 Distances to Target per Node:")
        #for i,(b,a) in enumerate(zip(self.l2_before,self.l2_after)):
         #   print(f"Node {i+1}: Before={b:.4f} | After={a:.4f} | Improvement={b-a:.4f}")


# ---------------- RUN ----------------
if __name__=="__main__":
    optimizer=UnifiedACORMultiplex(candidate_dims,D_graph,
                                    inner_archive_size,inner_offspring,
                                    outer_archive_size,outer_offspring,
                                    synthetic_targets,
                                    inner_learning,causal_flag=False)
    metrics_list = optimizer.run()
#optimizer.run()
    optimizer.plot_pointwise_minmax_elite(top_k=21)
    optimizer.plot_nested_activations()
    optimizer.plot_outer_fuzzy_graph()
    optimizer.plot_nested_vs_target()
    optimizer.print_l2_summary()

"""**About the results**

Notice that the outputs diverge from the targets in some instances and that they also deviate more. It is worth mentioning here that an inter layer of mutual information has been added and it functionally addresses the evolution process of each inner dimension FCM-graph. About the outputs, the goodness of fit on the target can be tweaked, which was mentioned but now is exemplified; that can be done by using the hyperparameters in the configuration. In this instance, the main objective was to find the most expesive service; this isnt completely it, however, it should be close

"""

########################################################################################################################################

import numpy as np
import matplotlib.pyplot as plt
import networkx as nx

# ---------------- CONFIG ----------------
candidate_dims = [6] * 10
D_graph = 4
inner_archive_size = 40
inner_offspring = 40
outer_archive_size = 40
outer_offspring = 40
inner_iters_per_outer = 50
outer_generations = 10
outer_cost_limit = 10000
inner_learning = 1.25
gamma_interlayer = 0.33  # NEW: scaling for inter-layer MI penalty
seed = 42
np.random.seed(seed)

# ---------------- METRICS EVALUATOR ----------------
class MetricsEvaluator:
    def __init__(self, data_matrix):
        self.data_matrix = data_matrix
        self.num_features = data_matrix.shape[1]
        self.W0 = 10.0
        self.T0 = 100.0
        self.U0 = 0.5

    def compute_node_metrics(self, node_idx, y=None):
        row = self.data_matrix[node_idx % self.data_matrix.shape[0]]
        k = self.num_features
        base = max(1, k // 3)
        wait_cols = list(range(0, base))
        thr_cols = list(range(base, 2*base))
        util_cols = list(range(2*base, k))
        wait_signal = np.mean(row[wait_cols])
        throughput_signal = np.mean(row[thr_cols])
        util_signal = np.mean(row[util_cols])
        if y is None:
            y = np.array([0.5, 0.5, 0.5])
        else:
            y = np.array(y[:3]) if len(y) >= 3 else np.pad(y, (0, 3 - len(y)), constant_values=0.5)
        wait = self.W0 * (1 + 1.2 * wait_signal + 0.8 * y[0])
        throughput = self.T0 * (1 + 1.1 * throughput_signal + 0.6 * y[1] - 0.4 * wait_signal)
        util = self.U0 + 0.8 * util_signal + 0.6 * y[2]
        wait = float(np.clip(wait, 0, 100))
        throughput = float(np.clip(throughput, 0, 150))
        util = float(np.clip(util, 0, 1))
        score = -wait + throughput + util
        return {'wait': wait, 'throughput': throughput, 'util': util, 'score': score}

# ---------------- GAMMA INTERLAYER ----------------
class InterLayer:
    def __init__(self, D_graph, max_inner_dim, inter_dim=None, edge_threshold=0.02, gamma=1.0, seed=42):
        np.random.seed(seed)
        self.D_graph = D_graph
        self.max_input = 2 * max_inner_dim
        self.edge_threshold = edge_threshold
        self.gamma = gamma
        self.inter_dim = inter_dim if inter_dim is not None else max_inner_dim
        self.weights = {}
        self.bias = {}
        for i in range(D_graph):
            for j in range(D_graph):
                if i == j:
                    continue
                self.weights[(i,j)] = np.random.uniform(-0.6, 0.6, (self.inter_dim, self.max_input))
                self.bias[(i,j)] = np.random.uniform(-0.3, 0.3, self.inter_dim)

    def compute_edge_activation(self, i, j, nested_reps):
        concat = np.concatenate([nested_reps[i], nested_reps[j]])
        if len(concat) < self.max_input:
            concat = np.pad(concat, (0, self.max_input - len(concat)))
        else:
            concat = concat[:self.max_input]
        v = self.weights[(i,j)].dot(concat) + self.bias[(i,j)]
        return 1 / (1 + np.exp(-v))

    def build_inter_activations(self, Gmat, nested_reps):
        acts = {}
        for i in range(self.D_graph):
            for j in range(self.D_graph):
                if i == j: continue
                if abs(Gmat[i,j]) > self.edge_threshold:
                    acts[(i,j)] = self.compute_edge_activation(i, j, nested_reps)
        return acts

    @staticmethod
    def pairwise_squared_corr(acts):
        if len(acts) < 2:
            return 0.0
        A = np.stack(list(acts.values()))
        A_centered = A - A.mean(axis=1, keepdims=True)
        stds = np.sqrt(np.sum(A_centered**2, axis=1)/(A.shape[1]-1) + 1e-12)
        cov = A_centered @ A_centered.T / (A.shape[1]-1)
        corr = cov / (np.outer(stds, stds) + 1e-12)
        np.fill_diagonal(corr, 0)
        return float((corr**2).sum())

    def mi_for_graph(self, Gmat, nested_reps):
        acts = self.build_inter_activations(Gmat, nested_reps)
        if len(acts) == 0: return 0.0
        return self.gamma * self.pairwise_squared_corr(acts)

# ---------------- UNIFIED ACOR MULTIPLEX ----------------
class UnifiedACORMultiplex:
    def __init__(self, candidate_dims, D_graph, inner_archive_size, inner_offspring,
                 outer_archive_size, outer_offspring, synthetic_targets, inner_learning,
                 gamma_interlayer=1.0, causal_flag=True):
        self.candidate_dims = candidate_dims
        self.D_graph = D_graph
        self.inner_archive_size = inner_archive_size
        self.inner_offspring = inner_offspring
        self.outer_archive_size = outer_archive_size
        self.outer_offspring = outer_offspring
        self.synthetic_targets = synthetic_targets
        self.inner_learning = inner_learning
        self.causal_flag = causal_flag
        self.nested_reps = [np.zeros(max(candidate_dims)) for _ in range(D_graph)]
        self.best_dim_per_node = [candidate_dims[0] for _ in range(D_graph)]
        self.inter_layer = InterLayer(D_graph, max_inner_dim=max(candidate_dims), gamma=gamma_interlayer)
        self.chosen_Gmat = np.random.uniform(-0.5, 0.5, (D_graph, D_graph))
        np.fill_diagonal(self.chosen_Gmat, 0)
        self.l2_before, self.l2_after = [], []

    @staticmethod
    def fcm_propagate(x, W, steps=30):
        y = x.copy()
        for _ in range(steps):
            y = 1 / (1 + np.exp(-(W.dot(y) + x)))
        return y

    @staticmethod
    def behavioral_update(W, y, alpha=0.6, lr=0.1, decay=0.01, causal_mask=None, eps=1e-6):
        C = y.copy()
        D = np.abs(y - np.mean(y))
        S = alpha*C + (1-alpha)*D
        RC = (y - np.mean(y)) / (np.std(y)+eps)
        RI = (S - np.mean(S)) / (np.std(S)+eps)
        delta = lr*np.outer(RC, RI) - decay*W
        np.fill_diagonal(delta, 0)
        W_new = W + delta
        if causal_mask is not None:
            W_new = np.sign(causal_mask) * np.abs(W_new)
        np.clip(W_new, -1, 1)
        np.fill_diagonal(W_new, 0)
        return W_new

    def run_inner(self, node_idx, target, D_fcm):
        self.l2_before.append(np.linalg.norm(self.nested_reps[node_idx] - target))
        x = target.copy()
        W = np.random.uniform(-0.6, 0.6, (D_fcm, D_fcm))
        np.fill_diagonal(W, 0)
        for _ in range(inner_iters_per_outer):
            y = self.fcm_propagate(x, W)
            grad_y = 2.0 * (y - target)
            x -= self.inner_learning * grad_y
            x = np.clip(x, 0, 1)
            W -= 0.05 * np.outer(grad_y, y)
            np.fill_diagonal(W, 0)
            W = np.clip(W, -1, 1)
        self.nested_reps[node_idx] = y
        self.l2_after.append(np.linalg.norm(y - target))
        mi_score = self.inter_layer.mi_for_graph(self.chosen_Gmat, self.nested_reps)
        return x, W, y, mi_score

    def run_outer(self):
        node_metrics_list = []
        raw_scores = []
        metrics_evaluator = MetricsEvaluator(DATA_MATRIX)
        for i, y in enumerate(self.nested_reps):
            metrics = metrics_evaluator.compute_node_metrics(i, y=y)
            node_metrics_list.append(metrics)
            raw_scores.append(metrics['score'])
        raw_scores = np.array(raw_scores)
        total_raw = raw_scores.sum()
        if total_raw > outer_cost_limit:
            scale_factor = outer_cost_limit / total_raw
            scaled_scores = raw_scores * scale_factor
            for i,s in enumerate(scaled_scores):
                node_metrics_list[i]['score'] = s
            total_capped = scaled_scores.sum()
        else:
            total_capped = total_raw
        return node_metrics_list, total_capped

    def run(self, outer_generations=outer_generations):
        final_metrics_list = None
        for gen in range(outer_generations):
            mi_scores = []
            for node_idx in range(self.D_graph):
                dim = self.best_dim_per_node[node_idx]
                target = self.synthetic_targets[node_idx][dim]
                _, _, _, mi_score = self.run_inner(node_idx, target, dim)
                mi_scores.append(mi_score)
            metrics_list, capped_score = self.run_outer()
            print(f"\n--- Generation {gen} Metrics ---")
            for i, m in enumerate(metrics_list):
                metric_str = " | ".join([f"{k}: {v:.2f}" for k, v in m.items()])
                print(f"Node {i} | {metric_str}")
            print(f"Outer Score (global capped): {capped_score:.3f}")
            final_metrics_list = metrics_list
        return final_metrics_list

        # ---------------- HELPER: Collect Pointwise Min/Max Elite ----------------
    def collect_pointwise_minmax_elite(self, node_idx, dim_idx, top_k=21):
        """
        Collects pointwise min and max values across top_k elite activations
        for a given node.
        """
        reps = []
        base = self.nested_reps[node_idx]
        for _ in range(top_k):
            reps.append(np.clip(base + np.random.normal(0,0.05,len(base)),0,1))
        reps = np.array(reps)
        y_min = reps.min(axis=0)
        y_max = reps.max(axis=0)
        return y_min, y_max

    # ---------------- PLOTTING ----------------
    def plot_pointwise_minmax_elite(self, top_k=21):
        plt.figure(figsize=(14,3))
        for i in range(self.D_graph):
            dim = self.best_dim_per_node[i]
            y_min, y_max = self.collect_pointwise_minmax_elite(i, dim, top_k)
            y_sel = self.nested_reps[i]
            y_true = self.synthetic_targets[i][dim]
            if len(y_true) < len(y_sel):
                y_true = np.pad(y_true,(0,len(y_sel)-len(y_true)),"constant")
            elif len(y_true) > len(y_sel):
                y_true = y_true[:len(y_sel)]
            x = np.arange(len(y_min))
            plt.subplot(1,self.D_graph,i+1)
            plt.fill_between(x, y_min, y_max, color='skyblue', alpha=0.4, label='Elite Interval')
            plt.plot(x, y_sel, 'k-', lw=2, label='Estimated Activation')
            plt.plot(x, y_true, 'r--', lw=2, label='True Activation')
            plt.ylim(0,1.05)
            plt.title(f"Node {i+1} | Dim {dim}")
            if i == 0: plt.legend()
        plt.tight_layout()
        plt.show()

    # ---------------- PLOTTING ----------------
    def plot_nested_activations(self):
        plt.figure(figsize=(12,3))
        for i, rep in enumerate(self.nested_reps):
            plt.subplot(1,self.D_graph,i+1)
            plt.bar(range(len(rep)), rep, color=plt.cm.plasma(rep))
            plt.ylim(0,1)
            plt.title(f"Node {i+1} Activations")
        plt.tight_layout()
        plt.show()

    def plot_outer_fuzzy_graph(self):
        G = nx.DiGraph()
        for i in range(self.D_graph): G.add_node(i)
        for i in range(self.D_graph):
            for j in range(self.D_graph):
                if i != j and abs(self.chosen_Gmat[i,j]) > 0.02:
                    G.add_edge(i,j,weight=self.chosen_Gmat[i,j])
        node_sizes = [self.best_dim_per_node[i]*200 for i in range(self.D_graph)]
        edge_colors = ['green' if d['weight']>0 else 'red' for _,_,d in G.edges(data=True)]
        edge_widths = [abs(d['weight'])*3 for _,_,d in G.edges(data=True)]
        pos = nx.circular_layout(G)
        plt.figure(figsize=(6,6))
        nx.draw(G,pos,node_size=node_sizes,node_color='skyblue',
                edge_color=edge_colors,width=edge_widths,arrows=True,with_labels=True)
        plt.title("Outer Fuzzy Multiplex Graph")
        plt.show()

    def plot_nested_vs_target(self):
        plt.figure(figsize=(12,4))
        for i in range(self.D_graph):
            best_dim = self.best_dim_per_node[i]
            y_actual = self.nested_reps[i]
            y_target = self.synthetic_targets[i][best_dim]
            if len(y_target) < len(y_actual):
                y_target = np.pad(y_target,(0,len(y_actual)-len(y_target)),"constant")
            elif len(y_target) > len(y_actual):
                y_target = y_target[:len(y_actual)]
            plt.subplot(1,self.D_graph,i+1)
            plt.plot(range(len(y_actual)), y_actual,'o-',label='FCM Output')
            plt.plot(range(len(y_target)), y_target,'x--',label='Target')
            plt.ylim(0,1.1)
            plt.title(f"Node {i+1} | Dim {best_dim}")
            if i == 0: plt.legend()
        plt.tight_layout()
        plt.show()
    # ---------------- ADDITION: Pointwise Min/Max Elite Plot ----------------


# ---------------- ATTACH TO CLASS ----------------

# ---------------- USAGE ----------------
# After optimizer.run() and other plots:


    def print_l2_summary(self):
        print("\nL2 Distances to Target per Node:")
      #  for i,(b,a) in enumerate(zip(self.l2_before,self.l2_after)):
       #     print(f"Node {i+1}: Before={b:.4f} | After={a:.4f} | Improvement={b-a:.4f}")

# ---------------- RUN ----------------
if __name__ == "__main__":
    optimizer = UnifiedACORMultiplex(
        candidate_dims, D_graph,
        inner_archive_size, inner_offspring,
        outer_archive_size, outer_offspring,
        synthetic_targets,
        inner_learning, gamma_interlayer=gamma_interlayer,
        causal_flag=False
    )
    metrics_list = optimizer.run()
    optimizer.plot_pointwise_minmax_elite(top_k=21)

    optimizer.plot_nested_activations()
    optimizer.plot_outer_fuzzy_graph()
    optimizer.plot_nested_vs_target()
    optimizer.print_l2_summary()

#############################################################################################################################################

"""
### Here is an FCM that optimizes the nodes, i.e. service in this case, based on processes overall score"""

import numpy as np

def baseline_outer_objective_normalized(metrics_evaluator, D_graph, alpha_w=1.0, alpha_t=1.0, alpha_u=1.0, scale_factor=5.0):
    """
    Compute baseline outer objective for each node, normalize metrics like optimizer,
    scale like FCM activations, rank nodes, and return total score.

    Args:
        metrics_evaluator: instance of MetricsEvaluator
        D_graph: number of nodes
        alpha_w, alpha_t, alpha_u: weighting factors for wait, throughput, util
        scale_factor: factor to scale the objective to match FCM activations

    Returns:
        outer_obj: raw baseline outer objective per node
        traits: scaled FCM-like activations
        top_idx: node indices sorted by descending objective
        total_score: sum of per-node scores (like optimizer total)
    """
    outer_obj = []
    raw_scores = []

    for i in range(D_graph):
        metrics = metrics_evaluator.compute_node_metrics(i)
        # normalized metrics
        wait_norm = metrics['wait'] / metrics_evaluator.W0
        throughput_norm = metrics['throughput'] / metrics_evaluator.T0
        util_norm = metrics['util'] / metrics_evaluator.U0

        # baseline outer objective (negative is better)
        obj = - (alpha_w * wait_norm - alpha_t * throughput_norm + alpha_u * util_norm)
        outer_obj.append(obj)

        # raw score for total comparison
        score = -metrics['wait'] + metrics['throughput'] + metrics['util']
        raw_scores.append(score)

    outer_obj = np.array(outer_obj)
    traits = outer_obj * scale_factor  # scale like FCM activations
    raw_scores = np.array(raw_scores)
    total_score = raw_scores.sum()    # total outer score, comparable to optimizer

    # rank nodes
    top_idx = np.argsort(-outer_obj)

    # print top nodes
    print("\n=== Baseline Method: Normalized Outer Objective ===")
    print("Top nodes by outer objective:")
    for rank, idx in enumerate(top_idx[:D_graph], 1):
        m = metrics_evaluator.compute_node_metrics(idx)
        print(f"Rank {rank} | Node {idx} | OuterObj {outer_obj[idx]:.4f} | Trait {traits[idx]:.4f} | "
              f"Wait {m['wait']:.2f} | Throughput {m['throughput']:.2f} | Util {m['util']:.2f} | "
              f"Score {raw_scores[idx]:.2f}")

    print(f"\nTotal baseline outer score: {total_score:.3f}")

    return outer_obj, traits, top_idx, raw_scores, total_score


# ---------------- Example Usage ----------------
D_graph = 4
baseline_obj, baseline_traits, baseline_top_idx, baseline_raw_scores, baseline_total = \
    baseline_outer_objective_normalized(metrics_evaluator, D_graph)

"""# Baseline vs Fuzzy multiplex optimizer

Here some graphs about the quality of node solution as well as a comparison is illustrated. In addition, the two cost solutions are also compared from which one can deduce that the optimizer provides better and more hollistic solutions, despite the outer graph influence that is in place. Needless to say that this was expected, because the baseline FCM is a greedy algorithm while the fuzzy multiplex optimizer is a top-tier meta-heuristic system.
"""

import numpy as np
from scipy.stats import spearmanr
import matplotlib.pyplot as plt
import seaborn as sns

# ---------------- Step 0: Setup ----------------
# D_graph = number of nodes
# metrics_evaluator = your MetricsEvaluator instance
# metrics_list = list of dicts from UnifiedACORMultiplex optimizer run
# Each dict has keys: 'wait', 'throughput', 'util', 'score'

# Compute baseline metrics per node
baseline_metrics = [metrics_evaluator.compute_node_metrics(i) for i in range(D_graph)]

# ---------------- Step 1: Compute per-node deltas and percentage improvements ----------------
deltas = []
pct_improvements = []

for i in range(D_graph):
    baseline = baseline_metrics[i]
    optimized = metrics_list[i]

    # Outer objectives (minimization)
    delta_wait = baseline['wait'] - optimized['wait']      # positive = improvement
    delta_score = baseline['score'] - optimized['score']  # positive = improvement

    # Inner objectives (maximization)
    delta_thrpt = optimized['throughput'] - baseline['throughput']
    delta_util = optimized['util'] - baseline['util']

    delta = {
        'wait': delta_wait,
        'throughput': delta_thrpt,
        'util': delta_util,
        'score': delta_score
    }
    deltas.append(delta)

    pct = {
        'wait_pct': (delta_wait / baseline['wait'] * 100) if baseline['wait'] != 0 else 0,
        'throughput_pct': (delta_thrpt / baseline['throughput'] * 100) if baseline['throughput'] != 0 else 0,
        'util_pct': (delta_util / baseline['util'] * 100) if baseline['util'] != 0 else 0,
        'score_pct': (delta_score / baseline['score'] * 100) if baseline['score'] != 0 else 0
    }
    pct_improvements.append(pct)

# ---------------- Step 2: System-level improvements ----------------
system_delta = {k: sum(d[k] for d in deltas) for k in deltas[0]}
system_pct = {k: sum(p[k+'_pct'] for p in pct_improvements)/D_graph for k in ['wait','throughput','util','score']}

# ---------------- Step 3: Node rankings and Spearman correlation ----------------
optimizer_scores = np.array([m['score'] for m in metrics_list])
baseline_scores = np.array([m['score'] for m in baseline_metrics])
optimizer_rank = np.argsort(optimizer_scores)  # lower score = better
baseline_rank = np.argsort(baseline_scores)
rank_corr, _ = spearmanr(optimizer_rank, baseline_rank)

# ---------------- Step 4: Compute efficiency ratios (score/util) ----------------
efficiency = []
for i in range(D_graph):
    eff_base = baseline_metrics[i]['score'] / baseline_metrics[i]['util'] if baseline_metrics[i]['util'] > 0 else np.nan
    eff_opt = metrics_list[i]['score'] / metrics_list[i]['util'] if metrics_list[i]['util'] > 0 else np.nan
    efficiency.append({'baseline': eff_base, 'optimizer': eff_opt})

# ---------------- Step 5: Compute overall improvement scores per node ----------------
w_score, w_thrpt, w_wait = 0.5, 0.3, 0.2  # weighted contributions
overall_improvement_per_node = [
    w_score*d['score'] + w_thrpt*d['throughput'] + w_wait*d['wait'] for d in deltas
]
overall_improvement_system = sum(overall_improvement_per_node)

# ---------------- Step 6: Print per-node comparison ----------------
print("\n--- Per-Node Comparison ---")
print(f"{'Node':<5}{'Delta Wait':>12}{'Delta Thrpt':>12}{'Delta Util':>12}{'Delta Score':>14}{'Eff Baseline':>14}{'Eff Optimizer':>14}{'OverallImp':>14}")
for i in range(D_graph):
    print(f"{i:<5}"
          f"{deltas[i]['wait']:>12.2f}"
          f"{deltas[i]['throughput']:>12.2f}"
          f"{deltas[i]['util']:>12.2f}"
          f"{deltas[i]['score']:>14.2f}"
          f"{efficiency[i]['baseline']:>14.2f}"
          f"{efficiency[i]['optimizer']:>14.2f}"
          f"{overall_improvement_per_node[i]:>14.2f}")

# ---------------- Step 7: Print system-level summary ----------------
print("\n--- System-Level Summary ---")
for k,v in system_delta.items():
    print(f"Total Delta {k}: {v:.2f}")
for k,v in system_pct.items():
    print(f"Average Percent Improvement {k}: {v:.2f}%")
print(f"System Overall Improvement Score: {overall_improvement_system:.2f}")
print(f"Spearman Rank Correlation (Node Scores): {rank_corr:.3f}")

# ---------------- Step 8: Visualizations ----------------
nodes = np.arange(D_graph)

# Per-node delta bars
fig, ax = plt.subplots(figsize=(10,4))
width = 0.2
ax.bar(nodes - 1.5*width, [d['wait'] for d in deltas], width, label='Delta Wait')
ax.bar(nodes - 0.5*width, [d['throughput'] for d in deltas], width, label='Delta Throughput')
ax.bar(nodes + 0.5*width, [d['util'] for d in deltas], width, label='Delta Util')
ax.bar(nodes + 1.5*width, [d['score'] for d in deltas], width, label='Delta Score')
ax.set_xticks(nodes)
ax.set_xticklabels([f"Node {i}" for i in nodes])
ax.set_ylabel("Delta Value")
ax.set_title("Per-Node Metric Deltas (Optimizer vs Baseline)")
ax.legend()
plt.tight_layout()
plt.show()

# Overall improvement per node
plt.figure(figsize=(8,4))
plt.bar(nodes, overall_improvement_per_node, color='skyblue')
plt.xticks(nodes, [f"Node {i}" for i in nodes])
plt.ylabel("Overall Improvement Score")
plt.title("Per-Node Overall Improvement")
plt.tight_layout()
plt.show()

# Heatmap of percentage improvements
pct_matrix = np.array([[p['wait_pct'], p['throughput_pct'], p['util_pct'], p['score_pct']] for p in pct_improvements])
plt.figure(figsize=(8,3))
sns.heatmap(pct_matrix, annot=True, fmt=".1f", cmap="RdYlGn",
            xticklabels=['Wait %','Thrpt %','Util %','Score %'],
            yticklabels=[f"Node {i}" for i in nodes])
plt.title("Per-Node Percentage Improvements")
plt.tight_layout()
plt.show()

