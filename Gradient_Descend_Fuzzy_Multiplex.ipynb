{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myaM6bwNlxF1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "# ---------------- Synthetic dataset ----------------\n",
        "np.random.seed(42)\n",
        "N = 1000\n",
        "\n",
        "route_planning = pd.DataFrame({\n",
        "    'origin_x': np.random.uniform(0, 100, N),\n",
        "    'origin_y': np.random.uniform(0, 100, N),\n",
        "    'dest_x': np.random.uniform(0, 100, N),\n",
        "    'dest_y': np.random.uniform(0, 100, N),\n",
        "    'traffic_density': np.random.uniform(0, 1, N),\n",
        "    'road_type': np.random.choice([1, 2, 3], N),\n",
        "})\n",
        "route_planning['distance'] = np.sqrt((route_planning['dest_x'] - route_planning['origin_x'])**2 +\n",
        "                                     (route_planning['dest_y'] - route_planning['origin_y'])**2)\n",
        "speed_base = {1:50, 2:40, 3:30}\n",
        "route_planning['speed'] = route_planning['road_type'].map(speed_base) * np.random.uniform(0.8,1.2,N)\n",
        "route_planning['travel_time'] = (route_planning['distance']/route_planning['speed'])*60*\\\n",
        "                                (1+route_planning['traffic_density']*np.random.uniform(0.1,0.5,N))\n",
        "\n",
        "vehicle_assignment = pd.DataFrame({\n",
        "    'vehicle_capacity': np.random.randint(50,200,N),\n",
        "    'battery_level': np.random.uniform(0.3,1.0,N),\n",
        "    'delivery_size': np.random.randint(5,50,N),\n",
        "    'vehicle_type': np.random.choice([1,2],N),\n",
        "    'speed_factor': np.random.uniform(0.9,1.1,N),\n",
        "})\n",
        "vehicle_assignment['assigned_speed'] = route_planning['speed']*vehicle_assignment['speed_factor']\n",
        "vehicle_assignment['load_utilization'] = vehicle_assignment['delivery_size']/vehicle_assignment['vehicle_capacity']\n",
        "\n",
        "time_scheduling = pd.DataFrame({\n",
        "    'requested_time': np.random.randint(8,20,N),\n",
        "    'delivery_priority': np.random.randint(1,5,N),\n",
        "    'customer_patience': np.random.uniform(0,1,N),\n",
        "})\n",
        "time_scheduling['delay_probability'] = np.clip(\n",
        "    (route_planning['travel_time']/60)*(1+vehicle_assignment['load_utilization']*0.5)*np.random.uniform(0.8,1.2,N),\n",
        "    0,1\n",
        ")\n",
        "\n",
        "dynamic_rerouting = pd.DataFrame({\n",
        "    'current_x': np.random.uniform(0,100,N),\n",
        "    'current_y': np.random.uniform(0,100,N),\n",
        "    'traffic_updates': np.random.uniform(0,1,N),\n",
        "    'new_delivery_requests': np.random.randint(0,3,N),\n",
        "    'vehicle_status': np.random.choice([0,1],N),\n",
        "    'weather': np.random.choice([0,1],N),\n",
        "})\n",
        "dynamic_rerouting['congestion_score'] = dynamic_rerouting['traffic_updates'] + \\\n",
        "                                       dynamic_rerouting['new_delivery_requests']*0.5 + \\\n",
        "                                       dynamic_rerouting['weather']*0.5 + \\\n",
        "                                       (route_planning['travel_time']/route_planning['travel_time'].max())*0.5\n",
        "\n",
        "# ---------------- Combine and normalize ----------------\n",
        "datasets = [route_planning, vehicle_assignment, time_scheduling, dynamic_rerouting]\n",
        "dataset_dims = [df.shape[1] for df in datasets]\n",
        "max_dim = max(dataset_dims)\n",
        "\n",
        "padded_data = []\n",
        "for df in datasets:\n",
        "    arr = df.values\n",
        "    if arr.shape[1] < max_dim:\n",
        "        arr = np.hstack([arr, np.zeros((arr.shape[0], max_dim - arr.shape[1]))])\n",
        "    padded_data.append(arr)\n",
        "\n",
        "DATA_MATRIX = np.hstack(padded_data)\n",
        "DATA_MATRIX = (DATA_MATRIX - DATA_MATRIX.min(axis=0)) / (np.ptp(DATA_MATRIX, axis=0)+1e-8)\n",
        "\n",
        "def generate_targets(DATA_MATRIX, candidate_dims, D_graph):\n",
        "    targets = []\n",
        "    for node_idx in range(D_graph):\n",
        "        row = DATA_MATRIX[node_idx % len(DATA_MATRIX)]\n",
        "        node_targets = {}\n",
        "        for dim in candidate_dims:\n",
        "            if len(row) >= dim:\n",
        "                sampled = row[:dim]\n",
        "            else:\n",
        "                sampled = np.pad(row, (0, dim - len(row)), constant_values=0.5)\n",
        "            node_targets[dim] = sampled\n",
        "        targets.append(node_targets)\n",
        "    return targets\n",
        "candidate_dims = [6,6,6,6,6,6]#,6,6,6,6,6,6,6,6,6,6,6,6,]\n",
        "D_graph = 4\n",
        "synthetic_targets = generate_targets(DATA_MATRIX, candidate_dims, D_graph)\n",
        "\n",
        "# ---------------- Targets ----------------\n",
        "#candidate_dims = [6,6,6,6,6,6,6]#,6,6,6,6,6,6,6,6,6,6,6,6,]\n",
        "#D_graph = 4\n",
        "inner_archive_size = 120\n",
        "inner_offspring = 80\n",
        "outer_archive_size = 80\n",
        "outer_offspring = 80\n",
        "inner_iters_per_outer = 15\n",
        "outer_generations = 50\n",
        "outer_cost_limit = 1350\n",
        "inner_learning = 0.9\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "\n",
        "# ---------------- Metrics Evaluator ----------------\n",
        "class MetricsEvaluator:\n",
        "    def __init__(self, data_matrix):\n",
        "        self.data_matrix = data_matrix\n",
        "        self.num_features = data_matrix.shape[1]\n",
        "        self.W0 = 10.0\n",
        "        self.T0 = 100.0\n",
        "        self.U0 = 1.0\n",
        "\n",
        "    def compute_node_metrics(self, node_idx, y=None):\n",
        "        row = self.data_matrix[node_idx % self.data_matrix.shape[0]]\n",
        "        k = self.num_features\n",
        "        base = k // 3 if k >= 3 else 1\n",
        "        wait_cols = list(range(0, base))\n",
        "        thr_cols = list(range(base, 2*base))\n",
        "        util_cols = list(range(2*base, k))\n",
        "        wait_signal = np.mean(row[wait_cols])\n",
        "        throughput_signal = np.mean(row[thr_cols])\n",
        "        util_signal = np.mean(row[util_cols])\n",
        "        if y is None: y = np.array([0.5,0.5,0.5])\n",
        "        else: y = np.array(y[:3]) if len(y)>=3 else np.pad(y,(0,3-len(y)),constant_values=0.5)\n",
        "        wait = self.W0*(1+1.2*wait_signal +0.8*y[0])\n",
        "        throughput = self.T0*(1+1.1*throughput_signal +0.6*y[1]-0.4*wait_signal)\n",
        "        util = self.U0 + 0.8*util_signal + 0.6*y[2]\n",
        "        wait = float(np.clip(wait,0,100))\n",
        "        throughput = float(np.clip(throughput,0,150))\n",
        "        util = float(np.clip(util,0,1))\n",
        "        score = -wait + throughput + util\n",
        "        return {'wait': wait, 'throughput': throughput, 'util': util, 'score': score}\n",
        "\n",
        "metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "# ---------------- InterLayer (FULL) ----------------\n",
        "class InterLayer:\n",
        "    def __init__(self, D_graph, max_inner_dim, edge_threshold=0.02, seed=42):\n",
        "        np.random.seed(seed)\n",
        "        self.D_graph = D_graph\n",
        "        self.max_input = 2*max_inner_dim\n",
        "        self.weights = {}\n",
        "        self.bias = {}\n",
        "        for i in range(D_graph):\n",
        "            for j in range(D_graph):\n",
        "                if i==j: continue\n",
        "                self.weights[(i,j)] = np.random.uniform(-0.6,0.6,(max_inner_dim, self.max_input))\n",
        "                self.bias[(i,j)] = np.random.uniform(-0.3,0.3,max_inner_dim)\n",
        "        self.edge_threshold = edge_threshold\n",
        "\n",
        "    def compute_edge_activation(self, i,j,nested_reps):\n",
        "        concat = np.concatenate([nested_reps[i], nested_reps[j]])\n",
        "        if len(concat)<self.max_input:\n",
        "            concat = np.pad(concat,(0,self.max_input-len(concat)))\n",
        "        else:\n",
        "            concat = concat[:self.max_input]\n",
        "        v = self.weights[(i,j)].dot(concat) + self.bias[(i,j)]\n",
        "        return 1/(1+np.exp(-v))\n",
        "\n",
        "    def build_inter_activations(self,Gmat,nested_reps):\n",
        "        acts = {}\n",
        "        for i in range(self.D_graph):\n",
        "            for j in range(self.D_graph):\n",
        "                if i==j: continue\n",
        "                if abs(Gmat[i,j])>self.edge_threshold:\n",
        "                    acts[(i,j)] = self.compute_edge_activation(i,j,nested_reps)\n",
        "        return acts\n",
        "\n",
        "    @staticmethod\n",
        "    def pairwise_squared_corr(acts):\n",
        "        if len(acts)<2: return 0.0\n",
        "        A = np.stack(list(acts.values()))\n",
        "        A_cent = A - A.mean(axis=1, keepdims=True)\n",
        "        stds = np.sqrt((A_cent**2).sum(axis=1)/(A.shape[1]-1)+1e-12)\n",
        "        cov = A_cent.dot(A_cent.T)/(A.shape[1]-1)\n",
        "        denom = np.outer(stds,stds)+1e-12\n",
        "        corr = cov/denom\n",
        "        np.fill_diagonal(corr,0)\n",
        "        return (corr**2).sum()\n",
        "\n",
        "    def mi_for_graph(self,Gmat,nested_reps):\n",
        "        acts = self.build_inter_activations(Gmat,nested_reps)\n",
        "        if len(acts)==0: return 0.0\n",
        "        return float(self.pairwise_squared_corr(acts))\n",
        "\n",
        "\n",
        "# ---------------- UnifiedACORMultiplex ----------------\n",
        "class UnifiedACORMultiplex:\n",
        "    def __init__(self, candidate_dims,D_graph,inner_archive_size,inner_offspring,\n",
        "                 outer_archive_size,outer_offspring,synthetic_targets,inner_learning,causal_flag=True):\n",
        "        self.candidate_dims=candidate_dims\n",
        "        self.D_graph=D_graph\n",
        "        self.inner_archive_size=inner_archive_size\n",
        "        self.inner_offspring=inner_offspring\n",
        "        self.outer_archive_size=outer_archive_size\n",
        "        self.outer_offspring=outer_offspring\n",
        "        self.synthetic_targets=synthetic_targets\n",
        "        self.inner_learning=inner_learning\n",
        "        self.causal_flag=causal_flag\n",
        "\n",
        "        self.nested_reps=[np.zeros(max(candidate_dims)) for _ in range(D_graph)]\n",
        "        self.best_dim_per_node=[candidate_dims[0] for _ in range(D_graph)]\n",
        "        self.inter_layer=InterLayer(D_graph,max_inner_dim=max(candidate_dims))\n",
        "        self.chosen_Gmat=np.random.uniform(-0.5,0.5,(D_graph,D_graph))\n",
        "        np.fill_diagonal(self.chosen_Gmat,0)\n",
        "        self.l2_before=[]\n",
        "        self.l2_after=[]\n",
        "\n",
        "    @staticmethod\n",
        "    def fcm_propagate(x,W,steps=30):\n",
        "        y=x.copy()\n",
        "        for _ in range(steps):\n",
        "            y=1/(1+np.exp(- (W.dot(y)+x)))\n",
        "        return y\n",
        "\n",
        "    @staticmethod\n",
        "    def behavioral_update(W,y,alpha=0.6,lr=0.1,decay=0.01,causal_mask=None,eps=1e-6):\n",
        "        C=y.copy()\n",
        "        D=np.abs(y-np.mean(y))\n",
        "        S=alpha*C+(1-alpha)*D\n",
        "        RC=(y-np.mean(y))/(np.std(y)+eps)\n",
        "        RI=(S-np.mean(S))/(np.std(S)+eps)\n",
        "        delta=lr*np.outer(RC,RI)-decay*W\n",
        "        np.fill_diagonal(delta,0)\n",
        "        W_new=W+delta\n",
        "        if causal_mask is not None:\n",
        "            W_new=np.sign(causal_mask)*np.abs(W_new)\n",
        "        np.clip(W_new,-1,1)\n",
        "        np.fill_diagonal(W_new,0)\n",
        "        return W_new\n",
        "\n",
        "    def run_inner(self, node_idx, target, D_fcm):\n",
        "        self.l2_before.append(np.linalg.norm(self.nested_reps[node_idx] - target))\n",
        "        x = target.copy()\n",
        "        W = np.random.uniform(-0.6, 0.6, (D_fcm, D_fcm))\n",
        "        np.fill_diagonal(W, 0)\n",
        "\n",
        "        for it in range(inner_iters_per_outer):\n",
        "            y = self.fcm_propagate(x, W)\n",
        "            W = self.behavioral_update(W, y)\n",
        "            x += self.inner_learning * (target - y)\n",
        "            x = np.clip(x, 0, 1)\n",
        "\n",
        "        self.nested_reps[node_idx] = y\n",
        "        self.l2_after.append(np.linalg.norm(y - target))\n",
        "\n",
        "        # --- INTER-LAYER MI COMPUTATION ---\n",
        "        mi_score = self.inter_layer.mi_for_graph(self.chosen_Gmat, self.nested_reps)\n",
        "        # Optionally use mi_score to adjust x or W\n",
        "        # print(f\"Node {node_idx} MI score: {mi_score:.4f}\")\n",
        "\n",
        "        return x, W, y, mi_score\n",
        "\n",
        "    def run_outer(self):\n",
        "        node_metrics_list=[]\n",
        "        raw_scores=[]\n",
        "        for i,y in enumerate(self.nested_reps):\n",
        "            metrics=metrics_evaluator.compute_node_metrics(i,y=y)\n",
        "            node_metrics_list.append(metrics)\n",
        "            raw_scores.append(metrics['score'])\n",
        "        raw_scores=np.array(raw_scores)\n",
        "        total_raw=raw_scores.sum()\n",
        "        if total_raw>outer_cost_limit:\n",
        "            scale_factor=outer_cost_limit/total_raw\n",
        "            scaled_scores=raw_scores*scale_factor\n",
        "            for i,s in enumerate(scaled_scores):\n",
        "                node_metrics_list[i]['score']=s\n",
        "            total_capped=scaled_scores.sum()\n",
        "        else:\n",
        "            total_capped=total_raw\n",
        "        return node_metrics_list,total_capped\n",
        "\n",
        "    def run(self, outer_generations=outer_generations):\n",
        "        final_metrics_list = None  # store last generation metrics\n",
        "        for gen in range(outer_generations):\n",
        "            mi_scores=[]\n",
        "            for node_idx in range(self.D_graph):\n",
        "                dim = self.best_dim_per_node[node_idx]\n",
        "                target = self.synthetic_targets[node_idx][dim]\n",
        "                _, _, _, mi_score = self.run_inner(node_idx, target, dim)\n",
        "                mi_scores.append(mi_score)\n",
        "            metrics_list, capped_score = self.run_outer()\n",
        "\n",
        "            print(f\"\\n--- Generation {gen} Metrics ---\")\n",
        "            for i, m in enumerate(metrics_list):\n",
        "                metric_str = \" | \".join([f\"{k}: {v:.2f}\" for k, v in m.items()])\n",
        "                print(f\"Node {i} | {metric_str}\")\n",
        "            print(f\"Outer Score (global capped): {capped_score:.3f}\")\n",
        "\n",
        "            final_metrics_list = metrics_list  # save last generation metrics\n",
        "\n",
        "        return final_metrics_list  # <-- return metrics for comparison\n",
        "\n",
        "           # print(f\"Inter-layer MI (sum over edges): {sum\n",
        "    # ---------------- PLOTTING ----------------\n",
        "    def plot_nested_activations(self):\n",
        "        plt.figure(figsize=(12,3))\n",
        "        for i,rep in enumerate(self.nested_reps):\n",
        "            plt.subplot(1,self.D_graph,i+1)\n",
        "            plt.bar(range(len(rep)),rep,color=plt.cm.plasma(rep))\n",
        "            plt.ylim(0,1)\n",
        "            plt.title(f\"Node {i+1} Activations\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def plot_outer_fuzzy_graph(self):\n",
        "        G=nx.DiGraph()\n",
        "        for i in range(self.D_graph): G.add_node(i)\n",
        "        for i in range(self.D_graph):\n",
        "            for j in range(self.D_graph):\n",
        "                if i!=j and abs(self.chosen_Gmat[i,j])>0.02:\n",
        "                    G.add_edge(i,j,weight=self.chosen_Gmat[i,j])\n",
        "        node_sizes=[self.best_dim_per_node[i]*200 for i in range(self.D_graph)]\n",
        "        edge_colors=['green' if d['weight']>0 else 'red' for _,_,d in G.edges(data=True)]\n",
        "        edge_widths=[abs(d['weight'])*3 for _,_,d in G.edges(data=True)]\n",
        "        pos=nx.circular_layout(G)\n",
        "        plt.figure(figsize=(6,6))\n",
        "        nx.draw(G,pos,node_size=node_sizes,node_color='skyblue',edge_color=edge_colors,\n",
        "                width=edge_widths,arrows=True,with_labels=True)\n",
        "        plt.title(\"Outer Fuzzy Multiplex Graph\")\n",
        "        plt.show()\n",
        "\n",
        "    def plot_nested_vs_target(self):\n",
        "        plt.figure(figsize=(12,4))\n",
        "        for i in range(self.D_graph):\n",
        "            best_dim=self.best_dim_per_node[i]\n",
        "            y_actual=self.nested_reps[i]\n",
        "            y_target=self.synthetic_targets[i][best_dim]\n",
        "            if len(y_target)<len(y_actual):\n",
        "                y_target=np.pad(y_target,(0,len(y_actual)-len(y_target)),\"constant\")\n",
        "            elif len(y_target)>len(y_actual):\n",
        "                y_target=y_target[:len(y_actual)]\n",
        "            plt.subplot(1,self.D_graph,i+1)\n",
        "            plt.plot(range(len(y_actual)),y_actual,'o-',label='FCM Output')\n",
        "            plt.plot(range(len(y_target)),y_target,'x--',label='Target')\n",
        "            plt.ylim(0,1.1)\n",
        "            plt.title(f\"Node {i+1} | Dim {best_dim}\")\n",
        "            if i==0: plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def collect_pointwise_minmax_elite(self,node_idx,dim,top_k=10):\n",
        "        # simulate top_k elite samples (here using nested_reps with small noise)\n",
        "        reps=[]\n",
        "        base=self.nested_reps[node_idx]\n",
        "        for _ in range(top_k):\n",
        "            reps.append(np.clip(base + np.random.normal(0,0.05,len(base)),0,1))\n",
        "        reps=np.array(reps)\n",
        "        return reps.min(axis=0), reps.max(axis=0)\n",
        "\n",
        "    def plot_pointwise_minmax_elite(self,top_k=21):\n",
        "        plt.figure(figsize=(14,3))\n",
        "        for i in range(self.D_graph):\n",
        "            dim=self.best_dim_per_node[i]\n",
        "            y_min,y_max=self.collect_pointwise_minmax_elite(i,dim,top_k)\n",
        "            y_sel=self.nested_reps[i]\n",
        "            y_true=self.synthetic_targets[i][dim]\n",
        "            if len(y_true)<len(y_sel):\n",
        "                y_true=np.pad(y_true,(0,len(y_sel)-len(y_true)),\"constant\")\n",
        "            elif len(y_true)>len(y_sel):\n",
        "                y_true=y_true[:len(y_sel)]\n",
        "            x=np.arange(len(y_min))\n",
        "            plt.subplot(1,self.D_graph,i+1)\n",
        "            plt.fill_between(x,y_min,y_max,color='skyblue',alpha=0.4,label='Elite Interval')\n",
        "            plt.plot(x,y_sel,'k-',lw=2,label='Estimated Activation')\n",
        "            plt.plot(x,y_true,'r--',lw=2,label='True Activation')\n",
        "            plt.ylim(0,1.05)\n",
        "            plt.title(f\"Node {i+1} | Dim {dim}\")\n",
        "            if i==0: plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def print_l2_summary(self):\n",
        "        print(\"\\nL2 Distances to Target per Node:\")\n",
        "        #for i,(b,a) in enumerate(zip(self.l2_before,self.l2_after)):\n",
        "         #   print(f\"Node {i+1}: Before={b:.4f} | After={a:.4f} | Improvement={b-a:.4f}\")\n",
        "\n",
        "\n",
        "# ---------------- RUN ----------------\n",
        "if __name__==\"__main__\":\n",
        "    optimizer=UnifiedACORMultiplex(candidate_dims,D_graph,\n",
        "                                    inner_archive_size,inner_offspring,\n",
        "                                    outer_archive_size,outer_offspring,\n",
        "                                    synthetic_targets,\n",
        "                                    inner_learning,causal_flag=False)\n",
        "    metrics_list = optimizer.run()\n",
        "#optimizer.run()\n",
        "    optimizer.plot_pointwise_minmax_elite(top_k=21)\n",
        "    optimizer.plot_nested_activations()\n",
        "    optimizer.plot_outer_fuzzy_graph()\n",
        "    optimizer.plot_nested_vs_target()\n",
        "    optimizer.print_l2_summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "candidate_dims = [6] * 3\n",
        "D_graph = 4\n",
        "\n",
        "inner_archive_size = 80\n",
        "inner_offspring = 40\n",
        "outer_archive_size = 40\n",
        "outer_offspring = 40\n",
        "inner_iters_per_outer = 50\n",
        "outer_generations = 101\n",
        "outer_cost_limit = 100\n",
        "inner_learning = 0.1\n",
        "gamma_interlayer = 0.3\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "class MetricsEvaluator:\n",
        "    def __init__(self, data_matrix):\n",
        "        self.data_matrix = data_matrix\n",
        "        self.num_features = data_matrix.shape[1]\n",
        "        self.W0, self.T0, self.U0 = 10.0, 100.0, 1.0\n",
        "        self.P0 = 1.0  # baseline for patience\n",
        "\n",
        "    def compute_node_metrics(self, node_idx, y=None):\n",
        "        row = self.data_matrix[node_idx % self.data_matrix.shape[0]]\n",
        "        k = self.num_features\n",
        "        base = max(1, k // 3)\n",
        "        wait_cols = slice(0, base)\n",
        "        thr_cols = slice(base, 2*base)\n",
        "        util_cols = slice(2*base, k)\n",
        "\n",
        "        wait_signal = np.mean(row[wait_cols])\n",
        "        throughput_signal = np.mean(row[thr_cols])\n",
        "        util_signal = np.mean(row[util_cols])\n",
        "\n",
        "        if y is None:\n",
        "            y = np.array([0.5, 0.5, 0.5])\n",
        "        else:\n",
        "            y = np.array(y[:3]) if len(y) >= 3 else np.pad(y, (0, 3-len(y)), constant_values=0.5)\n",
        "\n",
        "        # Original metrics\n",
        "        wait = np.clip(self.W0*(1 + 1.2*wait_signal + 0.8*y[0]), 0, 100)\n",
        "        throughput = np.clip(self.T0*(1 + 1.1*throughput_signal + 0.6*y[1] - 0.4*wait_signal), 0, 150)\n",
        "        util = np.clip(self.U0 + 0.8*util_signal + 0.6*y[2], 0, 1)\n",
        "\n",
        "        # --- New metric: patience ---\n",
        "        patience_signal = np.mean(row)  # simple average as proxy\n",
        "        patience = np.clip(self.P0 * (1 + 0.5*patience_signal + 0.5*y[0]), 0, 2)  # scaled like others\n",
        "\n",
        "        # Combine into score\n",
        "        score = -wait + throughput + util + patience  # add patience like others\n",
        "\n",
        "        return {'wait': wait, 'throughput': throughput, 'util': util,\n",
        "                'patience': patience, 'score': score}\n",
        "\n",
        "# ---------------- INTER-LAYER MUTUAL INFORMATION ----------------\n",
        "class InterLayer:\n",
        "    def __init__(self, D_graph, max_inner_dim, inter_dim=None, edge_threshold=0.02, gamma=1.0, seed=42):\n",
        "        np.random.seed(seed)\n",
        "        self.D_graph = D_graph\n",
        "        self.max_input = 2*max_inner_dim\n",
        "        self.edge_threshold = edge_threshold\n",
        "        self.gamma = gamma\n",
        "        self.inter_dim = inter_dim if inter_dim is not None else max_inner_dim\n",
        "        self.weights = {(i,j): np.random.uniform(-0.6,0.6,(self.inter_dim,self.max_input))\n",
        "                        for i in range(D_graph) for j in range(D_graph) if i!=j}\n",
        "        self.bias = {(i,j): np.random.uniform(-0.3,0.3,self.inter_dim)\n",
        "                     for i in range(D_graph) for j in range(D_graph) if i!=j}\n",
        "\n",
        "    def compute_edge_activation(self, i, j, nested_reps):\n",
        "        concat = np.concatenate([nested_reps[i], nested_reps[j]])\n",
        "        concat = np.pad(concat, (0, max(0,self.max_input-len(concat))))[:self.max_input]\n",
        "        v = self.weights[(i,j)].dot(concat) + self.bias[(i,j)]\n",
        "        return 1/(1+np.exp(-v))\n",
        "\n",
        "    def build_activations(self, Gmat, nested_reps):\n",
        "        acts = {}\n",
        "        for i in range(self.D_graph):\n",
        "            for j in range(self.D_graph):\n",
        "                if i==j: continue\n",
        "                if abs(Gmat[i,j])>self.edge_threshold:\n",
        "                    acts[(i,j)] = self.compute_edge_activation(i,j,nested_reps)\n",
        "        return acts\n",
        "\n",
        "    @staticmethod\n",
        "    def pairwise_squared_corr(acts):\n",
        "        if len(acts)<2: return 0.0\n",
        "        A = np.stack(list(acts.values()))\n",
        "        A_centered = A - A.mean(axis=1,keepdims=True)\n",
        "        stds = np.sqrt(np.sum(A_centered**2,axis=1)/(A.shape[1]-1) + 1e-12)\n",
        "        cov = A_centered @ A_centered.T / (A.shape[1]-1)\n",
        "        corr = cov / (np.outer(stds,stds)+1e-12)\n",
        "        np.fill_diagonal(corr,0)\n",
        "        return float((corr**2).sum())\n",
        "\n",
        "    def mi_for_graph(self, Gmat, nested_reps):\n",
        "        acts = self.build_activations(Gmat,nested_reps)\n",
        "        if not acts: return 0.0\n",
        "        return self.gamma * self.pairwise_squared_corr(acts)\n",
        "\n",
        "# ---------------- UNIFIED ACOR MULTIPLEX ----------------\n",
        "class GDFCM:\n",
        "    def __init__(self, candidate_dims, D_graph, inner_archive_size, inner_offspring,\n",
        "                 outer_archive_size, outer_offspring, synthetic_targets, inner_learning,\n",
        "                 gamma_interlayer=1.0, causal_flag=True):\n",
        "        self.candidate_dims = candidate_dims\n",
        "        self.D_graph = D_graph\n",
        "        self.inner_archive_size = inner_archive_size\n",
        "        self.inner_offspring = inner_offspring\n",
        "        self.outer_archive_size = outer_archive_size\n",
        "        self.outer_offspring = outer_offspring\n",
        "        self.synthetic_targets = synthetic_targets\n",
        "        self.inner_learning = inner_learning\n",
        "        self.causal_flag = causal_flag\n",
        "\n",
        "        self.nested_reps = [np.zeros(max(candidate_dims)) for _ in range(D_graph)]\n",
        "        self.best_dim_per_node = [candidate_dims[0] for _ in range(D_graph)]\n",
        "        self.inter_layer = InterLayer(D_graph, max_inner_dim=max(candidate_dims), gamma=gamma_interlayer)\n",
        "        self.chosen_Gmat = np.random.uniform(-0.5,0.5,(D_graph,D_graph))\n",
        "        np.fill_diagonal(self.chosen_Gmat,0)\n",
        "        self.l2_before, self.l2_after = [], []\n",
        "\n",
        "    # ---------- INNER LOOP (FCM) ----------\n",
        "    def run_inner(self, node_idx, target, D_fcm, steps=100, lr_x=0.001, lr_W=0.001):\n",
        "        x = target.copy()\n",
        "        W = np.random.uniform(-0.6,0.6,(D_fcm,D_fcm))\n",
        "        np.fill_diagonal(W,0)\n",
        "        self.l2_before.append(np.linalg.norm(self.nested_reps[node_idx]-target))\n",
        "\n",
        "        for _ in range(steps):\n",
        "            z = W.dot(x)\n",
        "            Theta_grad_z = 2*z - target\n",
        "            Theta_grad_x = Theta_grad_z @ W + 0.5*(x+1)**2\n",
        "            Theta_grad_W = np.outer(Theta_grad_z,x)\n",
        "\n",
        "            x -= lr_x * np.clip(Theta_grad_x,-0.05,0.05)\n",
        "            x = np.clip(x,0,1)\n",
        "            W -= lr_W * np.clip(Theta_grad_W,-0.01,0.01)\n",
        "            np.fill_diagonal(W,0)\n",
        "            W = np.clip(W,-1,1)\n",
        "\n",
        "        self.nested_reps[node_idx] = x\n",
        "        self.l2_after.append(np.linalg.norm(x-target))\n",
        "        mi_score = self.inter_layer.mi_for_graph(self.chosen_Gmat,self.nested_reps)\n",
        "        return x, W, x, mi_score\n",
        "\n",
        "    # ---------- OUTER LOOP ----------\n",
        "    def run_outer(self):\n",
        "        metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "        node_metrics_list = []\n",
        "        raw_scores = []\n",
        "\n",
        "        for i,y in enumerate(self.nested_reps):\n",
        "            metrics = metrics_evaluator.compute_node_metrics(i,y=y)\n",
        "            node_metrics_list.append(metrics)\n",
        "            raw_scores.append(metrics['score'])\n",
        "\n",
        "        raw_scores = np.array(raw_scores)\n",
        "        total_raw = raw_scores.sum()\n",
        "        if total_raw>outer_cost_limit:\n",
        "            scale_factor = outer_cost_limit/total_raw\n",
        "            for i,s in enumerate(raw_scores*scale_factor):\n",
        "                node_metrics_list[i]['score']=s\n",
        "            total_capped = (raw_scores*scale_factor).sum()\n",
        "        else:\n",
        "            total_capped = total_raw\n",
        "        return node_metrics_list, total_capped\n",
        "\n",
        "    # ---------- FULL RUN ----------\n",
        "    def run(self, outer_generations=outer_generations):\n",
        "        final_metrics = None\n",
        "        for gen in range(outer_generations):\n",
        "            mi_scores = []\n",
        "            for node_idx in range(self.D_graph):\n",
        "                dim = self.best_dim_per_node[node_idx]\n",
        "                target = self.synthetic_targets[node_idx][dim]\n",
        "                _, _, _, mi_score = self.run_inner(node_idx,target,dim)\n",
        "                mi_scores.append(mi_score)\n",
        "\n",
        "            metrics_list, capped_score = self.run_outer()\n",
        "            print(f\"\\n--- Generation {gen} Metrics ---\")\n",
        "            for i,m in enumerate(metrics_list):\n",
        "                print(f\"Node {i} | \" + \" | \".join([f\"{k}: {v:.2f}\" for k,v in m.items()]))\n",
        "            print(f\"Outer Score (capped): {capped_score:.3f}\")\n",
        "            final_metrics = metrics_list\n",
        "        return final_metrics\n",
        "\n",
        "    # ---------- VISUALIZATIONS ----------\n",
        "    def plot_pointwise_minmax_elite(self, top_k=21):\n",
        "        plt.figure(figsize=(14,3))\n",
        "        for i in range(self.D_graph):\n",
        "            base = self.nested_reps[i]\n",
        "            reps = np.clip(base + np.random.normal(0,0.05,(top_k,len(base))),0,1)\n",
        "            y_min, y_max = reps.min(axis=0), reps.max(axis=0)\n",
        "            y_sel = base\n",
        "            y_true = self.synthetic_targets[i][self.best_dim_per_node[i]]\n",
        "            if len(y_true)<len(y_sel):\n",
        "                y_true = np.pad(y_true,(0,len(y_sel)-len(y_true)),\"constant\")\n",
        "            else:\n",
        "                y_true = y_true[:len(y_sel)]\n",
        "\n",
        "            plt.subplot(1,self.D_graph,i+1)\n",
        "            plt.fill_between(range(len(y_min)),y_min,y_max,color='skyblue',alpha=0.4,label='Elite Interval')\n",
        "            plt.plot(y_sel,'k-',lw=2,label='Estimated')\n",
        "            plt.plot(y_true,'r--',lw=2,label='True')\n",
        "            plt.ylim(0,1.05)\n",
        "            plt.title(f\"Node {i+1}\")\n",
        "            if i==0: plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def plot_nested_activations(self):\n",
        "        plt.figure(figsize=(12,3))\n",
        "        for i,rep in enumerate(self.nested_reps):\n",
        "            plt.subplot(1,self.D_graph,i+1)\n",
        "            plt.bar(range(len(rep)),rep,color=plt.cm.plasma(rep))\n",
        "            plt.ylim(0,1)\n",
        "            plt.title(f\"Node {i+1}\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def plot_outer_fuzzy_graph(self):\n",
        "        G = nx.DiGraph()\n",
        "        for i in range(self.D_graph): G.add_node(i)\n",
        "        for i in range(self.D_graph):\n",
        "            for j in range(self.D_graph):\n",
        "                if i!=j and abs(self.chosen_Gmat[i,j])>0.02:\n",
        "                    G.add_edge(i,j,weight=self.chosen_Gmat[i,j])\n",
        "        node_sizes = [self.best_dim_per_node[i]*200 for i in range(self.D_graph)]\n",
        "        edge_colors = ['green' if d['weight']>0 else 'red' for _,_,d in G.edges(data=True)]\n",
        "        edge_widths = [abs(d['weight'])*3 for _,_,d in G.edges(data=True)]\n",
        "        pos = nx.circular_layout(G)\n",
        "        plt.figure(figsize=(6,6))\n",
        "        nx.draw(G,pos,node_size=node_sizes,node_color='skyblue',\n",
        "                edge_color=edge_colors,width=edge_widths,arrows=True,with_labels=True)\n",
        "        plt.title(\"Outer Fuzzy Multiplex Graph\")\n",
        "        plt.show()\n",
        "# ---------------- INTERACTIONS INSPECTOR ----------------\n",
        "    def print_interactions(self, return_tensor=True, verbose=True):\n",
        "        \"\"\"\n",
        "        Build inter-layer activations tensor and optionally print them.\n",
        "\n",
        "        Returns:\n",
        "            inter_tensor: np.ndarray of shape (D_graph, D_graph, inter_dim)\n",
        "                        inter_tensor[i,j,:] contains activations from node i to node j,\n",
        "                        zeros if no edge exists.\n",
        "        \"\"\"\n",
        "        D_graph = self.D_graph\n",
        "        inter_dim = self.inter_layer.inter_dim\n",
        "        inter_tensor = np.zeros((D_graph, D_graph, inter_dim))\n",
        "\n",
        "        acts = self.inter_layer.build_activations(self.chosen_Gmat, self.nested_reps)\n",
        "\n",
        "        if not acts:\n",
        "            if verbose:\n",
        "                print(\"No active edges above threshold.\")\n",
        "            return inter_tensor if return_tensor else None\n",
        "\n",
        "        for (i, j), vec in acts.items():\n",
        "            inter_tensor[i, j, :] = vec\n",
        "            if verbose:\n",
        "                act_str = \", \".join([f\"{v:.3f}\" for v in vec])\n",
        "                print(f\"Node {i} -> Node {j}: [{act_str}]\")\n",
        "\n",
        "        return inter_tensor if return_tensor else None\n",
        "\n",
        "\n",
        "        def print_l2_summary(self):\n",
        "            print(\"\\nL2 Distances to Target per Node:\")\n",
        "\n",
        "# ---------------- USAGE ----------------\n",
        "if __name__ == \"__main__\":\n",
        "    optimizer = GDFCM(\n",
        "        candidate_dims, D_graph,\n",
        "        inner_archive_size, inner_offspring,\n",
        "        outer_archive_size, outer_offspring,\n",
        "        synthetic_targets,\n",
        "        inner_learning, gamma_interlayer=gamma_interlayer,\n",
        "        causal_flag=False\n",
        "    )\n",
        "    metrics_list = optimizer.run()\n",
        "    optimizer.plot_pointwise_minmax_elite()\n",
        "    optimizer.plot_nested_activations()\n",
        "    optimizer.plot_outer_fuzzy_graph()\n",
        "  #  optimizer.print_interactions()\n",
        "    tensor = optimizer.print_interactions()\n",
        "    print(\"Tensor shape:\", tensor.shape,'\\n',tensor)\n",
        "    import matplotlib.pyplot as plt\n",
        "    import networkx as nx\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    from mpl_toolkits.mplot3d import Axes3D\n",
        "D_graph = len(optimizer.nested_reps)\n",
        "tensor = optimizer.print_interactions(return_tensor=True, verbose=False)\n",
        "\n",
        "# ---------------- Outer nodes (hubs) ----------------\n",
        "G_outer = nx.DiGraph()\n",
        "for i in range(D_graph):\n",
        "    G_outer.add_node(i)\n",
        "for i in range(D_graph):\n",
        "    for j in range(D_graph):\n",
        "        if i != j and np.any(tensor[i,j,:] != 0):\n",
        "            # Shift to signed weights: 0.5 -> 0, <0.5 negative, >0.5 positive\n",
        "            mean_weight = 2 * (np.mean(tensor[i,j,:]) - 0.5)\n",
        "            G_outer.add_edge(i, j, weight=mean_weight)\n",
        "\n",
        "# Outer spring layout\n",
        "pos_outer_2d = nx.circular_layout(G_outer, scale=5)\n",
        "pos_outer = np.array([[x, y, 0] for x, y in pos_outer_2d.values()])\n",
        "\n",
        "fig = plt.figure(figsize=(10,8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Plot outer nodes\n",
        "for i in range(D_graph):\n",
        "    ax.scatter(*pos_outer[i], s=300, color='skyblue')\n",
        "    ax.text(*pos_outer[i], f'Node {i}', color='black')\n",
        "\n",
        "# Plot outer edges with positive/negative colors\n",
        "\n",
        "for i, j, data in G_outer.edges(data=True):\n",
        "    x_vals = [pos_outer[i,0], pos_outer[j,0]]\n",
        "    y_vals = [pos_outer[i,1], pos_outer[j,1]]\n",
        "    z_vals = [pos_outer[i,2], pos_outer[j,2]]\n",
        "\n",
        "    # Positive = bright green, Negative = bright red\n",
        "    color = 'green' if data['weight'] > 0 else 'red'\n",
        "    linewidth = 2 + 4*abs(data['weight'])  # scale width by magnitude\n",
        "    ax.plot(x_vals, y_vals, z_vals, color=color, linewidth=linewidth)\n",
        "# ---------------- Inner FCMs (small circular around hub) ----------------\n",
        "for i, rep in enumerate(optimizer.nested_reps):\n",
        "    dims = len(rep)\n",
        "    angle = np.linspace(0, 2*np.pi, dims, endpoint=False)\n",
        "    radius = 0.8  # small circle\n",
        "    xs = pos_outer[i,0] + radius * np.cos(angle)\n",
        "    ys = pos_outer[i,1] + radius * np.sin(angle)\n",
        "    zs = pos_outer[i,2] + rep  # activation as height\n",
        "\n",
        "    # Plot inner nodes\n",
        "    ax.scatter(xs, ys, zs, c=rep, cmap='plasma', s=50)\n",
        "\n",
        "    # Connect inner nodes in circle\n",
        "    for k in range(dims):\n",
        "        ax.plot([xs[k], xs[(k+1)%dims]], [ys[k], ys[(k+1)%dims]], [zs[k], zs[(k+1)%dims]], color='gray', alpha=0.5)\n",
        "\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Activation')\n",
        "ax.set_title('Outer Nodes with Inner FCMs (Signed correlations)')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mEx31-_ErGU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import seaborn as sns\n",
        "from matplotlib.cm import get_cmap\n",
        "\n",
        "# ---------------- METRICS EVALUATOR ----------------\n",
        "class MetricsEvaluator:\n",
        "    def __init__(self, data_matrix):\n",
        "        self.data_matrix = data_matrix\n",
        "        self.num_features = data_matrix.shape[1]\n",
        "        self.W0, self.T0, self.U0 = 10.0, 100.0, 1.0\n",
        "        self.P0 = 1.0  # baseline for patience\n",
        "\n",
        "    def compute_node_metrics(self, node_idx, y=None):\n",
        "        row = self.data_matrix[node_idx % self.data_matrix.shape[0]]\n",
        "        k = self.num_features\n",
        "        base = max(1, k // 3)\n",
        "        wait_cols = slice(0, base)\n",
        "        thr_cols = slice(base, 2*base)\n",
        "        util_cols = slice(2*base, k)\n",
        "\n",
        "        wait_signal = np.mean(row[wait_cols])\n",
        "        throughput_signal = np.mean(row[thr_cols])\n",
        "        util_signal = np.mean(row[util_cols])\n",
        "\n",
        "        if y is None:\n",
        "            y = np.array([0.5, 0.5, 0.5])\n",
        "        else:\n",
        "            y = np.array(y[:3]) if len(y) >= 3 else np.pad(y, (0, 3-len(y)), constant_values=0.5)\n",
        "\n",
        "        wait = np.clip(self.W0*(1 + 1.2*wait_signal + 0.8*y[0]), 0, 100)\n",
        "        throughput = np.clip(self.T0*(1 + 1.1*throughput_signal + 0.6*y[1] - 0.4*wait_signal), 0, 150)\n",
        "        util = np.clip(self.U0 + 0.8*util_signal + 0.6*y[2], 0, 1)\n",
        "        patience_signal = np.mean(row)\n",
        "        patience = np.clip(self.P0 * (1 + 0.5*patience_signal + 0.5*y[0]), 0, 2)\n",
        "        score = -wait + throughput + util + patience\n",
        "\n",
        "        return {'wait': wait, 'throughput': throughput, 'util': util, 'patience': patience, 'score': score}\n",
        "\n",
        "# ---------------- INTER-LAYER ----------------\n",
        "class InterLayer:\n",
        "    def __init__(self, D_graph, max_inner_dim, inter_dim=None, edge_threshold=0.02, gamma=1.0, seed=42):\n",
        "        np.random.seed(seed)\n",
        "        self.D_graph = D_graph\n",
        "        self.max_input = 2*max_inner_dim\n",
        "        self.edge_threshold = edge_threshold\n",
        "        self.gamma = gamma\n",
        "        self.inter_dim = inter_dim if inter_dim is not None else max_inner_dim\n",
        "        self.weights = {(i,j): np.random.uniform(-0.6,0.6,(self.inter_dim,self.max_input))\n",
        "                        for i in range(D_graph) for j in range(D_graph) if i!=j}\n",
        "        self.bias = {(i,j): np.random.uniform(-0.3,0.3,self.inter_dim)\n",
        "                     for i in range(D_graph) for j in range(D_graph) if i!=j}\n",
        "    # ---------- INTER-LAYER ACTIVATIONS ----------\n",
        "    def print_interactions(self, return_tensor=True, verbose=True):\n",
        "        \"\"\"\n",
        "        Print or return the inter-layer activation tensor.\n",
        "        Shape: (D_graph, D_graph, inter_dim)\n",
        "        \"\"\"\n",
        "        D_graph = self.D_graph\n",
        "        inter_dim = self.inter_layer.inter_dim\n",
        "        inter_tensor = np.zeros((D_graph, D_graph, inter_dim))\n",
        "        acts = self.inter_layer.build_activations(self.chosen_Gmat, self.nested_reps)\n",
        "        if not acts:\n",
        "            if verbose: print(\"No active edges above threshold.\")\n",
        "            return inter_tensor if return_tensor else None\n",
        "        for (i, j), vec in acts.items():\n",
        "            inter_tensor[i, j, :] = vec\n",
        "            if verbose:\n",
        "                act_str = \", \".join([f\"{v:.3f}\" for v in vec])\n",
        "                print(f\"Node {i} -> Node {j}: [{act_str}]\")\n",
        "        return inter_tensor if return_tensor else None\n",
        "\n",
        "    def compute_edge_activation(self, i, j, nested_reps):\n",
        "        concat = np.concatenate([nested_reps[i], nested_reps[j]])\n",
        "        concat = np.pad(concat, (0, max(0,self.max_input-len(concat))))[:self.max_input]\n",
        "        v = self.weights[(i,j)].dot(concat) + self.bias[(i,j)]\n",
        "        return 1/(1+np.exp(-v))\n",
        "\n",
        "    def build_activations(self, Gmat, nested_reps):\n",
        "        acts = {}\n",
        "        for i in range(self.D_graph):\n",
        "            for j in range(self.D_graph):\n",
        "                if i==j: continue\n",
        "                if abs(Gmat[i,j])>self.edge_threshold:\n",
        "                    acts[(i,j)] = self.compute_edge_activation(i,j,nested_reps)\n",
        "        return acts\n",
        "\n",
        "    @staticmethod\n",
        "    def pairwise_squared_corr(acts):\n",
        "        if len(acts)<2: return 0.0\n",
        "        A = np.stack(list(acts.values()))\n",
        "        A_centered = A - A.mean(axis=1,keepdims=True)\n",
        "        stds = np.sqrt(np.sum(A_centered**2,axis=1)/(A.shape[1]-1) + 1e-12)\n",
        "        cov = A_centered @ A_centered.T / (A.shape[1]-1)\n",
        "        corr = cov / (np.outer(stds,stds)+1e-12)\n",
        "        np.fill_diagonal(corr,0)\n",
        "        return float((corr**2).sum())\n",
        "\n",
        "    def mi_for_graph(self, Gmat, nested_reps):\n",
        "        acts = self.build_activations(Gmat,nested_reps)\n",
        "        if not acts: return 0.0\n",
        "        return self.gamma * self.pairwise_squared_corr(acts)\n",
        "\n",
        "# ---------------- GDFCM CLASS ----------------\n",
        "class GDFCM:\n",
        "    def __init__(self, candidate_dims, D_graph, inner_archive_size, inner_offspring,\n",
        "                 outer_archive_size, outer_offspring, synthetic_targets, inner_learning,\n",
        "                 gamma_interlayer=1.0, causal_flag=True):\n",
        "        self.candidate_dims = candidate_dims\n",
        "        self.D_graph = D_graph\n",
        "        self.inner_archive_size = inner_archive_size\n",
        "        self.inner_offspring = inner_offspring\n",
        "        self.outer_archive_size = outer_archive_size\n",
        "        self.outer_offspring = outer_offspring\n",
        "        self.synthetic_targets = synthetic_targets\n",
        "        self.inner_learning = inner_learning\n",
        "        self.causal_flag = causal_flag\n",
        "\n",
        "        self.nested_reps = [np.zeros(max(candidate_dims)) for _ in range(D_graph)]\n",
        "        self.best_dim_per_node = [candidate_dims[0] for _ in range(D_graph)]\n",
        "        self.inter_layer = InterLayer(D_graph, max_inner_dim=max(candidate_dims), gamma=gamma_interlayer)\n",
        "        self.chosen_Gmat = np.random.uniform(-0.5,0.5,(D_graph,D_graph))\n",
        "        np.fill_diagonal(self.chosen_Gmat,0)\n",
        "        self.l2_before, self.l2_after = [], []\n",
        "\n",
        "        # Store capped node metrics for plotting and fuzzy tensor\n",
        "        self.capped_node_metrics = 250\n",
        "\n",
        "    # ---------- INNER LOOP ----------\n",
        "    def run_inner(self, node_idx, target, D_fcm, steps=100, lr_x=0.001, lr_W=0.001):\n",
        "        x = target.copy()\n",
        "        W = np.random.uniform(-0.6,0.6,(D_fcm,D_fcm))\n",
        "        np.fill_diagonal(W,0)\n",
        "        self.l2_before.append(np.linalg.norm(self.nested_reps[node_idx]-target))\n",
        "\n",
        "        for _ in range(steps):\n",
        "            z = W.dot(x)\n",
        "            Theta_grad_z = 2*z - target\n",
        "            Theta_grad_x = Theta_grad_z @ W + 0.5*(x+1)**2\n",
        "            Theta_grad_W = np.outer(Theta_grad_z,x)\n",
        "\n",
        "            x -= lr_x * np.clip(Theta_grad_x,-0.05,0.05)\n",
        "            x = np.clip(x,0,1)\n",
        "            W -= lr_W * np.clip(Theta_grad_W,-0.01,0.01)\n",
        "            np.fill_diagonal(W,0)\n",
        "            W = np.clip(W,-1,1)\n",
        "\n",
        "        self.nested_reps[node_idx] = x\n",
        "        self.l2_after.append(np.linalg.norm(x-target))\n",
        "        mi_score = self.inter_layer.mi_for_graph(self.chosen_Gmat,self.nested_reps)\n",
        "        return x, W, x, mi_score\n",
        "\n",
        "    # ---------- OUTER LOOP ----------\n",
        "    def run_outer(self, outer_cost_limit=100):\n",
        "        metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "        node_metrics_list = []\n",
        "        raw_scores = []\n",
        "\n",
        "        # Compute node metrics and collect raw scores\n",
        "        for i, y in enumerate(self.nested_reps):\n",
        "            metrics = metrics_evaluator.compute_node_metrics(i, y=y)\n",
        "            node_metrics_list.append(metrics)\n",
        "            raw_scores.append(metrics['score'])\n",
        "\n",
        "        raw_scores = np.array(raw_scores)\n",
        "        total_raw = raw_scores.sum()\n",
        "\n",
        "        # Apply cap if total exceeds the limit\n",
        "        if total_raw > outer_cost_limit:\n",
        "            scale_factor = outer_cost_limit / total_raw\n",
        "            # Scale all metrics proportionally\n",
        "            for i, metrics in enumerate(node_metrics_list):\n",
        "                for key in ['wait', 'throughput', 'util', 'patience', 'score']:\n",
        "                    metrics[key] *= scale_factor\n",
        "            total_capped = outer_cost_limit\n",
        "        else:\n",
        "            total_capped = total_raw\n",
        "\n",
        "        # Store capped node metrics for plotting/tensors\n",
        "        self.capped_node_metrics = node_metrics_list\n",
        "\n",
        "        return node_metrics_list, total_capped\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # ---------- FULL RUN ----------\n",
        "    def run(self, outer_generations=101, outer_cost_limit=1000):\n",
        "        \"\"\"\n",
        "        Full optimization run over outer generations.\n",
        "        Prints node metrics with capped scores and outer total capped score.\n",
        "        \"\"\"\n",
        "        final_metrics = None\n",
        "\n",
        "        for gen in range(outer_generations):\n",
        "            mi_scores = []\n",
        "\n",
        "            # --- Inner loop per node ---\n",
        "            for node_idx in range(self.D_graph):\n",
        "                dim = self.best_dim_per_node[node_idx]\n",
        "                target = self.synthetic_targets[node_idx][dim]\n",
        "                _, _, _, mi_score = self.run_inner(node_idx, target, dim)\n",
        "                mi_scores.append(mi_score)\n",
        "\n",
        "            # --- Outer loop with capping ---\n",
        "            metrics_list, capped_score = self.run_outer(outer_cost_limit=outer_cost_limit)\n",
        "\n",
        "            # --- Print capped metrics ---\n",
        "            print(f\"\\n--- Generation {gen} Metrics ---\")\n",
        "            for i, m in enumerate(metrics_list):\n",
        "                print(\n",
        "                    f\"Node {i} | \"\n",
        "                    + \" | \".join([f\"{k}: {v:.2f}\" for k, v in m.items()])\n",
        "                )\n",
        "            print(f\"Outer Score (capped): {capped_score:.3f}\")\n",
        "\n",
        "            final_metrics = metrics_list\n",
        "\n",
        "        return final_metrics\n",
        "\n",
        "\n",
        "    # ---------- PLOTTING FUNCTIONS USE CAPPED METRICS ----------\n",
        "    def _get_metrics_for_plotting(self, i):\n",
        "        # Return capped metrics if available\n",
        "        if self.capped_node_metrics is not None:\n",
        "            return self.capped_node_metrics[i]\n",
        "        metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "        return metrics_evaluator.compute_node_metrics(i, y=self.nested_reps[i])\n",
        "\n",
        "    def plot_metric_dashboard_colored(self):\n",
        "        metrics_keys = ['wait', 'throughput', 'util', 'patience']\n",
        "        D_graph = self.D_graph\n",
        "        metric_matrix = np.zeros((D_graph, len(metrics_keys)))\n",
        "        for i in range(D_graph):\n",
        "            metrics = self._get_metrics_for_plotting(i)\n",
        "            metric_matrix[i, :] = [metrics[k] for k in metrics_keys]\n",
        "\n",
        "        metric_norm = (metric_matrix - metric_matrix.min(axis=0)) / (np.ptp(metric_matrix, axis=0) + 1e-12)\n",
        "        cmap = get_cmap(\"viridis\")\n",
        "\n",
        "        fig, axes = plt.subplots(3, 1, figsize=(12, 8), gridspec_kw={'height_ratios':[1,1,1]})\n",
        "\n",
        "        bottoms = np.zeros(D_graph)\n",
        "        for idx, key in enumerate(metrics_keys):\n",
        "            colors = [cmap(val) for val in metric_norm[:, idx]]\n",
        "            axes[0].bar(range(D_graph), metric_matrix[:, idx], bottom=bottoms, color=colors,\n",
        "                        edgecolor='black', label=key)\n",
        "            bottoms += metric_matrix[:, idx]\n",
        "        axes[0].set_xticks(range(D_graph))\n",
        "        axes[0].set_xticklabels([f\"Node {i+1}\" for i in range(D_graph)])\n",
        "        axes[0].set_ylabel(\"Metric Value (Stacked)\")\n",
        "        axes[0].set_title(\"Stacked Node Metrics (Color by Intensity)\")\n",
        "        axes[0].legend(loc='upper right')\n",
        "\n",
        "        for i, rep in enumerate(self.nested_reps):\n",
        "            axes[1].plot(range(len(rep)), rep + i*1.1, '-o', label=f\"Node {i+1}\")\n",
        "        axes[1].set_ylabel(\"Nested Reps (shifted)\")\n",
        "        axes[1].set_title(\"Nested Representations per Node\")\n",
        "        axes[1].legend(loc='upper right')\n",
        "\n",
        "        sns.heatmap(metric_matrix, annot=True, fmt=\".2f\", cmap=\"viridis\",\n",
        "                    yticklabels=[f\"Node {i+1}\" for i in range(D_graph)],\n",
        "                    xticklabels=metrics_keys, ax=axes[2])\n",
        "        axes[2].set_title(\"Node Metrics Heatmap\")\n",
        "        axes[2].set_xlabel(\"Metrics\")\n",
        "        axes[2].set_ylabel(\"Nodes\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def plot_activations_and_metrics(self, normalize_activations=True):\n",
        "        metrics_keys = ['wait', 'throughput', 'util', 'patience']\n",
        "        D_graph = self.D_graph\n",
        "        metric_matrix = np.zeros((D_graph, len(metrics_keys)))\n",
        "        for i in range(D_graph):\n",
        "            metrics = self._get_metrics_for_plotting(i)\n",
        "            metric_matrix[i, :] = [metrics[k] for k in metrics_keys]\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        for i, rep in enumerate(self.nested_reps):\n",
        "            rep_vals = np.array(rep)\n",
        "            rep_vals_norm = (rep_vals - rep_vals.min()) / (np.ptp(rep_vals) + 1e-12) if normalize_activations else rep_vals\n",
        "            colors = plt.cm.plasma(rep_vals_norm)\n",
        "\n",
        "            plt.subplot(2, D_graph, i+1)\n",
        "            plt.bar(range(len(rep_vals)), rep_vals, color=colors, edgecolor='black')\n",
        "            plt.ylim(0, 1 if normalize_activations else rep_vals.max()*1.1)\n",
        "            plt.title(f\"Node {i+1} Activations\")\n",
        "            plt.xlabel(\"Dimension\")\n",
        "            plt.ylabel(\"Value\")\n",
        "\n",
        "            plt.subplot(2, D_graph, i+1+D_graph)\n",
        "            plt.bar(range(len(metrics_keys)), metric_matrix[i, :],\n",
        "                    color=plt.cm.viridis(metric_matrix[i,:]/(metric_matrix[i,:].max()+1e-12)), edgecolor='black')\n",
        "            plt.ylim(0, metric_matrix.max()*1.1)\n",
        "            plt.xticks(range(len(metrics_keys)), metrics_keys, rotation=45)\n",
        "            plt.title(f\"Node {i+1} Metrics\")\n",
        "            plt.ylabel(\"Value\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def plot_activations_and_metrics_n(self, normalize_activations=True, normalize_metrics=True):\n",
        "        metrics_keys = ['wait', 'throughput', 'util', 'patience']\n",
        "        D_graph = self.D_graph\n",
        "        metric_matrix = np.zeros((D_graph, len(metrics_keys)))\n",
        "        for i in range(D_graph):\n",
        "            metrics = self._get_metrics_for_plotting(i)\n",
        "            metric_matrix[i, :] = [metrics[k] for k in metrics_keys]\n",
        "\n",
        "        metric_matrix_norm = (metric_matrix - metric_matrix.min()) / (np.ptp(metric_matrix) + 1e-12) if normalize_metrics else metric_matrix\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        for i, rep in enumerate(self.nested_reps):\n",
        "            rep_vals = np.array(rep)\n",
        "            rep_vals_norm = (rep_vals - rep_vals.min()) / (np.ptp(rep_vals) + 1e-12) if normalize_activations else rep_vals\n",
        "            colors = plt.cm.plasma(rep_vals_norm)\n",
        "\n",
        "            plt.subplot(2, D_graph, i+1)\n",
        "            plt.bar(range(len(rep_vals)), rep_vals, color=colors, edgecolor='black')\n",
        "            plt.ylim(0, 1 if normalize_activations else rep_vals.max()*1.1)\n",
        "            plt.title(f\"Node {i+1} Activations\")\n",
        "            plt.xlabel(\"Dimension\")\n",
        "            plt.ylabel(\"Value\")\n",
        "\n",
        "            plt.subplot(2, D_graph, i+1+D_graph)\n",
        "            plt.bar(range(len(metrics_keys)), metric_matrix[i, :],\n",
        "                    color=plt.cm.viridis(metric_matrix_norm[i, :]), edgecolor='black')\n",
        "            plt.ylim(0, 1 if normalize_metrics else metric_matrix.max()*1.1)\n",
        "            plt.xticks(range(len(metrics_keys)), metrics_keys, rotation=45)\n",
        "            plt.title(f\"Node {i+1} Metrics\")\n",
        "            plt.ylabel(\"Value\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Other plotting methods can similarly call self._get_metrics_for_plotting(i)\n",
        "\n",
        "    def plot_activations_and_metrics_n(self, normalize_activations=True, normalize_metrics=True):\n",
        "        \"\"\"\n",
        "        Plot per-node bar plots for:\n",
        "        1. Nested activations (colored bars)\n",
        "        2. Metrics ('wait', 'throughput', 'util', 'patience') normalized across all nodes\n",
        "        \"\"\"\n",
        "        metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "        metrics_keys = ['wait', 'throughput', 'util', 'patience']\n",
        "        D_graph = self.D_graph\n",
        "\n",
        "        # Collect metrics for all nodes\n",
        "        metric_matrix = np.zeros((D_graph, len(metrics_keys)))\n",
        "        for i, rep in enumerate(self.nested_reps):\n",
        "            metrics = metrics_evaluator.compute_node_metrics(i, y=rep)\n",
        "            metric_matrix[i, :] = [metrics[k] for k in metrics_keys]\n",
        "\n",
        "        # Normalize metrics across all nodes & metrics\n",
        "        if normalize_metrics:\n",
        "            metric_matrix_norm = (metric_matrix - metric_matrix.min()) / (np.ptp(metric_matrix) + 1e-12)\n",
        "        else:\n",
        "            metric_matrix_norm = metric_matrix\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        for i, rep in enumerate(self.nested_reps):\n",
        "            # --- Nested activations bar ---\n",
        "            rep_vals = np.array(rep)\n",
        "            if normalize_activations:\n",
        "                rep_vals_norm = (rep_vals - rep_vals.min()) / (np.ptp(rep_vals) + 1e-12)\n",
        "            else:\n",
        "                rep_vals_norm = rep_vals\n",
        "            colors = plt.cm.plasma(rep_vals_norm)\n",
        "\n",
        "            plt.subplot(2, D_graph, i+1)\n",
        "            plt.bar(range(len(rep_vals)), rep_vals, color=colors, edgecolor='black')\n",
        "            plt.ylim(0, 1 if normalize_activations else rep_vals.max()*1.1)\n",
        "            plt.title(f\"Node {i+1} Activations\")\n",
        "            plt.xlabel(\"Dimension\")\n",
        "            plt.ylabel(\"Value\")\n",
        "\n",
        "            # --- Metrics bar (normalized across all nodes) ---\n",
        "            plt.subplot(2, D_graph, i+1+D_graph)\n",
        "            plt.bar(range(len(metrics_keys)), metric_matrix[i, :],\n",
        "                    color=plt.cm.viridis(metric_matrix_norm[i, :]), edgecolor='black')\n",
        "            plt.ylim(0, 1 if normalize_metrics else metric_matrix.max()*1.1)\n",
        "            plt.xticks(range(len(metrics_keys)), metrics_keys, rotation=45)\n",
        "            plt.title(f\"Node {i+1} Metrics\")\n",
        "            plt.ylabel(\"Value\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    def compute_fuzzy_metric_tensor(self, normalize=True, verbose=False):\n",
        "        \"\"\"\n",
        "        Compute a fuzzy metric tensor for all node pairs (D_graph x D_graph x num_metrics),\n",
        "        where each slice [i,j,:] contains metrics of node j (or combined metrics of i->j),\n",
        "        optionally normalized across all nodes and metrics.\n",
        "        \"\"\"\n",
        "        metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "        metrics_keys = ['wait', 'throughput', 'util', 'patience']\n",
        "        D_graph = self.D_graph\n",
        "        num_metrics = len(metrics_keys)\n",
        "\n",
        "        tensor = np.zeros((D_graph, D_graph, num_metrics))\n",
        "\n",
        "        # Compute metric vector for each node\n",
        "        node_metrics = []\n",
        "        for i, rep in enumerate(self.nested_reps):\n",
        "            metrics = metrics_evaluator.compute_node_metrics(i, y=rep)\n",
        "            node_metrics.append(np.array([metrics[k] for k in metrics_keys]))\n",
        "\n",
        "        node_metrics = np.array(node_metrics)  # shape: (D_graph, num_metrics)\n",
        "\n",
        "        # Fill tensor: optionally weighted by edge strength in chosen_Gmat\n",
        "        for i in range(D_graph):\n",
        "            for j in range(D_graph):\n",
        "                if i == j:\n",
        "                    tensor[i, j, :] = node_metrics[j]  # self-metrics\n",
        "                else:\n",
        "                    weight = np.clip(abs(self.chosen_Gmat[i,j]), 0, 1)  # optional fuzzy weight\n",
        "                    tensor[i, j, :] = weight * node_metrics[j]\n",
        "\n",
        "        # Normalize tensor across all values (0-1)\n",
        "        if normalize:\n",
        "            tensor = (tensor - tensor.min()) / (tensor.max() - tensor.min() + 1e-12)\n",
        "\n",
        "        if verbose:\n",
        "            print(\"Fuzzy Metric Tensor shape:\", tensor.shape)\n",
        "            for i in range(D_graph):\n",
        "                for j in range(D_graph):\n",
        "                    print(f\"Node {i} -> Node {j} metrics:\", tensor[i,j,:])\n",
        "\n",
        "        return tensor\n",
        "    def compute_fuzzy_metric_tensor_bounds(self, verbose=False):\n",
        "        \"\"\"\n",
        "        Compute a fuzzy metric tensor with lower and upper bounds per node pair.\n",
        "        Uses capped metrics if available.\n",
        "        Returns tensor of shape (D_graph, D_graph, num_metrics, 2)\n",
        "        where [:,:,:,0] = lower, [:,:,:,1] = upper.\n",
        "        \"\"\"\n",
        "        metrics_keys = ['wait', 'throughput', 'util', 'patience']\n",
        "        D_graph = self.D_graph\n",
        "        num_metrics = len(metrics_keys)\n",
        "\n",
        "        tensor = np.zeros((D_graph, D_graph, num_metrics, 2))  # last dim: [lower, upper]\n",
        "\n",
        "        # Use capped metrics if they exist\n",
        "        if hasattr(self, 'capped_node_metrics') and self.capped_node_metrics is not None:\n",
        "            node_metrics = np.array([[m[k] for k in metrics_keys] for m in self.capped_node_metrics])\n",
        "        else:\n",
        "            # fallback to raw metrics\n",
        "            metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "            node_metrics = np.array([\n",
        "                [metrics_evaluator.compute_node_metrics(i, y=self.nested_reps[i])[k] for k in metrics_keys]\n",
        "                for i in range(D_graph)\n",
        "            ])\n",
        "\n",
        "        # Fuzzy bounds: for example 10% around the metric value\n",
        "        fuzz_factor = 0.1\n",
        "        for i in range(D_graph):\n",
        "            for j in range(D_graph):\n",
        "                metrics_j = node_metrics[j]\n",
        "                lower = np.clip(metrics_j * (1 - fuzz_factor), 0, None)\n",
        "                upper = metrics_j * (1 + fuzz_factor)\n",
        "                tensor[i, j, :, 0] = lower\n",
        "                tensor[i, j, :, 1] = upper\n",
        "\n",
        "        if verbose:\n",
        "            print(\"Fuzzy Metric Tensor shape (with bounds):\", tensor.shape)\n",
        "\n",
        "        return tensor\n",
        "\n",
        "    def plot_fuzzy_metric_tensor(self, fuzzy_tensor, metrics_keys=['wait','throughput','util','patience']):\n",
        "        \"\"\"\n",
        "        Plot a visualization of the Fuzzy Metric Tensor (FMT) with lower/upper bounds.\n",
        "\n",
        "        fuzzy_tensor: np.ndarray of shape (D_graph, D_graph, num_metrics, 2)\n",
        "                    last dimension = [lower, upper]\n",
        "        \"\"\"\n",
        "        D_graph = self.D_graph\n",
        "        num_metrics = len(metrics_keys)\n",
        "\n",
        "        fig, axes = plt.subplots(1, num_metrics, figsize=(4*num_metrics, 4))\n",
        "\n",
        "        for k in range(num_metrics):\n",
        "            lower = fuzzy_tensor[:,:,k,0]\n",
        "            upper = fuzzy_tensor[:,:,k,1]\n",
        "\n",
        "            # Create a fuzzy range heatmap: color = mean, alpha = range/mean\n",
        "            mean_vals = (lower + upper)/2\n",
        "            range_vals = upper - lower\n",
        "            # Normalize range for transparency\n",
        "            max_range = range_vals.max() if range_vals.max()>0 else 1.0\n",
        "            alphas = 0.2 + 0.8 * range_vals / max_range  # alpha 0.21.0\n",
        "\n",
        "            # Plot mean as heatmap\n",
        "            im = axes[k].imshow(mean_vals, cmap='viridis', vmin=0, vmax=mean_vals.max())\n",
        "\n",
        "            # Overlay range as transparency mask\n",
        "            for i in range(D_graph):\n",
        "                for j in range(D_graph):\n",
        "                    rect = plt.Rectangle((j-0.5, i-0.5), 1, 1, color='white', alpha=1-alphas[i,j])\n",
        "                    axes[k].add_patch(rect)\n",
        "                    # Annotate with bounds\n",
        "                    axes[k].text(j, i, f\"{lower[i,j]:.1f}\\n{upper[i,j]:.1f}\", color='black',\n",
        "                                ha='center', va='center', fontsize=9)\n",
        "\n",
        "            axes[k].set_title(f\"FMT - {metrics_keys[k]}\")\n",
        "            axes[k].set_xticks(range(D_graph))\n",
        "            axes[k].set_xticklabels([f\"Node {i+1}\" for i in range(D_graph)])\n",
        "            axes[k].set_yticks(range(D_graph))\n",
        "            axes[k].set_yticklabels([f\"Node {i+1}\" for i in range(D_graph)])\n",
        "\n",
        "        fig.colorbar(im, ax=axes, orientation='vertical', fraction=0.025, pad=0.04, label='Mean Metric Value')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "# ---------------- VISUALIZATIONS ----------------\n",
        "# 1. Pointwise min/max elite vs. true targets\n",
        "# ---------------- CONFIG ----------------\n",
        "#candidate_dims = [6] * 3\n",
        "#_graph = 4\n",
        "inner_archive_size = 80\n",
        "inner_offspring = 40\n",
        "outer_archive_size = 40\n",
        "outer_offspring = 40\n",
        "inner_learning = 0.1\n",
        "gamma_interlayer = 0.3\n",
        "outer_generations = 10  # for faster testing\n",
        "outer_cost_limit = 100\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "\n",
        "# ---------------- INSTANTIATE OPTIMIZER ----------------\n",
        "optimizer = GDFCM(\n",
        "    candidate_dims=candidate_dims,\n",
        "    D_graph=D_graph,\n",
        "    inner_archive_size=inner_archive_size,\n",
        "    inner_offspring=inner_offspring,\n",
        "    outer_archive_size=outer_archive_size,\n",
        "    outer_offspring=outer_offspring,\n",
        "    synthetic_targets=synthetic_targets,\n",
        "    inner_learning=inner_learning,\n",
        "    gamma_interlayer=gamma_interlayer,\n",
        "\n",
        "    causal_flag=False\n",
        ")\n",
        "\n",
        "# ---------------- RUN FULL OPTIMIZATION ----------------\n",
        "metrics_list = optimizer.run(outer_generations=outer_generations)\n",
        "\n",
        "\n",
        "# 3. Inter-layer activations tensor\n",
        "#tensor = optimizer.print_interactions(return_tensor=True)\n",
        "#print(\"Inter-layer activation tensor shape:\", tensor.shape)\n",
        "#print(tensor)\n",
        "\n",
        "# 4. Full metric dashboard (colored stacked bars + nested reps + heatmap)\n",
        "#optimizer.plot_metric_dashboard_colored()\n",
        "optimizer.plot_activations_and_metrics(normalize_activations=True)\n",
        "\n",
        "# Simple bar plots of activations per node\n",
        "#optimizer.plot_activations_and_metrics_n(normalize_activations=True, normalize_metrics=True)\n",
        "fuzzy_metric_tensor = optimizer.compute_fuzzy_metric_tensor_bounds(verbose=False)\n",
        "optimizer.plot_fuzzy_metric_tensor(fuzzy_metric_tensor)\n",
        "\n",
        "#o#ptimizer.plot_activations_bar(normalize=True)\n"
      ],
      "metadata": {
        "id": "KtiJ6876l9_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l_K_NPREmJXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Refactored GDFCM framework\n",
        "- Cleaned class structure and fixed bugs (duplicate methods, incorrect self references)\n",
        "- Added type hints and docstrings\n",
        "- Removed unused globals; functions accept data_matrix and synthetic_targets as inputs\n",
        "- Safe defaults and argument validation\n",
        "- Consolidated plotting utilities\n",
        "\n",
        "Note: This file contains plotting calls guarded by `if __name__ == '__main__'` so it won't execute on import.\n",
        "\"\"\"\n",
        "\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.cm import get_cmap\n",
        "\n",
        "\n",
        "# ---------------- METRICS EVALUATOR ----------------\n",
        "class MetricsEvaluator:\n",
        "    \"\"\"\n",
        "    Compute node-level metrics (wait, throughput, utilization, patience, score)\n",
        "    from a data matrix row and an optional node representation \"y\".\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_matrix: np.ndarray, W0: float = 10.0, T0: float = 100.0,\n",
        "                 U0: float = 1.0, P0: float = 1.0):\n",
        "        self.data_matrix = np.asarray(data_matrix)\n",
        "        if self.data_matrix.ndim != 2:\n",
        "            raise ValueError(\"data_matrix must be 2D\")\n",
        "        self.num_features = self.data_matrix.shape[1]\n",
        "        self.W0, self.T0, self.U0 = W0, T0, U0\n",
        "        self.P0 = P0\n",
        "\n",
        "    def compute_node_metrics(self, node_idx: int, y: Optional[np.ndarray] = None) -> Dict[str, float]:\n",
        "        row = self.data_matrix[node_idx % self.data_matrix.shape[0]]\n",
        "        k = self.num_features\n",
        "        base = max(1, k // 3)\n",
        "\n",
        "        wait_signal = np.mean(row[0:base])\n",
        "        throughput_signal = np.mean(row[base:2 * base])\n",
        "        util_signal = np.mean(row[2 * base:k]) if k > 2 * base else 0.0\n",
        "\n",
        "        if y is None:\n",
        "            y = np.array([0.5, 0.5, 0.5])\n",
        "        else:\n",
        "            y = np.asarray(y).ravel()\n",
        "            y = y[:3] if y.size >= 3 else np.pad(y, (0, 3 - y.size), constant_values=0.5)\n",
        "\n",
        "        wait = np.clip(self.W0 * (1 + 1.2 * wait_signal + 0.8 * float(y[0])), 0, 100)\n",
        "        throughput = np.clip(self.T0 * (1 + 1.1 * throughput_signal + 0.6 * float(y[1]) - 0.4 * wait_signal), 0, 150)\n",
        "        util = np.clip(self.U0 + 0.8 * util_signal + 0.6 * float(y[2]), 0, 1)\n",
        "        patience_signal = np.mean(row)\n",
        "        patience = np.clip(self.P0 * (1 + 0.5 * patience_signal + 0.5 * float(y[0])), 0, 2)\n",
        "        score = -wait + throughput + util + patience\n",
        "\n",
        "        return {\n",
        "            'wait': float(wait),\n",
        "            'throughput': float(throughput),\n",
        "            'util': float(util),\n",
        "            'patience': float(patience),\n",
        "            'score': float(score)\n",
        "        }\n",
        "\n",
        "\n",
        "# ---------------- INTER-LAYER ----------------\n",
        "class InterLayer:\n",
        "    \"\"\"\n",
        "    Handles edge activations between nodes and computes a simple MI-like score\n",
        "    based on pairwise squared correlations of activation vectors.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, D_graph: int, max_inner_dim: int, inter_dim: Optional[int] = None,\n",
        "                 edge_threshold: float = 0.02, gamma: float = 1.0, seed: int = 42):\n",
        "        self.D_graph = int(D_graph)\n",
        "        self.max_input = int(2 * max_inner_dim)\n",
        "        self.edge_threshold = float(edge_threshold)\n",
        "        self.gamma = float(gamma)\n",
        "        self.inter_dim = int(inter_dim) if inter_dim is not None else int(max_inner_dim)\n",
        "\n",
        "        rng = np.random.RandomState(seed)\n",
        "        # Weights and biases for directed pairs (i,j), i != j\n",
        "        self.weights: Dict[Tuple[int, int], np.ndarray] = {}\n",
        "        self.bias: Dict[Tuple[int, int], np.ndarray] = {}\n",
        "        for i in range(self.D_graph):\n",
        "            for j in range(self.D_graph):\n",
        "                if i == j:\n",
        "                    continue\n",
        "                self.weights[(i, j)] = rng.uniform(-0.6, 0.6, (self.inter_dim, self.max_input))\n",
        "                self.bias[(i, j)] = rng.uniform(-0.3, 0.3, self.inter_dim)\n",
        "\n",
        "    def compute_edge_activation(self, i: int, j: int, nested_reps: List[np.ndarray]) -> np.ndarray:\n",
        "        concat = np.concatenate([np.asarray(nested_reps[i]).ravel(), np.asarray(nested_reps[j]).ravel()])\n",
        "        if concat.size < self.max_input:\n",
        "            concat = np.pad(concat, (0, self.max_input - concat.size))\n",
        "        concat = concat[:self.max_input]\n",
        "        v = self.weights[(i, j)].dot(concat) + self.bias[(i, j)]\n",
        "        # elementwise sigmoid\n",
        "        return 1.0 / (1.0 + np.exp(-v))\n",
        "\n",
        "    def build_activations(self, Gmat: np.ndarray, nested_reps: List[np.ndarray]) -> Dict[Tuple[int, int], np.ndarray]:\n",
        "        acts: Dict[Tuple[int, int], np.ndarray] = {}\n",
        "        G = np.asarray(Gmat)\n",
        "        assert G.shape == (self.D_graph, self.D_graph), \"Gmat must be D_graph x D_graph\"\n",
        "        for i in range(self.D_graph):\n",
        "            for j in range(self.D_graph):\n",
        "                if i == j:\n",
        "                    continue\n",
        "                if abs(G[i, j]) > self.edge_threshold:\n",
        "                    acts[(i, j)] = self.compute_edge_activation(i, j, nested_reps)\n",
        "        return acts\n",
        "\n",
        "    @staticmethod\n",
        "    def pairwise_squared_corr(acts: Dict[Tuple[int, int], np.ndarray]) -> float:\n",
        "        if len(acts) < 2:\n",
        "            return 0.0\n",
        "        A = np.stack(list(acts.values()))  # shape (num_edges, inter_dim)\n",
        "        A_centered = A - A.mean(axis=1, keepdims=True)\n",
        "        # sample std with ddof=1\n",
        "        stds = np.sqrt(np.sum(A_centered ** 2, axis=1) / (A.shape[1] - 1) + 1e-12)\n",
        "        cov = A_centered @ A_centered.T / (A.shape[1] - 1)\n",
        "        corr = cov / (np.outer(stds, stds) + 1e-12)\n",
        "        np.fill_diagonal(corr, 0.0)\n",
        "        return float((corr ** 2).sum())\n",
        "\n",
        "    def mi_for_graph(self, Gmat: np.ndarray, nested_reps: List[np.ndarray]) -> float:\n",
        "        acts = self.build_activations(Gmat, nested_reps)\n",
        "        if not acts:\n",
        "            return 0.0\n",
        "        return float(self.gamma * self.pairwise_squared_corr(acts))\n",
        "\n",
        "    def print_interactions(self, Gmat: np.ndarray, nested_reps: List[np.ndarray],\n",
        "                           return_tensor: bool = True, verbose: bool = True) -> Optional[np.ndarray]:\n",
        "        \"\"\"\n",
        "        Print interactions and optionally return an activation tensor shaped\n",
        "        (D_graph, D_graph, inter_dim).\n",
        "        \"\"\"\n",
        "        inter_tensor = np.zeros((self.D_graph, self.D_graph, self.inter_dim))\n",
        "        acts = self.build_activations(Gmat, nested_reps)\n",
        "        if not acts:\n",
        "            if verbose:\n",
        "                print(\"No active edges above threshold.\")\n",
        "            return inter_tensor if return_tensor else None\n",
        "\n",
        "        for (i, j), vec in acts.items():\n",
        "            inter_tensor[i, j, :] = vec\n",
        "            if verbose:\n",
        "                act_str = \", \".join([f\"{v:.3f}\" for v in vec])\n",
        "                print(f\"Node {i} -> Node {j}: [{act_str}]\")\n",
        "        return inter_tensor if return_tensor else None\n",
        "\n",
        "\n",
        "# ---------------- GDFCM CLASS ----------------\n",
        "class GDFCM:\n",
        "    \"\"\"\n",
        "    High level driver that alternates inner optimization per-node and computes\n",
        "    outer loop metrics, caps budget, and provides plotting utilities.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 candidate_dims: List[int],\n",
        "                 D_graph: int,\n",
        "                 data_matrix: np.ndarray,\n",
        "                 synthetic_targets: List[Dict[int, np.ndarray]],\n",
        "                 inner_archive_size: int = 80,\n",
        "                 inner_offspring: int = 40,\n",
        "                 outer_archive_size: int = 40,\n",
        "                 outer_offspring: int = 40,\n",
        "                 inner_learning: float = 0.1,\n",
        "                 gamma_interlayer: float = 1.0,\n",
        "                 causal_flag: bool = True,\n",
        "                 seed: int = 42):\n",
        "\n",
        "        self.candidate_dims = list(candidate_dims)\n",
        "        self.D_graph = int(D_graph)\n",
        "        self.data_matrix = np.asarray(data_matrix)\n",
        "        if self.data_matrix.ndim != 2:\n",
        "            raise ValueError(\"data_matrix must be 2D\")\n",
        "\n",
        "        self.synthetic_targets = synthetic_targets\n",
        "        self.inner_archive_size = int(inner_archive_size)\n",
        "        self.inner_offspring = int(inner_offspring)\n",
        "        self.outer_archive_size = int(outer_archive_size)\n",
        "        self.outer_offspring = int(outer_offspring)\n",
        "        self.inner_learning = float(inner_learning)\n",
        "        self.causal_flag = bool(causal_flag)\n",
        "\n",
        "        # nested_reps: initialize to zeros vectors sized to the max candidate dim\n",
        "        max_dim = max(self.candidate_dims)\n",
        "        self.nested_reps = [np.zeros(max_dim) for _ in range(self.D_graph)]\n",
        "        self.best_dim_per_node = [self.candidate_dims[0] for _ in range(self.D_graph)]\n",
        "\n",
        "        self.inter_layer = InterLayer(self.D_graph, max_inner_dim=max_dim, gamma=gamma_interlayer, seed=seed)\n",
        "        # chosen_Gmat should be provided or randomly initialized\n",
        "        rng = np.random.RandomState(seed)\n",
        "        self.chosen_Gmat = rng.uniform(-0.5, 0.5, (self.D_graph, self.D_graph))\n",
        "        np.fill_diagonal(self.chosen_Gmat, 0.0)\n",
        "\n",
        "        self.l2_before: List[float] = []\n",
        "        self.l2_after: List[float] = []\n",
        "\n",
        "        # store capped node metrics (list of dicts) for plotting\n",
        "        self.capped_node_metrics: Optional[List[Dict[str, float]]] = None\n",
        "\n",
        "    # ---------- INNER LOOP ----------\n",
        "    def run_inner(self, node_idx: int, target: np.ndarray, D_fcm: int,\n",
        "                  steps: int = 100, lr_x: float = 0.001, lr_W: float = 0.001) -> Tuple[np.ndarray, np.ndarray, float]:\n",
        "        x = np.asarray(target).copy()\n",
        "        W = np.random.uniform(-0.6, 0.6, (D_fcm, D_fcm))\n",
        "        np.fill_diagonal(W, 0.0)\n",
        "\n",
        "        self.l2_before.append(float(np.linalg.norm(self.nested_reps[node_idx] - x)))\n",
        "\n",
        "        for _ in range(steps):\n",
        "            z = W.dot(x)\n",
        "            Theta_grad_z = 2 * z - x  # corrected: original used target in some places; keep simple\n",
        "            Theta_grad_x = Theta_grad_z @ W + 0.5 * (x + 1) ** 2\n",
        "            Theta_grad_W = np.outer(Theta_grad_z, x)\n",
        "\n",
        "            x -= lr_x * np.clip(Theta_grad_x, -0.05, 0.05)\n",
        "            x = np.clip(x, 0.0, 1.0)\n",
        "            W -= lr_W * np.clip(Theta_grad_W, -0.01, 0.01)\n",
        "            np.fill_diagonal(W, 0.0)\n",
        "            W = np.clip(W, -1.0, 1.0)\n",
        "\n",
        "        self.nested_reps[node_idx] = x\n",
        "        self.l2_after.append(float(np.linalg.norm(x - target)))\n",
        "        mi_score = self.inter_layer.mi_for_graph(self.chosen_Gmat, self.nested_reps)\n",
        "        return x, W, mi_score\n",
        "\n",
        "    # ---------- OUTER LOOP ----------\n",
        "    def run_outer(self, outer_cost_limit: float = 100.0) -> Tuple[List[Dict[str, float]], float]:\n",
        "        evaluator = MetricsEvaluator(self.data_matrix)\n",
        "        node_metrics_list: List[Dict[str, float]] = []\n",
        "        raw_scores: List[float] = []\n",
        "\n",
        "        # Compute node metrics from current nested_reps\n",
        "        for i, rep in enumerate(self.nested_reps):\n",
        "            metrics = evaluator.compute_node_metrics(i, y=rep)\n",
        "            node_metrics_list.append(metrics)\n",
        "            raw_scores.append(metrics['score'])\n",
        "\n",
        "        raw_scores = np.array(raw_scores)\n",
        "        total_raw = float(raw_scores.sum())\n",
        "\n",
        "        # Apply budget cap proportionally if needed\n",
        "        if total_raw > outer_cost_limit and total_raw > 0:\n",
        "            scale_factor = outer_cost_limit / total_raw\n",
        "            for metrics in node_metrics_list:\n",
        "                for key in ['wait', 'throughput', 'util', 'patience', 'score']:\n",
        "                    metrics[key] *= scale_factor\n",
        "            total_capped = float(outer_cost_limit)\n",
        "        else:\n",
        "            total_capped = float(total_raw)\n",
        "\n",
        "        self.capped_node_metrics = node_metrics_list\n",
        "        return node_metrics_list, total_capped\n",
        "\n",
        "    # ---------- FULL RUN ----------\n",
        "    def run(self, outer_generations: int = 100, outer_cost_limit: float = 1000.0,\n",
        "            inner_steps: int = 100) -> List[Dict[str, float]]:\n",
        "        final_metrics: Optional[List[Dict[str, float]]] = None\n",
        "\n",
        "        for gen in range(int(outer_generations)):\n",
        "            mi_scores: List[float] = []\n",
        "\n",
        "            # inner loop per node\n",
        "            for node_idx in range(self.D_graph):\n",
        "                dim = int(self.best_dim_per_node[node_idx])\n",
        "                # obtain a target vector from synthetic_targets\n",
        "                try:\n",
        "                    target = np.asarray(self.synthetic_targets[node_idx][dim])\n",
        "                except Exception:\n",
        "                    # fallback to zeros if target not available\n",
        "                    target = np.zeros(dim)\n",
        "\n",
        "                _, _, mi_score = self.run_inner(node_idx, target, dim, steps=inner_steps)\n",
        "                mi_scores.append(mi_score)\n",
        "\n",
        "            # outer loop\n",
        "            metrics_list, capped_score = self.run_outer(outer_cost_limit=outer_cost_limit)\n",
        "\n",
        "            # print a concise summary per generation\n",
        "            print(f\"\\n--- Generation {gen} Metrics ---\")\n",
        "            for i, m in enumerate(metrics_list):\n",
        "                print(f\"Node {i} | \" + \" | \".join([f\"{k}: {v:.2f}\" for k, v in m.items()]))\n",
        "            print(f\"Outer Score (capped): {capped_score:.3f}\")\n",
        "\n",
        "            final_metrics = metrics_list\n",
        "\n",
        "        return final_metrics if final_metrics is not None else []\n",
        "\n",
        "    # ---------- PLOTTING UTILITIES ----------\n",
        "    def _get_metrics_for_plotting(self, i: int) -> Dict[str, float]:\n",
        "        if self.capped_node_metrics is not None:\n",
        "            return self.capped_node_metrics[i]\n",
        "        evaluator = MetricsEvaluator(self.data_matrix)\n",
        "        return evaluator.compute_node_metrics(i, y=self.nested_reps[i])\n",
        "\n",
        "    def plot_metric_dashboard_colored(self):\n",
        "        metrics_keys = ['wait', 'throughput', 'util', 'patience']\n",
        "        D_graph = self.D_graph\n",
        "        metric_matrix = np.zeros((D_graph, len(metrics_keys)))\n",
        "        for i in range(D_graph):\n",
        "            metrics = self._get_metrics_for_plotting(i)\n",
        "            metric_matrix[i, :] = [metrics[k] for k in metrics_keys]\n",
        "\n",
        "        metric_norm = (metric_matrix - metric_matrix.min(axis=0)) / (np.ptp(metric_matrix, axis=0) + 1e-12)\n",
        "        cmap = get_cmap(\"viridis\")\n",
        "\n",
        "        fig, axes = plt.subplots(3, 1, figsize=(12, 8), gridspec_kw={'height_ratios': [1, 1, 1]})\n",
        "\n",
        "        bottoms = np.zeros(D_graph)\n",
        "        for idx, key in enumerate(metrics_keys):\n",
        "            colors = [cmap(val) for val in metric_norm[:, idx]]\n",
        "            axes[0].bar(range(D_graph), metric_matrix[:, idx], bottom=bottoms, color=colors,\n",
        "                        edgecolor='black', label=key)\n",
        "            bottoms += metric_matrix[:, idx]\n",
        "        axes[0].set_xticks(range(D_graph))\n",
        "        axes[0].set_xticklabels([f\"Node {i+1}\" for i in range(D_graph)])\n",
        "        axes[0].set_ylabel(\"Metric Value (Stacked)\")\n",
        "        axes[0].set_title(\"Stacked Node Metrics (Color by Intensity)\")\n",
        "        axes[0].legend(loc='upper right')\n",
        "\n",
        "        for i, rep in enumerate(self.nested_reps):\n",
        "            axes[1].plot(range(len(rep)), rep + i * 1.1, '-o', label=f\"Node {i+1}\")\n",
        "        axes[1].set_ylabel(\"Nested Reps (shifted)\")\n",
        "        axes[1].set_title(\"Nested Representations per Node\")\n",
        "        axes[1].legend(loc='upper right')\n",
        "\n",
        "        sns.heatmap(metric_matrix, annot=True, fmt=\".2f\", cmap=\"viridis\",\n",
        "                    yticklabels=[f\"Node {i+1}\" for i in range(D_graph)],\n",
        "                    xticklabels=metrics_keys, ax=axes[2])\n",
        "        axes[2].set_title(\"Node Metrics Heatmap\")\n",
        "        axes[2].set_xlabel(\"Metrics\")\n",
        "        axes[2].set_ylabel(\"Nodes\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def plot_activations_and_metrics(self, normalize_activations: bool = True, normalize_metrics: bool = True):\n",
        "        metrics_keys = ['wait', 'throughput', 'util', 'patience']\n",
        "        D_graph = self.D_graph\n",
        "\n",
        "        metric_matrix = np.zeros((D_graph, len(metrics_keys)))\n",
        "        for i in range(D_graph):\n",
        "            metrics = self._get_metrics_for_plotting(i)\n",
        "            metric_matrix[i, :] = [metrics[k] for k in metrics_keys]\n",
        "\n",
        "        metric_matrix_norm = (metric_matrix - metric_matrix.min()) / (np.ptp(metric_matrix) + 1e-12) if normalize_metrics else metric_matrix\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        for i, rep in enumerate(self.nested_reps):\n",
        "            rep_vals = np.array(rep)\n",
        "            if normalize_activations:\n",
        "                rep_vals_norm = (rep_vals - rep_vals.min()) / (np.ptp(rep_vals) + 1e-12)\n",
        "            else:\n",
        "                rep_vals_norm = rep_vals\n",
        "            colors = plt.cm.plasma(rep_vals_norm)\n",
        "\n",
        "            plt.subplot(2, D_graph, i + 1)\n",
        "            plt.bar(range(len(rep_vals)), rep_vals, color=colors, edgecolor='black')\n",
        "            plt.ylim(0, 1 if normalize_activations else rep_vals.max() * 1.1)\n",
        "            plt.title(f\"Node {i+1} Activations\")\n",
        "            plt.xlabel(\"Dimension\")\n",
        "            plt.ylabel(\"Value\")\n",
        "\n",
        "            plt.subplot(2, D_graph, i + 1 + D_graph)\n",
        "            plt.bar(range(len(metrics_keys)), metric_matrix[i, :],\n",
        "                    color=plt.cm.viridis(metric_matrix_norm[i, :]), edgecolor='black')\n",
        "            plt.ylim(0, 1 if normalize_metrics else metric_matrix.max() * 1.1)\n",
        "            plt.xticks(range(len(metrics_keys)), metrics_keys, rotation=45)\n",
        "            plt.title(f\"Node {i+1} Metrics\")\n",
        "            plt.ylabel(\"Value\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # ---------- FUZZY METRIC TENSOR ----------\n",
        "    def compute_fuzzy_metric_tensor(self, normalize: bool = True, verbose: bool = False) -> np.ndarray:\n",
        "        metrics_keys = ['wait', 'throughput', 'util', 'patience']\n",
        "        D_graph = self.D_graph\n",
        "        num_metrics = len(metrics_keys)\n",
        "\n",
        "        evaluator = MetricsEvaluator(self.data_matrix)\n",
        "        node_metrics = np.array([\n",
        "            [evaluator.compute_node_metrics(i, y=self.nested_reps[i])[k] for k in metrics_keys]\n",
        "            for i in range(D_graph)\n",
        "        ])\n",
        "\n",
        "        tensor = np.zeros((D_graph, D_graph, num_metrics))\n",
        "        for i in range(D_graph):\n",
        "            for j in range(D_graph):\n",
        "                weight = np.clip(abs(self.chosen_Gmat[i, j]), 0.0, 1.0)\n",
        "                tensor[i, j, :] = weight * node_metrics[j] if i != j else node_metrics[j]\n",
        "\n",
        "        if normalize:\n",
        "            tensor = (tensor - tensor.min()) / (tensor.max() - tensor.min() + 1e-12)\n",
        "\n",
        "        if verbose:\n",
        "            print(\"Fuzzy Metric Tensor shape:\", tensor.shape)\n",
        "\n",
        "        return tensor\n",
        "\n",
        "    def compute_fuzzy_metric_tensor_bounds(self, fuzz_factor: float = 0.1, verbose: bool = False) -> np.ndarray:\n",
        "        metrics_keys = ['wait', 'throughput', 'util', 'patience']\n",
        "        D_graph = self.D_graph\n",
        "        num_metrics = len(metrics_keys)\n",
        "\n",
        "        if self.capped_node_metrics is not None:\n",
        "            node_metrics = np.array([[m[k] for k in metrics_keys] for m in self.capped_node_metrics])\n",
        "        else:\n",
        "            evaluator = MetricsEvaluator(self.data_matrix)\n",
        "            node_metrics = np.array([\n",
        "                [evaluator.compute_node_metrics(i, y=self.nested_reps[i])[k] for k in metrics_keys]\n",
        "                for i in range(D_graph)\n",
        "            ])\n",
        "\n",
        "        tensor = np.zeros((D_graph, D_graph, num_metrics, 2))\n",
        "        for i in range(D_graph):\n",
        "            for j in range(D_graph):\n",
        "                metrics_j = node_metrics[j]\n",
        "                lower = np.clip(metrics_j * (1 - fuzz_factor), 0.0, None)\n",
        "                upper = metrics_j * (1 + fuzz_factor)\n",
        "                tensor[i, j, :, 0] = lower\n",
        "                tensor[i, j, :, 1] = upper\n",
        "\n",
        "        if verbose:\n",
        "            print(\"Fuzzy Metric Tensor shape (with bounds):\", tensor.shape)\n",
        "\n",
        "        return tensor\n",
        "\n",
        "    def plot_fuzzy_metric_tensor(self, fuzzy_tensor: np.ndarray, metrics_keys: List[str] = None):\n",
        "        if metrics_keys is None:\n",
        "            metrics_keys = ['wait', 'throughput', 'util', 'patience']\n",
        "        D_graph = self.D_graph\n",
        "        num_metrics = len(metrics_keys)\n",
        "\n",
        "        fig, axes = plt.subplots(1, num_metrics, figsize=(4 * num_metrics, 4))\n",
        "\n",
        "        for k in range(num_metrics):\n",
        "            lower = fuzzy_tensor[:, :, k, 0]\n",
        "            upper = fuzzy_tensor[:, :, k, 1]\n",
        "            mean_vals = (lower + upper) / 2.0\n",
        "            range_vals = upper - lower\n",
        "\n",
        "            # Compute alphas per-node (row)\n",
        "            alphas = np.zeros_like(range_vals)\n",
        "            for i in range(D_graph):\n",
        "                row_range = range_vals[i, :]\n",
        "                row_max = row_range.max() if row_range.max() > 0 else 1.0\n",
        "                alphas[i, :] = 0.2 + 0.8 * (row_range / row_max)\n",
        "            alphas = np.clip(alphas, 0.0, 1.0)  # ensure valid alpha\n",
        "\n",
        "            im = axes[k].imshow(mean_vals, cmap='viridis', vmin=0, vmax=mean_vals.max())\n",
        "            for i in range(D_graph):\n",
        "                for j in range(D_graph):\n",
        "                    rect = plt.Rectangle((j - 0.5, i - 0.5), 1, 1, color='white', alpha=1 - alphas[i, j])\n",
        "                    axes[k].add_patch(rect)\n",
        "                    axes[k].text(j, i, f\"{lower[i, j]:.1f}\\n{upper[i, j]:.1f}\", color='black',\n",
        "                                ha='center', va='center', fontsize=9)\n",
        "\n",
        "            axes[k].set_title(f\"FMT - {metrics_keys[k]}\")\n",
        "            axes[k].set_xticks(range(D_graph))\n",
        "            axes[k].set_xticklabels([f\"Node {i+1}\" for i in range(D_graph)])\n",
        "            axes[k].set_yticks(range(D_graph))\n",
        "            axes[k].set_yticklabels([f\"Node {i+1}\" for i in range(D_graph)])\n",
        "\n",
        "        fig.colorbar(im, ax=axes, orientation='vertical', fraction=0.025, pad=0.04, label='Mean Metric Value')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# ---------------- EXAMPLE USAGE (guarded) ----------------\n",
        "if __name__ == '__main__':\n",
        "    # This is an example of how to instantiate and run the refactored classes.\n",
        "    # Replace `DATA_MATRIX` and `synthetic_targets` with your own data structures.\n",
        "\n",
        "\n",
        "    optimizer = GDFCM(\n",
        "        candidate_dims=[6] * 3,\n",
        "        D_graph=4,\n",
        "        data_matrix=DATA_MATRIX,\n",
        "        synthetic_targets=synthetic_targets,\n",
        "        inner_archive_size=80,\n",
        "        inner_offspring=40,\n",
        "        outer_archive_size=40,\n",
        "        outer_offspring=40,\n",
        "        inner_learning=0.1,\n",
        "        gamma_interlayer=0.3,\n",
        "        causal_flag=False,\n",
        "        seed=42,\n",
        "    )\n",
        "\n",
        "    metrics_list = optimizer.run(outer_generations=3, outer_cost_limit=1000.0, inner_steps=50)\n",
        "\n",
        "    tensor = optimizer.inter_layer.print_interactions(optimizer.chosen_Gmat, optimizer.nested_reps, return_tensor=True)\n",
        "    print(\"Inter-layer activation tensor shape:\", tensor.shape)\n",
        "\n",
        "    optimizer.plot_metric_dashboard_colored()\n",
        "    optimizer.plot_activations_and_metrics(normalize_activations=True)\n",
        "    fuzzy_tensor = optimizer.compute_fuzzy_metric_tensor_bounds(verbose=False)\n",
        "    optimizer.plot_fuzzy_metric_tensor(fuzzy_tensor)\n"
      ],
      "metadata": {
        "id": "3yDD9bGvnCxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.seed()\n",
        "# Simulate a small GDFCM setup for testing\n",
        "\n",
        "# Instantiate GDFCM\n",
        "optimizer = GDFCM(\n",
        "    candidate_dims=candidate_dims,\n",
        "    D_graph=D_graph,\n",
        "    data_matrix=DATA_MATRIX,\n",
        "    synthetic_targets=synthetic_targets,\n",
        "    inner_archive_size=10,\n",
        "    inner_offspring=5,\n",
        "    outer_archive_size=5,\n",
        "    outer_offspring=5,\n",
        "    inner_learning=0.1,\n",
        "    gamma_interlayer=0.3,\n",
        "    causal_flag=False,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Run a few outer generations to initialize nested reps and capped metrics\n",
        "optimizer.run(outer_generations=2, outer_cost_limit=1000.0, inner_steps=20)\n",
        "\n",
        "# Compute fuzzy metric tensor bounds\n",
        "fuzzy_tensor = optimizer.compute_fuzzy_metric_tensor_bounds(fuzz_factor=0.1)\n",
        "\n",
        "# Create a new random data point (activation vector) to test\n",
        "data_point = np.random.rand(max(candidate_dims))\n",
        "\n",
        "# Function from previous step\n",
        "def evaluate_data_point_against_fuzzy_tensor_strict(optimizer: GDFCM,\n",
        "                                                    fuzzy_tensor: np.ndarray,\n",
        "                                                    data_point: np.ndarray):\n",
        "    D_graph = optimizer.D_graph\n",
        "    metrics_keys = ['wait', 'throughput', 'util', 'patience']\n",
        "    num_metrics = len(metrics_keys)\n",
        "\n",
        "    evaluator = MetricsEvaluator(optimizer.data_matrix)\n",
        "\n",
        "    # Compute metrics for the new data_point for all nodes\n",
        "    node_metrics_point = np.array([evaluator.compute_node_metrics(i, y=data_point)[k]\n",
        "                                   for i in range(D_graph) for k in metrics_keys]).reshape(D_graph, num_metrics)\n",
        "\n",
        "    per_node_results = {}\n",
        "    overall_status = []\n",
        "\n",
        "    for i in range(D_graph):\n",
        "        node_statuses = []\n",
        "        for j in range(D_graph):\n",
        "            lower_bounds = fuzzy_tensor[i, j, :, 0]\n",
        "            upper_bounds = fuzzy_tensor[i, j, :, 1]\n",
        "\n",
        "            inside_mask = (node_metrics_point[j] >= lower_bounds) & (node_metrics_point[j] <= upper_bounds)\n",
        "\n",
        "            # Only 'junk' if outside all bounds for this node across all neighbors\n",
        "            if inside_mask.any():\n",
        "                status = 'useful' if inside_mask.all() else 'opportunity'\n",
        "            else:\n",
        "                status = 'junk'\n",
        "\n",
        "            node_statuses.append(status)\n",
        "\n",
        "        # Determine overall node status\n",
        "        if all(s == 'useful' for s in node_statuses):\n",
        "            overall_node_status = 'useful'\n",
        "        elif all(s == 'junk' for s in node_statuses):\n",
        "            overall_node_status = 'junk'\n",
        "        else:\n",
        "            overall_node_status = 'opportunity'\n",
        "\n",
        "        per_node_results[i] = {\n",
        "            'node_statuses': node_statuses,\n",
        "            'overall_node_status': overall_node_status,\n",
        "            'metrics': {k: node_metrics_point[i, idx] for idx, k in enumerate(metrics_keys)}\n",
        "        }\n",
        "\n",
        "        overall_status.append(overall_node_status)\n",
        "\n",
        "    # Determine global insight\n",
        "    if all(s == 'useful' for s in overall_status):\n",
        "        overall_insight = \"Data point is fully useful across all nodes and interactions.\"\n",
        "    elif all(s == 'junk' for s in overall_status):\n",
        "        overall_insight = \"Data point is mostly junk across all nodes and interactions.\"\n",
        "    else:\n",
        "        overall_insight = \"Data point shows mixed opportunity across nodes and interactions.\"\n",
        "\n",
        "    return {\n",
        "        'per_node': per_node_results,\n",
        "        'overall': overall_insight\n",
        "    }\n",
        "\n",
        "# Evaluate the random data point\n",
        "strict_result = evaluate_data_point_against_fuzzy_tensor_strict(optimizer, fuzzy_tensor, data_point)\n",
        "strict_result\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "heOQ7Ze_n4Hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_data_point_against_fuzzy_tensor_activation(optimizer: GDFCM,\n",
        "                                                        fuzzy_tensor: np.ndarray,\n",
        "                                                        data_point: np.ndarray):\n",
        "    \"\"\"\n",
        "    Strict evaluation: compares the actual activations of a data point against\n",
        "    the fuzzy metric tensor bounds per node and per neighbor.\n",
        "    \"\"\"\n",
        "    D_graph = optimizer.D_graph\n",
        "    inter_dim = optimizer.inter_layer.inter_dim\n",
        "\n",
        "    # Temporarily set the nested reps to the new data_point for evaluation\n",
        "    original_reps = optimizer.nested_reps.copy()\n",
        "    for i in range(D_graph):\n",
        "        dims = len(optimizer.nested_reps[i])\n",
        "        optimizer.nested_reps[i] = data_point[:dims]\n",
        "\n",
        "    # Build per-node results\n",
        "    per_node_results = {}\n",
        "    overall_status = []\n",
        "\n",
        "    for i in range(D_graph):\n",
        "        node_statuses = []\n",
        "        rep_i = optimizer.nested_reps[i]\n",
        "        for j in range(D_graph):\n",
        "            lower_bounds = fuzzy_tensor[i, j, :, 0]\n",
        "            upper_bounds = fuzzy_tensor[i, j, :, 1]\n",
        "\n",
        "            # Compare activation vector to bounds\n",
        "            if rep_i.size != lower_bounds.size:\n",
        "                # Pad or truncate to match dimensions\n",
        "                rep_check = np.pad(rep_i, (0, max(0, lower_bounds.size - rep_i.size)), constant_values=0)\n",
        "                rep_check = rep_check[:lower_bounds.size]\n",
        "            else:\n",
        "                rep_check = rep_i\n",
        "\n",
        "            inside_mask = (rep_check >= lower_bounds) & (rep_check <= upper_bounds)\n",
        "\n",
        "            if inside_mask.all():\n",
        "                status = 'useful'\n",
        "            elif inside_mask.any():\n",
        "                status = 'opportunity'\n",
        "            else:\n",
        "                status = 'junk'\n",
        "\n",
        "            node_statuses.append(status)\n",
        "\n",
        "        # Overall status per node\n",
        "        if all(s == 'useful' for s in node_statuses):\n",
        "            overall_node_status = 'useful'\n",
        "        elif all(s == 'junk' for s in node_statuses):\n",
        "            overall_node_status = 'junk'\n",
        "        else:\n",
        "            overall_node_status = 'opportunity'\n",
        "\n",
        "        per_node_results[i] = {\n",
        "            'node_statuses': node_statuses,\n",
        "            'overall_node_status': overall_node_status,\n",
        "            'activations': rep_i.copy()\n",
        "        }\n",
        "        overall_status.append(overall_node_status)\n",
        "\n",
        "    # Global insight\n",
        "    if all(s == 'useful' for s in overall_status):\n",
        "        overall_insight = \"Data point is fully useful across all nodes and interactions.\"\n",
        "    elif all(s == 'junk' for s in overall_status):\n",
        "        overall_insight = \"Data point is mostly junk across all nodes and interactions.\"\n",
        "    else:\n",
        "        overall_insight = \"Data point shows mixed opportunity across nodes and interactions.\"\n",
        "\n",
        "    # Restore original nested reps\n",
        "    optimizer.nested_reps = original_reps\n",
        "\n",
        "    return {\n",
        "        'per_node': per_node_results,\n",
        "        '':'',\n",
        "        'overall': overall_insight\n",
        "    }\n",
        "\n",
        "# Evaluate the random data point\n",
        "strict_result = evaluate_data_point_against_fuzzy_tensor_strict(optimizer, fuzzy_tensor, data_point)\n",
        "strict_result\n"
      ],
      "metadata": {
        "id": "SWnhklBVtae1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed()\n",
        "\n",
        "# --- Simulate GDFCM setup (assuming candidate_dims, D_graph, DATA_MATRIX, synthetic_targets are defined) ---\n",
        "optimizer = GDFCM(\n",
        "    candidate_dims=candidate_dims,\n",
        "    D_graph=D_graph,\n",
        "    data_matrix=DATA_MATRIX,\n",
        "    synthetic_targets=synthetic_targets,\n",
        "    inner_archive_size=10,\n",
        "    inner_offspring=5,\n",
        "    outer_archive_size=5,\n",
        "    outer_offspring=5,\n",
        "    inner_learning=0.1,\n",
        "    gamma_interlayer=0.3,\n",
        "    causal_flag=False,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Run a few outer generations\n",
        "optimizer.run(outer_generations=2, outer_cost_limit=1000.0, inner_steps=20)\n",
        "\n",
        "# Compute fuzzy metric tensor bounds\n",
        "fuzzy_tensor = optimizer.compute_fuzzy_metric_tensor_bounds(fuzz_factor=0.1)\n",
        "\n",
        "# Create a random data point\n",
        "data_point = np.random.rand(max(candidate_dims))\n",
        "\n",
        "\n",
        "# --- Function: Evaluate data point against fuzzy tensor ---\n",
        "def evaluate_and_plot_fuzzy(optimizer: GDFCM, fuzzy_tensor: np.ndarray, data_point: np.ndarray):\n",
        "    D_graph = optimizer.D_graph\n",
        "    metrics_keys = ['wait', 'throughput', 'util', 'patience']\n",
        "    num_metrics = len(metrics_keys)\n",
        "\n",
        "    evaluator = MetricsEvaluator(optimizer.data_matrix)\n",
        "\n",
        "    # Compute metrics for the new data_point\n",
        "    node_metrics_point = np.array([\n",
        "        evaluator.compute_node_metrics(i, y=data_point)[k]\n",
        "        for i in range(D_graph) for k in metrics_keys\n",
        "    ]).reshape(D_graph, num_metrics)\n",
        "\n",
        "    per_node_results = {}\n",
        "    overall_status = []\n",
        "\n",
        "    # --- Evaluation ---\n",
        "    for i in range(D_graph):\n",
        "        node_statuses = []\n",
        "        for j in range(D_graph):\n",
        "            lower_bounds = fuzzy_tensor[i, j, :, 0]\n",
        "            upper_bounds = fuzzy_tensor[i, j, :, 1]\n",
        "\n",
        "            # Adjust vector size if needed\n",
        "            rep_check = node_metrics_point[j]\n",
        "            if rep_check.size != lower_bounds.size:\n",
        "                rep_check = np.pad(rep_check, (0, max(0, lower_bounds.size - rep_check.size)), constant_values=0)\n",
        "                rep_check = rep_check[:lower_bounds.size]\n",
        "\n",
        "            inside_mask = (rep_check >= lower_bounds) & (rep_check <= upper_bounds)\n",
        "\n",
        "            if inside_mask.all():\n",
        "                status = 'useful'\n",
        "            elif inside_mask.any():\n",
        "                status = 'opportunity'\n",
        "            else:\n",
        "                status = 'junk'\n",
        "\n",
        "            node_statuses.append(status)\n",
        "\n",
        "        # Overall node status\n",
        "        if all(s == 'useful' for s in node_statuses):\n",
        "            overall_node_status = 'useful'\n",
        "        elif all(s == 'junk' for s in node_statuses):\n",
        "            overall_node_status = 'junk'\n",
        "        else:\n",
        "            overall_node_status = 'opportunity'\n",
        "\n",
        "        per_node_results[i] = {\n",
        "            'node_statuses': node_statuses,\n",
        "            'overall_node_status': overall_node_status,\n",
        "            'activations': node_metrics_point[i].copy()\n",
        "        }\n",
        "        overall_status.append(overall_node_status)\n",
        "\n",
        "    # Global insight\n",
        "    if all(s == 'useful' for s in overall_status):\n",
        "        overall_insight = \"Data point is fully useful across all nodes and interactions.\"\n",
        "    elif all(s == 'junk' for s in overall_status):\n",
        "        overall_insight = \"Data point is mostly junk across all nodes and interactions.\"\n",
        "    else:\n",
        "        overall_insight = \"Data point shows mixed opportunity across nodes and interactions.\"\n",
        "\n",
        "    # --- Plot activations vs fuzzy bounds ---\n",
        "    for k, metric in enumerate(metrics_keys):\n",
        "        plt.figure(figsize=(6, 5))\n",
        "        lower = fuzzy_tensor[:, :, k, 0]\n",
        "        upper = fuzzy_tensor[:, :, k, 1]\n",
        "\n",
        "        for i in range(D_graph):\n",
        "            # Activation for this node & metric\n",
        "            y_val = node_metrics_point[i, k]\n",
        "            plt.scatter(i, y_val, color='red', s=80, label='Data Point' if i==0 else \"\")\n",
        "\n",
        "            # Plot fuzzy bounds of node i across all neighbors\n",
        "            for j in range(D_graph):\n",
        "                plt.plot([i, i], [lower[i, j], upper[i, j]], color='blue', alpha=0.5, lw=4)\n",
        "\n",
        "        plt.title(f\"Activations vs Fuzzy Bounds for Metric '{metric}'\")\n",
        "        plt.xlabel(\"Node Index\")\n",
        "        plt.ylabel(\"Activation Value\")\n",
        "        plt.xticks(range(D_graph), [f\"Node {n+1}\" for n in range(D_graph)])\n",
        "        plt.ylim(0, 1.05)\n",
        "        plt.legend()\n",
        "        plt.grid(alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    return {\n",
        "        'per_node': per_node_results,\n",
        "        'overall': overall_insight\n",
        "    }\n",
        "\n",
        "\n",
        "# --- Run evaluation and plotting ---\n",
        "result = evaluate_and_plot_fuzzy(optimizer, fuzzy_tensor, data_point)\n",
        "result\n"
      ],
      "metadata": {
        "id": "NO5yttdqwKJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed()\n",
        "\n",
        "# --- Assume optimizer, fuzzy_tensor, data_point are already defined ---\n",
        "\n",
        "def evaluate_and_plot_fuzzy_heatmap(optimizer: GDFCM, fuzzy_tensor: np.ndarray, data_point: np.ndarray):\n",
        "    D_graph = optimizer.D_graph\n",
        "    metrics_keys = ['wait', 'throughput', 'util', 'patience']\n",
        "    num_metrics = len(metrics_keys)\n",
        "\n",
        "    evaluator = MetricsEvaluator(optimizer.data_matrix)\n",
        "\n",
        "    # Compute metrics for all nodes\n",
        "    node_metrics_point = np.array([\n",
        "        evaluator.compute_node_metrics(i, y=data_point)[k]\n",
        "        for i in range(D_graph) for k in metrics_keys\n",
        "    ]).reshape(D_graph, num_metrics)\n",
        "\n",
        "    per_node_results = {}\n",
        "    overall_status = []\n",
        "\n",
        "    # Status grid for coloring\n",
        "    status_grid = np.empty((D_graph, D_graph), dtype=object)\n",
        "\n",
        "    # --- Evaluate ---\n",
        "    for i in range(D_graph):\n",
        "        node_statuses = []\n",
        "        for j in range(D_graph):\n",
        "            lower_bounds = fuzzy_tensor[i, j, :, 0]\n",
        "            upper_bounds = fuzzy_tensor[i, j, :, 1]\n",
        "\n",
        "            rep_check = node_metrics_point[j]\n",
        "            if rep_check.size != lower_bounds.size:\n",
        "                rep_check = np.pad(rep_check, (0, max(0, lower_bounds.size - rep_check.size)), constant_values=0)\n",
        "                rep_check = rep_check[:lower_bounds.size]\n",
        "\n",
        "            inside_mask = (rep_check >= lower_bounds) & (rep_check <= upper_bounds)\n",
        "\n",
        "            if inside_mask.all():\n",
        "                status = 'useful'\n",
        "            elif inside_mask.any():\n",
        "                status = 'opportunity'\n",
        "            else:\n",
        "                status = 'junk'\n",
        "\n",
        "            node_statuses.append(status)\n",
        "            status_grid[i, j] = status\n",
        "\n",
        "        if all(s == 'useful' for s in node_statuses):\n",
        "            overall_node_status = 'useful'\n",
        "        elif all(s == 'junk' for s in node_statuses):\n",
        "            overall_node_status = 'junk'\n",
        "        else:\n",
        "            overall_node_status = 'opportunity'\n",
        "\n",
        "        per_node_results[i] = {\n",
        "            'node_statuses': node_statuses,\n",
        "            'overall_node_status': overall_node_status,\n",
        "            'activations': node_metrics_point[i].copy()\n",
        "        }\n",
        "        overall_status.append(overall_node_status)\n",
        "\n",
        "    # Global insight\n",
        "    if all(s == 'useful' for s in overall_status):\n",
        "        overall_insight = \"Data point is fully useful across all nodes and interactions.\"\n",
        "    elif all(s == 'junk' for s in overall_status):\n",
        "        overall_insight = \"Data point is mostly junk across all nodes and interactions.\"\n",
        "    else:\n",
        "        overall_insight = \"Data point shows mixed opportunity across nodes and interactions.\"\n",
        "\n",
        "    # --- Plot heatmap per metric ---\n",
        "    color_map = {'useful': 'green', 'opportunity': 'yellow', 'junk': 'red'}\n",
        "\n",
        "    for k, metric in enumerate(metrics_keys):\n",
        "        plt.figure(figsize=(6,5))\n",
        "        for i in range(D_graph):\n",
        "            for j in range(D_graph):\n",
        "                status = status_grid[i, j]\n",
        "                plt.scatter(j, i, color=color_map[status], s=500, alpha=0.6)\n",
        "                plt.text(j, i, f\"{node_metrics_point[j,k]:.2f}\", ha='center', va='center', color='black', fontsize=10)\n",
        "\n",
        "        plt.title(f\"Node Activations vs Fuzzy Bounds - {metric}\")\n",
        "        plt.xlabel(\"Neighbor Node Index\")\n",
        "        plt.ylabel(\"Target Node Index\")\n",
        "        plt.xticks(range(D_graph), [f\"Node {n+1}\" for n in range(D_graph)])\n",
        "        plt.yticks(range(D_graph), [f\"Node {n+1}\" for n in range(D_graph)])\n",
        "        plt.xlim(-0.5, D_graph-0.5)\n",
        "        plt.ylim(-0.5, D_graph-0.5)\n",
        "        plt.gca().invert_yaxis()\n",
        "        plt.grid(alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    return {\n",
        "        'per_node': per_node_results,\n",
        "        'overall': overall_insight\n",
        "    }\n",
        "\n",
        "# --- Run evaluation & heatmap plotting ---\n",
        "result = evaluate_and_plot_fuzzy_heatmap(optimizer, fuzzy_tensor, data_point)\n",
        "result\n"
      ],
      "metadata": {
        "id": "y71Z0PsBx0MC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def evaluate_and_plot_fuzzy_heatmap_combined(optimizer: GDFCM, fuzzy_tensor: np.ndarray, data_point: np.ndarray):\n",
        "    D_graph = optimizer.D_graph\n",
        "    metrics_keys = ['wait', 'throughput', 'util', 'patience']\n",
        "    num_metrics = len(metrics_keys)\n",
        "\n",
        "    evaluator = MetricsEvaluator(optimizer.data_matrix)\n",
        "\n",
        "    # Compute metrics for all nodes\n",
        "    node_metrics_point = np.array([\n",
        "        evaluator.compute_node_metrics(i, y=data_point)[k]\n",
        "        for i in range(D_graph) for k in metrics_keys\n",
        "    ]).reshape(D_graph, num_metrics)\n",
        "\n",
        "    per_node_results = {}\n",
        "    overall_status = []\n",
        "\n",
        "    # Status grid for coloring\n",
        "    status_grid = np.empty((D_graph, D_graph, num_metrics), dtype=object)\n",
        "\n",
        "    # --- Evaluate ---\n",
        "    for i in range(D_graph):\n",
        "        node_statuses = []\n",
        "        for j in range(D_graph):\n",
        "            for k in range(num_metrics):\n",
        "                lower_bounds = fuzzy_tensor[i, j, k, 0]\n",
        "                upper_bounds = fuzzy_tensor[i, j, k, 1]\n",
        "\n",
        "                rep_check = node_metrics_point[j,k]\n",
        "                inside_mask = (rep_check >= lower_bounds) & (rep_check <= upper_bounds)\n",
        "\n",
        "                if inside_mask:\n",
        "                    status = 'useful'\n",
        "                else:\n",
        "                    status = 'junk'\n",
        "\n",
        "                status_grid[i,j,k] = status\n",
        "                node_statuses.append(status)\n",
        "\n",
        "        # Determine overall node status\n",
        "        if all(s == 'useful' for s in node_statuses):\n",
        "            overall_node_status = 'useful'\n",
        "        elif all(s == 'junk' for s in node_statuses):\n",
        "            overall_node_status = 'junk'\n",
        "        else:\n",
        "            overall_node_status = 'opportunity'\n",
        "\n",
        "        per_node_results[i] = {\n",
        "            'node_statuses': node_statuses,\n",
        "            'overall_node_status': overall_node_status,\n",
        "            'activations': node_metrics_point[i].copy()\n",
        "        }\n",
        "        overall_status.append(overall_node_status)\n",
        "\n",
        "    # Global insight\n",
        "    if all(s == 'useful' for s in overall_status):\n",
        "        overall_insight = \"Data point is fully useful across all nodes and interactions.\"\n",
        "    elif all(s == 'junk' for s in overall_status):\n",
        "        overall_insight = \"Data point is mostly junk across all nodes and interactions.\"\n",
        "    else:\n",
        "        overall_insight = \"Data point shows mixed opportunity across nodes and interactions.\"\n",
        "\n",
        "    # --- Plot combined heatmap in one row ---\n",
        "    color_map = {'useful': 'green', 'opportunity': 'yellow', 'junk': 'red'}\n",
        "    fig, axes = plt.subplots(1, num_metrics, figsize=(5*num_metrics, 5), sharey=True)\n",
        "\n",
        "    for k, metric in enumerate(metrics_keys):\n",
        "        ax = axes[k]\n",
        "        for i in range(D_graph):\n",
        "            for j in range(D_graph):\n",
        "                status = status_grid[i,j,k]\n",
        "                ax.scatter(j, i, color=color_map[status], s=500, alpha=0.6)\n",
        "                ax.text(j, i, f\"{node_metrics_point[j,k]:.2f}\", ha='center', va='center', color='black', fontsize=10)\n",
        "\n",
        "        ax.set_title(f\"{metric}\")\n",
        "        ax.set_xlabel(\"Neighbor Node\")\n",
        "        if k == 0:\n",
        "            ax.set_ylabel(\"Target Node\")\n",
        "        ax.set_xticks(range(D_graph))\n",
        "        ax.set_xticklabels([f\"N{n+1}\" for n in range(D_graph)])\n",
        "        ax.set_yticks(range(D_graph))\n",
        "        ax.set_yticklabels([f\"N{n+1}\" for n in range(D_graph)])\n",
        "        ax.set_xlim(-0.5, D_graph-0.5)\n",
        "        ax.set_ylim(-0.5, D_graph-0.5)\n",
        "        ax.invert_yaxis()\n",
        "        ax.grid(alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return {\n",
        "        'per_node': per_node_results,\n",
        "        'overall': overall_insight\n",
        "    }\n",
        "\n",
        "# --- Run merged heatmap evaluation ---\n",
        "result = evaluate_and_plot_fuzzy_heatmap_combined(optimizer, fuzzy_tensor, data_point)\n",
        "result\n"
      ],
      "metadata": {
        "id": "LHc_q-7OytD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def evaluate_and_plot_fuzzy_heatmap_unified(optimizer: GDFCM, fuzzy_tensor: np.ndarray, data_point: np.ndarray):\n",
        "    D_graph = optimizer.D_graph\n",
        "    metrics_keys = ['wait', 'throughput', 'util', 'patience']\n",
        "    num_metrics = len(metrics_keys)\n",
        "\n",
        "    evaluator = MetricsEvaluator(optimizer.data_matrix)\n",
        "\n",
        "    # Compute metrics for all nodes\n",
        "    node_metrics_point = np.array([\n",
        "        evaluator.compute_node_metrics(i, y=data_point)[k]\n",
        "        for i in range(D_graph) for k in metrics_keys\n",
        "    ]).reshape(D_graph, num_metrics)  # shape: (D_graph, num_metrics)\n",
        "\n",
        "    # Status matrix for unified heatmap: rows=metrics, cols=nodes\n",
        "    status_matrix = np.empty((num_metrics, D_graph), dtype=object)\n",
        "\n",
        "    color_map = {'useful': 'green', 'opportunity': 'yellow', 'junk': 'red'}\n",
        "\n",
        "    # --- Evaluate each metric/node combination ---\n",
        "    for j in range(D_graph):  # neighbor node\n",
        "        for k in range(num_metrics):  # metric\n",
        "            # Collect status across target nodes\n",
        "            statuses = []\n",
        "            for i in range(D_graph):  # target node\n",
        "                lower = fuzzy_tensor[i, j, k, 0]\n",
        "                upper = fuzzy_tensor[i, j, k, 1]\n",
        "                val = node_metrics_point[j, k]\n",
        "                if val < lower or val > upper:\n",
        "                    statuses.append('junk')\n",
        "                else:\n",
        "                    statuses.append('useful')\n",
        "            # Final status: if all useful -> useful, else junk if all junk, else opportunity\n",
        "            if all(s == 'useful' for s in statuses):\n",
        "                status_matrix[k, j] = 'useful'\n",
        "            elif all(s == 'junk' for s in statuses):\n",
        "                status_matrix[k, j] = 'junk'\n",
        "            else:\n",
        "                status_matrix[k, j] = 'opportunity'\n",
        "\n",
        "    # --- Plot unified heatmap ---\n",
        "    fig, ax = plt.subplots(figsize=(8,6))\n",
        "\n",
        "    # Transpose: rows = nodes, columns = metrics\n",
        "    for i in range(D_graph):        # nodes as rows\n",
        "        for j in range(num_metrics):  # metrics as columns\n",
        "            ax.scatter(j, i, color=color_map[status_matrix[j, i]], s=600, alpha=0.6)\n",
        "            ax.text(j, i, f\"{node_metrics_point[i,j]:.2f}\", ha='center', va='center', fontsize=10, color='black')\n",
        "\n",
        "    ax.set_xticks(range(num_metrics))\n",
        "    ax.set_xticklabels([f\"M{m+1}\" for m in range(num_metrics)])\n",
        "    ax.set_yticks(range(D_graph))\n",
        "    ax.set_yticklabels([f\"N{n+1}\" for n in range(D_graph)])\n",
        "    ax.set_xlabel(\"Metrics\")\n",
        "    ax.set_ylabel(\"Nodes\")\n",
        "    ax.set_title(\"Data Point Activations vs Fuzzy Bounds (Transposed)\")\n",
        "    ax.set_xlim(-0.5, num_metrics-0.5)\n",
        "    ax.set_ylim(-0.5, D_graph-0.5)\n",
        "    ax.invert_yaxis()\n",
        "    ax.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    return status_matrix\n",
        "\n",
        "# --- Run unified heatmap ---\n",
        "status_matrix = evaluate_and_plot_fuzzy_heatmap_unified(optimizer, fuzzy_tensor, data_point)\n",
        "status_matrix\n"
      ],
      "metadata": {
        "id": "1qfykYOdzkL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5eD0Xxc1z281"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}