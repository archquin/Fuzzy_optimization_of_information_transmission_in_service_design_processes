{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Replace Bellow cells 1 by 1 with domain specific knowlledge/information/data"
      ],
      "metadata": {
        "id": "16DS9q3X--lL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxOzVIYCVTOc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "\n",
        "# Reset random seed for 2025\n",
        "np.random.seed(2025)\n",
        "\n",
        "# =====================================================\n",
        "# 1. COFFEE SERVICE TOUCHPOINTS (6 NODES)\n",
        "# =====================================================\n",
        "TOUCHPOINTS = [\n",
        "    \"Order_Intake\",      # 0: POS entry & Queue\n",
        "    \"Brewing_Prep\",      # 1: Extraction & Milk work\n",
        "    \"Quality_Audit\",     # 2: Visual & Temp check\n",
        "    \"Payment_Finalize\",  # 3: Transaction & Digital Tip\n",
        "    \"Handover_Vibe\",     # 4: Customer interaction\n",
        "    \"Inventory_Log\"      # 5: Waste/Stock deduction\n",
        "]\n",
        "\n",
        "# =====================================================\n",
        "# 2. METRIC CONFIGURATION\n",
        "# =====================================================\n",
        "METRIC_KEYS = [\n",
        "    \"Throughput_Speed\",    # 0\n",
        "    \"Product_Quality\",     # 1\n",
        "    \"Customer_Experience\", # 2\n",
        "    \"Operational_Margin\"   # 3\n",
        "]\n",
        "\n",
        "METRIC_FEATURE_MAP = {\n",
        "    \"Order_Intake\":     ['queue_wait_sec'],      # Speed\n",
        "    \"Brewing_Prep\":     ['extraction_accuracy'], # Quality\n",
        "    \"Quality_Audit\":    ['rejection_rate'],      # Quality/Experience\n",
        "    \"Payment_Finalize\": ['transaction_speed'],   # Speed\n",
        "    \"Handover_Vibe\":    ['loyalty_opt_in'],      # Experience\n",
        "    \"Inventory_Log\":    ['waste_variance']       # Margin\n",
        "}\n",
        "\n",
        "# METRIC TARGET MATRIX (6x4)\n",
        "METRIC_TARGET = np.array([\n",
        "    [1, 0, 0, 0], # 0. Order -> Speed\n",
        "    [0, 1, 0, 0], # 1. Prep -> Quality\n",
        "    [0, 1, 1, 0], # 2. Audit -> Quality + Exp\n",
        "    [1, 0, 0, 0], # 3. Payment -> Speed\n",
        "    [0, 0, 1, 0], # 4. Handover -> Experience\n",
        "    [0, 0, 0, 1]  # 5. Inventory -> Margin\n",
        "])\n",
        "\n",
        "# METRIC FORMULAS (Scales 0.0 - 1.0)\n",
        "METRIC_FORMULAS = [\n",
        "    lambda x: np.exp(-x / 300.0),       # 0. Order: Faster = better\n",
        "    lambda x: x / 100.0,                # 1. Prep: Accuracy %\n",
        "    lambda x: 1.0 - x,                  # 2. Audit: Low rejection = high score\n",
        "    lambda x: 60 / (60 + x),            # 3. Payment: Speed normalization\n",
        "    lambda x: 1.0 if x > 0.5 else 0.4,  # 4. Handover: Loyalty threshold\n",
        "    lambda x: np.exp(-x / 10.0)         # 5. Inventory: Low waste = high margin\n",
        "]\n",
        "\n",
        "# =====================================================\n",
        "# 3. COFFEE DSM (DEPENDENCY STRUCTURE)\n",
        "# =====================================================\n",
        "# Defines the flow and the \"Production Knot\" (Rework loop 1 <-> 2)\n",
        "DSM = np.array([\n",
        "    #0    1    2    3    4    5\n",
        "    [0,   1,   0,   0,   0,   0],   # Order -> Prep\n",
        "    [0,   0,   1,   0,   0,   0],   # Prep -> Audit\n",
        "    [0,   0.4, 0,   1,   0,   0],   # Audit Feedback to Prep (Rework Loop)\n",
        "    [0,   0,   0,   0,   1,   0],   # Payment -> Handover\n",
        "    [0,   0,   0,   0,   0,   1],   # Handover -> Log\n",
        "    [0,   0,   0,   0,   0,   0],   # Log (End)\n",
        "])\n",
        "\n",
        "# =====================================================\n",
        "# 4. SIMULATOR CLASS\n",
        "# =====================================================\n",
        "\n",
        "class CoffeeSimulator:\n",
        "    def __init__(self):\n",
        "        self.generators = {\n",
        "            \"Order_Intake\":     lambda: {\"queue_wait_sec\": np.random.exponential(120)},\n",
        "            \"Brewing_Prep\":     lambda: {\"extraction_accuracy\": np.random.uniform(80, 100)},\n",
        "            \"Quality_Audit\":    lambda: {\"rejection_rate\": np.random.beta(1, 10)},\n",
        "            \"Payment_Finalize\": lambda: {\"transaction_speed\": np.random.uniform(5, 45)},\n",
        "            \"Handover_Vibe\":    lambda: {\"loyalty_opt_in\": np.random.uniform(0, 1)},\n",
        "            \"Inventory_Log\":    lambda: {\"waste_variance\": np.random.rayleigh(2)}\n",
        "        }\n",
        "\n",
        "    def run(self, dsm_matrix):\n",
        "        G = nx.DiGraph()\n",
        "        for i, u in enumerate(TOUCHPOINTS):\n",
        "            for j, v in enumerate(TOUCHPOINTS):\n",
        "                if dsm_matrix[i, j] > 0:\n",
        "                    G.add_edge(u, v)\n",
        "\n",
        "        # Cluster Detection (Strongly Connected Components)\n",
        "        sccs = list(nx.strongly_connected_components(G))\n",
        "        node_to_mod = {n: i for i, scc in enumerate(sccs) for n in scc}\n",
        "\n",
        "        MG = nx.DiGraph()\n",
        "        for u, v in G.edges():\n",
        "            if node_to_mod[u] != node_to_mod[v]:\n",
        "                MG.add_edge(node_to_mod[u], node_to_mod[v])\n",
        "\n",
        "        levels = list(nx.topological_generations(MG))\n",
        "        results = []\n",
        "\n",
        "        for lvl in levels:\n",
        "            for mod_idx in lvl:\n",
        "                nodes = list(sccs[mod_idx])\n",
        "                is_rework_cluster = len(nodes) > 1\n",
        "\n",
        "                for node_name in nodes:\n",
        "                    node_idx = TOUCHPOINTS.index(node_name)\n",
        "                    data_dict = self.generators[node_name]()\n",
        "\n",
        "                    primary_key = METRIC_FEATURE_MAP[node_name][0]\n",
        "                    raw_val = data_dict[primary_key]\n",
        "                    score = METRIC_FORMULAS[node_idx](raw_val)\n",
        "\n",
        "                    for m_idx, active in enumerate(METRIC_TARGET[node_idx]):\n",
        "                        if active:\n",
        "                            results.append({\n",
        "                                \"Touchpoint\": node_name,\n",
        "                                \"Feature\": primary_key,\n",
        "                                \"Value\": round(raw_val, 2),\n",
        "                                \"Score\": round(score, 3),\n",
        "                                \"Metric\": METRIC_KEYS[m_idx],\n",
        "                                \"Rework_Risk\": is_rework_cluster\n",
        "                            })\n",
        "\n",
        "        return pd.DataFrame(results), sccs\n",
        "\n",
        "# =====================================================\n",
        "# 5. EXECUTION\n",
        "# =====================================================\n",
        "\n",
        "sim = CoffeeSimulator()\n",
        "df, sccs = sim.run(DSM)\n",
        "\n",
        "print(\"\\n\" + \"=\"*75)\n",
        "print(\"COFFEE SHOP OPERATIONS: KPI & REWORK ANALYSIS\")\n",
        "print(\"=\"*75)\n",
        "\n",
        "print(\"\\n1. KPI SCORECARD BY PROCESS STEP\")\n",
        "print(df[['Touchpoint', 'Feature', 'Value', 'Score', 'Metric']].to_string(index=False))\n",
        "\n",
        "print(\"\\n2. AGGREGATED BUSINESS HEALTH\")\n",
        "print(df.groupby('Metric')['Score'].mean().reset_index().round(3))\n",
        "\n",
        "print(\"\\n3. OPERATIONAL BOTTLENECKS (Coupled Production Knot)\")\n",
        "clusters = [list(c) for c in sccs if len(c) > 1]\n",
        "if clusters:\n",
        "    for i, c in enumerate(clusters):\n",
        "        print(f\"   ⚠️ Cluster {i+1} (Rework Loop): {c}\")\n",
        "else:\n",
        "    print(\"   ✅ No complex rework loops detected.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*75)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "\n",
        "# Reset random seed for 2025\n",
        "np.random.seed(2025)\n",
        "\n",
        "# =====================================================\n",
        "# 1. SERVICE TOUCHPOINTS (6 NODES)\n",
        "# =====================================================\n",
        "TOUCHPOINTS = [\n",
        "    \"Order_Intake\",      # 0: Customer queuing & POS entry\n",
        "    \"Brewing_Prep\",      # 1: Grinding & Espresso extraction\n",
        "    \"Milk_Texturing\",    # 2: Frothing & Temp control\n",
        "    \"Quality_Audit\",     # 3: Visual & Flavor verification\n",
        "    \"Handover_Vibe\",     # 4: Customer name-call & interaction\n",
        "    \"Inventory_Log\"      # 5: Waste tracking & stock update\n",
        "]\n",
        "\n",
        "# =====================================================\n",
        "# 2. COFFEE DSM (DEPENDENCY STRUCTURE MATRIX)\n",
        "# =====================================================\n",
        "# High values = High likelihood of a step being impacted by another.\n",
        "# Note the loop between 1, 2, and 3 (The \"Production Knot\")\n",
        "DSM_MATRIX = np.array([\n",
        "    #0    1    2    3    4    5\n",
        "    [0,   1,   0,   0,   0,   0],   # 0: Order -> Prep\n",
        "    [0,   0,   0.8, 0.9, 0,   0],   # 1: Prep -> Milk & Audit\n",
        "    [0,   0.7, 0,   0.9, 0,   0],   # 2: Milk -> Prep (Feedback) & Audit\n",
        "    [0,   0.5, 0.5, 0,   1,   0.4], # 3: Audit -> Rework Loop to 1/2 or Handover\n",
        "    [0,   0,   0,   0,   0,   1],   # 4: Handover -> Inventory\n",
        "    [0,   0,   0,   0,   0,   0],   # 5: Inventory (End)\n",
        "])\n",
        "\n",
        "# =====================================================\n",
        "# 3. GRAPH ENGINE & MODULE DETECTION\n",
        "# =====================================================\n",
        "G = nx.DiGraph()\n",
        "for i, u in enumerate(TOUCHPOINTS):\n",
        "    for j, v in enumerate(TOUCHPOINTS):\n",
        "        if DSM_MATRIX[i, j] > 0:\n",
        "            G.add_edge(u, v, weight=DSM_MATRIX[i, j])\n",
        "\n",
        "# Identify Rework Loops via Strongly Connected Components\n",
        "SCCS = list(nx.strongly_connected_components(G))\n",
        "\n",
        "# Topological Leveling for Modular Execution\n",
        "module_map = {node: idx for idx, comp in enumerate(SCCS) for node in comp}\n",
        "MG = nx.DiGraph()\n",
        "for u, v in G.edges():\n",
        "    mu, mv = module_map[u], module_map[v]\n",
        "    if mu != mv: MG.add_edge(mu, mv)\n",
        "\n",
        "LEVELS = list(nx.topological_generations(MG))\n",
        "\n",
        "# =====================================================\n",
        "# 4. DATA GENERATORS (Coffee Specifics)\n",
        "# =====================================================\n",
        "def gen_intake():\n",
        "    return {\"queue_length\": np.random.poisson(5), \"upsell_success\": np.random.random() > 0.7}\n",
        "\n",
        "def gen_prep():\n",
        "    return {\"extraction_time\": np.random.normal(28, 3), \"temp_celsius\": np.random.uniform(90, 96)}\n",
        "\n",
        "def gen_milk():\n",
        "    return {\"froth_density\": np.random.beta(8, 2), \"milk_waste_ml\": np.random.exponential(20)}\n",
        "\n",
        "def gen_audit():\n",
        "    return {\"defect_detected\": np.random.choice([0, 1], p=[0.9, 0.1]), \"aesthetic_score\": np.random.uniform(0.7, 1.0)}\n",
        "\n",
        "def gen_handover():\n",
        "    return {\"sentiment_score\": np.random.uniform(3.5, 5.0), \"wait_time_total\": np.random.normal(180, 40)}\n",
        "\n",
        "def gen_log():\n",
        "    return {\"inventory_accuracy\": 0.99, \"carbon_footprint_index\": np.random.uniform(0.1, 0.5)}\n",
        "\n",
        "GENERATOR_MAP = {\n",
        "    \"Order_Intake\": gen_intake, \"Brewing_Prep\": gen_prep, \"Milk_Texturing\": gen_milk,\n",
        "    \"Quality_Audit\": gen_audit, \"Handover_Vibe\": gen_handover, \"Inventory_Log\": gen_log\n",
        "}\n",
        "\n",
        "# =====================================================\n",
        "# 5. SIMULATION & METRICS\n",
        "# =====================================================\n",
        "class CoffeeMetricsEvaluator:\n",
        "    def compute_metrics(self, df):\n",
        "        # Extract mean values for key indicators\n",
        "        extract_time = df[df['variable'] == 'extraction_time']['value'].mean() or 28\n",
        "        waste = df[df['variable'] == 'milk_waste_ml']['value'].mean() or 0\n",
        "        sentiment = df[df['variable'] == 'sentiment_score']['value'].mean() or 4.0\n",
        "\n",
        "        # Operational Risk: High waste and inconsistent extraction\n",
        "        op_risk = (waste * 0.05) + (abs(28 - extract_time) * 0.1)\n",
        "        # Quality Score: Sentiment and precision\n",
        "        quality = (sentiment / 5.0) * 0.7 + (1.0 if 25 <= extract_time <= 31 else 0.5) * 0.3\n",
        "\n",
        "        return np.clip(op_risk, 0, 1), np.clip(quality, 0, 1)\n",
        "\n",
        "def run_modular_sim(levels, sccs):\n",
        "    rows = []\n",
        "    for lvl_idx, modules in enumerate(levels, 1):\n",
        "        for m_idx in modules:\n",
        "            nodes = list(sccs[m_idx])\n",
        "            for tp in nodes:\n",
        "                for k, v in GENERATOR_MAP[tp]().items():\n",
        "                    rows.append({\n",
        "                        \"touchpoint\": tp, \"level\": lvl_idx,\n",
        "                        \"type\": \"Knot (Rework)\" if len(nodes) > 1 else \"Linear\",\n",
        "                        \"variable\": k, \"value\": v\n",
        "                    })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# Execute\n",
        "df_sim = run_modular_sim(LEVELS, SCCS)\n",
        "evaluator = CoffeeMetricsEvaluator()\n",
        "risk, qual = evaluator.compute_metrics(df_sim)\n",
        "\n",
        "# =====================================================\n",
        "# 6. REPORT\n",
        "# =====================================================\n",
        "print(\"\\n=== COFFEE SHOP ARCHITECTURE OPTIMIZATION ===\")\n",
        "print(f\"Total Workflow Steps: {len(TOUCHPOINTS)}\")\n",
        "print(f\"System Coordination Complexity (DSM Sum): {DSM_MATRIX.sum():.2f}\")\n",
        "\n",
        "print(\"\\n1. MODULAR STRUCTURE (Strongly Connected Components)\")\n",
        "for i, lvl in enumerate(LEVELS, 1):\n",
        "    for mod_idx in lvl:\n",
        "        nodes = list(SCCS[mod_idx])\n",
        "        type_str = \"⚠️ REWORK KNOT\" if len(nodes) > 1 else \"✅ LINEAR STEP\"\n",
        "        print(f\"  Level {i} | {type_str:<15} | {nodes}\")\n",
        "\n",
        "print(\"\\n2. SYSTEM HEALTH INDEX\")\n",
        "print(f\"  Operational Risk Index: {risk:.4f} (Target: <0.2000)\")\n",
        "print(f\"  Service Quality Score:  {qual:.4f} (Target: >0.8500)\")\n",
        "\n",
        "print(\"\\n3. SAMPLE DATA (Knot Nodes)\")\n",
        "print(df_sim[df_sim['type'] == 'Knot (Rework)'].head(5).to_string(index=False))"
      ],
      "metadata": {
        "id": "TyiLuN-7VTz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import numpy as np\n",
        "\n",
        "# ======================================================\n",
        "# 1. METRIC KEYS (The 6-Node Coffee & Risk String)\n",
        "# ======================================================\n",
        "METRIC_KEYS = [\n",
        "    \"Order_Logistics\",       # 0: Input Phase (Footfall & Queue)\n",
        "    \"Brewing_Hub\",            # 1: HUB - Technical Extraction (The \"MRI\")\n",
        "    \"Safety_Audit_HACCP\",    # 2: HUB - Regulatory Compliance\n",
        "    \"Liability_Underwriting\",# 3: HUB - Financial Risk & Insurance\n",
        "    \"Service_Handover\",      # 4: Experience & Sentiment\n",
        "    \"Aggregate_Margin\"       # 5: Output Summary Index\n",
        "]\n",
        "\n",
        "# ======================================================\n",
        "# 2. FEATURE KEYS (Operations + Actuarial Domain)\n",
        "# ======================================================\n",
        "FEATURE_KEYS = [\n",
        "    # Node 0: Logistics\n",
        "    'pedestrian_footfall', 'pos_latency_ms',\n",
        "    # Node 1 & 2: Production/Safety Knot\n",
        "    'boiler_pressure_stability', 'water_tds_level',\n",
        "    'inspection_violation_count', 'sanitation_sop_score',\n",
        "    # Node 3: Risk/Insurance\n",
        "    'premium_per_revenue', 'liability_exposure_index',\n",
        "    'risk_retention_limit', 'inventory_shrinkage_rate',\n",
        "    # Node 4 & 5: Sentiment & Margin\n",
        "    'sentiment_score', 'loyalty_opt_in',\n",
        "    'cogs_reduction_target', 'waste_variance'\n",
        "]\n",
        "\n",
        "# ======================================================\n",
        "# 3. MAPPING (Node -> Features)\n",
        "# ======================================================\n",
        "metric_feature_map = {\n",
        "    \"Order_Logistics\":       ['pedestrian_footfall', 'pos_latency_ms'],\n",
        "    \"Brewing_Hub\":           ['boiler_pressure_stability', 'water_tds_level'],\n",
        "    \"Safety_Audit_HACCP\":    ['inspection_violation_count', 'sanitation_sop_score'],\n",
        "    \"Liability_Underwriting\":['premium_per_revenue', 'liability_exposure_index'],\n",
        "    \"Service_Handover\":      ['sentiment_score', 'loyalty_opt_in'],\n",
        "    \"Aggregate_Margin\":      ['cogs_reduction_target', 'waste_variance']\n",
        "}\n",
        "# ======================================================\n",
        "# 5. TARGET MATRIX (6x6 Interaction Graph)\n",
        "# ======================================================\n",
        "METRIC_TARGET = [\n",
        "    # 0  1  2  3  4  5\n",
        "    [1, 1, 0, 0, 0, 1], # 0: Logistics -> Prep & Margin\n",
        "    [0, 1, 1, 0, 1, 1], # 1: Brewing -> Audit, Handover, Margin\n",
        "    [0, 1, 1, 1, 0, 1], # 2: Audit -> Feedback to Prep, Risk, Margin\n",
        "    [1, 0, 1, 1, 0, 1], # 3: Liability -> Feedbacks to Logistics & Audit\n",
        "    [0, 0, 0, 0, 1, 1], # 4: Handover -> Margin\n",
        "    [0, 0, 0, 0, 0, 1]  # 5: Aggregate\n",
        "]\n",
        "\n",
        "# Formulas (Actuarial & Operational Logic)\n",
        "METRIC_FORMULAS = [\n",
        "    lambda x: np.log(x + 1),            # 0. Logistics benefit\n",
        "    lambda x: np.tanh(x) * 2.0,         # 1. Brewing Stability (The MEP of Coffee)\n",
        "    lambda x: np.exp(-x * 3.0),         # 2. Safety (The Regulatory Accred of Coffee)\n",
        "    lambda x: 1.0 / (x * 0.5 + 0.1),    # 3. Liability (Lower exposure = Higher Score)\n",
        "    lambda x: x / 5.0,                  # 4. Sentiment (1-5 Scale)\n",
        "    lambda x: np.mean(x)                # 5. Aggregate Roll-up\n",
        "]\n",
        "# ======================================================\n",
        "# 6. OPTIMIZER (Branch & Bound)\n",
        "# ======================================================\n",
        "class BranchBoundOptimizer:\n",
        "    def __init__(self, tol=1e-3, max_depth=20, minimize=True, value_range=(0.0, 10.0)):\n",
        "        self.tol = tol\n",
        "        self.max_depth = max_depth\n",
        "        self.minimize = minimize\n",
        "        self.value_range = value_range\n",
        "\n",
        "    def optimize(self, features, y=None, metric_mask=None):\n",
        "        y = np.zeros(3) if y is None else np.array(y[:3])\n",
        "        base = np.mean(list(features.values())) + np.mean(y)\n",
        "\n",
        "        a0, b0 = self.value_range\n",
        "        a0 += base\n",
        "        b0 += base\n",
        "\n",
        "        work = [(a0, b0, 0)]\n",
        "        best_x = None\n",
        "        best_score = np.inf if self.minimize else -np.inf\n",
        "\n",
        "        def better(s1, s2):\n",
        "            return s1 < s2 if self.minimize else s1 > s2\n",
        "\n",
        "        while work:\n",
        "            a, b, depth = work.pop()\n",
        "            mid = 0.5 * (a + b)\n",
        "\n",
        "            # Apply Formula if mask is active\n",
        "            mv = [f(mid) if m else 0.0 for f, m in zip(METRIC_FORMULAS, metric_mask)]\n",
        "            score = sum(mv)\n",
        "\n",
        "            if best_x is None or better(score, best_score):\n",
        "                best_x = mid\n",
        "                best_score = score\n",
        "\n",
        "            if depth >= self.max_depth or (b - a) < self.tol:\n",
        "                continue\n",
        "\n",
        "            work.append((a, mid, depth + 1))\n",
        "            work.append((mid, b, depth + 1))\n",
        "\n",
        "        return best_x\n",
        "\n",
        "# ======================================================\n",
        "# 8. EVALUATOR\n",
        "# ======================================================\n",
        "class MetricsEvaluator:\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_matrix,\n",
        "        metric_formulas=METRIC_FORMULAS,\n",
        "        metric_feature_map=metric_feature_map,\n",
        "        feature_keys=FEATURE_KEYS,\n",
        "        feature_target=None,\n",
        "        metric_target=None,\n",
        "        tol=1e-3,\n",
        "        max_depth=20,\n",
        "        minimize=False,\n",
        "        value_range=(0.0, 5.0)\n",
        "    ):\n",
        "        self.data_matrix = data_matrix\n",
        "        self.metric_formulas = metric_formulas\n",
        "        self.metric_feature_map = metric_feature_map\n",
        "        self.feature_keys = feature_keys\n",
        "\n",
        "        # Defaults\n",
        "        self.feature_target = feature_target or [[1]*len(feature_keys) for _ in range(data_matrix.shape[0])]\n",
        "        self.metric_target = metric_target or METRIC_TARGET\n",
        "        self.num_nodes = data_matrix.shape[0]\n",
        "\n",
        "        self.optimizer = BranchBoundOptimizer(\n",
        "            tol=tol,\n",
        "            max_depth=max_depth,\n",
        "            minimize=minimize,\n",
        "            value_range=value_range\n",
        "        )\n",
        "\n",
        "    def extract_features(self, node_idx):\n",
        "        if node_idx >= len(self.data_matrix): return {}\n",
        "        row = self.data_matrix[node_idx]\n",
        "        mask = self.feature_target[node_idx]\n",
        "        features = {k: v for k, v, m in zip(self.feature_keys, row, mask) if m}\n",
        "        return features\n",
        "\n",
        "    def compute_node_metrics(self, node_idx, y=None):\n",
        "        features = self.extract_features(node_idx)\n",
        "        metric_mask = self.metric_target[node_idx] if node_idx < len(self.metric_target) else [0]*len(METRIC_KEYS)\n",
        "        metric_values = {}\n",
        "\n",
        "        for key, formula, mask in zip(METRIC_KEYS, self.metric_formulas, metric_mask):\n",
        "            if mask and key in self.metric_feature_map:\n",
        "                relevant_keys = self.metric_feature_map[key]\n",
        "                relevant_features = [features[f] for f in relevant_keys if f in features]\n",
        "                x = np.mean(relevant_features) if relevant_features else 0.0\n",
        "\n",
        "                # Optimize\n",
        "                opt_value = self.optimizer.optimize(features={key: x}, y=y, metric_mask=[1])\n",
        "                metric_values[key] = formula(opt_value)\n",
        "            else:\n",
        "                metric_values[key] = 0.0\n",
        "\n",
        "        metric_values['score'] = sum(metric_values.values())\n",
        "        return metric_values\n",
        "# ======================================================\n",
        "# 8. EXECUTION\n",
        "# ======================================================\n",
        "\n",
        "# Generate Mock Data (15 Nodes x 25 Features)\n",
        "# ======================================================\n",
        "# 9. DIMENSIONALITY CONFIGURATION (15 Nodes)\n",
        "# ======================================================\n",
        "\n",
        "# Dimensions assigned based on node complexity and coupling depth.\n",
        "# \"Hub\" nodes and Financial nodes get higher dimensions.\n",
        "\n",
        "# ======================================================\n",
        "# 4. DIMENSIONALITY CONFIGURATION (6 Nodes)\n",
        "# ======================================================\n",
        "candidate_dims = [\n",
        "    [4],   # 0: Location_Logistics (Linear Input)\n",
        "    [16],  # 1: Brewing_Hub (HUB: High Variable Density)\n",
        "    [16],  # 2: Safety_Audit_HACCP (HUB: Regulatory Risk)\n",
        "    [16],  # 3: Liability_Underwriting (HUB: Financial Shield)\n",
        "    [8],   # 4: Service_Handover (Coupled Experience)\n",
        "    [4]    # 5: Aggregate_Margin (System Output)\n",
        "]\n",
        "\n",
        "D_graph = len(candidate_dims)"
      ],
      "metadata": {
        "id": "Q0fGU91EVabm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Reset random seed for 2025\n",
        "np.random.seed(2025)\n",
        "\n",
        "# ======================================================\n",
        "# 1. CORE SYSTEM CONFIGURATION (Names must be EXACT)\n",
        "# ======================================================\n",
        "# These names serve as the \"Source of Truth\" for the entire script.\n",
        "TOUCHPOINTS = [\n",
        "    \"Order_Intake\",      # 0\n",
        "    \"Brewing_Prep\",      # 1\n",
        "    \"Milk_Texturing\",    # 2\n",
        "    \"Quality_Audit\",     # 3\n",
        "    \"Handover_Vibe\",     # 4\n",
        "    \"Inventory_Log\"      # 5\n",
        "]\n",
        "\n",
        "# Hub nodes (Brewing & Quality) require higher dimensions (16)\n",
        "candidate_dims = [[4], [16], [8], [16], [8], [4]]\n",
        "\n",
        "# ======================================================\n",
        "# 2. SYNCHRONIZED GENERATOR MAP\n",
        "# ======================================================\n",
        "# FIX: Keys here must match TOUCHPOINTS exactly to avoid KeyError.\n",
        "GENERATOR_MAP = {\n",
        "    \"Order_Intake\": lambda: {\n",
        "        \"footfall\": np.random.poisson(40),\n",
        "        \"pos_latency\": np.random.exponential(5)\n",
        "    },\n",
        "    \"Brewing_Prep\": lambda: {\n",
        "        \"pressure\": np.random.normal(9, 0.5),\n",
        "        \"temp\": np.random.normal(92, 1.5),\n",
        "        \"extraction_vol\": np.random.uniform(28, 32)\n",
        "    },\n",
        "    \"Milk_Texturing\": lambda: {\n",
        "        \"froth_quality\": np.random.beta(8, 2),\n",
        "        \"steam_temp\": np.random.normal(65, 2)\n",
        "    },\n",
        "    \"Quality_Audit\": lambda: {\n",
        "        \"violation_risk\": np.random.beta(1, 10),\n",
        "        \"sanitation_score\": np.random.uniform(80, 100)\n",
        "    },\n",
        "    \"Handover_Vibe\": lambda: {\n",
        "        \"sentiment\": np.random.uniform(3, 5),\n",
        "        \"wait_time\": np.random.rayleigh(120)\n",
        "    },\n",
        "    \"Inventory_Log\": lambda: {\n",
        "        \"cogs_ratio\": np.random.uniform(0.2, 0.35),\n",
        "        \"labor_cost\": np.random.normal(15, 2)\n",
        "    }\n",
        "}\n",
        "\n",
        "# ======================================================\n",
        "# 3. DATA MATRIX GENERATION & NORMALIZATION\n",
        "# ======================================================\n",
        "num_samples = 100\n",
        "feature_list = []\n",
        "\n",
        "for tp in TOUCHPOINTS:\n",
        "    # Now correctly finds keys like 'Order_Intake'\n",
        "    samples = [GENERATOR_MAP[tp]() for _ in range(num_samples)]\n",
        "    df = pd.DataFrame(samples)\n",
        "\n",
        "    # Handle categoricals if any\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype == 'object':\n",
        "            df[col] = pd.factorize(df[col])[0]\n",
        "    feature_list.append(df)\n",
        "\n",
        "DATA_MATRIX_RAW = pd.concat(feature_list, axis=1).to_numpy()\n",
        "# Normalization logic\n",
        "DATA_MATRIX = (DATA_MATRIX_RAW - DATA_MATRIX_RAW.min(axis=0)) / (np.ptp(DATA_MATRIX_RAW, axis=0) + 1e-8)\n",
        "\n",
        "print(f\"Success! Coffee Operational Matrix Created: {DATA_MATRIX.shape}\")\n",
        "\n",
        "# ======================================================\n",
        "# 4. HIGH-FIDELITY SYNTHETIC TARGET GENERATOR\n",
        "# ======================================================\n",
        "def generate_coffee_targets(DATA_MATRIX, candidate_dims):\n",
        "    dims_flat = [i[0] for i in candidate_dims]\n",
        "    num_nodes = len(dims_flat)\n",
        "    targets = []\n",
        "    current_col = 0\n",
        "\n",
        "    for node_idx in range(num_nodes):\n",
        "        dim = dims_flat[node_idx]\n",
        "\n",
        "        # Row selection: use the specific row index for the touchpoint\n",
        "        row = DATA_MATRIX[node_idx % DATA_MATRIX.shape[0]]\n",
        "\n",
        "        # Slicing the contiguous block for target signature\n",
        "        block = row[current_col : current_col + dim]\n",
        "\n",
        "        # HUB LOGIC: Reflective padding for 16D nodes\n",
        "        if len(block) < dim:\n",
        "            block = np.pad(block, (0, dim - len(block)), mode='reflect')\n",
        "\n",
        "        targets.append({\n",
        "            'node_id': node_idx,\n",
        "            'touchpoint': TOUCHPOINTS[node_idx],\n",
        "            'dim_required': dim,\n",
        "            'target': np.round(block, 4)\n",
        "        })\n",
        "\n",
        "        # Modular wrap for feature variance recycling\n",
        "        current_col = (current_col + dim) % DATA_MATRIX.shape[1]\n",
        "\n",
        "    return targets\n",
        "\n",
        "# Execute generation\n",
        "synthetic_targets = generate_coffee_targets(DATA_MATRIX, candidate_dims)\n",
        "\n",
        "# ======================================================\n",
        "# 5. DIAGNOSTIC REPORT\n",
        "# ======================================================\n",
        "print(f\"\\n--- Coffee Operations Optimization Matrix (6-Node Compact) ---\")\n",
        "print(f\"Total Operational Features Analyzed: {DATA_MATRIX.shape[1]}\")\n",
        "print(\"-\" * 75)\n",
        "\n",
        "for t in synthetic_targets:\n",
        "    is_hub = \"[HUB-KNOT]\" if t['dim_required'] == 16 else \"          \"\n",
        "    node_desc = f\"{t['node_id']:01d} | {t['touchpoint']:<22} | {is_hub}\"\n",
        "    # Print the first 4 elements of the signature\n",
        "    print(f\"Node {node_desc} | Size: {t['dim_required']:02d} | Sig: {t['target'][:4]}...\")\n",
        "\n",
        "print(\"-\" * 75)"
      ],
      "metadata": {
        "id": "rCIcB9UMVjhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Replace above cells with Expert knowledge__\n",
        "# Service Design Generator Simulation"
      ],
      "metadata": {
        "id": "G5m6U-HipINy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "# Target Mask (which metrics apply to which node)\n",
        "\n",
        "def top_k_masked_probs(weights, k):\n",
        "    \"\"\"\n",
        "    Keep only top-k weights, zero out the rest, renormalize.\n",
        "    \"\"\"\n",
        "    if k >= len(weights):\n",
        "        return weights / (weights.sum() + 1e-12)\n",
        "\n",
        "    idx = np.argpartition(weights, -k)[-k:]\n",
        "    mask = np.zeros_like(weights)\n",
        "    mask[idx] = weights[idx]\n",
        "\n",
        "    s = mask.sum()\n",
        "    if s > 0:\n",
        "        mask /= s\n",
        "    return mask\n",
        "def get_all_paths(G, C, node_types, start=0, end=None):\n",
        "    \"\"\"\n",
        "    Return all simple paths that respect coupling constraints.\n",
        "    \"\"\"\n",
        "    if end is None:\n",
        "        end = G.shape[0] - 1\n",
        "\n",
        "    D = G.shape[0]\n",
        "    paths = []\n",
        "\n",
        "    def coupling_ok(i, j):\n",
        "        label = C[i, j]\n",
        "\n",
        "        # Default direct coupling\n",
        "        if \"->\" in label and \"->C\" not in label:\n",
        "            src, dst = label.split(\"->\")\n",
        "            return src == node_types[i] and dst == node_types[j]\n",
        "\n",
        "        # Escalation case (B->B->C)\n",
        "        if label == \"B->B->C\":\n",
        "            return node_types[i] == \"B\" and node_types[j] == \"B\"\n",
        "\n",
        "        return False\n",
        "\n",
        "    def dfs(node, path, visited):\n",
        "        if node == end:\n",
        "            paths.append(path.copy())\n",
        "            return\n",
        "\n",
        "        for nxt in range(D):\n",
        "            if G[node, nxt] > 0 and nxt not in visited:\n",
        "                if not coupling_ok(node, nxt):\n",
        "                    continue\n",
        "\n",
        "                visited.add(nxt)\n",
        "                dfs(nxt, path + [nxt], visited)\n",
        "                visited.remove(nxt)\n",
        "\n",
        "    dfs(start, [start], {start})\n",
        "    return paths\n",
        "\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# EXTERNAL DEPENDENCIES & CONFIGURATION\n",
        "# =============================================================================\n",
        "# These variables are referenced in the original code but not defined.\n",
        "# Assumed to be present in the execution environment.\n",
        "# -----------------------------------------------------------------------------\n",
        "# D_graph = ...\n",
        "# DATA_MATRIX = ...\n",
        "# METRIC_KEYS = [...]\n",
        "# METRIC_TARGET = [...]\n",
        "# METRIC_FORMULAS = [...]\n",
        "# METRIC_INVERSES = {...}\n",
        "# synthetic_targets = ...\n",
        "# MetricsEvaluator = ... (Class)\n",
        "# -----------------------------------------------------------------------------\n",
        "# =============================================================================\n",
        "# NEW: DETERMINISTIC DFS & PATH EVALUATOR\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "def evaluate_fixed_paths(paths, node_metrics, beta=0.3):\n",
        "    \"\"\"\n",
        "    Replaces collect_and_select_best_walks.\n",
        "    Scores the specific paths found by DFS.\n",
        "    \"\"\"\n",
        "    walks = []\n",
        "    best = None\n",
        "\n",
        "    for path in paths:\n",
        "        cost = 0.0\n",
        "        quality = 0.0\n",
        "\n",
        "        # Calculate Path Metrics\n",
        "        for node_idx in path:\n",
        "            m = node_metrics[node_idx]\n",
        "            cost += m.get(\"cost\", 0.0)\n",
        "            quality += m.get(\"quality\", 0.0)\n",
        "\n",
        "        score = quality - 1.0 * cost # using default lambda_cost=1.0\n",
        "\n",
        "        # DFS paths are unique, so count is always 1\n",
        "        count = 1\n",
        "        adjusted_score = score / (1 + beta * count)\n",
        "\n",
        "        record = {\n",
        "            \"path\": path,\n",
        "            \"score\": score,\n",
        "            \"adjusted_score\": adjusted_score,\n",
        "            \"cost\": cost,\n",
        "            \"quality\": quality,\n",
        "            \"count\": count\n",
        "        }\n",
        "        walks.append(record)\n",
        "\n",
        "        if best is None or adjusted_score > best[\"adjusted_score\"]:\n",
        "            best = record\n",
        "\n",
        "    return walks, best\n",
        "\n",
        "def dsm_walk(G, node_metrics, start=0, max_steps=50, lambda_cost=1.0):\n",
        "    D = G.shape[0]\n",
        "    target_node = D - 1  # Define Nlast\n",
        "\n",
        "    # Force start at 0 if not provided\n",
        "    current = start\n",
        "    path = [current]\n",
        "    visited = {current}\n",
        "\n",
        "    cost = 0.0\n",
        "    quality = 0.0\n",
        "\n",
        "    # Metrics for the start node\n",
        "    m_start = node_metrics[current]\n",
        "    cost += m_start.get(\"cost\", 0.0)\n",
        "    quality += m_start.get(\"quality\", 0.0)\n",
        "\n",
        "    reached_target = False\n",
        "\n",
        "    for _ in range(max_steps):\n",
        "        # If we reached the last node, stop successfully\n",
        "        if current == target_node:\n",
        "            reached_target = True\n",
        "            break\n",
        "\n",
        "        weights = np.zeros(D)\n",
        "\n",
        "        for j in range(D):\n",
        "            # Allow visiting the target even if visited (though unlikely to revisit in DAG)\n",
        "            # But generally prevent cycles\n",
        "            if j == 0:\n",
        "                continue\n",
        "            if j in visited:\n",
        "                continue\n",
        "\n",
        "            q = node_metrics[j].get(\"quality\", 0.0)\n",
        "            c = node_metrics[j].get(\"cost\", 0.0)\n",
        "\n",
        "            # Standard probability weight\n",
        "            weights[j] = G[current, j] * (q / (1 + c))\n",
        "\n",
        "        if weights.sum() == 0:\n",
        "            break\n",
        "\n",
        "        weights /= weights.sum()\n",
        "        nxt = np.random.choice(D, p=weights)\n",
        "\n",
        "        m = node_metrics[nxt]\n",
        "        cost += m.get(\"cost\", 0.0)\n",
        "        quality += m.get(\"quality\", 0.0)\n",
        "\n",
        "        path.append(nxt)\n",
        "        visited.add(nxt)\n",
        "        current = nxt\n",
        "\n",
        "    # Recalculate score based on success\n",
        "    score = quality - lambda_cost * cost\n",
        "\n",
        "    # Heavy penalty if the walk did not reach Nlast\n",
        "    if not reached_target:\n",
        "        score = -1e9\n",
        "\n",
        "    return path, score, cost, quality\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "def collect_and_select_best_walks(G, node_metrics, n_walks=300, beta=0.3):\n",
        "    walks = []\n",
        "    best = None\n",
        "    path_counts = defaultdict(int)\n",
        "    D = G.shape[0]\n",
        "\n",
        "    for _ in range(n_walks):\n",
        "        # Force start at 0\n",
        "        start = 0\n",
        "        path, score, cost, quality = dsm_walk(\n",
        "            G, node_metrics, start\n",
        "        )\n",
        "\n",
        "        # Only record if it successfully reached the last node\n",
        "        # (Score is -1e9 if it failed, per updated dsm_walk)\n",
        "        if path[-1] != (D - 1):\n",
        "            continue\n",
        "\n",
        "        key = tuple(path)\n",
        "        path_counts[key] += 1\n",
        "\n",
        "        # novelty penalty\n",
        "        adjusted_score = score / (1 + beta * path_counts[key])\n",
        "\n",
        "        record = {\n",
        "            \"path\": path,\n",
        "            \"score\": score,\n",
        "            \"adjusted_score\": adjusted_score,\n",
        "            \"cost\": cost,\n",
        "            \"quality\": quality,\n",
        "            \"count\": path_counts[key]\n",
        "        }\n",
        "        walks.append(record)\n",
        "\n",
        "        if best is None or adjusted_score > best[\"adjusted_score\"]:\n",
        "            best = record\n",
        "\n",
        "    return walks, best\n",
        "\n",
        "class CouplingState:\n",
        "    \"\"\"\n",
        "    One-step coupling automaton.\n",
        "    C = transient escalation after B->B->C\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.state = \"NORMAL\"\n",
        "\n",
        "    def update(self, coupling):\n",
        "        if coupling == \"B->B->C\":\n",
        "            self.state = \"C\"\n",
        "        else:\n",
        "            self.state = \"NORMAL\"\n",
        "\n",
        "    def allows(self, next_node_type):\n",
        "        \"\"\"\n",
        "        Rule:\n",
        "        C → random, but NOT B\n",
        "        \"\"\"\n",
        "        if self.state == \"C\" and next_node_type == \"B\":\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "def infer_node_types(node_metrics):\n",
        "    \"\"\"\n",
        "    A = quality-dominant\n",
        "    B = cost-dominant\n",
        "    \"\"\"\n",
        "    node_types = []\n",
        "    for m in node_metrics:\n",
        "        q = m.get(\"quality\", 0.0)\n",
        "        c = m.get(\"cost\", 0.0)\n",
        "        node_types.append(\"A\" if q >= c else \"B\")\n",
        "    return node_types\n",
        "\n",
        "def build_coupling_matrix(node_types):\n",
        "    \"\"\"\n",
        "    Builds a D x D coupling-label matrix.\n",
        "    C exists only as a coupling escalation (B->B->C).\n",
        "    \"\"\"\n",
        "    D = len(node_types)\n",
        "    C = np.empty((D, D), dtype=object)\n",
        "\n",
        "    for i in range(D):\n",
        "        for j in range(D):\n",
        "            src = node_types[i]\n",
        "            dst = node_types[j]\n",
        "\n",
        "            if src == \"B\" and dst == \"B\":\n",
        "                C[i, j] = \"B->B->C\"\n",
        "            else:\n",
        "                C[i, j] = f\"{src}->{dst}\"\n",
        "\n",
        "    return C\n",
        "\n",
        "\n",
        "def coupling_weight(coupling_label, metrics_i, metrics_j):\n",
        "    \"\"\"\n",
        "    OHMIC GF: I = ΔV / R\n",
        "    Treats the transition as a vector field.\n",
        "    \"\"\"\n",
        "    # Vector extraction\n",
        "    v_i = np.array(list(metrics_i.values())) if isinstance(metrics_i, dict) else np.array(metrics_i)\n",
        "    v_j = np.array(list(metrics_j.values())) if isinstance(metrics_j, dict) else np.array(metrics_j)\n",
        "\n",
        "    # 1. Voltage (Potential Gain): Only conduct on improvement\n",
        "    v_diff = np.maximum(v_j - v_i, 0)\n",
        "    voltage = np.linalg.norm(v_diff) + 1e-6\n",
        "\n",
        "    # 2. Resistance (Node Friction): Destination cost/complexity\n",
        "    resistance = np.mean(v_j) + 0.1\n",
        "\n",
        "    # 3. Current (Conductance)\n",
        "    conductance = voltage / resistance\n",
        "\n",
        "    # Type A = High Conductance, Type B = High Damping\n",
        "    valve = 1.3 if \"A\" in coupling_label else 0.7\n",
        "    return max(conductance * valve, 0.0001)\n",
        "\n",
        "# Configuration\n",
        "#candidate_dims = [[2], [2], [2], [2], [2], [2], [2], [2], [1]]\n",
        "outer_generations = 1\n",
        "inner_learning = 0.1\n",
        "gamma_interlayer = 1\n",
        "top_k = 21\n",
        "\n",
        "# Initialize random state\n",
        "np.random.seed()\n",
        "seed = None  # Placeholder as per original logic\n",
        "\n",
        "# Placeholder for Data Matrix generation (from original snippet)\n",
        "# new_DATA_MATRIX = np.random.rand(D_graph, DATA_MATRIX.shape[1])\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# HELPER CLASSES\n",
        "# =============================================================================\n",
        "class DSM_Tracker:\n",
        "    \"\"\"\n",
        "    Tracks a DSM (Design Structure Matrix) layer and its residual.\n",
        "    \"\"\"\n",
        "    def __init__(self, multiplex_layer):\n",
        "        self.layer = multiplex_layer\n",
        "        self.primary_dsm = None\n",
        "        self.residual_dsm = None\n",
        "        self.update_dsms()\n",
        "\n",
        "    def update_dsms(self):\n",
        "        if self.primary_dsm is None:\n",
        "            # First time: store current DSM as reference\n",
        "            self.primary_dsm = self.layer.chosen_Gmat.copy()\n",
        "            self.residual_dsm = np.zeros_like(self.primary_dsm)\n",
        "        else:\n",
        "            # Update residual: current DSM minus primary\n",
        "            current = self.layer.chosen_Gmat\n",
        "            self.residual_dsm = current - self.primary_dsm\n",
        "\n",
        "    def get_matrices(self):\n",
        "        return self.primary_dsm, self.residual_dsm\n",
        "\n",
        "    def print_matrices(self):\n",
        "        print(\"\\n--- Primary DSM ---\")\n",
        "        print(self.primary_dsm)\n",
        "        print(\"\\n--- Residual DSM ---\")\n",
        "        print(self.residual_dsm)\n",
        "\n",
        "\n",
        "class DSM_Layer_Decomposer:\n",
        "    \"\"\"\n",
        "    Manages the additive decomposition of DSM layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, baseline_matrix, mode='additive'):\n",
        "        self.baseline_matrix = baseline_matrix.copy()\n",
        "        self.current_total = baseline_matrix.copy()\n",
        "        self.mode = mode\n",
        "        self.layers = []\n",
        "        self.residuals = []\n",
        "\n",
        "    def add_snapshot(self, new_total_matrix):\n",
        "        \"\"\"\n",
        "        Calculates the DELTA (change) between the new state and the previous state,\n",
        "        stores that delta as a layer.\n",
        "        \"\"\"\n",
        "        delta = new_total_matrix - self.current_total\n",
        "        self.layers.append(delta.copy())\n",
        "\n",
        "        # Update current tracker\n",
        "        self.current_total = new_total_matrix.copy()\n",
        "\n",
        "        # Calculate residual (Difference from the baseline)\n",
        "        residual = self.current_total - self.baseline_matrix\n",
        "        self.residuals.append(residual)\n",
        "\n",
        "        layer_id = len(self.layers) - 1\n",
        "        print(f\"\\n=== DSM LAYER {layer_id} CAPTURED ===\")\n",
        "        print(f\"Layer Contribution (Delta):\\n{np.round(delta, 3)}\")\n",
        "\n",
        "        return delta\n",
        "\n",
        "    def get_reconstruction(self):\n",
        "        return np.sum(self.layers, axis=0)\n",
        "\n",
        "\n",
        "class SVM:\n",
        "    \"\"\"\n",
        "    Metric-inverse multi-output SVM with epsilon-insensitive loss.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, output_dim=None, metric_keys=None, lr=0.001, epsilon=0.1):\n",
        "        self.input_dim = input_dim\n",
        "        self.metric_keys = metric_keys\n",
        "\n",
        "        if metric_keys is not None:\n",
        "            self.output_dim = len(metric_keys)\n",
        "        elif output_dim is not None:\n",
        "            self.output_dim = output_dim\n",
        "        else:\n",
        "            self.output_dim = 1\n",
        "\n",
        "        self.lr = lr\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        # Lazy initialization\n",
        "        self.X_train = None\n",
        "        self.y_train = None\n",
        "        self.alpha = None\n",
        "        self.b = None\n",
        "\n",
        "    def train_step(self, X, y_true):\n",
        "        X = np.array(X)\n",
        "        y_true = np.array(y_true)\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        if self.X_train is None:\n",
        "            self.X_train = X.copy()\n",
        "            self.y_train = y_true.copy()\n",
        "            self.alpha = np.zeros((X.shape[1], self.output_dim))\n",
        "            self.b = np.zeros(self.output_dim)\n",
        "\n",
        "        # Linear kernel\n",
        "        y_pred = X.dot(self.alpha) + self.b\n",
        "\n",
        "        # Epsilon-insensitive loss\n",
        "        diff = y_pred - y_true\n",
        "        mask = np.abs(diff) > self.epsilon\n",
        "        diff *= mask\n",
        "\n",
        "        grad_alpha = X.T.dot(diff) / n_samples\n",
        "        grad_b = diff.mean(axis=0)\n",
        "\n",
        "        self.alpha -= self.lr * grad_alpha\n",
        "        self.b -= self.lr * grad_b\n",
        "\n",
        "        loss = np.mean(np.maximum(0, np.abs(y_pred - y_true) - self.epsilon))\n",
        "        return loss\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.array(X)\n",
        "        y_pred = X.dot(self.alpha) + self.b\n",
        "\n",
        "        if self.metric_keys is None:\n",
        "            return y_pred\n",
        "\n",
        "        # Apply metric inverses\n",
        "        y_transformed = np.zeros_like(y_pred)\n",
        "        for i, key in enumerate(self.metric_keys):\n",
        "            inverse_fn = METRIC_INVERSES[key]\n",
        "            # Handle potential list return from inverse_fn\n",
        "            val_func = lambda y: inverse_fn(y)[0] if isinstance(inverse_fn(y), list) else inverse_fn(y)\n",
        "            y_transformed[:, i] = np.array([val_func(y) for y in y_pred[:, i]])\n",
        "\n",
        "        return y_transformed\n",
        "\n",
        "\n",
        "class InterLayer:\n",
        "    def __init__(self, D_graph, max_inner_dim, inter_dim=None, edge_threshold=0.02, gamma=1.0, seed=42):\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "\n",
        "        self.D_graph = D_graph\n",
        "        self.edge_threshold = edge_threshold\n",
        "        self.gamma = gamma\n",
        "\n",
        "        # Handle list vs int inputs for dimensions\n",
        "        m_dim = max_inner_dim[0] if isinstance(max_inner_dim, list) else max_inner_dim\n",
        "\n",
        "        if inter_dim is not None:\n",
        "            self.inter_dim = inter_dim[0] if isinstance(inter_dim, list) else inter_dim\n",
        "        else:\n",
        "            self.inter_dim = m_dim\n",
        "\n",
        "        self.max_input = 2 * m_dim\n",
        "\n",
        "        # Initialize weights\n",
        "        self.weights = {}\n",
        "        self.bias = {}\n",
        "        for i in range(D_graph):\n",
        "            for j in range(D_graph):\n",
        "                if i != j:\n",
        "                    w_init = np.random.uniform(-0.1, 0.1, (self.inter_dim, self.max_input))\n",
        "                    self.weights[(i, j)] = w_init\n",
        "                    self.bias[(i, j)] = np.zeros(self.inter_dim)\n",
        "\n",
        "    def compute_edge_activation(self, i, j, nested_reps):\n",
        "        concat = np.concatenate([nested_reps[i], nested_reps[j]])\n",
        "        # Pad or truncation\n",
        "        if len(concat) < self.max_input:\n",
        "            concat = np.pad(concat, (0, self.max_input - len(concat)))\n",
        "        else:\n",
        "            concat = concat[:self.max_input]\n",
        "\n",
        "        # Normalize\n",
        "        concat = (concat - np.mean(concat)) / (np.std(concat) + 1e-12)\n",
        "\n",
        "        # Activation\n",
        "        v = self.weights[(i, j)].dot(concat) + self.bias[(i, j)]\n",
        "        input_strength = np.clip(np.mean(np.abs(concat)), 0, 1)\n",
        "        v = v * input_strength\n",
        "\n",
        "        return 1 / (1 + np.exp(-v))\n",
        "\n",
        "    def build_activations(self, Gmat, nested_reps):\n",
        "        acts = {}\n",
        "        for i in range(self.D_graph):\n",
        "            for j in range(self.D_graph):\n",
        "                if i == j:\n",
        "                    continue\n",
        "                if abs(Gmat[i, j]) > self.edge_threshold:\n",
        "                    acts[(i, j)] = self.compute_edge_activation(i, j, nested_reps)\n",
        "        return acts\n",
        "\n",
        "    @staticmethod\n",
        "    def pairwise_squared_corr(acts):\n",
        "        if len(acts) < 2:\n",
        "            return 0.0\n",
        "        A = np.stack(list(acts.values()))\n",
        "        A_centered = A - A.mean(axis=1, keepdims=True)\n",
        "        stds = np.sqrt(np.sum(A_centered**2, axis=1) / (A.shape[1]-1) + 1e-12)\n",
        "        cov = A_centered @ A_centered.T / (A.shape[1]-1)\n",
        "        corr = cov / (np.outer(stds, stds) + 1e-12)\n",
        "        np.fill_diagonal(corr, 0)\n",
        "        return float((corr**2).sum())\n",
        "\n",
        "    def mi_for_graph(self, Gmat, nested_reps):\n",
        "        acts = self.build_activations(Gmat, nested_reps)\n",
        "        if not acts:\n",
        "            return 0.0\n",
        "        return self.gamma * self.pairwise_squared_corr(acts)\n",
        "\n",
        "def build_dsm_from_walks(D, paths):\n",
        "    \"\"\"\n",
        "    Constructs a DSM where entry [i,j] is the probability\n",
        "    that a successful process moves from i to j.\n",
        "    \"\"\"\n",
        "    flow = np.zeros((D, D))\n",
        "\n",
        "    # Count transitions\n",
        "    for path in paths:\n",
        "        for k in range(len(path) - 1):\n",
        "            u, v = path[k], path[k+1]\n",
        "            flow[u, v] += 1\n",
        "\n",
        "    # Normalize by the total number of successful walks.\n",
        "    # This prevents 'saturation'—if an edge is rarely used, it stays small.\n",
        "    n_paths = len(paths)\n",
        "    if n_paths > 0:\n",
        "        flow = flow / n_paths\n",
        "\n",
        "    np.fill_diagonal(flow, 0.0)\n",
        "    return flow\n",
        "# =============================================================================\n",
        "# MAIN OPTIMIZER CLASS\n",
        "# =============================================================================\n",
        "class MetricDrivenRandomWalk:\n",
        "    def __init__(self, coupling_matrix, node_metrics, max_steps=50):\n",
        "        self.C = coupling_matrix\n",
        "        self.node_metrics = node_metrics\n",
        "        self.D = coupling_matrix.shape[0]\n",
        "        self.max_steps = max_steps\n",
        "\n",
        "    def transition_probs(self, i, k=2):\n",
        "        w = np.zeros(self.D)\n",
        "\n",
        "        for j in range(self.D):\n",
        "            if i == j:\n",
        "                continue\n",
        "\n",
        "            weight = coupling_weight(self.C[i, j], self.node_metrics[j])\n",
        "\n",
        "            # Directional heuristic (keep yours)\n",
        "            if j > i:\n",
        "                weight *= 1.2\n",
        "            elif j < i:\n",
        "                weight *= 0.8\n",
        "\n",
        "            w[j] = weight\n",
        "\n",
        "        # If nothing viable, fallback to uniform\n",
        "        if w.sum() == 0:\n",
        "            return np.ones(self.D) / self.D\n",
        "\n",
        "        # Emphasize strong edges\n",
        "        w = w ** 2\n",
        "\n",
        "        # 🔒 HARD SPARSITY CONSTRAINT (TOP-K)\n",
        "        w = top_k_masked_probs(w, k)\n",
        "\n",
        "        return w\n",
        "\n",
        "\n",
        "    def run(self, start=0):\n",
        "        path = [start]\n",
        "        current = start\n",
        "        target_node = self.D - 1\n",
        "\n",
        "        for _ in range(self.max_steps):\n",
        "            if current == target_node:\n",
        "                break\n",
        "\n",
        "            probs = self.transition_probs(current)\n",
        "\n",
        "            # Move\n",
        "            current = np.random.choice(self.D, p=probs)\n",
        "            path.append(current)\n",
        "\n",
        "            if current == target_node:\n",
        "                break\n",
        "\n",
        "        return path\n",
        "class MetricDrivenRandomWalk:\n",
        "    def __init__(self, coupling_matrix, node_metrics, max_steps=50, top_k=2):\n",
        "        self.C = coupling_matrix\n",
        "        self.node_metrics = node_metrics\n",
        "        self.D = coupling_matrix.shape[0]\n",
        "        self.max_steps = max_steps\n",
        "        self.top_k = top_k  # <--- FIX: Store the parameter\n",
        "\n",
        "    def transition_probs(self, i):\n",
        "        w = np.zeros(self.D)\n",
        "\n",
        "        for j in range(self.D):\n",
        "            if i == j:\n",
        "                continue\n",
        "\n",
        "            weight = coupling_weight(self.C[i, j], self.node_metrics[j])\n",
        "\n",
        "            # Directional heuristic\n",
        "            if j > i:\n",
        "                weight *= 1.2\n",
        "            elif j < i:\n",
        "                weight *= 0.8\n",
        "\n",
        "            w[j] = weight\n",
        "\n",
        "        if w.sum() == 0:\n",
        "            return np.ones(self.D) / self.D\n",
        "\n",
        "        w = w ** 2\n",
        "\n",
        "        # 🔒 FIX: Use the stored self.top_k\n",
        "        w = top_k_masked_probs(w, self.top_k)\n",
        "\n",
        "        return w\n",
        "\n",
        "    def run(self, start=0):\n",
        "        path = [start]\n",
        "        current = start\n",
        "        target_node = self.D - 1\n",
        "\n",
        "        for _ in range(self.max_steps):\n",
        "            if current == target_node:\n",
        "                break\n",
        "\n",
        "            probs = self.transition_probs(current)\n",
        "\n",
        "            # Move\n",
        "            current = np.random.choice(self.D, p=probs)\n",
        "            path.append(current)\n",
        "\n",
        "            if current == target_node:\n",
        "                break\n",
        "\n",
        "        return path\n",
        "class MetricDrivenRandomWalk:\n",
        "    def __init__(self, coupling_matrix, node_metrics, metric_targets, max_steps=100, top_k=2):\n",
        "        self.C = coupling_matrix\n",
        "        self.node_metrics = node_metrics\n",
        "        self.metric_targets = metric_targets\n",
        "        self.D = coupling_matrix.shape[0]\n",
        "        self.max_steps = max_steps\n",
        "        self.top_k = top_k\n",
        "\n",
        "        # ⚡ Precompute the base Ohmic Conductance (The 'Clean' Pipe)\n",
        "        self.Gamma_base = self._precompute_conductance()\n",
        "\n",
        "    def _precompute_conductance(self):\n",
        "        gamma_mat = np.zeros((self.D, self.D))\n",
        "        for i in range(self.D):\n",
        "            for j in range(self.D):\n",
        "                if i != j and self.metric_targets[i][j] != 0:\n",
        "                    # Calculate I = V / R using the means of the metric vectors\n",
        "                    gamma_mat[i, j] = coupling_weight(self.C[i, j], self.node_metrics[i], self.node_metrics[j])\n",
        "        return gamma_mat\n",
        "\n",
        "    def run(self, start=0):\n",
        "        path = [start]\n",
        "        current = start\n",
        "        target_node = self.D - 1\n",
        "\n",
        "        for _ in range(self.max_steps):\n",
        "            if current == target_node:\n",
        "                break\n",
        "\n",
        "            # 1. Grab precomputed conductances for 'current'\n",
        "            w = self.Gamma_base[current, :].copy()\n",
        "\n",
        "            # 2. APPLY MEANS-BASED FATIGUE (The Singularity Breaker)\n",
        "            # We penalize nodes based on the MEAN of their appearance in the path\n",
        "            for j in range(self.D):\n",
        "                if j in path:\n",
        "                    # Every visit reduces the conductance by a mean-factor\n",
        "                    # This increases the 'Resistance' of the loop\n",
        "                    count = path.count(j)\n",
        "                    w[j] *= (1.0 / (1.0 + count))\n",
        "\n",
        "            # 3. DIRECTIONAL BIAS (The Gravity Jump)\n",
        "            # Ensure the jump 'means' something by favoring forward progress\n",
        "            indices = np.arange(self.D)\n",
        "            w[indices > current] *= 1.5  # Potential to jump ahead\n",
        "            w[indices < current] *= 0.2  # Heavy friction for jumping back\n",
        "\n",
        "            # 4. EXECUTE THE JUMP\n",
        "            if w.sum() == 0:\n",
        "                break # Circuit broken\n",
        "\n",
        "            # Sharpen the distribution and pick the best K\n",
        "            probs = top_k_masked_probs(w**2, self.top_k)\n",
        "            current = np.random.choice(self.D, p=probs)\n",
        "            path.append(current)\n",
        "\n",
        "        return path\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MetricDrivenRandomWalk:\n",
        "    def __init__(self, coupling_matrix, node_metrics, metric_targets, max_steps=100, top_k=2):\n",
        "        self.C = coupling_matrix\n",
        "        self.node_metrics = node_metrics\n",
        "        self.metric_targets = metric_targets\n",
        "        self.D = coupling_matrix.shape[0]\n",
        "        self.max_steps = max_steps\n",
        "        self.top_k = top_k\n",
        "\n",
        "        # 1. Internal Enigma State (The 'Rotor' Configuration)\n",
        "        # This permutation vector re-maps the node indices on every jump\n",
        "        self.enigma_state = np.arange(self.D)\n",
        "\n",
        "        # 2. Precompute the Base Ohmic Conductance (Static Wiring)\n",
        "        self.Gamma_base = self._precompute_conductance()\n",
        "\n",
        "    def _precompute_conductance(self):\n",
        "        \"\"\"Standard Ohmic Flow: Current = Voltage / Mean(Metrics)\"\"\"\n",
        "        gamma_mat = np.zeros((self.D, self.D))\n",
        "        for i in range(self.D):\n",
        "            for j in range(self.D):\n",
        "                if i != j and self.metric_targets[i][j] != 0:\n",
        "                    # Ohmic GF: delta Quality / Mean Friction\n",
        "                    gamma_mat[i, j] = coupling_weight(self.C[i, j], self.node_metrics[i], self.node_metrics[j])\n",
        "        return gamma_mat\n",
        "\n",
        "    def rotate_rotors(self, current_node):\n",
        "        \"\"\"\n",
        "        The Enigma Shift: Permutes the state based on the 'Mean' of the node.\n",
        "        This changes the available 'exits' for the next step.\n",
        "        \"\"\"\n",
        "        metrics = self.node_metrics[current_node]\n",
        "        # Use the mean as the 'Notch' on the rotor\n",
        "        mean_val = np.mean(list(metrics.values())) if isinstance(metrics, dict) else np.mean(metrics)\n",
        "\n",
        "        # Shift the state machine by the intensity of the current node\n",
        "        shift = int(mean_val * 10) % self.D\n",
        "        self.enigma_state = np.roll(self.enigma_state, shift)\n",
        "\n",
        "    def run(self, start=0):\n",
        "        path = [start]\n",
        "        current = start\n",
        "        target_node = self.D - 1\n",
        "\n",
        "        for step in range(self.max_steps):\n",
        "            if current == target_node:\n",
        "                break\n",
        "\n",
        "            # ⚡ STEP 1: Rotate the Enigma State Machine\n",
        "            self.rotate_rotors(current)\n",
        "\n",
        "            # ⚡ STEP 2: Get Base Weights\n",
        "            w = self.Gamma_base[current, :].copy()\n",
        "\n",
        "            # ⚡ STEP 3: Permutation Mapping\n",
        "            # Scramble the available flows through the current Enigma configuration\n",
        "            # This makes the 'Singularity' impossible because the weights shift every visit\n",
        "            w = w[self.enigma_state]\n",
        "\n",
        "            # ⚡ STEP 4: Directional Bias (Inertia)\n",
        "            indices = np.arange(self.D)\n",
        "            w[indices > current] *= 2.0  # Forward Pull\n",
        "            w[indices < current] *= 0.1  # Backward Resistance (Rework)\n",
        "\n",
        "            if w.sum() == 0:\n",
        "                break # Open Circuit\n",
        "\n",
        "            # ⚡ STEP 5: Jump\n",
        "            probs = top_k_masked_probs(w**2, self.top_k)\n",
        "            current = np.random.choice(self.D, p=probs)\n",
        "            path.append(current)\n",
        "\n",
        "        return path\n",
        "\n",
        "class MetricDrivenRandomWalk:\n",
        "    def __init__(self, coupling_matrix, node_metrics, metric_targets, max_steps=100, top_k=2):\n",
        "        self.C = coupling_matrix\n",
        "        self.node_metrics = node_metrics\n",
        "        self.metric_targets = metric_targets\n",
        "        self.D = coupling_matrix.shape[0]\n",
        "        self.max_steps = max_steps\n",
        "        self.top_k = top_k\n",
        "\n",
        "        # ⚡ PRE-COMPUTED MANIFOLD (The Speed Fix)\n",
        "        # We build the 'Enigma' logic directly into the static weights\n",
        "        self.Gamma_Enigma = self._precompute_enigma_manifold()\n",
        "\n",
        "    def _precompute_enigma_manifold(self):\n",
        "        \"\"\"\n",
        "        Calculates the entire state-space once.\n",
        "        Incorporates 'Mean Resistance' into the structure.\n",
        "        \"\"\"\n",
        "        manifold = np.zeros((self.D, self.D))\n",
        "        for i in range(self.D):\n",
        "            m_i = self.node_metrics[i]\n",
        "            # Mean Friction of source node acts as a Rotor Notch\n",
        "            friction_i = np.mean(list(m_i.values())) if isinstance(m_i, dict) else np.mean(m_i)\n",
        "\n",
        "            for j in range(self.D):\n",
        "                if i != j and self.metric_targets[i][j] != 0:\n",
        "                    # Ohmic Conductance\n",
        "                    base_i = coupling_weight(self.C[i, j], m_i, self.node_metrics[j])\n",
        "\n",
        "                    # ENIGMA BIAS: Favor jumps that 'match' the rotor phase\n",
        "                    # This replaces the slow np.roll() inside the run loop\n",
        "                    phase_shift = (i + j) % self.D\n",
        "                    enigma_mod = 1.0 + (0.1 * np.sin(phase_shift * friction_i))\n",
        "\n",
        "                    # GRAVITY BIAS: Forward progress is structurally cheaper\n",
        "                    gravity = 2.0 if j > i else 0.2\n",
        "\n",
        "                    manifold[i, j] = base_i * enigma_mod * gravity\n",
        "        return manifold\n",
        "\n",
        "    def run(self, start=0):\n",
        "        path = [start]\n",
        "        current = start\n",
        "        target = self.D - 1\n",
        "\n",
        "        # Pre-slice Gamma to avoid lookups\n",
        "        G = self.Gamma_Enigma\n",
        "\n",
        "        for _ in range(self.max_steps):\n",
        "            if current == target:\n",
        "                break\n",
        "\n",
        "            # ⚡ O(1) LOOKUP: No math, no counts, no state shifts\n",
        "            w = G[current].copy()\n",
        "\n",
        "            # Apply a light novelty penalty only if we hit a loop\n",
        "            if current in path[:-1]:\n",
        "                w *= 0.1 # Instant circuit breaker\n",
        "\n",
        "            if w.sum() == 0:\n",
        "                break\n",
        "\n",
        "            # Probabilistic Jump\n",
        "            probs = top_k_masked_probs(w**2, self.top_k)\n",
        "            current = np.random.choice(self.D, p=probs)\n",
        "            path.append(current)\n",
        "\n",
        "        return path"
      ],
      "metadata": {
        "id": "7Z_mRbD4VpM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    D_GRAPH = D_graph\n",
        "except:\n",
        "    D_graph=D_graph\n",
        "def build_coupling_weight(label, m_i, m_j):\n",
        "    \"\"\"\n",
        "    Interpretation: Pascal's Law P = F / A\n",
        "    Force (F) = Potential delta in metrics (Improvement)\n",
        "    Area (A) = Complexity/Friction of the destination node\n",
        "    Valve = Orifice efficiency based on coupling type\n",
        "    \"\"\"\n",
        "    # 1. THE NODAL VECTORS\n",
        "    v_i = np.array(list(m_i.values())) if isinstance(m_i, dict) else np.array(m_i)\n",
        "    v_j = np.array(list(m_j.values())) if isinstance(m_j, dict) else np.array(m_j)\n",
        "\n",
        "    # 2. THE DRIVING FORCE (F)\n",
        "    # The pressure only builds if the destination offers more \"potential\"\n",
        "    # F = || max(0, v_j - v_i) ||\n",
        "    force = np.linalg.norm(np.maximum(v_j - v_i, 0)) + 1e-9\n",
        "\n",
        "    # 3. THE SURFACE AREA (A)\n",
        "    # The 'wider' the destination node (more complex/costly),\n",
        "    # the more the force is distributed, lowering the pressure.\n",
        "    surface_area = np.mean(v_j) + 0.1\n",
        "\n",
        "    # 4. THE VALVE ORIFICE (Efficiency)\n",
        "    # A-type couplings are 'Wide Nozzles' (High pressure translation)\n",
        "    # B-type couplings are 'Constricted Nozzles' (Damped flow)\n",
        "    label_str = str(label) if label is not None else \"\"\n",
        "    if \"A\" in label_str:\n",
        "        motivator = force # High flow efficiency\n",
        "    elif \"B\" in label_str:\n",
        "        motivator =  1.0  # Damped efficiency\n",
        "    else:\n",
        "        motivator = surface_area# Standard atmospheric pressure\n",
        "\n",
        "    # 5. THE SYSTEM PRESSURE (P)\n",
        "    # (i) Power driver\n",
        "    # (iii) Force driver\n",
        "    # (ii) Pressure\n",
        "    pressure = motivator * (force / surface_area)\n",
        "\n",
        "    return max(pressure, 0.0001)\n",
        "def infer_node_types(node_metrics):\n",
        "    \"\"\"\n",
        "    Determines node type based on Cost vs Quality dominance.\n",
        "    A = quality-dominant, B = cost-dominant.\n",
        "    \"\"\"\n",
        "    node_types = []\n",
        "    for m in node_metrics:\n",
        "        # Handle dict or list input\n",
        "        if isinstance(m, dict):\n",
        "            q = m.get(\"quality\", 0.0)\n",
        "            c = m.get(\"cost\", 0.0)\n",
        "        else:\n",
        "            # Assuming [cost, quality] or similar if list,\n",
        "            # but defaulting to A if structure unknown\n",
        "            q, c = 1, 0\n",
        "        node_types.append(\"A\" if q >= c else \"B\")\n",
        "    return node_types\n",
        "\n",
        "def build_coupling_matrix(node_types):\n",
        "    \"\"\"\n",
        "    Builds a D x D coupling-label matrix.\n",
        "    C exists only as a coupling escalation (B->B->C).\n",
        "    \"\"\"\n",
        "    D = len(node_types)\n",
        "    C = np.empty((D, D), dtype=object)\n",
        "\n",
        "    for i in range(D):\n",
        "        for j in range(D):\n",
        "            src = node_types[i]\n",
        "            dst = node_types[j]\n",
        "\n",
        "            if src == \"B\" and dst == \"B\":\n",
        "                C[i, j] = \"B->B->C\"\n",
        "            else:\n",
        "                C[i, j] = f\"{src}->{dst}\"\n",
        "    return C\n",
        "\n",
        "def build_dsm_from_walks(D, paths):\n",
        "    \"\"\"\n",
        "    Constructs a DSM (Design Structure Matrix) from walk paths.\n",
        "    Entry [i,j] is the probability that a process moves from i to j.\n",
        "    \"\"\"\n",
        "    flow = np.zeros((D, D))\n",
        "    for path in paths:\n",
        "        for k in range(len(path) - 1):\n",
        "            u, v = path[k], path[k+1]\n",
        "            flow[u, v] += 1\n",
        "\n",
        "    n_paths = len(paths)\n",
        "    if n_paths > 0:\n",
        "        flow = flow / n_paths\n",
        "\n",
        "    np.fill_diagonal(flow, 0.0)\n",
        "    return flow\n",
        "\n",
        "# =============================================================================\n",
        "# 2. THE METRIC-DRIVEN RANDOM WALKER (Consolidated)\n",
        "# =============================================================================\n",
        "\n",
        "class CouplingState:\n",
        "    \"\"\"\n",
        "    One-step coupling automaton to enforce B->B->C rules.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.state = \"NORMAL\"\n",
        "\n",
        "    def update(self, coupling):\n",
        "        if coupling == \"B->B->C\":\n",
        "            self.state = \"C\"\n",
        "        else:\n",
        "            self.state = \"NORMAL\"\n",
        "\n",
        "    def allows(self, next_node_type):\n",
        "        # Rule: After C state, random is allowed, but strictly NOT B\n",
        "        if self.state == \"C\" and next_node_type == \"B\":\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "class MetricDrivenRandomWalk:\n",
        "    def __init__(self, coupling_matrix, node_metrics, metric_targets=None, max_steps=50, top_k=2, temperature=1.0):\n",
        "        \"\"\"\n",
        "        Unified Walker Class.\n",
        "        :param coupling_matrix: DxD matrix of transition labels.\n",
        "        :param node_metrics: List of dicts/vectors for node values.\n",
        "        :param metric_targets: (Optional) Binary mask or Target Matrix.\n",
        "        :param max_steps: Maximum path length.\n",
        "        :param top_k: Branching factor constraint.\n",
        "        :param temperature: Control randomness (Higher = more random, Lower = more deterministic).\n",
        "        \"\"\"\n",
        "        self.C = coupling_matrix\n",
        "        self.node_metrics = node_metrics\n",
        "        self.node_types = infer_node_types(node_metrics)\n",
        "\n",
        "        self.D = coupling_matrix.shape[0]\n",
        "        self.max_steps = max_steps\n",
        "        self.top_k = top_k\n",
        "        self.temperature = temperature\n",
        "\n",
        "        # Handle Mask: If None, allow all connections (ones)\n",
        "        if metric_targets is not None:\n",
        "            self.mask = np.array(metric_targets)\n",
        "        else:\n",
        "            self.mask = np.ones((self.D, self.D))\n",
        "\n",
        "    def run(self, start=0):\n",
        "        path = [start]\n",
        "        current = start\n",
        "        target_node = self.D - 1\n",
        "\n",
        "        # Initialize Automaton for this specific run\n",
        "        automaton = CouplingState()\n",
        "\n",
        "        for _ in range(self.max_steps):\n",
        "            if current == target_node:\n",
        "                break\n",
        "\n",
        "            # 1. Calculate Raw Weights (Ohmic Conductance)\n",
        "            weights = np.zeros(self.D)\n",
        "            for j in range(self.D):\n",
        "                if current == j: continue\n",
        "\n",
        "                # A. Structural Mask Check (User provided mask)\n",
        "                if self.mask[current, j] == 0:\n",
        "                    continue\n",
        "\n",
        "                # B. Automaton Rule Check\n",
        "                if not automaton.allows(self.node_types[j]):\n",
        "                    continue\n",
        "\n",
        "                # C. Compute Physics-based Weight\n",
        "                base_w = build_coupling_weight(self.C[current, j], self.node_metrics[current], self.node_metrics[j])\n",
        "\n",
        "                # D. Directional Bias (Gravity)\n",
        "                gravity = 1.5 if j > current else 0.4\n",
        "\n",
        "                # E. Loop Friction (Novelty check)\n",
        "                friction = 0.1 if j in path else 1.0\n",
        "\n",
        "                weights[j] = base_w * gravity * friction\n",
        "\n",
        "            # 2. Check for Dead End\n",
        "            if weights.sum() == 0:\n",
        "                break\n",
        "\n",
        "            # 3. Apply Temperature (Softmax-ish scaling) for True Randomness\n",
        "            # Normalize first to avoid overflow in exp\n",
        "            weights = weights / (weights.max() + 1e-12)\n",
        "            exp_weights = np.exp(weights / self.temperature)\n",
        "\n",
        "            # Zero out the ones that were originally zero\n",
        "            exp_weights[weights == 0] = 0\n",
        "\n",
        "            # 4. Top-K Sparsity & Normalization\n",
        "            probs = top_k_masked_probs(exp_weights, self.top_k)\n",
        "\n",
        "            # 5. Stochastic Jump\n",
        "            nxt = np.random.choice(self.D, p=probs)\n",
        "\n",
        "            # Update state\n",
        "            automaton.update(self.C[current, nxt])\n",
        "            path.append(nxt)\n",
        "            current = nxt\n",
        "\n",
        "        return path\n",
        "from collections import deque\n",
        "\n",
        "        #return True\n",
        "# --- Inner Loop (FCM & Learning) ---\n",
        "INNER_FCM_STEPS = 1000       # Iterations per node simulation\n",
        "INNER_LR_X = 1.0             # Learning rate for State X\n",
        "INNER_LR_Y = 0.01           # Learning rate for State Y\n",
        "INNER_LR_W = 1.0             # Learning rate for Weights\n",
        "INNER_SVM_LR = 0.01          # SVM Learning Rate\n",
        "INNER_GAMMA = 1.0            # Inter-layer neural connection strength\n",
        "\n",
        "# --- Random Walk & Pathfinding ---\n",
        "WALK_BETA = 0.3              # Novelty penalty (dampens repeated paths)\n",
        "WALK_LAMBDA_COST = 1.0       # Penalty weight for cost metrics\n",
        "\n",
        "# --- Outer Loop (Topology Optimization) ---\n",
        "OUTER_GENERATIONS = 1        # Iterations per Layer\n",
        "OUTER_COST_LIMIT = 1000      # Normalization ceiling for scores\n",
        "INTER_EDGE_THRESH = 0.02     # Min DSM weight to trigger neural link\n",
        "\n",
        "for ijk in range(D_GRAPH):\n",
        "    for jik in range(5,30):\n",
        "        print(50*'_',ijk,50*'-',jik,50*'=')\n",
        "        #===============================================================================\n",
        "        OUTER_N_SIMS = 1000          # More simulations to find the \"Hidden Gem\" paths\n",
        "        WALK_MAX_STEPS = 100          # Let the walker explore complex relationships deeply\n",
        "        DSM_TARGET_EDGES = jik        # Allow HIGHER density (Complexity is allowed!)\n",
        "        OUTER_DSM_LAYERS = 1         # Balanced hierarchy (Structure -> Systems -> Skin)\n",
        "        DSM_ADDITIVE_RATE = 0.45      # Low Learning Rate: Learn slowly, don't panic.\n",
        "        DSM_FEEDBACK_STR = 0.05      # Weak Feedback: Listen to problems, but don't obsess.\n",
        "        WALK_TOP_K = 2               # Soft Sparsity: Consider more options per step.\n",
        "        DSM_FEEDBACK_FILTER = 0.1    # Only react to major issues.\n",
        "        DSM_PRUNE_THRESH = 0.02      # Keep subtle connections.\n",
        "        DSM_INIT_RANGE = 0.2         # Start with a blanker slate.\n",
        "        STARTING_POINT = ijk           # START AT SITE ANALYSIS (Respect the Land).\n",
        "        #=============================================================================\n",
        "        class Fuzzy_Hierarchical_Multiplex:\n",
        "            def __init__(self, candidate_dims, D_graph,\n",
        "                        synthetic_targets,\n",
        "                        gamma_interlayer=1.0, causal_flag=False,\n",
        "                        metrics=METRIC_KEYS, metric_mask=METRIC_TARGET):\n",
        "\n",
        "                self.candidate_dims = candidate_dims\n",
        "                self.D_graph = D_graph\n",
        "                self.synthetic_targets = synthetic_targets\n",
        "                self.causal_flag = causal_flag\n",
        "                self.best_dim_per_node = [len(t)-1 for t in synthetic_targets]\n",
        "                self.MM = metric_mask\n",
        "                self.MK = metrics\n",
        "                self.MKI = metrics + ['score']\n",
        "\n",
        "                self.PLM = [[] for _ in range(self.D_graph)]\n",
        "                self.PLMS = [[] for _ in range(self.D_graph)]\n",
        "                self.nested_reps = [np.zeros(c[0]) for c in candidate_dims]\n",
        "\n",
        "                # Inter-layer setup\n",
        "                self.inter_layer = InterLayer(D_graph, max_inner_dim=max(candidate_dims), gamma=gamma_interlayer)\n",
        "                self.chosen_Gmat = np.random.uniform(0.0, 0.3, (D_graph, D_graph))\n",
        "                np.fill_diagonal(self.chosen_Gmat, 0)\n",
        "\n",
        "                self.l2_before, self.l2_after = [], []\n",
        "                self.max_target_len = max(len(t['target']) for t in synthetic_targets)\n",
        "                self.svm_lr = 0.01\n",
        "\n",
        "                self.metric_traces = {k: [] for k in metrics}\n",
        "                self.metric_traces_per_node = [{} for _ in range(self.D_graph)]\n",
        "\n",
        "                # DSM optimization hyperparameters\n",
        "                self.dsm_lr = 0.1\n",
        "                self.dsm_l1 = 0.02\n",
        "                self.dsm_clip = 1.0\n",
        "                self.dsm_history = []\n",
        "                self.dsm_cost_weight = 0.05\n",
        "\n",
        "            def print_dsm_basic(self):\n",
        "                D = self.D_graph\n",
        "                print(\"\\n=== DESIGN STRUCTURE MATRIX (DSM) : Gmat ===\")\n",
        "                header = \"     \" + \" \".join([f\"N{j:>4}\" for j in range(D)])\n",
        "                print(header)\n",
        "                for i in range(D):\n",
        "                    row = \"N{:>2} | \".format(i)\n",
        "                    for j in range(D):\n",
        "                        row += f\"{self.chosen_Gmat[i, j]:>5.2f} \"\n",
        "                    print(row)\n",
        "\n",
        "            # ---------- INNER LOOP (FCM) ----------\n",
        "            def run_inner(self, node_idx, target, D_fcm,\n",
        "                        steps=INNER_FCM_STEPS, lr_x=INNER_LR_X, lr_y=INNER_LR_Y, lr_W=INNER_LR_W,\n",
        "                        decorrelate_metrics=False):\n",
        "\n",
        "                # --- Initialize activations ---\n",
        "                x = np.random.uniform(-0.6, 0.6, D_fcm)\n",
        "                y = np.random.uniform(-0.1, 0.1, D_fcm)\n",
        "\n",
        "                # L2 tracking\n",
        "                self.l2_before.append(np.linalg.norm(self.nested_reps[node_idx][:len(target)] - target))\n",
        "\n",
        "                # --- FCM updates ---\n",
        "                W = np.random.uniform(-0.6, 0.6, (D_fcm, D_fcm))\n",
        "                np.fill_diagonal(W, 0)\n",
        "\n",
        "                for _ in range(steps):\n",
        "                    z = y.dot(W) + x\n",
        "                    Theta_grad_z = z - target\n",
        "                    Theta_grad_x = Theta_grad_z\n",
        "                    Theta_grad_y = Theta_grad_z.dot(W.T)\n",
        "                    Theta_grad_W = np.outer(y, Theta_grad_z)\n",
        "\n",
        "                    x -= lr_x * np.clip(Theta_grad_x, -0.05, 0.05)\n",
        "                    y -= lr_y * np.clip(Theta_grad_y, -0.05, 0.05)\n",
        "                    W -= lr_W * np.clip(Theta_grad_W, -0.01, 0.01)\n",
        "\n",
        "                    x = np.clip(x, 0, 1)\n",
        "                    y = np.clip(y, 0, 1)\n",
        "                    np.fill_diagonal(W, 0)\n",
        "                    W = np.clip(W, -1, 1)\n",
        "\n",
        "                # --- Update nested representation ---\n",
        "                self.nested_reps[node_idx][:len(x)] = x\n",
        "                self.l2_after.append(np.linalg.norm(x - target))\n",
        "\n",
        "                # --- Extract node features ---\n",
        "                # Assuming MetricsEvaluator is a global or imported class\n",
        "                metrics_evaluator = MetricsEvaluator(data_matrix=DATA_MATRIX)\n",
        "                features = metrics_evaluator.extract_features(node_idx)\n",
        "                feat_vals = np.array(list(features.values()))\n",
        "\n",
        "                # --- Compute metrics scaled by activations + features ---\n",
        "                metric_mask = METRIC_TARGET[node_idx]\n",
        "                metric_values = {}\n",
        "\n",
        "                for key, formula, mask in zip(METRIC_KEYS, METRIC_FORMULAS, metric_mask):\n",
        "                    if mask:\n",
        "                        weighted_input = np.mean(feat_vals)\n",
        "                        # Outer scale check\n",
        "                        outer_scale = getattr(self, 'best_node_weights', {}).get(node_idx, 1.0)\n",
        "                        if isinstance(outer_scale, (list, np.ndarray)):\n",
        "                            # fallback if it was stored incorrectly in previous context\n",
        "                            outer_scale = 1.0\n",
        "\n",
        "                        weighted_input *= outer_scale\n",
        "                        metric_val = formula(weighted_input)\n",
        "                        metric_values[key] = metric_val\n",
        "\n",
        "                        # STORE DATAPOINT\n",
        "                        self.metric_traces[key].append((weighted_input, metric_val))\n",
        "                    else:\n",
        "                        metric_values[key] = 0.0\n",
        "\n",
        "                # --- Total score ---\n",
        "                metric_values['score'] = sum(metric_values.values())\n",
        "\n",
        "                # --- Build SVM Training Data ---\n",
        "                metric_output_vals = np.array(\n",
        "                    [v for k, v in metric_values.items() if k not in ['score', 'x', 'feat_vals']]\n",
        "                )\n",
        "\n",
        "                # Lazy init per-node SVM\n",
        "                if not hasattr(self, \"node_svms\"):\n",
        "                    self.node_svms = {}\n",
        "\n",
        "                if node_idx not in self.node_svms:\n",
        "                    self.node_svms[node_idx] = SVM(\n",
        "                        input_dim=len(self.MK),\n",
        "                        output_dim=self.candidate_dims[node_idx][0],\n",
        "                        lr=self.svm_lr\n",
        "                    )\n",
        "\n",
        "                svm = self.node_svms[node_idx]\n",
        "\n",
        "                # Build SVM Input/Output\n",
        "                x_in_full = np.zeros(len(self.MK))\n",
        "                x_in_full[:len(metric_output_vals)] = metric_output_vals\n",
        "                x_in = x_in_full.reshape(1, -1)\n",
        "\n",
        "                y_out_full = np.zeros(self.candidate_dims[node_idx][0])\n",
        "                y_out_full[:len(x)] = x\n",
        "                y_out = y_out_full.reshape(1, -1)\n",
        "\n",
        "                # Train SVM\n",
        "                _ = svm.train_step(x_in, y_out)\n",
        "\n",
        "                # --- Store PLMS trace ---\n",
        "                self.PLMS[node_idx].append((float(weighted_input), metric_output_vals))\n",
        "\n",
        "                if len(self.PLMS[node_idx]) % 100 == 0:\n",
        "                    print(f\"Node {node_idx}, samples learned:\", len(self.PLMS[node_idx]))\n",
        "\n",
        "                # --- Compute inter-layer MI ---\n",
        "                mi_score = self.inter_layer.mi_for_graph(self.chosen_Gmat, self.nested_reps)\n",
        "\n",
        "                return x, y, W, mi_score, metric_values\n",
        "\n",
        "            # ---------- OUTER LOOP (Topology Optimization) ----------\n",
        "            def run_outer(self, outer_cost_limit=OUTER_COST_LIMIT, alpha=0.0, additive_rate=DSM_ADDITIVE_RATE, n_simulations=OUTER_N_SIMS):\n",
        "                \"\"\"\n",
        "                CORRECTED LOGIC (Distributed Flow):\n",
        "                1. Uses Random Walk to find high-probability metric flows starting from ANY node.\n",
        "                2. Injects FEEDBACK LOOPS to create Coupled Blocks.\n",
        "                3. Prunes weak edges to prevent 'Total Chaos'.\n",
        "\n",
        "                *Update:* Removed 'Start at 0' handicap. Now samples flows from all subsystems.\n",
        "                \"\"\"\n",
        "                node_metrics_list = self.capped_node_metrics\n",
        "                D = self.D_graph\n",
        "\n",
        "                # =========================================================\n",
        "                # 1. METRIC SCORING\n",
        "                # =========================================================\n",
        "                raw_scores = np.array([m['score'] for m in node_metrics_list])\n",
        "                total_raw = raw_scores.sum()\n",
        "                if total_raw > outer_cost_limit:\n",
        "                    scale_factor = outer_cost_limit / total_raw\n",
        "                    for metrics in node_metrics_list:\n",
        "                        for key in self.MKI:\n",
        "                            metrics[key] *= scale_factor\n",
        "                    raw_scores *= scale_factor\n",
        "\n",
        "                fuzzy_tensor = self.compute_fuzzy_metric_tensor(normalize=False)\n",
        "                self.weighted_fmt = fuzzy_tensor.copy()\n",
        "\n",
        "                # Calculate Contributions\n",
        "                node_contributions = np.zeros(D)\n",
        "                for i in range(D):\n",
        "                    own_score = raw_scores[i]\n",
        "                    fmt_contrib = fuzzy_tensor[i, :, :].sum() - fuzzy_tensor[i, i, :].sum()\n",
        "                    node_contributions[i] = own_score + self.inter_layer.gamma * fmt_contrib\n",
        "                self.node_score_contributions = node_contributions\n",
        "                self.correlation_penalty = 0.0\n",
        "\n",
        "                # =========================================================\n",
        "                # 2. PROBABILISTIC PATHFINDING (Distributed Random Walk)\n",
        "                # =========================================================\n",
        "\n",
        "                node_metrics = self.capped_node_metrics\n",
        "                node_types = infer_node_types(node_metrics)\n",
        "                C_matrix = build_coupling_matrix(node_types)\n",
        "                print(node_types)\n",
        "                # Initialize Walker\n",
        "                # Inside run_outer method:\n",
        "\n",
        "# ... (Previous metric scoring logic) ...\n",
        "\n",
        "                # Pass the mask (METRIC_TARGET) explicitly\n",
        "                # Assuming 'self.MM' holds the METRIC_TARGET data structure you provided\n",
        "                walker = MetricDrivenRandomWalk(\n",
        "                    C_matrix,\n",
        "                    node_metrics,\n",
        "                    metric_targets=self.MM, # <--- PASS THE MASK HERE\n",
        "                    max_steps=WALK_MAX_STEPS,\n",
        "                    top_k=WALK_TOP_K\n",
        "                )\n",
        "\n",
        "# ... (Rest of pathfinding logic) ...\n",
        "\n",
        "                # --- VALIDATOR (RELAXED) ---\n",
        "                                # 1. Define the Tiers of Validation\n",
        "                def is_path_valid(path):\n",
        "                    # 1. Topology Checks\n",
        "                    if len(path) < 2: return False        # A path must go somewhere\n",
        "                    if path[-1] != (D - 1): return False  # Must still converge to Project Completion\n",
        "                    if len(set(path)) != len(path): return False # No self-cycles\n",
        "\n",
        "                    # 2. Coupling Constraints\n",
        "                    state_machine = CouplingState()\n",
        "                    for k in range(len(path) - 1):\n",
        "                        u, v = path[k], path[k+1]\n",
        "                        type_u, type_v = node_types[u], node_types[v]\n",
        "\n",
        "                        if not state_machine.allows(type_v): return False\n",
        "\n",
        "                        coupling = \"B->B->C\" if (type_u == \"B\" and type_v == \"B\") else f\"{type_u}->{type_v}\"\n",
        "                        state_machine.update(coupling)\n",
        "                    return True\n",
        "\n",
        "\n",
        "                # --- DISTRIBUTED SAMPLING ---\n",
        "                valid_paths = []\n",
        "                print(f\" [Optimizer] Sampling {n_simulations} distributed paths (Any Start -> End)...\")\n",
        "\n",
        "                # Potential start nodes: 0 to D-2 (Any node except the final Sink node)\n",
        "                possible_starts = list(range(D - 1))\n",
        "\n",
        "                for _ in range(n_simulations):\n",
        "                    # Randomly select a starting subsystem to ensure \"All Processes Included\"\n",
        "\n",
        "                    path = walker.run(start=STARTING_POINT)\n",
        "\n",
        "                    if is_path_valid(path):\n",
        "                        valid_paths.append(path)\n",
        "\n",
        "                # FALLBACK\n",
        "                if len(valid_paths) == 0:\n",
        "                    print(\" [WARNING] Strict constraints failed. Synthesizing default backbone.\")\n",
        "                    valid_paths.append(list(range(D)))\n",
        "\n",
        "                # Deduplicate\n",
        "                unique_paths = sorted(list(set(tuple(p) for p in valid_paths)), key=lambda x: len(x), reverse=True)\n",
        "                valid_paths = [list(p) for p in unique_paths]\n",
        "\n",
        "                # Deduplicate\n",
        "                unique_paths = sorted(list(set(tuple(p) for p in valid_paths)), key=lambda x: len(x), reverse=True)\n",
        "                valid_paths = [list(p) for p in unique_paths]\n",
        "\n",
        "                # =========================================================\n",
        "                # 3. LAYER CONSTRUCTION & FEEDBACK INJECTION\n",
        "                # =========================================================\n",
        "\n",
        "                G_layer = build_dsm_from_walks(D, valid_paths)\n",
        "\n",
        "                # Feedback Loops\n",
        "                feedback_strength = DSM_FEEDBACK_STR\n",
        "                G_feedback = G_layer.T * feedback_strength\n",
        "\n",
        "                # Filter feedback\n",
        "                G_feedback[G_layer < DSM_FEEDBACK_FILTER] = 0.0\n",
        "\n",
        "                # Combine\n",
        "                G_layer_final = G_layer + G_feedback\n",
        "\n",
        "                # =========================================================\n",
        "                # 4. UPDATE, AMPLIFY & PRUNE\n",
        "                # =========================================================\n",
        "\n",
        "                self.chosen_Gmat = self.chosen_Gmat + (additive_rate * G_layer_final)\n",
        "\n",
        "                if np.max(self.chosen_Gmat) > 0:\n",
        "                    self.chosen_Gmat /= np.max(self.chosen_Gmat)\n",
        "\n",
        "                # Top-K Pruning\n",
        "                TARGET_EDGES = DSM_TARGET_EDGES\n",
        "                flat = self.chosen_Gmat.ravel()\n",
        "                if len(flat) > TARGET_EDGES:\n",
        "                    threshold = np.partition(flat, -TARGET_EDGES)[-TARGET_EDGES]\n",
        "                    self.chosen_Gmat[self.chosen_Gmat < threshold] = 0.0\n",
        "\n",
        "                # Noise Pruning\n",
        "                self.chosen_Gmat[self.chosen_Gmat < DSM_PRUNE_THRESH] = 0.0\n",
        "\n",
        "                density = np.count_nonzero(self.chosen_Gmat)\n",
        "                print(f\" [Optimizer] Matrix Updated. Density: {density} edges. Max Val: {np.max(self.chosen_Gmat):.2f}\")\n",
        "\n",
        "                self.walks, self.best_walk = collect_and_select_best_walks(\n",
        "                    self.chosen_Gmat,\n",
        "                    self.capped_node_metrics,\n",
        "                    beta=WALK_BETA\n",
        "                )\n",
        "\n",
        "                self.print_dsm_basic()\n",
        "\n",
        "                if not hasattr(self, \"_node_contributions_history\"):\n",
        "                    self._node_contributions_history = []\n",
        "                self._node_contributions_history.append(node_contributions.copy())\n",
        "\n",
        "                return node_metrics_list, 0.0, node_contributions\n",
        "\n",
        "            def run(self, outer_generations=OUTER_GENERATIONS, num_dsm_layers=OUTER_DSM_LAYERS):\n",
        "                best_score = -np.inf\n",
        "\n",
        "                # 1. FIX INITIALIZATION:\n",
        "                # Use the random initial state as the baseline.\n",
        "                # This ensures Layer 0 captures the \"Jump\" from noise to structure.\n",
        "                baseline = self.chosen_Gmat.copy()\n",
        "                dsm_decomposer = DSM_Layer_Decomposer(baseline, mode='additive')\n",
        "                dsm_decomposer.current_total = baseline.copy()\n",
        "\n",
        "                print(f\"Starting Optimization: {num_dsm_layers} Layers x {outer_generations} Gens\")\n",
        "\n",
        "                # Define the \"Building Blocks\" for the 3 layers (based on your 12-15 node stack)\n",
        "                # Layer 0: Structure (Nodes 0-5), Layer 1: Systems (Nodes 6-10), Layer 2: Skin/Ops (Nodes 11-14)\n",
        "                nodes_per_layer = np.array_split(range(self.D_graph), num_dsm_layers)\n",
        "\n",
        "                for layer_idx in range(num_dsm_layers):\n",
        "                    print(f\"\\n>>> COMPILING LAYER {layer_idx + 1}: {['STRUCTURE', 'SYSTEMS', 'SKIN'][layer_idx]} <<<\")\n",
        "\n",
        "                    # Determine the nodes active in this specific layer\n",
        "                    active_nodes = nodes_per_layer[layer_idx]\n",
        "\n",
        "                    for gen in range(outer_generations):\n",
        "                        # 1. Inner Loop (Targeting active nodes for this layer)\n",
        "                        node_metrics_list = []\n",
        "                        for node_idx in range(self.D_graph):\n",
        "                            full_target = self.synthetic_targets[node_idx]['target']\n",
        "                            D_fcm = self.candidate_dims[node_idx][0]\n",
        "                            target = full_target[:D_fcm]\n",
        "\n",
        "                            # We simulate everything, but the \"Learning\" is focused on the active layer\n",
        "                            _, _, _, _, metrics = self.run_inner(node_idx, target, D_fcm)\n",
        "                            node_metrics_list.append(metrics)\n",
        "\n",
        "                        self.capped_node_metrics = node_metrics_list\n",
        "\n",
        "                        # 2. Outer Loop (Topology Optimization)\n",
        "                        # We pass the layer_idx to run_outer if you want to adjust the WALK_TOP_K\n",
        "                        # or additive_rate per layer (e.g., higher for structure, lower for skin)\n",
        "                        _, capped_score, _ = self.run_outer()\n",
        "\n",
        "                        best_score = max(best_score, capped_score)\n",
        "                        print(f\" [Gen {gen+1}] Score: {capped_score:.4f}\", end='\\r')\n",
        "\n",
        "                    print(\"\")\n",
        "\n",
        "                    # 3. SNAPSHOT: The Decomposer captures the \"Delta\" for this layer\n",
        "                    # This is where the MUX/DEMUX logic is voucher-ed.\n",
        "                    dsm_decomposer.add_snapshot(self.chosen_Gmat)\n",
        "\n",
        "                self.dsm_layers = dsm_decomposer.layers\n",
        "                print(\"\\nOptimization Complete. All 3 Layers Compiled.\")\n",
        "                return best_score\n",
        "\n",
        "            # ---------- VISUALIZATIONS & ANALYSIS ----------\n",
        "\n",
        "            def plot_pointwise_minmax_elite(self, top_k=21):\n",
        "                plt.figure(figsize=(14, 3))\n",
        "                for i in range(self.D_graph):\n",
        "                    dim_i = self.candidate_dims[i][0]\n",
        "                    base = self.nested_reps[i][:dim_i]\n",
        "                    reps = np.clip(base + np.random.normal(0, 0.05, (top_k, len(base))), 0, 1)\n",
        "                    y_min, y_max = reps.min(axis=0), reps.max(axis=0)\n",
        "                    y_sel = base\n",
        "\n",
        "                    y_true = self.synthetic_targets[i]['target'][:len(y_sel)]\n",
        "                    if len(y_true) < len(y_sel):\n",
        "                        y_true = np.pad(y_true, (0, len(y_sel) - len(y_true)), \"constant\")\n",
        "                    else:\n",
        "                        y_true = y_true[:len(y_sel)]\n",
        "\n",
        "                    plt.subplot(1, self.D_graph, i + 1)\n",
        "                    plt.fill_between(range(len(y_min)), y_min, y_max, color='skyblue', alpha=0.4, label='Elite Interval')\n",
        "                    plt.plot(y_sel, 'k-', lw=2, label='Estimated')\n",
        "                    plt.plot(y_true, 'r--', lw=2, label='True')\n",
        "                    plt.ylim(0, 1.05)\n",
        "                    plt.title(f\"Node {i + 1}\")\n",
        "                    if i == 0: plt.legend()\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "            def plot_nested_activations(self):\n",
        "                plt.figure(figsize=(12, 3))\n",
        "                for i, rep in enumerate(self.nested_reps):\n",
        "                    dim_i = self.candidate_dims[i][0]\n",
        "                    rep_i = rep[:dim_i]\n",
        "                    plt.subplot(1, self.D_graph, i + 1)\n",
        "                    plt.bar(range(len(rep_i)), rep_i, color=plt.cm.plasma(rep_i))\n",
        "                    plt.ylim(0, 1)\n",
        "                    plt.title(f\"Node {i + 1}\")\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "            def plot_outer_fuzzy_graph(self):\n",
        "                G = nx.DiGraph()\n",
        "                for i in range(self.D_graph): G.add_node(i)\n",
        "                for i in range(self.D_graph):\n",
        "                    for j in range(self.D_graph):\n",
        "                        if i != j and abs(self.chosen_Gmat[i, j]) > 0.02:\n",
        "                            G.add_edge(i, j, weight=self.chosen_Gmat[i, j])\n",
        "\n",
        "                node_sizes = [self.best_dim_per_node[i] * 200 for i in range(self.D_graph)]\n",
        "                edge_colors = ['green' if d['weight'] > 0 else 'red' for _, _, d in G.edges(data=True)]\n",
        "                edge_widths = [abs(d['weight']) * 3 for _, _, d in G.edges(data=True)]\n",
        "\n",
        "                pos = nx.spring_layout(G)\n",
        "                plt.figure(figsize=(6, 6))\n",
        "                nx.draw(G, pos, node_size=node_sizes, node_color='skyblue',\n",
        "                        edge_color=edge_colors, width=edge_widths, arrows=True, with_labels=True)\n",
        "                plt.title(\"Outer Fuzzy Multiplex Graph\")\n",
        "                plt.show()\n",
        "\n",
        "            def print_interactions(self, return_tensor=True, verbose=True):\n",
        "                D_graph = self.D_graph\n",
        "                inter_dim = self.inter_layer.inter_dim\n",
        "                inter_tensor = np.zeros((D_graph, D_graph, inter_dim))\n",
        "\n",
        "                acts = self.inter_layer.build_activations(self.chosen_Gmat, self.nested_reps)\n",
        "                if not acts:\n",
        "                    if verbose:\n",
        "                        print(\"No active edges above threshold.\")\n",
        "                    return inter_tensor if return_tensor else None\n",
        "\n",
        "                for (i, j), vec in acts.items():\n",
        "                    inter_tensor[i, j, :] = vec\n",
        "                    if verbose:\n",
        "                        act_str = \", \".join([f\"{v:.3f}\" for v in vec])\n",
        "                        print(f\"Node {i} -> Node {j}: [{act_str}]\")\n",
        "                return inter_tensor if return_tensor else None\n",
        "\n",
        "            def compute_fuzzy_metric_tensor(self, normalize=True, verbose=False):\n",
        "                metrics_keys = self.MK\n",
        "                D = self.D_graph\n",
        "                num_metrics = len(metrics_keys)\n",
        "                tensor = np.zeros((D, D, num_metrics))\n",
        "\n",
        "                metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "\n",
        "                node_metrics = []\n",
        "                for i, rep in enumerate(self.nested_reps):\n",
        "                    metrics = metrics_evaluator.compute_node_metrics(i, y=rep)\n",
        "                    node_metrics.append(np.array([metrics[k] for k in metrics_keys]))\n",
        "                node_metrics = np.array(node_metrics)\n",
        "\n",
        "                for i in range(D):\n",
        "                    for j in range(D):\n",
        "                        if i == j:\n",
        "                            tensor[i, j, :] = node_metrics[j]\n",
        "                        else:\n",
        "                            weight = np.clip(abs(self.chosen_Gmat[i, j]), 0, 1)\n",
        "                            tensor[i, j, :] = weight * node_metrics[j]\n",
        "\n",
        "                if normalize:\n",
        "                    tensor = (tensor - tensor.min()) / (tensor.max() - tensor.min() + 1e-12)\n",
        "\n",
        "                if verbose:\n",
        "                    print(\"Fuzzy Metric Tensor shape:\", tensor.shape)\n",
        "\n",
        "                return tensor\n",
        "\n",
        "            def plot_fuzzy_metric_tensor_heatmaps(self, fuzzy_tensor=None, metrics_keys=None):\n",
        "                if metrics_keys is None:\n",
        "                    metrics_keys = self.MK\n",
        "                if fuzzy_tensor is None:\n",
        "                    fuzzy_tensor = self.compute_fuzzy_metric_tensor(normalize=True)\n",
        "\n",
        "                D = self.D_graph\n",
        "                num_metrics = len(metrics_keys)\n",
        "\n",
        "                fig, axes = plt.subplots(1, num_metrics, figsize=(4 * num_metrics, 4))\n",
        "                if num_metrics == 1: axes = [axes]\n",
        "\n",
        "                im = None\n",
        "                for k, key in enumerate(metrics_keys):\n",
        "                    data = fuzzy_tensor[:, :, k]\n",
        "                    im = axes[k].imshow(data, cmap='viridis', vmin=0, vmax=1)\n",
        "                    for i in range(D):\n",
        "                        for j in range(D):\n",
        "                            axes[k].text(j, i, f\"{data[i, j]:.2f}\", ha='center', va='center', color='white', fontsize=9)\n",
        "                    axes[k].set_xticks(range(D))\n",
        "                    axes[k].set_yticks(range(D))\n",
        "                    axes[k].set_xticklabels([f'Node {j}' for j in range(D)])\n",
        "                    axes[k].set_yticklabels([f'Node {i}' for i in range(D)])\n",
        "                    axes[k].set_title(f'FMT - {key}')\n",
        "\n",
        "                fig.colorbar(im, ax=axes, orientation='vertical', fraction=0.025, pad=0.04, label='Normalized Metric Value')\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "            def compute_fmt_with_elite_bounds(self, top_k=21):\n",
        "                metrics_keys = self.MK\n",
        "                D = self.D_graph\n",
        "                num_metrics = len(metrics_keys)\n",
        "                tensor_bounds = np.zeros((D, D, num_metrics, 2))\n",
        "\n",
        "                metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "\n",
        "                for i in range(D):\n",
        "                    base = self.nested_reps[i]\n",
        "                    reps = np.clip(base + np.random.normal(0, 0.05, (top_k, len(base))), 0, 1)\n",
        "\n",
        "                    metrics_matrix = np.zeros((top_k, num_metrics))\n",
        "                    for idx, rep in enumerate(reps):\n",
        "                        m = metrics_evaluator.compute_node_metrics(i, y=rep)\n",
        "                        metrics_matrix[idx, :] = [m[k] for k in metrics_keys]\n",
        "\n",
        "                    lower_i = metrics_matrix.min(axis=0)\n",
        "                    upper_i = metrics_matrix.max(axis=0)\n",
        "\n",
        "                    for j in range(D):\n",
        "                        tensor_bounds[i, j, :, 0] = lower_i\n",
        "                        tensor_bounds[i, j, :, 1] = upper_i\n",
        "\n",
        "                return tensor_bounds\n",
        "\n",
        "            def plot_fmt_with_bounds(self, fmt_tensor_bounds):\n",
        "                D = self.D_graph\n",
        "                metrics_keys = self.MK\n",
        "                M_actual = len(metrics_keys)\n",
        "\n",
        "                mean_vals = (fmt_tensor_bounds[:, :, :, 0] + fmt_tensor_bounds[:, :, :, 1]) / 2\n",
        "                mean_vals = mean_vals.mean(axis=1)  # mean across targets\n",
        "                mean_vals = mean_vals.mean(axis=0, keepdims=True)  # mean across nodes\n",
        "\n",
        "                if hasattr(self, 'best_alpha') and hasattr(self, 'best_w_contrib'):\n",
        "                    mean_weight = (self.best_alpha * self.best_w_contrib).mean()\n",
        "                    mean_vals = mean_vals * mean_weight\n",
        "\n",
        "                fig, ax = plt.subplots(figsize=(1.2 * M_actual + 4, 2))\n",
        "                im = ax.imshow(mean_vals, cmap='viridis', aspect='auto')\n",
        "\n",
        "                vmin, vmax = mean_vals.min(), mean_vals.max()\n",
        "                for i in range(mean_vals.shape[0]):\n",
        "                    for k in range(M_actual):\n",
        "                        val = mean_vals[i, k]\n",
        "                        color = 'white' if val < (vmin + 0.5 * (vmax - vmin)) else 'black'\n",
        "                        ax.text(k, i, f\"{val:.2f}\", ha='center', va='center', color=color, fontsize=8)\n",
        "\n",
        "                ax.set_xticks(range(M_actual))\n",
        "                ax.set_xticklabels(metrics_keys[:M_actual], rotation=45, ha='right')\n",
        "                ax.set_yticks([0])\n",
        "                ax.set_yticklabels(['Mean across nodes'])\n",
        "                ax.set_title(\"Weighted FMT with Bounds\")\n",
        "                fig.colorbar(im, ax=ax, label='Weighted Mean Metric Value')\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "            def plot_node_score_contribution(self, metrics_keys=None):\n",
        "                if metrics_keys is None:\n",
        "                    metrics_keys = self.MK\n",
        "                D = self.D_graph\n",
        "                node_contributions = np.array(self.node_score_contributions)\n",
        "\n",
        "                if hasattr(self, 'weighted_fmt'):\n",
        "                    fuzzy_tensor = np.array(self.weighted_fmt)\n",
        "                else:\n",
        "                    fuzzy_tensor = self.compute_fuzzy_metric_tensor(normalize=True)\n",
        "\n",
        "                fuzzy_tensor_norm = (fuzzy_tensor - fuzzy_tensor.min()) / (fuzzy_tensor.max() - fuzzy_tensor.min() + 1e-12)\n",
        "                fmt_matrix = fuzzy_tensor_norm.sum(axis=2)\n",
        "                np.fill_diagonal(fmt_matrix, 0)\n",
        "\n",
        "                raw_matrix = np.zeros((D, D))\n",
        "                np.fill_diagonal(raw_matrix, node_contributions)\n",
        "\n",
        "                total_matrix = raw_matrix + fmt_matrix\n",
        "\n",
        "                fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "                matrices = [raw_matrix, fmt_matrix, total_matrix]\n",
        "                titles = [\"Raw Node Contribution\", \"Normalized FMT Contribution\", \"Total Contribution\"]\n",
        "\n",
        "                im = None\n",
        "                for ax, mat, title in zip(axes, matrices, titles):\n",
        "                    im = ax.imshow(mat, cmap='viridis', vmin=0, vmax=1)\n",
        "                    for i in range(D):\n",
        "                        for j in range(D):\n",
        "                            ax.text(j, i, f\"{mat[i, j]:.2f}\", ha='center', va='center', color='white', fontsize=8)\n",
        "                    ax.set_title(title)\n",
        "                    ax.set_xticks(range(D))\n",
        "                    ax.set_xticklabels([f\"Node {i + 1}\" for i in range(D)])\n",
        "                    ax.set_yticks(range(D))\n",
        "                    ax.set_yticklabels([f\"Node {i + 1}\" for i in range(D)])\n",
        "\n",
        "                fig.colorbar(im, ax=axes, orientation='vertical', fraction=0.025, pad=0.04, label='Contribution Value')\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "            def plot_fmt_with_run_metrics(self, metrics_keys=None):\n",
        "                if metrics_keys is None:\n",
        "                    metrics_keys = self.MK\n",
        "                D = self.D_graph\n",
        "                M_actual = len(metrics_keys)\n",
        "\n",
        "                if not hasattr(self, 'capped_node_metrics'):\n",
        "                    raise ValueError(\"No node metrics available. Run run_outer() first.\")\n",
        "\n",
        "                weighted_fmt = np.zeros((D, D, M_actual))\n",
        "                for i in range(D):\n",
        "                    for j in range(D):\n",
        "                        for k, key in enumerate(metrics_keys):\n",
        "                            val = self.capped_node_metrics[j][key]\n",
        "                            if i != j:\n",
        "                                val *= np.clip(abs(self.chosen_Gmat[i, j]), 0, 1)\n",
        "                            weighted_fmt[i, j, k] = val\n",
        "\n",
        "                for i in range(D):\n",
        "                    for k in range(M_actual):\n",
        "                        if not METRIC_TARGET[i][k]:\n",
        "                            weighted_fmt[i, :, k] = 0.0\n",
        "\n",
        "                mean_vals = weighted_fmt.mean(axis=1)\n",
        "\n",
        "                fig, ax = plt.subplots(figsize=(1.2 * M_actual + 4, 0.35 * D + 4))\n",
        "                im = ax.imshow(mean_vals, cmap='viridis', aspect='auto')\n",
        "\n",
        "                vmin, vmax = mean_vals.min(), mean_vals.max()\n",
        "                for i in range(D):\n",
        "                    for k in range(M_actual):\n",
        "                        val = mean_vals[i, k]\n",
        "                        color = 'white' if val < (vmin + 0.5 * (vmax - vmin)) else 'black'\n",
        "                        ax.text(k, i, f\"{val:.2f}\", ha='center', va='center', color=color, fontsize=8)\n",
        "\n",
        "                ax.set_xticks(range(M_actual))\n",
        "                ax.set_xticklabels(metrics_keys[:M_actual], rotation=45, ha='right')\n",
        "                ax.set_yticks(range(D))\n",
        "                ax.set_yticklabels([f\"Node {i}\" for i in range(D)])\n",
        "                ax.set_title(\"Weighted FMT Metrics (Actual Run Output)\")\n",
        "                fig.colorbar(im, ax=ax, label='Weighted Metric Value')\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "            def collect_fmt_datapoints(self):\n",
        "                self.fmt_datapoints = {k: [] for k in self.MK}\n",
        "                for node_idx in range(self.D_graph):\n",
        "                    if len(self.PLMS[node_idx]) == 0:\n",
        "                        continue\n",
        "                    for weighted_input_val, metric_vals in self.PLMS[node_idx]:\n",
        "                        for m, key in enumerate(self.MK):\n",
        "                            if m < len(metric_vals):\n",
        "                                self.fmt_datapoints[key].append((weighted_input_val, metric_vals[m]))\n",
        "\n",
        "            def collect_metric_traces_per_node(self):\n",
        "                self.metric_traces_per_node = [{} for _ in range(self.D_graph)]\n",
        "                for node_idx in range(self.D_graph):\n",
        "                    self.metric_traces_per_node[node_idx] = {k: [] for k in self.MK}\n",
        "                    if len(self.PLMS[node_idx]) == 0:\n",
        "                        continue\n",
        "                    for weighted_input_val, metric_vals in self.PLMS[node_idx]:\n",
        "                        for m, key in enumerate(self.MK):\n",
        "                            if m < len(metric_vals):\n",
        "                                self.metric_traces_per_node[node_idx][key].append((weighted_input_val, metric_vals[m]))\n",
        "\n",
        "            def plot_fmt_per_datapoint(self, top_k=21, span=0.3, grid_size=100):\n",
        "                if not hasattr(self, 'fmt_datapoints'):\n",
        "                    self.collect_fmt_datapoints()\n",
        "\n",
        "                for key, formula in zip(self.MK, METRIC_FORMULAS):\n",
        "                    if key not in self.fmt_datapoints or len(self.fmt_datapoints[key]) == 0:\n",
        "                        continue\n",
        "\n",
        "                    data = np.array(self.fmt_datapoints[key])\n",
        "                    x_data, y_data = data[:, 0], data[:, 1]\n",
        "\n",
        "                    x_curve = np.linspace(x_data.min() - span, x_data.max() + span, grid_size)\n",
        "                    y_curve = np.array([formula(x) for x in x_curve])\n",
        "\n",
        "                    plt.figure(figsize=(6, 4))\n",
        "                    plt.scatter(x_data, y_data, alpha=0.6, label=\"FMT datapoints\")\n",
        "                    plt.plot(x_curve, y_curve, 'r', lw=2, label=\"Metric equation\")\n",
        "                    plt.xlabel(\"Weighted Input\")\n",
        "                    plt.ylabel(f\"{key} (FMT)\")\n",
        "                    plt.title(f\"FMT per datapoint - Metric: {key}\")\n",
        "                    plt.legend()\n",
        "                    plt.grid(alpha=0.3)\n",
        "                    plt.tight_layout()\n",
        "                    plt.show()\n",
        "\n",
        "            def plot_metric_equations_per_node(self, grid_size=100, span=0.3):\n",
        "                if not hasattr(self, 'metric_traces_per_node'):\n",
        "                    self.collect_metric_traces_per_node()\n",
        "\n",
        "                for node_idx in range(self.D_graph):\n",
        "                    node_traces = self.metric_traces_per_node[node_idx]\n",
        "                    if all(len(v) == 0 for v in node_traces.values()):\n",
        "                        continue\n",
        "\n",
        "                    plt.figure(figsize=(6, 4))\n",
        "                    for key, formula in zip(self.MK, METRIC_FORMULAS):\n",
        "                        if key not in node_traces or len(node_traces[key]) == 0:\n",
        "                            continue\n",
        "                        data = np.array(node_traces[key])\n",
        "                        x_data, y_data = data[:, 0], data[:, 1]\n",
        "\n",
        "                        x_curve = np.linspace(x_data.min() - span, x_data.max() + span, grid_size)\n",
        "                        y_curve = np.array([formula(x) for x in x_curve])\n",
        "\n",
        "                        plt.scatter(x_data, y_data, alpha=0.6, label=f\"{key} datapoints\")\n",
        "                        plt.plot(x_curve, y_curve, 'r', lw=2, label=f\"{key} equation\")\n",
        "\n",
        "                    plt.xlabel(\"Weighted Input\")\n",
        "                    plt.ylabel(\"Metric Value\")\n",
        "                    plt.title(f\"Node {node_idx} - Metric Equations\")\n",
        "                    plt.legend()\n",
        "                    plt.grid(alpha=0.3)\n",
        "                    plt.tight_layout()\n",
        "                    plt.show()\n",
        "\n",
        "\n",
        "        # =============================================================================\n",
        "        # MAIN EXECUTION BLOCK\n",
        "        # =============================================================================\n",
        "\n",
        "        if __name__ == \"__main__\":\n",
        "            # Ensure necessary globals exist before running; otherwise this block is illustrative\n",
        "            try:\n",
        "                optimizer = Fuzzy_Hierarchical_Multiplex(\n",
        "                    candidate_dims, D_graph,\n",
        "                    synthetic_targets,\n",
        "                    gamma_interlayer=0,\n",
        "                    causal_flag=False\n",
        "                )\n",
        "\n",
        "                # Run Optimization\n",
        "                metrics_list = optimizer.run()\n",
        "\n",
        "                # Visualizations\n",
        "            # optimizer.plot_pointwise_minmax_elite()\n",
        "                #optimizer.plot_nested_activations()\n",
        "\n",
        "                # Compute FMT with elite bounds\n",
        "                #fmt_elite_bounds = optimizer.compute_fmt_with_elite_bounds(top_k=top_k + 10)\n",
        "\n",
        "                # Plot as heatmaps\n",
        "                #optimizer.plot_fmt_with_run_metrics()\n",
        "\n",
        "                # Compute fuzzy multiplex tensor\n",
        "                #fmt_tensor = optimizer.compute_fuzzy_metric_tensor(normalize=False)\n",
        "                #optimizer.plot_fuzzy_metric_tensor_heatmaps(fmt_tensor)\n",
        "\n",
        "                # Plot Contributions & Graph\n",
        "                #optimizer.plot_node_score_contribution()\n",
        "                optimizer.plot_outer_fuzzy_graph()\n",
        "\n",
        "                # Interactions\n",
        "            # tensor = optimizer.print_interactions()\n",
        "                #print(\"Tensor shape:\", tensor.shape, '\\n', tensor)\n",
        "\n",
        "                # Datapoints & Equations\n",
        "                #optimizer.collect_fmt_datapoints()\n",
        "                #optimizer.plot_fmt_per_datapoint()\n",
        "                #optimizer.collect_metric_traces_per_node()\n",
        "            # optimizer.plot_metric_equations_per_node()\n",
        "\n",
        "                # DSM Tracking Demo\n",
        "                dsm_tracker = DSM_Tracker(optimizer)\n",
        "\n",
        "                # Run extra DSM update\n",
        "                optimizer.run_outer()\n",
        "                dsm_tracker.update_dsms()\n",
        "\n",
        "                # Retrieve matrices\n",
        "                primary, residual = dsm_tracker.get_matrices()\n",
        "                #print(\"Primary DSM:\\n\", primary)\n",
        "                #rint(\"Residual DSM:\\n\", residual)\n",
        "\n",
        "            except NameError as e:\n",
        "                print(f\"Error: Missing external dependency definition. \\n{e}\")\n",
        "                print(\"Please ensure D_graph, DATA_MATRIX, METRIC_KEYS, etc. are defined.\")\n",
        "\n",
        "       # primary, residual = dsm_tracker.get_matrices()\n",
        "\n",
        "        import networkx as nx\n",
        "        import numpy as np\n",
        "\n",
        "        import networkx as nx\n",
        "        import numpy as np\n",
        "\n",
        "        class TopologicalEvaluator:\n",
        "            def __init__(self, dsm, node_metrics, node_types=None):\n",
        "                \"\"\"\n",
        "                :param dsm: The adjacency matrix (np.array)\n",
        "                :param node_metrics: List of dicts containing 'cost', 'quality', etc.\n",
        "                :param node_types: List of 'A' or 'B' types (inferred if None)\n",
        "                \"\"\"\n",
        "                self.dsm = np.array(dsm)\n",
        "                self.metrics = node_metrics\n",
        "                self.node_types = node_types if node_types else self._infer_types()\n",
        "                self.G = self._build_graph()\n",
        "\n",
        "            def _infer_types(self):\n",
        "                return [\"A\" if m.get('quality', 0) >= m.get('cost', 0) else \"B\" for m in self.metrics]\n",
        "\n",
        "            def _build_graph(self):\n",
        "                G = nx.DiGraph()\n",
        "                for i in range(len(self.dsm)):\n",
        "                    for j in range(len(self.dsm)):\n",
        "                        if self.dsm[i, j] > 0:\n",
        "                            G.add_edge(i, j, weight=self.dsm[i, j])\n",
        "                return G\n",
        "\n",
        "            def calculate_system_pressure(self):\n",
        "                \"\"\"Calculates total pressure based on P = F/A logic across the topology.\"\"\"\n",
        "                total_p = 0\n",
        "                edge_data = []\n",
        "\n",
        "                for u, v in self.G.edges():\n",
        "                    # Force (Potential delta)\n",
        "                    v_u = np.array(list(self.metrics[u].values()))\n",
        "                    v_v = np.array(list(self.metrics[v].values()))\n",
        "                    force = np.linalg.norm(np.maximum(v_v - v_u, 0)) + 1e-9\n",
        "\n",
        "                    # Area (Friction of destination)\n",
        "                    area = np.mean(v_v) + 0.1\n",
        "\n",
        "                    # Pressure\n",
        "                    p = self.dsm[u, v] * (force / area)\n",
        "                    total_p += p\n",
        "                    edge_data.append({'link': (u, v), 'pressure': p})\n",
        "\n",
        "                return total_p, edge_data\n",
        "\n",
        "            def evaluate_topology(self):\n",
        "                # 1. Component Analysis\n",
        "                sccs = list(nx.strongly_connected_components(self.G))\n",
        "                cycles = [list(c) for c in sccs if len(c) > 1]\n",
        "\n",
        "                # 2. Hierarchical Depth (Topological Generations)\n",
        "                condensed = nx.condensation(self.G)\n",
        "                layers = list(nx.topological_generations(condensed))\n",
        "\n",
        "                # 3. System Pressure\n",
        "                total_p, edge_pressures = self.calculate_system_pressure()\n",
        "\n",
        "                # 4. Criticality Score\n",
        "                # High score if high depth but low cycle intensity\n",
        "                fitness = (len(layers) * 10) - (len(cycles) * 5) - (total_p * 0.5)\n",
        "\n",
        "                return {\n",
        "                    \"hierarchy_depth\": len(layers),\n",
        "                    \"cycle_count\": len(cycles),\n",
        "                    \"system_pressure\": round(total_p, 4),\n",
        "                    \"fitness_score\": round(fitness, 2),\n",
        "                    \"bottlenecks\": sorted(edge_pressures, key=lambda x: x['pressure'], reverse=True)[:3]\n",
        "                }\n",
        "\n",
        "        import numpy as np\n",
        "\n",
        "        # 1. GENERATE ACTUAL STATE DATA\n",
        "        # We must turn the generator functions into a snapshot of data\n",
        "        generators = GENERATOR_MAP\n",
        "        current_state = {node: gen() for node, gen in generators.items()}\n",
        "\n",
        "        # 2. TRANSFORM STATE INTO EVALUATOR-FRIENDLY METRICS\n",
        "        # The evaluator looks for 'quality' and 'cost'. We map your architectural KPIs:\n",
        "        # Quality = performance-based metrics (safety, aesthetics, completeness)\n",
        "        # Cost = resource-based metrics (budget, tonnage, violations)\n",
        "        evaluator_metrics = []\n",
        "        METRIC_MAP = metric_feature_map\n",
        "        node_list = list(METRIC_MAP.keys())\n",
        "\n",
        "        for node in node_list:\n",
        "            node_data = current_state.get(node, {})\n",
        "            feat_key = METRIC_MAP[node][0]\n",
        "            category = METRIC_MAP[node][1]\n",
        "\n",
        "            val = node_data.get(feat_key, 0.5)\n",
        "\n",
        "            # Logic: If it's a 'Prevention' or 'Alpha' category, it's Quality.\n",
        "            # If it's 'Efficiency' or 'Total Cost', it relates to Cost.\n",
        "            if category in [\"Regulatory_Alpha\", \"Clinical_Safety\", \"Loss_Ratio_Prevention\"]:\n",
        "                evaluator_metrics.append({\"quality\": val, \"cost\": 0.1}) # Low baseline cost\n",
        "            else:\n",
        "                evaluator_metrics.append({\"quality\": 0.5, \"cost\": val / 1000.0}) # Scale budget to 0-1 range\n",
        "\n",
        "        # 3. RUN EVALUATOR\n",
        "        # Ensure 'primary' (your DSM) is indexed in the same order as node_list\n",
        "        evaluator = TopologicalEvaluator(primary, evaluator_metrics)\n",
        "        report = evaluator.evaluate_topology()\n",
        "\n",
        "\n",
        "        # 4. OUTPUT RESULTS\n",
        "                # Unify all analysis and print logic into a single command\n",
        "        import numpy as np\n",
        "\n",
        "        def run_unified_audit(primary_matrix, target_dsm, evaluator_metrics, node_list):\n",
        "            \"\"\"\n",
        "            Unified Topological Audit comparing the 'Primary' system against the 'Target DSM'.\n",
        "            \"\"\"\n",
        "            # 1. Setup Evaluators for both matrices\n",
        "            # We are comparing Primary (The baseline/current state) vs DSM (The target/proposed state)\n",
        "            eval_primary = TopologicalEvaluator(primary_matrix, evaluator_metrics)\n",
        "            eval_target = TopologicalEvaluator(target_dsm, evaluator_metrics)\n",
        "\n",
        "            # 2. Process Reports\n",
        "            res_p = eval_primary.evaluate_topology()\n",
        "            res_t = eval_target.evaluate_topology()\n",
        "\n",
        "            # 3. UNIFIED OUTPUT TABLE\n",
        "            print(f\"\\n{' TOPOLOGICAL SYSTEM AUDIT: PRIMARY VS. TARGET DSM ':=^75}\")\n",
        "            print(f\"{'KPI Indicator':<32} | {'PRIMARY':<15} | {'TARGET DSM':<15} | {'DELTA':<10}\")\n",
        "            print(\"-\" * 75)\n",
        "\n",
        "            metrics_map = [\n",
        "                (\"Fitness Score (0-100)\", res_p['fitness_score'], res_t['fitness_score']),\n",
        "                (\"System Pressure (Pa)\", res_p['system_pressure'], res_t['system_pressure']),\n",
        "                (\"Sequential Hierarchy Depth\", res_p['hierarchy_depth'], res_t['hierarchy_depth']),\n",
        "                (\"Coupled Feedback Cycles\", res_p['cycle_count'], res_t['cycle_count'])\n",
        "            ]\n",
        "\n",
        "            for label, p_val, t_val in metrics_map:\n",
        "                print(f\"{label:<32} | {p_val:<15.2f} | {t_val:<15.2f} | {t_val - p_val:+.2f}\")\n",
        "\n",
        "            print(\"=\" * 75)\n",
        "\n",
        "            # 4. PRIMARY BOTTLENECK\n",
        "            if res_p['bottlenecks']:\n",
        "                p_top = res_p['bottlenecks'][0]\n",
        "                p_src, p_dst = p_top['link']\n",
        "                print(f\"PRIMARY BOTTLENECK: {node_list[p_src]} → {node_list[p_dst]}\")\n",
        "                print(f\"Friction Magnitude: {p_top['pressure']:.4f}\")\n",
        "\n",
        "            print(\"-\" * 75)\n",
        "\n",
        "            # 5. TARGET DSM BOTTLENECK\n",
        "            if res_t['bottlenecks']:\n",
        "                t_top = res_t['bottlenecks'][0]\n",
        "                t_src, t_dst = t_top['link']\n",
        "                print(f\"TARGET DSM BOTTLENECK: {node_list[t_src]} → {node_list[t_dst]}\")\n",
        "                print(f\"Friction Magnitude: {t_top['pressure']:.4f}\")\n",
        "\n",
        "        # --- EXECUTION ---\n",
        "        # This compares the Primary matrix against the target DSM directly.\n",
        "        run_unified_audit(primary, DSM, evaluator_metrics, node_list)"
      ],
      "metadata": {
        "id": "uvgYhXoaVrxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DATA <--> METRICS <--> TP/DSM <--> GENERIC_SIMULATOR"
      ],
      "metadata": {
        "id": "9j9b5wHYe8I9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u_yrXF9qZW2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pIMnjU_Bc2XG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}