{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data generation for service evaluation"
      ],
      "metadata": {
        "id": "YvBjVLjnNI0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "np.random.seed()\n",
        "N = 1000   # number of synthetic ship logs\n",
        "\n",
        "# ============================\n",
        "# Domain 1: Route & Navigation\n",
        "# ============================\n",
        "route_nav = pd.DataFrame({\n",
        "    'distance_nm': np.random.uniform(50, 5000, N),         # nautical miles\n",
        "    'planned_speed_kn': np.random.uniform(10, 25, N),      # knots\n",
        "    'actual_speed_kn': np.random.uniform(8, 26, N),\n",
        "    'eta_hours': np.random.uniform(5, 500, N),\n",
        "    'route_risk_score': np.random.uniform(0, 1, N),        # piracy/weather risk\n",
        "    'fuel_capacity_tons': np.random.uniform(50, 300, N),\n",
        "    'fuel_used_tons': np.random.uniform(20, 290, N),\n",
        "})\n",
        "\n",
        "route_nav['speed_variance'] = (\n",
        "    route_nav['planned_speed_kn'] - route_nav['actual_speed_kn']\n",
        ")\n",
        "\n",
        "# ============================\n",
        "# Domain 2: Weather Conditions\n",
        "# ============================\n",
        "weather = pd.DataFrame({\n",
        "    'wave_height_m': np.random.uniform(0, 12, N),\n",
        "    'wind_speed_kn': np.random.uniform(0, 60, N),\n",
        "    'wind_direction_deg': np.random.uniform(0, 360, N),\n",
        "    'visibility_km': np.random.uniform(1, 20, N),\n",
        "    'storm_probability': np.random.uniform(0, 1, N),\n",
        "    'precipitation_mm': np.random.uniform(0, 100, N),\n",
        "})\n",
        "\n",
        "# ============================\n",
        "# Domain 3: Vessel Load & Capacity\n",
        "# ============================\n",
        "vessel_load = pd.DataFrame({\n",
        "    'max_capacity_tons': np.random.uniform(1000, 30000, N),\n",
        "    'current_load_tons': np.random.uniform(200, 29000, N),\n",
        "    'num_containers': np.random.randint(50, 2000, N),\n",
        "    'reefer_containers': np.random.randint(0, 300, N),\n",
        "    'bulk_cargo_tons': np.random.uniform(0, 5000, N),\n",
        "    'ballast_water_tons': np.random.uniform(0, 10000, N),\n",
        "})\n",
        "\n",
        "vessel_load['load_ratio'] = (\n",
        "    vessel_load['current_load_tons'] / vessel_load['max_capacity_tons']\n",
        ")\n",
        "\n",
        "# ============================\n",
        "# Domain 4: Cargo Storage & Loading\n",
        "# ============================\n",
        "cargo_types = [\n",
        "    \"containers\", \"bulk\", \"liquid\", \"hazardous\", \"reefer\",\n",
        "    \"vehicles\", \"general_goods\"\n",
        "]\n",
        "\n",
        "storage_loading = pd.DataFrame({\n",
        "    'cargo_type': np.random.choice(cargo_types, N),\n",
        "    'cargo_weight_tons': np.random.uniform(1, 500, N),\n",
        "    'loading_speed_tph': np.random.uniform(20, 200, N),     # tons per hour\n",
        "    'unloading_speed_tph': np.random.uniform(20, 200, N),\n",
        "    'storage_temp_req_C': np.random.uniform(-20, 30, N),    # some cargo needs cooling\n",
        "    'containerized': np.random.choice([0, 1], N),\n",
        "})\n",
        "\n",
        "# Convert cargo_type categorical → numeric index\n",
        "storage_loading['cargo_type_id'] = pd.factorize(storage_loading['cargo_type'])[0]\n",
        "\n",
        "# ============================\n",
        "# Domain 5: Temporal & Port Activity\n",
        "# ============================\n",
        "temporal_port = pd.DataFrame({\n",
        "    'arrival_hour': np.random.randint(0, 24, N),\n",
        "    'arrival_day': np.random.randint(0, 7, N),\n",
        "    'port_congestion_level': np.random.uniform(0, 1, N),\n",
        "    'docking_delay_hours': np.random.uniform(0, 48, N),\n",
        "    'tugboat_availability': np.random.choice([0, 1], N),\n",
        "})\n",
        "\n",
        "# ============================\n",
        "# Combine & Normalize\n",
        "# ============================\n",
        "datasets = [\n",
        "    route_nav,\n",
        "    weather,\n",
        "    vessel_load,\n",
        "    storage_loading.drop(columns=['cargo_type']),  # use numeric features only\n",
        "    temporal_port\n",
        "]\n",
        "\n",
        "DATA_MATRIX = np.hstack([df.values for df in datasets])\n",
        "DATA_MATRIX = (DATA_MATRIX - DATA_MATRIX.min(axis=0)) / (np.ptp(DATA_MATRIX, axis=0) + 1e-8)\n",
        "\n",
        "# ============================\n",
        "# Multi-node target generator\n",
        "# ============================\n",
        "# ============================\n",
        "# Graph-based Multi-node Targets\n",
        "# ============================\n",
        "\n",
        "D_graph = 3\n",
        "candidate_dims = [7, 6,6]# 6, 7, 6]  # updated target dimensions for 5 nodes\n",
        "\n",
        "def generate_targets_per_node(DATA_MATRIX, candidate_dims, D_graph):\n",
        "    targets = []\n",
        "    for node_idx in range(D_graph):\n",
        "        row = DATA_MATRIX[node_idx % len(DATA_MATRIX)]\n",
        "        dim = candidate_dims[node_idx]\n",
        "        if len(row) >= dim:\n",
        "            sampled = row[:dim]\n",
        "        else:\n",
        "            sampled = np.pad(row, (0, dim - len(row)), constant_values=0.5)\n",
        "        targets.append({'target': sampled})\n",
        "    return targets\n",
        "\n",
        "synthetic_targets = generate_targets_per_node(DATA_MATRIX, candidate_dims, D_graph)\n",
        "\n",
        "# Test sizes\n",
        "for i, t in enumerate(synthetic_targets):\n",
        "    print(f\"Node {i} target size: {len(t['target'])}\")\n",
        "candidate_dims = [[7]*10, [6]*10,[6]*10, ]#[7], [6]]  # updated target dimensions for 5 nodes\n"
      ],
      "metadata": {
        "id": "DtNuv6KRNL1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics Evaluator Using Branch and Bound"
      ],
      "metadata": {
        "id": "ZrG7o8dMMupa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiAi9jPvLSnp"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "# ---------------- FEATURE KEYS ----------------\n",
        "FEATURE_KEYS = ['Distance', 'Speed', 'Load', 'Capacity', 'TempRequirement', 'Containerization']\n",
        "\n",
        "FEATURE_TARGET = [[1]*len(FEATURE_KEYS) for _ in range(10)]  # 10 nodes, all features active\n",
        "\n",
        "def default_feature_values():\n",
        "    return {k: 0.5 for k in FEATURE_KEYS}\n",
        "\n",
        "\n",
        "METRIC_KEYS = [\n",
        "    'Reliability',          # Node reliability / uptime\n",
        "    'Efficiency',           # Energy conversion / utilization efficiency\n",
        "    'Cost',                 # Operational cost\n",
        "   # 'Flexibility',          # Ability to handle variable load / demand response\n",
        "   # 'Emissions',            # Environmental impact / CO2 equivalent\n",
        "   # 'RenewableUse',         # Utilization of renewable generation\n",
        "   # 'VoltageStability',     # Node voltage stability\n",
        "   # 'CongestionRisk',       # Risk of grid congestion\n",
        "   # 'BatteryWear',          # Battery degradation proxy\n",
        "   # 'DemandResponseSlack'   # Capacity left for demand response\n",
        "]\n",
        "\n",
        "METRIC_TARGET = [[1,1,1,1,1,],[1,1,1,0,0],[1,1,1,0,0]]#[[1]*len(FEATURE_KEYS) for _ in range(10)]\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "METRIC_FORMULAS = [\n",
        "    lambda x: np.exp(-((x-0.5)**2)/0.05),   # Reliability\n",
        "    #lambda x: np.exp(-((x-0.6)**2)/0.08),   # Efficiency\n",
        "    lambda x: 0.5 * x**2 + 0.1,             # Cost\n",
        "   # lambda x: 1 / (1 + 3*x),                # Flexibility\n",
        "   # lambda x: 0.7 * x**0.9,                 # Emissions\n",
        "  #  lambda x: np.log1p(x),                  # Renewable utilization\n",
        "#]#   lambda x: np.tanh(2*x),                 # Voltage stability\n",
        "  #  lambda x: np.exp(-x),                    # Congestion risk\n",
        "   # lambda x: np.sqrt(x),                    # Battery wear approximation\n",
        "    lambda x: (1 - x)**2                     # Demand response slack\n",
        "]\n",
        "\n",
        "\n",
        "metric_feature_map = {\n",
        "    # Reliability depends on stress vs capability\n",
        "    'Reliability': [\n",
        "        'Load',\n",
        "        'Capacity',\n",
        "        'TempRequirement'\n",
        "    ],\n",
        "\n",
        "    # Efficiency depends on speed under load\n",
        "    'Efficiency': [\n",
        "        'Speed',\n",
        "        'Load',\n",
        "        'Containerization'\n",
        "    ],\n",
        "\n",
        "    # Cost increases with distance, handling, and load\n",
        "    'Cost': [\n",
        "        'Distance',\n",
        "        'Load',\n",
        "        'Containerization'\n",
        "    ]\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class BranchBoundOptimizer:\n",
        "    \"\"\"\n",
        "    Exact, non-recursive Branch & Bound optimizer for 1D continuous problems.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        tol=1e-3,\n",
        "        max_depth=20,\n",
        "        minimize=True,\n",
        "        value_range=(0.0, 5.0)\n",
        "    ):\n",
        "        self.tol = tol\n",
        "        self.max_depth = max_depth\n",
        "        self.minimize = minimize\n",
        "        self.value_range = value_range\n",
        "\n",
        "    def optimize(self, features, y=None, metric_mask=None):\n",
        "        y = np.zeros(3) if y is None else np.array(y[:3])\n",
        "        base = np.mean(list(features.values())) + np.mean(y)\n",
        "\n",
        "        # Initial interval (shifted by base)\n",
        "        a0, b0 = self.value_range\n",
        "        a0 += base\n",
        "        b0 += base\n",
        "\n",
        "        # Work list: (a, b, depth)\n",
        "        work = [(a0, b0, 0)]\n",
        "\n",
        "        best_x = None\n",
        "        best_score = np.inf if self.minimize else -np.inf\n",
        "\n",
        "        def better(s1, s2):\n",
        "            return s1 < s2 if self.minimize else s1 > s2\n",
        "\n",
        "        while work:\n",
        "            a, b, depth = work.pop()\n",
        "\n",
        "            mid = 0.5 * (a + b)\n",
        "\n",
        "            # Evaluate midpoint\n",
        "            mv = [f(mid) if m else 0.0 for f, m in zip(METRIC_FORMULAS, metric_mask)]\n",
        "            score = sum(mv)\n",
        "\n",
        "            if best_x is None or better(score, best_score):\n",
        "                best_x = mid\n",
        "                best_score = score\n",
        "\n",
        "            # Termination condition\n",
        "            if depth >= self.max_depth or (b - a) < self.tol:\n",
        "                continue\n",
        "\n",
        "            # Branch (NO pruning — exact)\n",
        "            work.append((a, mid, depth + 1))\n",
        "            work.append((mid, b, depth + 1))\n",
        "\n",
        "        return best_x\n",
        "\n",
        "\n",
        "class MetricsEvaluator:\n",
        "    \"\"\"Compute node metrics using feature subsets and exact B&B optimizer per metric.\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_matrix,\n",
        "        metric_formulas=METRIC_FORMULAS,\n",
        "        metric_feature_map=metric_feature_map,\n",
        "        feature_keys=FEATURE_KEYS,\n",
        "        feature_target=None,\n",
        "        metric_target=None,\n",
        "        tol=1e-3,\n",
        "        max_depth=20,\n",
        "        minimize=False,\n",
        "        value_range=(0.0, 5.0)\n",
        "    ):\n",
        "        self.data_matrix = data_matrix\n",
        "        self.metric_formulas = metric_formulas\n",
        "        self.metric_feature_map = metric_feature_map\n",
        "        self.feature_keys = feature_keys\n",
        "        self.feature_target = feature_target or [[1]*len(feature_keys) for _ in range(data_matrix.shape[0])]\n",
        "        self.metric_target = metric_target or [[1]*len(metric_formulas) for _ in range(data_matrix.shape[0])]\n",
        "        self.num_nodes = data_matrix.shape[0]\n",
        "\n",
        "        # Create a B&B optimizer instance\n",
        "        self.optimizer = BranchBoundOptimizer(\n",
        "            tol=tol,\n",
        "            max_depth=max_depth,\n",
        "            minimize=minimize,\n",
        "            value_range=value_range\n",
        "        )\n",
        "\n",
        "    def extract_features(self, node_idx):\n",
        "        \"\"\"Return active features for a node as {feature_key: value}.\"\"\"\n",
        "        row = self.data_matrix[node_idx]\n",
        "        mask = self.feature_target[node_idx]\n",
        "        features = {k: v for k, v, m in zip(self.feature_keys, row, mask) if m}\n",
        "        return features\n",
        "\n",
        "    def compute_node_metrics(self, node_idx, y=None):\n",
        "        \"\"\"\n",
        "        Compute metrics for a single node using:\n",
        "        - relevant features from metric_feature_map\n",
        "        - B&B optimizer per metric\n",
        "        \"\"\"\n",
        "        features = self.extract_features(node_idx)\n",
        "        metric_mask = self.metric_target[node_idx]\n",
        "\n",
        "        metric_values = {}\n",
        "\n",
        "        for key, formula, mask in zip(METRIC_KEYS, self.metric_formulas, metric_mask):\n",
        "            if mask:\n",
        "                # Pick only the relevant features for this metric\n",
        "                relevant_features = [features[f] for f in self.metric_feature_map[key] if f in features]\n",
        "                x = np.mean(relevant_features) if relevant_features else 0.0\n",
        "\n",
        "                # Optimize the scalar using B&B\n",
        "                opt_value = self.optimizer.optimize(features={key: x}, y=y, metric_mask=[1])\n",
        "                metric_values[key] = formula(opt_value)\n",
        "            else:\n",
        "                metric_values[key] = 0.0\n",
        "\n",
        "        metric_values['score'] = sum(metric_values.values())\n",
        "        return metric_values\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "METRIC_TARGET"
      ],
      "metadata": {
        "id": "xHWu5PqJZsox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "synthetic_targets"
      ],
      "metadata": {
        "id": "bj-C_AigSMkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fuzzy Hierarchical Multiplex Model"
      ],
      "metadata": {
        "id": "zkzkHh2YNMNy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "h3zVe7LPNPIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "\n",
        "# ---------------- CONFI\n",
        "\n",
        "outer_generations = 1000\n",
        "outer_cost_limit = 10000\n",
        "inner_learning = 0.1\n",
        "gamma_interlayer = 1\n",
        "top_k = 21\n",
        "seed = np.random.seed()\n",
        "\n",
        "new_DATA_MATRIX = np.random.rand(D_graph, DATA_MATRIX.shape[1])\n",
        "\n",
        "\n",
        "\n",
        "class InterLayer:\n",
        "    def __init__(self, D_graph, max_inner_dim, inter_dim=None, edge_threshold=0.02, gamma=1.0, seed=42):\n",
        "        np.random.seed(seed)\n",
        "        self.D_graph = D_graph\n",
        "        self.edge_threshold = edge_threshold\n",
        "        self.gamma = gamma\n",
        "        self.inter_dim = inter_dim[0] if isinstance(inter_dim, list) else (inter_dim if inter_dim is not None else max_inner_dim[0] if isinstance(max_inner_dim, list) else max_inner_dim)\n",
        "        self.max_input = 2 * (max_inner_dim[0] if isinstance(max_inner_dim, list) else max_inner_dim)\n",
        "                    # Initialize weights proportional to synthetic correlation between nodes\n",
        "        self.weights = {}\n",
        "        self.bias = {}\n",
        "        for i in range(D_graph):\n",
        "            for j in range(D_graph):\n",
        "                if i != j:\n",
        "                    # small random + slight bias towards correlation\n",
        "                    w_init = np.random.uniform(-0.1, 0.1, (self.inter_dim, self.max_input))\n",
        "                    self.weights[(i,j)] = w_init\n",
        "                    self.bias[(i,j)] = np.zeros(self.inter_dim)\n",
        "\n",
        "    def compute_edge_activation(self, i, j, nested_reps):\n",
        "        concat = np.concatenate([nested_reps[i], nested_reps[j]])\n",
        "        concat = np.pad(concat, (0, max(0, self.max_input - len(concat))))[:self.max_input]\n",
        "\n",
        "        # Normalize input to improve correlation\n",
        "        concat = (concat - np.mean(concat)) / (np.std(concat) + 1e-12)\n",
        "\n",
        "        # Compute activation\n",
        "        v = self.weights[(i,j)].dot(concat) + self.bias[(i,j)]\n",
        "\n",
        "        # Scale by correlation strength with input signals\n",
        "        input_strength = np.clip(np.mean(np.abs(concat)), 0, 1)\n",
        "        v = v * input_strength\n",
        "\n",
        "        return 1 / (1 + np.exp(-v))\n",
        "\n",
        "    def build_activations(self, Gmat, nested_reps):\n",
        "        acts = {}\n",
        "        for i in range(self.D_graph):\n",
        "            for j in range(self.D_graph):\n",
        "                if i == j:\n",
        "                    continue\n",
        "                if abs(Gmat[i,j]) > self.edge_threshold:\n",
        "                    acts[(i,j)] = self.compute_edge_activation(i, j, nested_reps)\n",
        "        return acts\n",
        "\n",
        "    @staticmethod\n",
        "    def pairwise_squared_corr(acts):\n",
        "        if len(acts) < 2:\n",
        "            return 0.0\n",
        "        A = np.stack(list(acts.values()))\n",
        "        A_centered = A - A.mean(axis=1, keepdims=True)\n",
        "        stds = np.sqrt(np.sum(A_centered**2, axis=1) / (A.shape[1]-1) + 1e-12)\n",
        "        cov = A_centered @ A_centered.T / (A.shape[1]-1)\n",
        "        corr = cov / (np.outer(stds, stds) + 1e-12)\n",
        "        np.fill_diagonal(corr, 0)\n",
        "        return float((corr**2).sum())\n",
        "\n",
        "    def mi_for_graph(self, Gmat, nested_reps):\n",
        "        acts = self.build_activations(Gmat, nested_reps)\n",
        "        if not acts:\n",
        "            return 0.0\n",
        "        return self.gamma * self.pairwise_squared_corr(acts)\n",
        "\n",
        "    def correlate_shrink_interlayer(self, fmt_bounds=None, interaction_tensor=None, metrics_keys=None, verbose=True):\n",
        "        \"\"\"\n",
        "        Compute Pearson correlation per node & metric between:\n",
        "            - shrink factor (adaptive FMT)\n",
        "            - mean outgoing inter-layer activations\n",
        "        Returns: {node_idx: {metric: {'r':..., 'p':...}}}\n",
        "        \"\"\"\n",
        "        from scipy.stats import pearsonr\n",
        "\n",
        "        if metrics_keys is None:\n",
        "            metrics_keys =self.MK\n",
        "\n",
        "        D = self.D_graph\n",
        "\n",
        "        # 1. Compute FMT bounds if not given\n",
        "        if fmt_bounds is None:\n",
        "            fmt_bounds = self.compute_fmt_with_bounds_adaptive(top_k=top_k)\n",
        "\n",
        "        # 2. Get inter-layer activations if not provided\n",
        "        if interaction_tensor is None:\n",
        "            interaction_tensor = self.print_interactions(return_tensor=True, verbose=False)\n",
        "\n",
        "        inter_mean = interaction_tensor.mean(axis=2)  # (D,D)\n",
        "        shrink_factors = self.compute_fmt_shrink_factor(fmt_bounds, metrics_keys)  # (D, num_metrics)\n",
        "\n",
        "        correlations = {}\n",
        "\n",
        "        for i in range(D):\n",
        "            correlations[i] = {}\n",
        "            for k, key in enumerate(metrics_keys):\n",
        "                # FMT shrink for node i (broadcasted across outgoing edges)\n",
        "                shrink_vec = shrink_factors[i, k] * np.ones(D)\n",
        "                # Outgoing inter-layer activations from node i\n",
        "                inter_vec = inter_mean[i, :]\n",
        "                # Remove self-loop\n",
        "                mask = np.arange(D) != i\n",
        "                shrink_vec = shrink_vec[mask]\n",
        "                inter_vec = inter_vec[mask]\n",
        "\n",
        "                # Compute Pearson correlation\n",
        "                if np.std(inter_vec) > 1e-8:  # valid correlation\n",
        "                    r, p = pearsonr(shrink_vec, inter_vec)\n",
        "                else:\n",
        "                    r, p = 0.0, 1.0  # no variability\n",
        "\n",
        "                correlations[i][key] = {'r': r, 'p': p}\n",
        "                if verbose:\n",
        "                    print(f\"Node {i} | {key} shrink vs inter-layer: r={r:.3f}, p={p:.3e}\")\n",
        "\n",
        "        return correlations\n",
        "\n",
        "\n",
        "# ---------------- UNIFIED ACOR MULTIPLEX ----------------\n",
        "class Fuzzy_Hierarchical_Multiplex:\n",
        "    def __init__(self, candidate_dims, D_graph,\n",
        "                 synthetic_targets, inner_learning,\n",
        "                 gamma_interlayer=1.0, causal_flag=True,metrics=METRIC_KEYS,metric_mask=METRIC_TARGET):\n",
        "        self.candidate_dims = candidate_dims\n",
        "        self.D_graph = D_graph\n",
        "        self.synthetic_targets = synthetic_targets\n",
        "        self.inner_learning = inner_learning\n",
        "        self.causal_flag = causal_flag\n",
        "        self.best_dim_per_node = [len(t)-1 for t in synthetic_targets]  # last element as best dim\n",
        "        self.MM = metric_mask\n",
        "        self.MK = metrics\n",
        "        self.MKI = metrics+['score']\n",
        "\n",
        "        self.PLM = [[] for _ in range(self.D_graph)]\n",
        "        self.PLMS = [[] for _ in range(self.D_graph)]\n",
        "        self.nested_reps = [np.zeros(c[0]) for c in candidate_dims]\n",
        "      #  self.best_dim_per_node = [candidate_dims[0] for _ in range(D_graph)]\n",
        "        self.inter_layer = InterLayer(D_graph, max_inner_dim=max(candidate_dims), gamma=gamma_interlayer)\n",
        "        self.chosen_Gmat = np.random.uniform(-0.5,0.5,(D_graph,D_graph))\n",
        "        np.fill_diagonal(self.chosen_Gmat,0)\n",
        "        self.l2_before, self.l2_after = [], []\n",
        "        # ---------------- PER-NODE SIMPLE MLP ----------------\n",
        "        # ---------------- PER-NODE MLP ----------------\n",
        "        self.PLMS = [[] for _ in range(self.D_graph)]\n",
        "\n",
        "        self.mlp_hidden = 16\n",
        "        self.mlp_lr = 0.01\n",
        "# Max target length across all nodes\n",
        "        self.max_target_len = max(len(t['target']) for t in synthetic_targets)\n",
        "\n",
        "        input_dim = len(self.MK) # <-- IMPORTANT (1,3 input)\n",
        "        output_dim = self.max_target_len          # number of metrics\n",
        "\n",
        "        self.node_mlps = []\n",
        "\n",
        "        for node_idx in range(self.D_graph):\n",
        "            input_dim = len(self.MK)  # number of input metrics\n",
        "            output_dim = self.candidate_dims[node_idx][0]  # match node output\n",
        "            mlp = {\n",
        "                \"W1\": np.random.randn(input_dim, self.mlp_hidden) * 0.1,\n",
        "                \"b1\": np.zeros(self.mlp_hidden),\n",
        "                \"W2\": np.random.randn(self.mlp_hidden, output_dim) * 0.1,\n",
        "                \"b2\": np.zeros(output_dim)\n",
        "            }\n",
        "            self.node_mlps.append(mlp)\n",
        "\n",
        "\n",
        "\n",
        "    def mlp_forward(self, mlp, x):\n",
        "        \"\"\"\n",
        "        x: (1,1)\n",
        "        \"\"\"\n",
        "\n",
        "        z1 = x @ mlp[\"W1\"] + mlp[\"b1\"]\n",
        "        h = np.tanh(z1)\n",
        "        y = h @ mlp[\"W2\"] + mlp[\"b2\"]\n",
        "        return y, h\n",
        "\n",
        "    @staticmethod\n",
        "    def mlp_forward(mlp, x):\n",
        "        z1 = x @ mlp[\"W1\"] + mlp[\"b1\"]\n",
        "        h = np.tanh(z1)\n",
        "        y = h @ mlp[\"W2\"] + mlp[\"b2\"]\n",
        "        return y, h\n",
        "\n",
        "\n",
        "\n",
        "    def mlp_predict(mlp, weighted_input):\n",
        "        x = np.array(weighted_input, dtype=float).reshape(1, -1)  # <-- correct\n",
        "        y_pred, _ = Fuzzy_Hierarchical_Multiplex.mlp_forward(mlp, x)\n",
        "        return y_pred.flatten()\n",
        "\n",
        "    def mlp_train_step(self, mlp, x, y_true):\n",
        "        \"\"\"\n",
        "        One SGD step with MSE loss\n",
        "        x: (1,1)\n",
        "        y_true: (1,M)\n",
        "        \"\"\"\n",
        "        # Forward\n",
        "        y_pred, h = self.mlp_forward(mlp, x)\n",
        "\n",
        "        # Loss gradient\n",
        "        dy = y_pred - y_true\n",
        "\n",
        "        # Backprop\n",
        "        dW2 = h.T @ dy\n",
        "        db2 = dy[0]\n",
        "\n",
        "        dh = dy @ mlp[\"W2\"].T\n",
        "        dz1 = dh * (1 - h**2)\n",
        "\n",
        "        dW1 = x.T @ dz1\n",
        "        db1 = dz1[0]\n",
        "\n",
        "        # Update\n",
        "        mlp[\"W2\"] -= self.mlp_lr * dW2\n",
        "        mlp[\"b2\"] -= self.mlp_lr * db2\n",
        "        mlp[\"W1\"] -= self.mlp_lr * dW1\n",
        "        mlp[\"b1\"] -= self.mlp_lr * db1\n",
        "\n",
        "        return float(np.mean((y_pred - y_true) ** 2))\n",
        "\n",
        "\n",
        "    # ---------- INNER LOOP (FCM) ----------\n",
        "    def run_inner(self, node_idx, target, D_fcm,\n",
        "              steps=1000, lr_x=1, lr_y=0.01, lr_W=0.01,\n",
        "              decorrelate_metrics=True):\n",
        "\n",
        "        # --- Initialize activations ---\n",
        "        x =np.random.uniform(-0.6, 0.6, D_fcm)# target.copy()\n",
        "        y = np.random.uniform(-0.6, 0.6, D_fcm)\n",
        "\n",
        "        # Pad target for L2 computation\n",
        "        target_padded = np.pad(target, (0, len(self.nested_reps[node_idx]) - len(target)),\n",
        "                            mode='constant', constant_values=0.5)\n",
        "        self.l2_before.append(np.linalg.norm(self.nested_reps[node_idx] - target_padded))\n",
        "\n",
        "        # --- FCM updates ---\n",
        "        W = np.random.uniform(-0.6, 0.6, (D_fcm, D_fcm))\n",
        "        np.fill_diagonal(W, 0)\n",
        "\n",
        "        for _ in range(steps):\n",
        "            z = y.dot(W) + x\n",
        "            Theta_grad_z = z - target\n",
        "            Theta_grad_x = Theta_grad_z\n",
        "            Theta_grad_y = Theta_grad_z.dot(W.T)\n",
        "            Theta_grad_W = np.outer(y, Theta_grad_z)\n",
        "\n",
        "            x -= lr_x * np.clip(Theta_grad_x, -0.05, 0.05)\n",
        "            y -= lr_y * np.clip(Theta_grad_y, -0.05, 0.05)\n",
        "            W -= lr_W * np.clip(Theta_grad_W, -0.01, 0.01)\n",
        "\n",
        "            x = np.clip(x, 0, 1)\n",
        "            y = np.clip(y, 0, 1)\n",
        "            np.fill_diagonal(W, 0)\n",
        "            W = np.clip(W, -1, 1)\n",
        "\n",
        "        # --- Pad FCM output to nested representation ---\n",
        "        x_padded = np.pad(x, (0, len(self.nested_reps[node_idx]) - len(x)),\n",
        "                        mode='constant', constant_values=0.5)\n",
        "        self.nested_reps[node_idx] = x_padded\n",
        "        self.l2_after.append(np.linalg.norm(x_padded - target_padded))\n",
        "\n",
        "        # --- Extract node features ---\n",
        "        metrics_evaluator = MetricsEvaluator(data_matrix=DATA_MATRIX)\n",
        "        features = metrics_evaluator.extract_features(node_idx)\n",
        "        feat_vals = np.array(list(features.values()))\n",
        "\n",
        "        # --- Compute metrics scaled by activations + features ---\n",
        "        metric_mask = METRIC_TARGET[node_idx]\n",
        "        metric_values = {}\n",
        "\n",
        "        for key, formula, mask in zip(METRIC_KEYS, METRIC_FORMULAS, metric_mask):\n",
        "            if mask:\n",
        "                # Match activations to features\n",
        "                act_vals = x[:len(feat_vals)]\n",
        "                # --- Inner weighted sum ---\n",
        "                weighted_input = np.mean([1] * feat_vals)\n",
        "\n",
        "                # --- Apply outer scale if available ---\n",
        "                outer_scale = self.best_node_weights[node_idx] if hasattr(self, 'best_node_weights') else 1.0\n",
        "                weighted_input *= outer_scale\n",
        "\n",
        "                # Optional: small node bias for uniqueness\n",
        "               # weighted_input += 0.05 * node_idx\n",
        "\n",
        "                # Compute metric\n",
        "                metric_values[key] = formula(weighted_input)\n",
        "            else:\n",
        "                metric_values[key] = 0.0\n",
        "\n",
        "\n",
        "        # --- Total score ---\n",
        "        metric_values['score'] = sum(metric_values.values())\n",
        "\n",
        "        # --- Store inner activation for outer loop ---\n",
        "      #  metric_values['x'] = x_padded.copy()        # store padded activation\n",
        "       # metric_values['feat_vals'] = feat_vals.copy()  # store features# --- Build MLP training pair ---\n",
        "        metric_output_vals = np.array(\n",
        "            [v for k, v in metric_values.items()\n",
        "            if k not in ['score', 'x', 'feat_vals']]\n",
        "        )\n",
        "        mlp = self.node_mlps[node_idx]\n",
        "\n",
        "                # Input vector padded to MLP input size\n",
        "        x_in_full = np.zeros(len(self.MK))\n",
        "        x_in_full[:len(metric_output_vals)] = metric_output_vals\n",
        "        x_in = x_in_full.reshape(1, -1)\n",
        "\n",
        "        # Target vector padded or truncated to MLP output size\n",
        "        output_dim = self.node_mlps[node_idx][\"b2\"].shape[0]\n",
        "        y_out_full = np.zeros(output_dim)\n",
        "        y_out_full[:min(len(feat_vals), output_dim)] = feat_vals[:output_dim]\n",
        "        y_out = y_out_full.reshape(1, -1)\n",
        "\n",
        "        mlp_loss = self.mlp_train_step(mlp, x_in, y_out)\n",
        "\n",
        "\n",
        "\n",
        "        # --- Store PLMS trace (for diagnostics / replay) ---\n",
        "        self.PLMS[node_idx].append(\n",
        "            (float(weighted_input), metric_output_vals)\n",
        "        )\n",
        "\n",
        "        for i in range(self.D_graph):\n",
        "            print(f\"Node {i}, samples learned:\", len(self.PLMS[i]))\n",
        "\n",
        "        # --- Compute inter-layer MI ---\n",
        "        mi_score = self.inter_layer.mi_for_graph(self.chosen_Gmat, self.nested_reps)\n",
        "\n",
        "        return x, y, W, mi_score, metric_values\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def run_outer(self, outer_cost_limit=1000):\n",
        "        node_metrics_list = self.capped_node_metrics  # Use cached metrics from run_inner\n",
        "        raw_scores = np.array([m['score'] for m in node_metrics_list])\n",
        "\n",
        "        # --- Apply cap if needed ---\n",
        "        total_raw = raw_scores.sum()\n",
        "        if total_raw > outer_cost_limit:\n",
        "            scale_factor = outer_cost_limit / total_raw\n",
        "            for metrics in node_metrics_list:\n",
        "                for key in self.MKI:\n",
        "                    metrics[key] *= scale_factor\n",
        "            raw_scores *= scale_factor\n",
        "\n",
        "        # --- Compute Fuzzy Metric Tensor ---\n",
        "        fuzzy_tensor = self.compute_fuzzy_metric_tensor(normalize=False)\n",
        "                # ✅ Persist weights / tensor so they are not lost\n",
        "        self.weighted_fmt = fuzzy_tensor.copy()\n",
        "\n",
        "        self.saved_nested_reps = [rep.copy() for rep in self.nested_reps]\n",
        "        self.saved_Gmat = self.chosen_Gmat.copy()\n",
        "\n",
        "\n",
        "        D = self.D_graph\n",
        "\n",
        "        off_diag_mask = np.ones((D,D),dtype=bool)\n",
        "        np.fill_diagonal(off_diag_mask,0)\n",
        "        fmt_score_offdiag = fuzzy_tensor[off_diag_mask].sum()\n",
        "\n",
        "        # --- Compute per-node contributions ---\n",
        "        node_contributions = np.zeros(D)\n",
        "        for i in range(D):\n",
        "            own_score = raw_scores[i]\n",
        "            fmt_contrib = fuzzy_tensor[i,:,:].sum() - fuzzy_tensor[i,i,:].sum()\n",
        "            node_contributions[i] = own_score + self.inter_layer.gamma * fmt_contrib\n",
        "\n",
        "        # --- Correlation penalty ---\n",
        "        interaction_tensor = self.print_interactions(return_tensor=True, verbose=False)\n",
        "        fmt_mean = fuzzy_tensor.mean(axis=2)\n",
        "        inter_mean = interaction_tensor.mean(axis=2)\n",
        "        corr_penalty = 0.0\n",
        "        for i in range(D):\n",
        "            fmt_vec = fmt_mean[i,:]\n",
        "            inter_vec = inter_mean[i,:]\n",
        "            if np.std(fmt_vec) > 1e-8 and np.std(inter_vec) > 1e-8:\n",
        "                corr_penalty += abs(np.corrcoef(fmt_vec, inter_vec)[0,1])**2\n",
        "        corr_penalty /= D\n",
        "\n",
        "        combined_score = node_contributions.sum() - corr_penalty\n",
        "\n",
        "        self.node_score_contributions = node_contributions\n",
        "        self.correlation_penalty = corr_penalty\n",
        "\n",
        "        return node_metrics_list, combined_score, node_contributions\n",
        "\n",
        "\n",
        "\n",
        "    def run(self, outer_generations=outer_generations):\n",
        "        best_score = -np.inf\n",
        "\n",
        "        for gen in range(outer_generations):\n",
        "            node_metrics_list = []\n",
        "\n",
        "            for node_idx in range(self.D_graph):\n",
        "                full_target = self.synthetic_targets[node_idx]['target']\n",
        "                D_fcm = self.candidate_dims[node_idx][0]\n",
        "                target = full_target[:D_fcm]\n",
        "\n",
        "                _, _, _, _, metrics = self.run_inner(node_idx, target, D_fcm)\n",
        "                node_metrics_list.append(metrics)\n",
        "\n",
        "            # Outer loop\n",
        "            self.capped_node_metrics = node_metrics_list\n",
        "            _, capped_score, node_contributions = self.run_outer()\n",
        "\n",
        "            if capped_score > best_score:\n",
        "                best_score = capped_score\n",
        "            else:\n",
        "                print(f\"\\n--- Generation {gen} Metrics (NEW BEST) ---\")\n",
        "                for i, m in enumerate(node_metrics_list):\n",
        "                    out_str = []\n",
        "                    for k, v in m.items():\n",
        "                        if np.isscalar(v):\n",
        "                            out_str.append(f\"{k}: {v:.2f}\")\n",
        "                        elif isinstance(v, np.ndarray):\n",
        "                            pass\n",
        "                            out_str.append(f\"{k}: mean {v.mean():.2f}, shape {v.shape}\")\n",
        "                        else:\n",
        "                            out_str.append(f\"{k}: {v}\")\n",
        "                    print(f\"Node {i} | \" + \" | \".join(out_str))\n",
        "\n",
        "                print(f\"\\n--- Generation {gen} Node Contributions (NEW BEST) ---\")\n",
        "                for i, c in enumerate(node_contributions):\n",
        "                    print(f\"Node {i}: Contribution = {c:.4f}\")\n",
        "\n",
        "                print(f\"Outer Score (capped): {capped_score:.3f} <-- NEW BEST\")\n",
        "\n",
        "        return best_score\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # ---------- VISUALIZATIONS ----------\n",
        "    # ---------- VISUALIZATIONS ----------\n",
        "    def plot_pointwise_minmax_elite(self, top_k=21):\n",
        "        plt.figure(figsize=(14,3))\n",
        "        for i in range(self.D_graph):\n",
        "            # Node's actual dimension\n",
        "            dim_i = self.candidate_dims[i][0]  # ✅ integer\n",
        "            base = self.nested_reps[i][:dim_i]  # slice to candidate dim\n",
        "            reps = np.clip(base + np.random.normal(0,0.05,(top_k,len(base))),0,1)\n",
        "            y_min, y_max = reps.min(axis=0), reps.max(axis=0)\n",
        "            y_sel = base\n",
        "\n",
        "            # True target for this node, sliced to candidate dim\n",
        "            y_true = self.synthetic_targets[i]['target'][:len(y_sel)]\n",
        "            if len(y_true) < len(y_sel):\n",
        "                y_true = np.pad(y_true, (0, len(y_sel)-len(y_true)), \"constant\")\n",
        "            else:\n",
        "                y_true = y_true[:len(y_sel)]\n",
        "\n",
        "            plt.subplot(1,self.D_graph,i+1)\n",
        "            plt.fill_between(range(len(y_min)),y_min,y_max,color='skyblue',alpha=0.4,label='Elite Interval')\n",
        "            plt.plot(y_sel,'k-',lw=2,label='Estimated')\n",
        "            plt.plot(y_true,'r--',lw=2,label='True')\n",
        "            plt.ylim(0,1.05)\n",
        "            plt.title(f\"Node {i+1}\")\n",
        "            if i==0: plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def plot_nested_activations(self):\n",
        "        plt.figure(figsize=(12,3))\n",
        "        for i,rep in enumerate(self.nested_reps):\n",
        "            dim_i = self.candidate_dims[i][0]\n",
        "            rep_i = rep[:dim_i]  # slice to candidate dim\n",
        "            plt.subplot(1,self.D_graph,i+1)\n",
        "            plt.bar(range(len(rep_i)), rep_i, color=plt.cm.plasma(rep_i))\n",
        "            plt.ylim(0,1)\n",
        "            plt.title(f\"Node {i+1}\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def plot_outer_fuzzy_graph(self):\n",
        "        G = nx.DiGraph()\n",
        "        for i in range(self.D_graph): G.add_node(i)\n",
        "        for i in range(self.D_graph):\n",
        "            for j in range(self.D_graph):\n",
        "                if i!=j and abs(self.chosen_Gmat[i,j])>0.02:\n",
        "                    G.add_edge(i,j,weight=self.chosen_Gmat[i,j])\n",
        "        node_sizes = [self.best_dim_per_node[i]*200 for i in range(self.D_graph)]\n",
        "        edge_colors = ['green' if d['weight']>0 else 'red' for _,_,d in G.edges(data=True)]\n",
        "        edge_widths = [abs(d['weight'])*3 for _,_,d in G.edges(data=True)]\n",
        "        pos = nx.circular_layout(G)\n",
        "        plt.figure(figsize=(6,6))\n",
        "        nx.draw(G,pos,node_size=node_sizes,node_color='skyblue',\n",
        "                edge_color=edge_colors,width=edge_widths,arrows=True,with_labels=True)\n",
        "        plt.title(\"Outer Fuzzy Multiplex Graph\")\n",
        "        plt.show()\n",
        "# ---------------- INTERACTIONS INSPECTOR ----------------\n",
        "\n",
        "    def print_interactions(self, return_tensor=True, verbose=True):\n",
        "            D_graph = self.D_graph\n",
        "            inter_dim = self.inter_layer.inter_dim\n",
        "            inter_tensor = np.zeros((D_graph, D_graph, inter_dim))\n",
        "\n",
        "            acts = self.inter_layer.build_activations(self.chosen_Gmat, self.nested_reps)\n",
        "            if not acts:\n",
        "                if verbose:\n",
        "                    print(\"No active edges above threshold.\")\n",
        "                return inter_tensor if return_tensor else None\n",
        "\n",
        "            for (i, j), vec in acts.items():\n",
        "                inter_tensor[i, j, :] = vec\n",
        "                if verbose:\n",
        "                    act_str = \", \".join([f\"{v:.3f}\" for v in vec])\n",
        "                    print(f\"Node {i} -> Node {j}: [{act_str}]\")\n",
        "            return inter_tensor if return_tensor else None\n",
        "\n",
        "        # Move these outside of print_interactions (class-level)\n",
        "    def print_l2_summary(self):\n",
        "            print(\"\\nL2 Distances to Target per Node:\")\n",
        "            for idx, (before, after) in enumerate(zip(self.l2_before, self.l2_after)):\n",
        "                print(f\"Node {idx}: Before={before:.4f}, After={after:.4f}\")\n",
        "\n",
        "    def compute_fuzzy_metric_tensor(self, normalize=True, verbose=False):\n",
        "            \"\"\"\n",
        "            Computes a Fuzzy Metric Tensor (D_graph x D_graph x num_metrics)\n",
        "            using current nested reps and node metrics.\n",
        "            Each slice [i,j,:] represents metrics of node j (optionally weighted by Gmat[i,j])\n",
        "            \"\"\"\n",
        "            metrics_keys =self.MK\n",
        "            D = self.D_graph\n",
        "            num_metrics = len(metrics_keys)\n",
        "            tensor = np.zeros((D, D, num_metrics))\n",
        "\n",
        "            metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "\n",
        "            node_metrics = []\n",
        "            for i, rep in enumerate(self.nested_reps):\n",
        "                metrics = metrics_evaluator.compute_node_metrics(i, y=rep)\n",
        "                node_metrics.append(np.array([metrics[k] for k in metrics_keys]))\n",
        "            node_metrics = np.array(node_metrics)  # (D, num_metrics)\n",
        "\n",
        "            for i in range(D):\n",
        "                for j in range(D):\n",
        "                    if i==j:\n",
        "                        tensor[i,j,:] = node_metrics[j]\n",
        "                    else:\n",
        "                        weight = np.clip(abs(self.chosen_Gmat[i,j]), 0, 1)\n",
        "                        tensor[i,j,:] = weight * node_metrics[j]\n",
        "\n",
        "            if normalize:\n",
        "                tensor = (tensor - tensor.min()) / (tensor.max() - tensor.min() + 1e-12)\n",
        "\n",
        "            if verbose:\n",
        "                print(\"Fuzzy Metric Tensor shape:\", tensor.shape)\n",
        "\n",
        "            return tensor\n",
        "\n",
        "\n",
        "\n",
        "    def compute_fmt_shrink_factor(self, fmt_bounds, metrics_keys=None):\n",
        "        \"\"\"\n",
        "        Returns shrink factor per node and metric.\n",
        "        shrink_factor = 1 - (current_interval / original_interval)\n",
        "        \"\"\"\n",
        "        if metrics_keys is None:\n",
        "            metrics_keys = self.MK\n",
        "\n",
        "        D = self.D_graph\n",
        "        num_metrics = len(metrics_keys)\n",
        "        shrink_factors = np.zeros((D, num_metrics))\n",
        "\n",
        "        for i in range(D):\n",
        "            for k in range(num_metrics):\n",
        "                lower, upper = fmt_bounds[i, i, k, 0], fmt_bounds[i, i, k, 1]  # self-node interval\n",
        "                interval_width = upper - lower + 1e-12  # normalized [0,1]\n",
        "                shrink_factors[i, k] = 1 - interval_width  # more shrink = higher value\n",
        "\n",
        "        return shrink_factors\n",
        "\n",
        "    def compute_fmt_with_bounds_adaptive(self, top_k=21, max_shrink=0.5, metrics_keys=None):\n",
        "        \"\"\"\n",
        "        Computes FMT bounds using pointwise min/max across elite solutions,\n",
        "        and applies dynamic adaptive shrinking where variability is low.\n",
        "        Returns tensor shape (D,D,num_metrics,2) [lower, upper].\n",
        "        \"\"\"\n",
        "        if metrics_keys is None:\n",
        "            metrics_keys =self.MK\n",
        "\n",
        "        D = self.D_graph\n",
        "        num_metrics = len(metrics_keys)\n",
        "        tensor_bounds = np.zeros((D, D, num_metrics, 2))\n",
        "        metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "\n",
        "        variability = np.zeros((D, num_metrics))\n",
        "\n",
        "        # Step 1: compute bounds from perturbations\n",
        "        for i in range(D):\n",
        "            base = self.nested_reps[i]\n",
        "            reps = np.clip(base + np.random.normal(0, 0.05, (top_k, len(base))), 0, 1)\n",
        "            metrics_matrix = np.zeros((top_k, num_metrics))\n",
        "            for idx, rep in enumerate(reps):\n",
        "                m = metrics_evaluator.compute_node_metrics(i, y=rep)\n",
        "                metrics_matrix[idx, :] = [m[k] for k in metrics_keys]\n",
        "\n",
        "            lower_i = metrics_matrix.min(axis=0)\n",
        "            upper_i = metrics_matrix.max(axis=0)\n",
        "            tensor_bounds[i, :, :, 0] = lower_i[np.newaxis, :]  # broadcast to all j\n",
        "            tensor_bounds[i, :, :, 1] = upper_i[np.newaxis, :]\n",
        "            variability[i, :] = metrics_matrix.std(axis=0)\n",
        "\n",
        "        # Step 2: adaptive shrinking\n",
        "        for i in range(D):\n",
        "            for j in range(D):\n",
        "                for k in range(num_metrics):\n",
        "                    lower, upper = tensor_bounds[i,j,k,0], tensor_bounds[i,j,k,1]\n",
        "                    mean = (lower + upper)/2\n",
        "                    var_norm = min(1.0, variability[i,k]/(upper-lower + 1e-12))\n",
        "                    shrink_factor = max_shrink * (1 - var_norm)\n",
        "                    tensor_bounds[i,j,k,0] = mean - shrink_factor*(mean - lower)\n",
        "                    tensor_bounds[i,j,k,1] = mean + shrink_factor*(upper - mean)\n",
        "\n",
        "        return tensor_bounds\n",
        "\n",
        "\n",
        "\n",
        "    def plot_fuzzy_metric_tensor_heatmaps(self, fuzzy_tensor=None, metrics_keys=METRIC_KEYS):\n",
        "        \"\"\"\n",
        "        Plot a heatmap panel for each metric in the FMT.\n",
        "        Rows: source node i\n",
        "        Columns: target node j\n",
        "        \"\"\"\n",
        "        if fuzzy_tensor is None:\n",
        "            fuzzy_tensor = self.compute_fuzzy_metric_tensor(normalize=True)\n",
        "\n",
        "        D = self.D_graph\n",
        "        num_metrics = len(metrics_keys)\n",
        "\n",
        "        fig, axes = plt.subplots(1, num_metrics, figsize=(4*num_metrics,4))\n",
        "        if num_metrics == 1: axes = [axes]\n",
        "\n",
        "        for k, key in enumerate(metrics_keys):\n",
        "            data = fuzzy_tensor[:,:,k]\n",
        "            im = axes[k].imshow(data, cmap='viridis', vmin=0, vmax=1)\n",
        "            for i in range(D):\n",
        "                for j in range(D):\n",
        "                    axes[k].text(j,i,f\"{data[i,j]:.2f}\",ha='center',va='center',color='white',fontsize=9)\n",
        "            axes[k].set_xticks(range(D))\n",
        "            axes[k].set_yticks(range(D))\n",
        "            axes[k].set_xticklabels([f'Node {j}' for j in range(D)])\n",
        "            axes[k].set_yticklabels([f'Node {i}' for i in range(D)])\n",
        "            axes[k].set_title(f'FMT - {key}')\n",
        "\n",
        "        fig.colorbar(im, ax=axes, orientation='vertical', fraction=0.025, pad=0.04, label='Normalized Metric Value')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def compute_fmt_with_elite_bounds(self, top_k=21):\n",
        "        \"\"\"\n",
        "        Computes FMT bounds using pointwise min/max across elite solutions.\n",
        "        Returns tensor shape (D,D,num_metrics,2) [lower, upper].\n",
        "        \"\"\"\n",
        "        metrics_keys = self.MK\n",
        "        D = self.D_graph\n",
        "        num_metrics = len(metrics_keys)\n",
        "        tensor_bounds = np.zeros((D,D,num_metrics,2))\n",
        "\n",
        "        metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "\n",
        "        for i in range(D):\n",
        "            # Generate top_k perturbations around current nested_rep (like in plot_pointwise_minmax_elite)\n",
        "            base = self.nested_reps[i]\n",
        "            reps = np.clip(base + np.random.normal(0,0.05,(top_k,len(base))),0,1)\n",
        "\n",
        "            # Compute node metrics for each perturbed solution\n",
        "            metrics_matrix = np.zeros((top_k, num_metrics))\n",
        "            for idx, rep in enumerate(reps):\n",
        "                m = metrics_evaluator.compute_node_metrics(i, y=rep)\n",
        "                metrics_matrix[idx,:] = [m[k] for k in metrics_keys]\n",
        "\n",
        "            # Compute pointwise min/max across elite solutions\n",
        "            lower_i = metrics_matrix.min(axis=0)\n",
        "            upper_i = metrics_matrix.max(axis=0)\n",
        "\n",
        "            # Fill bounds tensor for all source nodes (i->j)\n",
        "            for j in range(D):\n",
        "                tensor_bounds[i,j,:,0] = lower_i\n",
        "                tensor_bounds[i,j,:,1] = upper_i\n",
        "\n",
        "        return tensor_bounds\n",
        "\n",
        "\n",
        "    def plot_fmt_with_bounds(self, fmt_tensor_bounds):\n",
        "        \"\"\"\n",
        "        Plot the FMT with lower/upper bounds, applying outer-loop weights if available.\n",
        "        Single row: mean across all source nodes\n",
        "        Columns: metrics (mean across target nodes)\n",
        "        Colors reflect actual weighted values (no normalization).\n",
        "        \"\"\"\n",
        "        import matplotlib.pyplot as plt\n",
        "        import numpy as np\n",
        "\n",
        "        D = self.D_graph\n",
        "        metrics_keys = self.MK\n",
        "        M_actual = len(metrics_keys)\n",
        "\n",
        "        # Compute mean value per metric across target nodes\n",
        "        mean_vals = (fmt_tensor_bounds[:, :, :, 0] + fmt_tensor_bounds[:, :, :, 1]) / 2  # shape (D, D, M_actual)\n",
        "        mean_vals = mean_vals.mean(axis=1)  # mean across targets -> (D, M_actual)\n",
        "\n",
        "        # Take mean across nodes to reduce to 1xM\n",
        "        mean_vals = mean_vals.mean(axis=0, keepdims=True)  # shape (1, M_actual)\n",
        "\n",
        "        # Apply outer-loop weights if available\n",
        "        if hasattr(self, 'best_alpha') and hasattr(self, 'best_w_contrib'):\n",
        "            mean_weight = (self.best_alpha * self.best_w_contrib).mean()\n",
        "            mean_vals = mean_vals * mean_weight\n",
        "\n",
        "        # Plot actual weighted FMT values (no per-column normalization)\n",
        "        fig, ax = plt.subplots(figsize=(1.2*M_actual + 4, 2))\n",
        "        im = ax.imshow(mean_vals, cmap='viridis', aspect='auto')\n",
        "\n",
        "        # Annotate cells with dynamic text color\n",
        "        vmin, vmax = mean_vals.min(), mean_vals.max()\n",
        "        for i in range(mean_vals.shape[0]):  # only 1 row\n",
        "            for k in range(M_actual):\n",
        "                val = mean_vals[i, k]\n",
        "                color = 'white' if val < (vmin + 0.5*(vmax - vmin)) else 'black'\n",
        "                ax.text(k, i, f\"{val:.2f}\", ha='center', va='center', color=color, fontsize=8)\n",
        "\n",
        "        ax.set_xticks(range(M_actual))\n",
        "        ax.set_xticklabels(metrics_keys[:M_actual], rotation=45, ha='right')\n",
        "        ax.set_yticks([0])\n",
        "        ax.set_yticklabels(['Mean across nodes'])\n",
        "        ax.set_title(\"Weighted FMT with Bounds (Collapsed to 1 Row)\")\n",
        "        fig.colorbar(im, ax=ax, label='Weighted Mean Metric Value')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def plot_node_score_contribution(self, metrics_keys=METRIC_KEYS):\n",
        "        \"\"\"\n",
        "        Plot per-node total score contribution:\n",
        "            - 3 panels: Raw, FMT (interaction), Total\n",
        "            - Diagonal represents raw contributions\n",
        "            - FMT scaled by best_weights if available\n",
        "            - Annotated cells\n",
        "            - Normalized across metrics for consistent visualization\n",
        "        \"\"\"\n",
        "        D = self.D_graph\n",
        "        node_contributions = np.array(self.node_score_contributions)\n",
        "\n",
        "        # --- FMT contribution ---\n",
        "        if hasattr(self, 'weighted_fmt'):\n",
        "            fuzzy_tensor = np.array(self.weighted_fmt)\n",
        "        else:\n",
        "            fuzzy_tensor = self.compute_fuzzy_metric_tensor(normalize=True)\n",
        "\n",
        "        # Normalize across entire tensor for plotting\n",
        "        fuzzy_tensor_norm = (fuzzy_tensor - fuzzy_tensor.min()) / (fuzzy_tensor.max() - fuzzy_tensor.min() + 1e-12)\n",
        "        fmt_matrix = fuzzy_tensor_norm.sum(axis=2)  # sum over metrics\n",
        "        np.fill_diagonal(fmt_matrix, 0)\n",
        "\n",
        "        # --- Raw matrix on diagonal ---\n",
        "        raw_matrix = np.zeros((D,D))\n",
        "        np.fill_diagonal(raw_matrix, node_contributions)\n",
        "\n",
        "        # --- Total contribution ---\n",
        "        total_matrix = raw_matrix + fmt_matrix\n",
        "\n",
        "        # --- Global min/max for color scale ---\n",
        "        global_min, global_max = total_matrix.min(), total_matrix.max()\n",
        "\n",
        "        # --- Plot ---\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "        matrices = [raw_matrix, fmt_matrix, total_matrix]\n",
        "        titles = [\"Raw Node Contribution\", \"Normalized FMT Contribution\", \"Total Contribution\"]\n",
        "\n",
        "        for ax, mat, title in zip(axes, matrices, titles):\n",
        "            im = ax.imshow(mat, cmap='viridis', vmin=0, vmax=1)  # normalized\n",
        "            for i in range(D):\n",
        "                for j in range(D):\n",
        "                    ax.text(j, i, f\"{mat[i,j]:.2f}\", ha='center', va='center', color='white', fontsize=8)\n",
        "            ax.set_title(title)\n",
        "            ax.set_xticks(range(D))\n",
        "            ax.set_xticklabels([f\"Node {i+1}\" for i in range(D)])\n",
        "            ax.set_yticks(range(D))\n",
        "            ax.set_yticklabels([f\"Node {i+1}\" for i in range(D)])\n",
        "\n",
        "        fig.colorbar(im, ax=axes, orientation='vertical', fraction=0.025, pad=0.04, label='Contribution Value')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def correlate_fmt_interactions_per_node(self, fmt_bounds=None, interaction_tensor=None, verbose=True):\n",
        "        \"\"\"\n",
        "        Correlate the FMT bounds with inter-layer interactions per node and per metric.\n",
        "        Returns a dict of shape: {node_idx: {metric: {'r':..., 'p':...}}}.\n",
        "        \"\"\"\n",
        "        from scipy.stats import pearsonr\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        metrics_keys = self.MK\n",
        "        D = self.D_graph\n",
        "\n",
        "        # Compute tensors if not provided\n",
        "        if fmt_bounds is None:\n",
        "            fmt_bounds = self.compute_fmt_with_elite_bounds(top_k=21)\n",
        "        if interaction_tensor is None:\n",
        "            interaction_tensor = self.print_interactions(return_tensor=True, verbose=False)\n",
        "\n",
        "        # Reduce interaction tensor along inter_dim\n",
        "        inter_mean = interaction_tensor.mean(axis=2)  # (D,D)\n",
        "\n",
        "        node_correlations = {}\n",
        "\n",
        "        for i in range(D):\n",
        "            node_correlations[i] = {}\n",
        "            for k, key in enumerate(metrics_keys):\n",
        "                # FMT bounds for target node j from source i (mean of lower/upper)\n",
        "                fmt_mean = fmt_bounds[i,:,k,:].mean(axis=1)  # shape (D,)\n",
        "                # Interaction tensor for edges from node i to j\n",
        "                inter_vec = inter_mean[i,:]  # shape (D,)\n",
        "                # Pearson correlation\n",
        "                corr, pval = pearsonr(fmt_mean, inter_vec)\n",
        "                node_correlations[i][key] = {'r': corr, 'p': pval}\n",
        "\n",
        "                if verbose:\n",
        "                    print(f\"Node {i} | {key}: r = {corr:.3f}, p = {pval:.3e}\")\n",
        "                    plt.figure(figsize=(4,3))\n",
        "                    plt.scatter(fmt_mean, inter_vec, alpha=0.7, edgecolor='k', color='skyblue')\n",
        "                    plt.xlabel(f\"FMT {key} (Node {i} -> others)\")\n",
        "                    plt.ylabel(f\"Interaction mean (Node {i} -> others)\")\n",
        "                    plt.title(f\"Node {i} | {key} correlation: r={corr:.3f}\")\n",
        "                    plt.grid(True)\n",
        "                    plt.show()\n",
        "\n",
        "        return node_correlations\n",
        "\n",
        "    def plot_fmt_with_run_metrics(self, metrics_keys=None):\n",
        "        \"\"\"\n",
        "        Plot FMT heatmaps using the actual metrics from the last run_outer.\n",
        "        Rows: source nodes\n",
        "        Columns: metrics (mean across target nodes)\n",
        "        \"\"\"\n",
        "        import matplotlib.pyplot as plt\n",
        "        import numpy as np\n",
        "\n",
        "        if metrics_keys is None:\n",
        "            metrics_keys = self.MK\n",
        "\n",
        "        D = self.D_graph\n",
        "        M_actual = len(metrics_keys)\n",
        "\n",
        "        if not hasattr(self, 'capped_node_metrics'):\n",
        "            raise ValueError(\"No node metrics available. Run run_outer() first.\")\n",
        "\n",
        "        # Build FMT from actual node metrics\n",
        "        weighted_fmt = np.zeros((D, D, M_actual))\n",
        "        for i in range(D):\n",
        "            for j in range(D):\n",
        "                for k, key in enumerate(metrics_keys):\n",
        "                    val = self.capped_node_metrics[j][key]  # take actual metric of node j\n",
        "                    # Apply Gmat weight for off-diagonal\n",
        "                    if i != j:\n",
        "                        val *= np.clip(abs(self.chosen_Gmat[i,j]), 0, 1)\n",
        "                    weighted_fmt[i,j,k] = val#*self.D_graph*\n",
        "\n",
        "        # Mask metrics if needed\n",
        "        for i in range(D):\n",
        "            for k in range(M_actual):\n",
        "                if not METRIC_TARGET[i][k]:\n",
        "                    weighted_fmt[i, :, k] = 0.0\n",
        "\n",
        "        # Average across target nodes\n",
        "        mean_vals = weighted_fmt.mean(axis=1)  # (D, M_actual)\n",
        "\n",
        "        # Plot\n",
        "        fig, ax = plt.subplots(figsize=(1.2*M_actual + 4, 0.35*D + 4))\n",
        "        im = ax.imshow(mean_vals, cmap='viridis', aspect='auto')\n",
        "\n",
        "        vmin, vmax = mean_vals.min(), mean_vals.max()\n",
        "        for i in range(D):\n",
        "            for k in range(M_actual):\n",
        "                val = mean_vals[i, k]\n",
        "                color = 'white' if val < (vmin + 0.5*(vmax - vmin)) else 'black'\n",
        "                ax.text(k, i, f\"{val:.2f}\", ha='center', va='center', color=color, fontsize=8)\n",
        "\n",
        "        ax.set_xticks(range(M_actual))\n",
        "        ax.set_xticklabels(metrics_keys[:M_actual], rotation=45, ha='right')\n",
        "        ax.set_yticks(range(D))\n",
        "        ax.set_yticklabels([f\"Node {i}\" for i in range(D)])\n",
        "        ax.set_title(\"Weighted FMT Metrics (Actual Run Output)\")\n",
        "        fig.colorbar(im, ax=ax, label='Weighted Metric Value')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def correlation_penalty(self, fmt_bounds=None, interaction_tensor=None):\n",
        "        \"\"\"\n",
        "        Computes a penalty term that is high if per-node FMT metrics correlate with interactions.\n",
        "        Returns total penalty to subtract from the outer score.\n",
        "        \"\"\"\n",
        "        from scipy.stats import pearsonr\n",
        "\n",
        "        D = self.D_graph\n",
        "        metrics_keys = self.MK\n",
        "        if fmt_bounds is None:\n",
        "            fmt_bounds = self.compute_fmt_with_elite_bounds(top_k=top_k)\n",
        "        if interaction_tensor is None:\n",
        "            interaction_tensor = self.print_interactions(return_tensor=True, verbose=False)\n",
        "\n",
        "        inter_mean = interaction_tensor.mean(axis=2)\n",
        "        total_penalty = 0.0\n",
        "\n",
        "        for i in range(D):\n",
        "            for k in range(len(metrics_keys)):\n",
        "                fmt_mean = fmt_bounds[i,:,k,:].mean(axis=1)\n",
        "                inter_vec = inter_mean[i,:]\n",
        "                if np.std(fmt_mean) > 1e-8 and np.std(inter_vec) > 1e-8:\n",
        "                    corr, _ = pearsonr(fmt_mean, inter_vec)\n",
        "                    total_penalty += abs(corr)  # penalize high correlation\n",
        "\n",
        "        # normalize by number of nodes × metrics\n",
        "        total_penalty /= (D * len(metrics_keys))**2\n",
        "        return total_penalty\n",
        "\n",
        "\n",
        "# ---------------- USAGE ----------------\n",
        "if __name__ == \"__main__\":\n",
        "    optimizer = Fuzzy_Hierarchical_Multiplex(\n",
        "        candidate_dims, D_graph,\n",
        "        synthetic_targets,\n",
        "        inner_learning, gamma_interlayer=0,\n",
        "        causal_flag=False\n",
        "    )\n",
        "    metrics_list = optimizer.run()\n",
        "    optimizer.plot_pointwise_minmax_elite()\n",
        "    optimizer.plot_nested_activations()\n",
        "    # Compute FMT with elite bounds\n",
        "    fmt_elite_bounds = optimizer.compute_fmt_with_elite_bounds(top_k=top_k+10)\n",
        "\n",
        "# Plot as heatmaps\n",
        "    optimizer.plot_fmt_with_run_metrics()\n",
        "\n",
        "    # Compute fuzzy multiplex tensor\n",
        "    fmt_tensor = optimizer.compute_fuzzy_metric_tensor(normalize=False)\n",
        "    optimizer.plot_fuzzy_metric_tensor_heatmaps(fmt_tensor)\n",
        "\n",
        "    # Compute FMT with bounds (minimax elite intervals)\n",
        "    optimizer.plot_node_score_contribution()\n",
        "    optimizer.plot_outer_fuzzy_graph()\n",
        "  #  optimizer.print_interactions()\n",
        "    tensor = optimizer.print_interactions()\n",
        "\n",
        "    print(\"Tensor shape:\", tensor.shape,'\\n',tensor)\n",
        "    # Compute tensors first\n",
        "\n",
        "    #fmt_elite_bounds = optimizer.compute_fmt_with_elite_bounds(top_k=top_k)\n",
        "    #interaction_tensor = optimizer.print_interactions(return_tensor=True, verbose=False)\n",
        "    #optimizer.plot_fmt_with_bounds(fmt_elite_bounds)\n",
        "\n",
        "    #interaction_tensor = optimizer.print_interactions(return_tensor=True, verbose=False)\n",
        "\n",
        "    # Get per-node, per-metric correlations\n",
        "    #node_metric_corrs = optimizer.correlate_fmt_interactions_per_node(\n",
        "     #   fmt_bounds=fmt_elite_bounds,\n",
        "      #  interaction_tensor=interaction_tensor\n",
        "   # )"
      ],
      "metadata": {
        "id": "5svhAWO7NO1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Estimates Target Activavtion Based on CRN desired metric numbers__"
      ],
      "metadata": {
        "id": "xKhn5A-hbqgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Choose a node and an input\n",
        "node_idx = 2\n",
        "weighted_input = np.array([0.85, 0.3, 0.2])\n",
        "\n",
        "\n",
        "# Grab that node's trained MLP\n",
        "mlp = optimizer.node_mlps[node_idx]\n",
        "\n",
        "# Use the class helper directly (NO self)\n",
        "predicted_metrics = Fuzzy_Hierarchical_Multiplex.mlp_predict(\n",
        "    mlp,\n",
        "    weighted_input\n",
        ")\n",
        "\n",
        "print(f\"Node {node_idx}\")\n",
        "print(\"Input weighted_input:\", weighted_input)\n",
        "print(\"Predicted metrics:\", np.round(predicted_metrics, 4))\n"
      ],
      "metadata": {
        "id": "T7XFhsT2vysC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Generate fresh synthetic targets ---\n",
        "synthetic_targets_new= synthetic_targets\n",
        "# --- Minimal FHM + FCM run with new targets ---\n",
        "optimizer = Fuzzy_Hierarchical_Multiplex(\n",
        "    candidate_dims=candidate_dims,\n",
        "    D_graph=D_graph,\n",
        "    synthetic_targets=synthetic_targets_new,\n",
        "    inner_learning=inner_learning,\n",
        "    gamma_interlayer=0.0,\n",
        "    causal_flag=False\n",
        ")\n",
        "\n",
        "# Store metrics during first run\n",
        "node_metrics_list = []\n",
        "\n",
        "for node_idx in range(D_graph):\n",
        "    full_target = synthetic_targets_new[node_idx]['target']\n",
        "    D_fcm = candidate_dims[node_idx][0]  # candidate dimension\n",
        "    target = full_target[:D_fcm]        # slice target to node FCM size\n",
        "\n",
        "    # Run inner FCM loop\n",
        "    x, y, W, mi_score, metrics = optimizer.run_inner(node_idx, target, D_fcm)\n",
        "    node_metrics_list.append(metrics)\n",
        "\n",
        "    #print(f\"\\nNode {node_idx} FHM output activations:\\n{x}\\n\")\n",
        "    print(f\"Node {node_idx} metrics:\\n{metrics}\\n\")\n",
        "\n",
        "# Compute scores and CRM tiers\n",
        "scores = [m['score'] for m in node_metrics_list]\n",
        "scores_norm = (np.array(scores) - min(scores)) / (max(scores) - min(scores) + 1e-12)\n",
        "\n",
        "for node_idx, score_norm in enumerate(scores_norm):\n",
        "    if score_norm > 0.66:\n",
        "        tier = 'High'\n",
        "    elif score_norm > 0.33:\n",
        "        tier = 'Medium'\n",
        "    else:\n",
        "        tier = 'Low'\n",
        "    print(f\"Node {node_idx}: CRM Tier = {tier}, Normalized Score = {score_norm:.3f}\")\n"
      ],
      "metadata": {
        "id": "VRaJFgmxV7rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FCM model"
      ],
      "metadata": {
        "id": "PpRiYl9ZNPuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class FCMWithTargets:\n",
        "    def __init__(self, synthetic_targets_new, seed=42):\n",
        "        \"\"\"\n",
        "        synthetic_targets: list of dicts with 'target' key\n",
        "        \"\"\"\n",
        "        np.random.seed(seed)\n",
        "        self.synthetic_targets = synthetic_targets\n",
        "        self.num_nodes = len(synthetic_targets)\n",
        "        self.max_dim = max(len(t['target']) for t in synthetic_targets)\n",
        "\n",
        "        # Initialize FCM weight matrices per node (square per node dim)\n",
        "        self.W = [np.random.uniform(-0.5, 0.5, (len(t['target']), len(t['target'])))\n",
        "                  for t in synthetic_targets]\n",
        "        for w in self.W:\n",
        "            np.fill_diagonal(w, 0)\n",
        "\n",
        "        # Node activations\n",
        "        self.activations = [np.zeros(len(t['target'])) for t in synthetic_targets]\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def run(self, steps=50, learning_rate=0.1):\n",
        "        \"\"\"\n",
        "        Run FCM updates towards targets\n",
        "        \"\"\"\n",
        "        for node_idx, t_dict in enumerate(self.synthetic_targets):\n",
        "            target = t_dict['target']\n",
        "            x = self.activations[node_idx]\n",
        "            W = self.W[node_idx]\n",
        "\n",
        "            for _ in range(steps):\n",
        "                input_signal = W.dot(x) + learning_rate * (target - x)\n",
        "                x = self.sigmoid(input_signal)\n",
        "\n",
        "            self.activations[node_idx] = x\n",
        "        return self.activations\n",
        "\n",
        "    def compute_node_scores(self):\n",
        "        \"\"\"\n",
        "        Example metric: average deviation from target\n",
        "        Higher score = closer to target\n",
        "        \"\"\"\n",
        "        scores = []\n",
        "        for node_idx, t_dict in enumerate(self.synthetic_targets):\n",
        "            target = t_dict['target']\n",
        "            x = self.activations[node_idx]\n",
        "            # score = inverse of L2 distance to target\n",
        "            score = 1 / (1 + np.linalg.norm(x - target))\n",
        "            scores.append(score)\n",
        "        return scores\n",
        "\n",
        "    def map_crm_tiers(self, scores, thresholds=(0.3, 0.6)):\n",
        "        \"\"\"\n",
        "        Map node scores to CRM tiers\n",
        "        \"\"\"\n",
        "        tiers = []\n",
        "        for s in scores:\n",
        "            if s > thresholds[1]:\n",
        "                tier = 'High'\n",
        "            elif s > thresholds[0]:\n",
        "                tier = 'Medium'\n",
        "            else:\n",
        "                tier = 'Low'\n",
        "            tiers.append(tier)\n",
        "        return tiers\n",
        "\n",
        "# ---------------- USAGE ----------------\n",
        "\n",
        "fcm = FCMWithTargets(synthetic_targets_new)\n",
        "activations = fcm.run(steps=100, learning_rate=0.2)\n",
        "\n",
        "scores = fcm.compute_node_scores()\n",
        "tiers = fcm.map_crm_tiers(scores)\n",
        "\n",
        "for i, (act, score, tier) in enumerate(zip(activations, scores, tiers)):\n",
        "    print(f\"Node {i} | Activations: {act} | \\nScore: {score:.3f} | CRM Tier: {tier}\\n\")\n"
      ],
      "metadata": {
        "id": "0i7k65n-NRa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "num_folds = 20\n",
        "D_graph = len(candidate_dims)  # number of nodes\n",
        "results_summary = []\n",
        "\n",
        "# --- Customer reference target simulation ---\n",
        "def generate_customer_targets(D_graph, candidate_dims):\n",
        "    synthetic_targets = []\n",
        "    for i in range(D_graph):\n",
        "        dim = candidate_dims[i][0]\n",
        "        base = np.random.rand(dim)\n",
        "        if i % 3 == 0:\n",
        "            target = 0.7 + 0.3 * base  # High preference\n",
        "        elif i % 3 == 1:\n",
        "            target = 0.3 + 0.4 * base  # Medium\n",
        "        else:\n",
        "            target = 0.0 + 0.3 * base  # Low\n",
        "        synthetic_targets.append({'target': target})\n",
        "    return synthetic_targets\n",
        "\n",
        "# --- FCM baseline class ---\n",
        "class FCMWithTargets:\n",
        "    def __init__(self, synthetic_targets):\n",
        "        self.synthetic_targets = synthetic_targets\n",
        "        self.num_nodes = len(synthetic_targets)\n",
        "        self.W = [np.random.uniform(-0.5, 0.5, (len(t['target']), len(t['target']))) for t in synthetic_targets]\n",
        "        for w in self.W: np.fill_diagonal(w, 0)\n",
        "        self.activations = [np.zeros(len(t['target'])) for t in synthetic_targets]\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def run(self, steps=100, learning_rate=0.2):\n",
        "        for node_idx, t_dict in enumerate(self.synthetic_targets):\n",
        "            target = t_dict['target']\n",
        "            x = self.activations[node_idx]\n",
        "            W = self.W[node_idx]\n",
        "            for _ in range(steps):\n",
        "                x = self.sigmoid(W.dot(x) + learning_rate*(target - x))\n",
        "            self.activations[node_idx] = x\n",
        "        return self.activations\n",
        "\n",
        "# --- Run comparison over folds ---\n",
        "for fold in range(num_folds):\n",
        "    synthetic_targets_new = generate_customer_targets(D_graph, candidate_dims)\n",
        "\n",
        "    # --- Minimal FHM run ---\n",
        "    optimizer = Fuzzy_Hierarchical_Multiplex(\n",
        "        candidate_dims=candidate_dims,\n",
        "        D_graph=D_graph,\n",
        "        synthetic_targets=synthetic_targets_new,\n",
        "        inner_learning=inner_learning,\n",
        "        gamma_interlayer=0.0,\n",
        "        causal_flag=False\n",
        "    )\n",
        "\n",
        "    node_metrics_list = []\n",
        "    for node_idx in range(D_graph):\n",
        "        full_target = synthetic_targets_new[node_idx]['target']\n",
        "        D_fcm = candidate_dims[node_idx][0]\n",
        "        target = full_target[:D_fcm]\n",
        "        _, _, _, _, metrics = optimizer.run_inner(node_idx, target, D_fcm)\n",
        "        node_metrics_list.append(metrics)\n",
        "\n",
        "    scores_fhm = np.array([m['score'] for m in node_metrics_list])\n",
        "    # Normalize to 0-1 for CRM tiers\n",
        "    norm_fhm = (scores_fhm - scores_fhm.min()) / (scores_fhm.max() - scores_fhm.min() + 1e-12)\n",
        "    tiers_fhm = ['High' if s>0.66 else 'Medium' if s>0.33 else 'Low' for s in norm_fhm]\n",
        "\n",
        "    # --- FCM baseline run ---\n",
        "    fcm = FCMWithTargets(synthetic_targets_new)\n",
        "    activations = fcm.run()\n",
        "\n",
        "    # --- Compute FCM node metrics using MetricsEvaluator with padded targets ---\n",
        "    max_len = max(len(t['target']) for t in synthetic_targets_new)\n",
        "    data_matrix = np.array([np.pad(t['target'], (0, max_len - len(t['target'])), 'constant')\n",
        "                            for t in synthetic_targets_new])\n",
        "\n",
        "    evaluator = MetricsEvaluator(\n",
        "        data_matrix=data_matrix,\n",
        "        feature_target=FEATURE_TARGET[:D_graph],\n",
        "        metric_target=METRIC_TARGET[:D_graph]\n",
        "    )\n",
        "\n",
        "    scores_fcm = np.array([evaluator.compute_node_metrics(i)['score'] for i in range(D_graph)])\n",
        "    norm_fcm = (scores_fcm - scores_fcm.min()) / (scores_fcm.max() - scores_fcm.min() + 1e-12)\n",
        "    tiers_fcm = ['High' if s>0.66 else 'Medium' if s>0.33 else 'Low' for s in norm_fcm]\n",
        "\n",
        "    # --- Store fold summary ---\n",
        "    results_summary.append({\n",
        "        'fold': fold+1,\n",
        "        'FHM_mean': scores_fhm.mean(),\n",
        "        'FHM_std': scores_fhm.std(),\n",
        "        'FHM_tiers': {'High': tiers_fhm.count('High'), 'Medium': tiers_fhm.count('Medium'), 'Low': tiers_fhm.count('Low')},\n",
        "        'FCM_mean': scores_fcm.mean(),\n",
        "        'FCM_std': scores_fcm.std(),\n",
        "        'FCM_tiers': {'High': tiers_fcm.count('High'), 'Medium': tiers_fcm.count('Medium'), 'Low': tiers_fcm.count('Low')}\n",
        "    })\n",
        "\n",
        "# --- Print concise summary ---\n",
        "for res in results_summary:\n",
        "    print(f\"Fold {res['fold']} | \"\n",
        "          f\"FHM Mean: {res['FHM_mean']:.3f}, Std: {res['FHM_std']:.3f}, Tiers: {res['FHM_tiers']} | \"\n",
        "          f\"FCM Mean: {res['FCM_mean']:.3f}, Std: {res['FCM_std']:.3f}, Tiers: {res['FCM_tiers']}\")\n"
      ],
      "metadata": {
        "id": "sBYXJbw9XRYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Techincal Report\n",
        "\n",
        "__Abstract__\n",
        "\n",
        "Here is a comparison between an FCM and the Fuzzy_Hierarchical_Multiplex with respect to a micro grid problem and four processes; the program uses user based inputs to determine service metrics and compatibility. In addition the CRM node-process evaluation is done using an FHM and FCM to. From the analysis it is determined that the FHM model is more flexible and efficient than an FCM in regards to assigning the better node/process, that is the process with desired attributes, to the customer.\n",
        "\n",
        "\n",
        "\n",
        "__Data Generator__\n",
        "In this script synthetic data that simulates a micro grid is generated and has four distinct patterns. The energy grid has four processes as a service let s say and so there each one has different attributes and priorities each time. To clarify, the data is the random, however, the way that the data is evaluated is directly stemming from the attributes that the user assigned. More specifically, in node one, attributes 1 to 5 are used to evaluate random data, and NOT generate it, etc. This way, every time the data is unrelated to the evaluation.\n",
        "\n",
        "\n",
        "\n",
        "__Metrics Evaluator__\n",
        "To determine the fitness of the targets an object is called which evaluates each given objective for the given data, or the overall node objectives that matter for each node, per node. To accomplish that, both the objective functions as well as the objectives are user inputs; furthermore, the objective evaluation is done using an exact method. To expound, the metrics are derived solving an optimization problem each time for each node. In this example Branch and bound was used to find best metrics per node.\n",
        "\n",
        "Note: (There are feature masks, but are not reflected on the plots and are reflected on the prints only)\n",
        "\n",
        "\n",
        "__FHM__\n",
        "Now the FHM model is used and it optimizes several things, firstly it accepts user inputs for evaluation and models each target process accordingly. It learns the weights while using metrics evaluator to optimize the metrics and then assignes the metrics to the respective activations. It has learned to transcend from activations to metrics this way. Then, it uses the outer objecitves and optimize node contributions to relearn the metrics from the scores and eventually. Now both activations and outer objectives can be used to calculate map the metric. Secondly, given input data from the data metrics, it analyzes the system services and provides an overall metric patron for the targets. Lastly, when a random input is the model can rerun it s formulas to calculate per node fitness and provide a normalized score.\n",
        "\n",
        "__FCM__\n",
        "The FCM tries to learn the targets and provide a reference for any relevant data. The targets are new data generated and the FCM is trained on this, it doesnt have a prior knowledge. It loses the comparison which will be explain after that. it has to be noted that this FCM does not learn the data so well.\n",
        "\n",
        "__Interpretation__\n",
        "The FHM has the ability to model data given user inputs which makes it a very strong model for deduction. On the other hand the FCM is data based; moreover, regardles off the activations the data fit is better on the FHM as it was tuned this way. Here it is important to distinguish between DATA and METRICS. While FHM maps the metrics the results only reflect that: __the data we want is provided for several attributes that one has assigned and nothing more.__ It is only the implications and no correlation, however, provided that the FHM mapped the data in a specific manner there is a score for each instance.\n",
        "\n",
        "__Remarks__\n",
        "The system has been used to model data and to determine outputs or more specifficaly service scores based on process. On the other hand the FCM uses raw data per node to calculate mean activations, which is a baseline. To attest, one can simply learn the data or maximize activations and there is another baseline with different score. Finally, the model is flexible enough, yet requires  some intervention. I think it works !!!\n",
        "\n"
      ],
      "metadata": {
        "id": "vxTQTYIHXtar"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-q9GNUcsj1UY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Node Features and Metric Mapping\n",
        "__based on chat translation__\n",
        "\n",
        "Each node \\(i\\) has a feature vector:\n",
        "\n",
        "$$\n",
        "\\mathbf{f}_i = [f_{i1}, f_{i2}, \\dots, f_{iF}] \\in [0,1]^F\n",
        "$$\n",
        "\n",
        "with a feature mask \\(M^{(f)}_i \\in \\{0,1\\}^F\\), so that the active features are:\n",
        "\n",
        "$$\n",
        "\\tilde{\\mathbf{f}}_i = \\mathbf{f}_i \\odot M^{(f)}_i\n",
        "$$\n",
        "\n",
        "For metric \\(k\\) with formula \\(g_k(\\cdot)\\) and relevant feature set \\(S_k\\):\n",
        "\n",
        "$$\n",
        "x_{ik} = \\frac{1}{|S_k \\cap \\text{active}(i)|} \\sum_{j \\in S_k \\cap \\text{active}(i)} f_{ij}\n",
        "$$\n",
        "\n",
        "$$\n",
        "m_{ik} = g_k(x_{ik})\n",
        "$$\n",
        "\n",
        "Node total score:\n",
        "\n",
        "$$\n",
        "\\text{score}_i = \\sum_{k=1}^{K} m_{ik}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "# Inner Loop: Fuzzy Cognitive Map (FCM)\n",
        "\n",
        "Each node \\(i\\) has inner activation vectors:\n",
        "\n",
        "$$\n",
        "\\mathbf{x}_i \\in [0,1]^{D_i}, \\quad \\mathbf{y}_i \\in [0,1]^{D_i}\n",
        "$$\n",
        "\n",
        "and FCM weight matrix:\n",
        "\n",
        "$$\n",
        "W_i \\in \\mathbb{R}^{D_i \\times D_i}, \\quad W_{ii} = 0\n",
        "$$\n",
        "\n",
        "**FCM updates per step \\(t\\):**\n",
        "\n",
        "$$\n",
        "\\mathbf{z}_i^{(t)} = \\mathbf{x}_i^{(t)} + W_i^{(t)} \\mathbf{y}_i^{(t)}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\Theta^{(t)}_{\\mathbf{x}} = \\mathbf{z}_i^{(t)} - \\mathbf{x}_i^\\text{target}, \\quad\n",
        "\\Theta^{(t)}_{\\mathbf{y}} = \\Theta^{(t)}_{\\mathbf{x}} \\cdot (W_i^{(t)})^T\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\Theta^{(t)}_{W} = \\mathbf{y}_i^{(t)} \\otimes \\Theta^{(t)}_{\\mathbf{x}}\n",
        "$$\n",
        "\n",
        "**Gradient updates:**\n",
        "\n",
        "$$\n",
        "\\mathbf{x}_i^{(t+1)} = \\mathbf{x}_i^{(t)} - \\eta_x \\cdot \\text{clip}(\\Theta^{(t)}_{\\mathbf{x}}, -\\delta_x, \\delta_x)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathbf{y}_i^{(t+1)} = \\mathbf{y}_i^{(t)} - \\eta_y \\cdot \\text{clip}(\\Theta^{(t)}_{\\mathbf{y}}, -\\delta_y, \\delta_y)\n",
        "$$\n",
        "\n",
        "$$\n",
        "W_i^{(t+1)} = \\text{clip}(W_i^{(t)} - \\eta_W \\cdot \\text{clip}(\\Theta^{(t)}_{W}, -\\delta_W, \\delta_W), -1, 1)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "# Inter-layer Activations\n",
        "\n",
        "Let the graph be \\(G\\) with adjacency \\(G_{ij}\\). The inter-layer activation from node \\(i\\) to \\(j\\) is:\n",
        "\n",
        "$$\n",
        "\\mathbf{a}_{ij} = \\sigma \\Big( W_{ij} \\cdot [\\mathbf{x}_i \\| \\mathbf{x}_j] + \\mathbf{b}_{ij} \\Big) \\cdot \\text{strength}(\\mathbf{x}_i, \\mathbf{x}_j)\n",
        "$$\n",
        "\n",
        "where \\(\\sigma\\) is the sigmoid function and \\(\\| \\) denotes concatenation.\n",
        "\n",
        "Mutual information / correlation across edges:\n",
        "\n",
        "$$\n",
        "MI(G) = \\gamma \\sum_{(i,j) \\in E} \\text{corr}^2(\\mathbf{a}_{ij}, \\mathbf{a}_{ji})\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "# Fuzzy Metric Tensor (FMT)\n",
        "\n",
        "Define FMT tensor:\n",
        "\n",
        "$$\n",
        "FMT_{i,j,k} =\n",
        "\\begin{cases}\n",
        "m_{jk}, & i = j \\\\\n",
        "|G_{ij}| \\cdot m_{jk}, & i \\neq j\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Normalized across metrics:\n",
        "\n",
        "$$\n",
        "\\hat{FMT}_{i,j,k} = \\frac{FMT_{i,j,k} - \\min(FMT)}{\\max(FMT) - \\min(FMT) + \\epsilon}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "# Outer Loop: Node Contribution\n",
        "\n",
        "Node contributions with outer weights \\(alpha_i\\):\n",
        "\n",
        "$$\n",
        "C_i = \\alpha_i \\left( \\text{raw_score}_i + \\gamma \\sum_{j \\neq i} \\hat{FMT}_{i,j,:} \\right)\n",
        "$$\n",
        "\n",
        "Outer objective (maximize total contribution minus correlation penalty):\n",
        "\n",
        "$$\n",
        "\\text{Score}_\\text{outer} = \\sum_i C_i - \\frac{1}{D \\cdot K} \\sum_i \\sum_k |\\text{corr}(FMT_{i,:,k}, A_{i,:})|\n",
        "$$\n"
      ],
      "metadata": {
        "id": "TU-XANvTkxIR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7dzJG_Fxj1Rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cefp0vTDXxM-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}