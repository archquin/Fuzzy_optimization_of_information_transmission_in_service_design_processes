{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "# ---------------- FEATURE KEYS ----------------\n",
        "FEATURE_KEYS = ['Distance', 'Speed', 'Load', 'Capacity', 'TempRequirement', 'Containerization']\n",
        "\n",
        "FEATURE_TARGET = [[1]*len(FEATURE_KEYS) for _ in range(10)]  # 10 nodes, all features active\n",
        "\n",
        "def default_feature_values():\n",
        "    return {k: 0.5 for k in FEATURE_KEYS}\n",
        "\n",
        "\n",
        "METRIC_KEYS = ['Wait', 'Utilization', 'Throughput', 'Patience','Energy']\n",
        "METRIC_TARGET = [\n",
        "    [1,1,1,1,1],\n",
        "    [1,1,1,1,1],\n",
        "    [1,1,1,1,1],\n",
        "    [1,1,1,1,1],\n",
        "    [1,1,1,1,1],\n",
        "]  # extend as needed\n",
        "\n",
        "# metric formulas\n",
        "METRIC_FORMULAS = [\n",
        "    lambda x: np.tanh(x),\n",
        "    lambda x: np.sqrt(x + 0.1),\n",
        "    lambda x: np.exp(-0.5 * x),\n",
        "    lambda x: 1 / (1 + x),\n",
        "    lambda x: x * 0.7\n",
        "]\n",
        "\n",
        "# ---------------- LSS OPTIMIZER ----------------\n",
        "class LSS:\n",
        "    \"\"\"List Sorting Search (beam-style deterministic optimizer using metric formulas).\"\"\"\n",
        "    def __init__(self, num_candidates=10, beam_width=15, minimize=False):\n",
        "        self.num_candidates = num_candidates\n",
        "        self.beam_width = beam_width\n",
        "        self.minimize = minimize\n",
        "\n",
        "    def optimize(self, features, y=None, metric_mask=None):\n",
        "        y = np.zeros(3) if y is None else np.array(y[:3])\n",
        "        base = np.mean(list(features.values())) + np.mean(y)\n",
        "\n",
        "        # generate initial candidate values\n",
        "        candidates = base + 0.1 * np.random.randn(self.num_candidates)\n",
        "        active_metric_count = sum(metric_mask) if metric_mask is not None else len(candidates)\n",
        "        candidates += active_metric_count * 0.01\n",
        "\n",
        "        # initialize beam: list of (candidate_value, metric_score)\n",
        "        beam = []\n",
        "        for val in candidates:\n",
        "            mv = [f(val) if m else 0.0 for f, m in zip(METRIC_FORMULAS, metric_mask)]\n",
        "            score = sum(mv)\n",
        "            beam.append((val, score))\n",
        "\n",
        "        # sort by metric-derived score and keep top beam_width\n",
        "        beam.sort(key=lambda x: x[1], reverse=not self.minimize)\n",
        "        beam = beam[:self.beam_width]\n",
        "\n",
        "        # pick best candidate from beam\n",
        "        best_value = beam[0][0]\n",
        "\n",
        "        return best_value\n",
        "\n",
        "\n",
        "# ---------------- METRICS EVALUATOR ----------------\n",
        "class MetricsEvaluator:\n",
        "    \"\"\"Compute node metrics with feature and metric masks, using LSS optimizer.\"\"\"\n",
        "    def __init__(self, data_matrix, num_candidates=10, beam_width=5, minimize=True):\n",
        "        self.data_matrix = data_matrix\n",
        "        self.num_nodes = data_matrix.shape[0]\n",
        "        self.optimizer = LSS(num_candidates=num_candidates, beam_width=beam_width, minimize=minimize)\n",
        "\n",
        "    def extract_features(self, node_idx):\n",
        "        node_data = self.data_matrix[node_idx, :]\n",
        "        defaults = default_feature_values()\n",
        "        features = {}\n",
        "        for i, key in enumerate(FEATURE_KEYS):\n",
        "            features[key] = node_data[i] if i < len(node_data) else defaults[key]\n",
        "        # normalize roughly\n",
        "        features = {k: v / (v + 1e-8) if v > 0 else defaults[k] for k,v in features.items()}\n",
        "        # apply feature mask\n",
        "        mask = FEATURE_TARGET[node_idx]\n",
        "        features = {k: v for k, v, m in zip(FEATURE_KEYS, features.values(), mask) if m}\n",
        "        return features\n",
        "\n",
        "    def compute_node_metrics(self, node_idx, y=None):\n",
        "        features = self.extract_features(node_idx)\n",
        "        metric_mask = METRIC_TARGET[node_idx]\n",
        "        opt_value = self.optimizer.optimize(features, y=y, metric_mask=metric_mask)\n",
        "\n",
        "        # generate metrics safely\n",
        "        metric_values = {}\n",
        "        for key, formula, mask in zip(METRIC_KEYS, METRIC_FORMULAS, metric_mask):\n",
        "            metric_values[key] = formula(opt_value) if mask else 0.0\n",
        "\n",
        "        metric_values['score'] = sum(metric_values.values())\n",
        "        return metric_values\n"
      ],
      "metadata": {
        "id": "-SMWpF04gwOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "np.random.seed()\n",
        "N = 1000   # number of synthetic ship logs\n",
        "\n",
        "# ============================\n",
        "# Domain 1: Route & Navigation\n",
        "# ============================\n",
        "route_nav = pd.DataFrame({\n",
        "    'distance_nm': np.random.uniform(50, 5000, N),         # nautical miles\n",
        "    'planned_speed_kn': np.random.uniform(10, 25, N),      # knots\n",
        "    'actual_speed_kn': np.random.uniform(8, 26, N),\n",
        "    'eta_hours': np.random.uniform(5, 500, N),\n",
        "    'route_risk_score': np.random.uniform(0, 1, N),        # piracy/weather risk\n",
        "    'fuel_capacity_tons': np.random.uniform(50, 300, N),\n",
        "    'fuel_used_tons': np.random.uniform(20, 290, N),\n",
        "})\n",
        "\n",
        "route_nav['speed_variance'] = (\n",
        "    route_nav['planned_speed_kn'] - route_nav['actual_speed_kn']\n",
        ")\n",
        "\n",
        "# ============================\n",
        "# Domain 2: Weather Conditions\n",
        "# ============================\n",
        "weather = pd.DataFrame({\n",
        "    'wave_height_m': np.random.uniform(0, 12, N),\n",
        "    'wind_speed_kn': np.random.uniform(0, 60, N),\n",
        "    'wind_direction_deg': np.random.uniform(0, 360, N),\n",
        "    'visibility_km': np.random.uniform(1, 20, N),\n",
        "    'storm_probability': np.random.uniform(0, 1, N),\n",
        "    'precipitation_mm': np.random.uniform(0, 100, N),\n",
        "})\n",
        "\n",
        "# ============================\n",
        "# Domain 3: Vessel Load & Capacity\n",
        "# ============================\n",
        "vessel_load = pd.DataFrame({\n",
        "    'max_capacity_tons': np.random.uniform(1000, 30000, N),\n",
        "    'current_load_tons': np.random.uniform(200, 29000, N),\n",
        "    'num_containers': np.random.randint(50, 2000, N),\n",
        "    'reefer_containers': np.random.randint(0, 300, N),\n",
        "    'bulk_cargo_tons': np.random.uniform(0, 5000, N),\n",
        "    'ballast_water_tons': np.random.uniform(0, 10000, N),\n",
        "})\n",
        "\n",
        "vessel_load['load_ratio'] = (\n",
        "    vessel_load['current_load_tons'] / vessel_load['max_capacity_tons']\n",
        ")\n",
        "\n",
        "# ============================\n",
        "# Domain 4: Cargo Storage & Loading\n",
        "# ============================\n",
        "cargo_types = [\n",
        "    \"containers\", \"bulk\", \"liquid\", \"hazardous\", \"reefer\",\n",
        "    \"vehicles\", \"general_goods\"\n",
        "]\n",
        "\n",
        "storage_loading = pd.DataFrame({\n",
        "    'cargo_type': np.random.choice(cargo_types, N),\n",
        "    'cargo_weight_tons': np.random.uniform(1, 500, N),\n",
        "    'loading_speed_tph': np.random.uniform(20, 200, N),     # tons per hour\n",
        "    'unloading_speed_tph': np.random.uniform(20, 200, N),\n",
        "    'storage_temp_req_C': np.random.uniform(-20, 30, N),    # some cargo needs cooling\n",
        "    'containerized': np.random.choice([0, 1], N),\n",
        "})\n",
        "\n",
        "# Convert cargo_type categorical → numeric index\n",
        "storage_loading['cargo_type_id'] = pd.factorize(storage_loading['cargo_type'])[0]\n",
        "\n",
        "# ============================\n",
        "# Domain 5: Temporal & Port Activity\n",
        "# ============================\n",
        "temporal_port = pd.DataFrame({\n",
        "    'arrival_hour': np.random.randint(0, 24, N),\n",
        "    'arrival_day': np.random.randint(0, 7, N),\n",
        "    'port_congestion_level': np.random.uniform(0, 1, N),\n",
        "    'docking_delay_hours': np.random.uniform(0, 48, N),\n",
        "    'tugboat_availability': np.random.choice([0, 1], N),\n",
        "})\n",
        "\n",
        "# ============================\n",
        "# Combine & Normalize\n",
        "# ============================\n",
        "datasets = [\n",
        "    route_nav,\n",
        "    weather,\n",
        "    vessel_load,\n",
        "    storage_loading.drop(columns=['cargo_type']),  # use numeric features only\n",
        "    temporal_port\n",
        "]\n",
        "\n",
        "DATA_MATRIX = np.hstack([df.values for df in datasets])\n",
        "DATA_MATRIX = (DATA_MATRIX - DATA_MATRIX.min(axis=0)) / (np.ptp(DATA_MATRIX, axis=0) + 1e-8)\n",
        "\n",
        "# ============================\n",
        "# Multi-node target generator\n",
        "# ============================\n",
        "# ============================\n",
        "# Graph-based Multi-node Targets\n",
        "# ============================\n",
        "\n",
        "D_graph = 5\n",
        "candidate_dims = [7, 6, 6, 7, 6]  # updated target dimensions for 5 nodes\n",
        "\n",
        "def generate_targets_per_node(DATA_MATRIX, candidate_dims, D_graph):\n",
        "    targets = []\n",
        "    for node_idx in range(D_graph):\n",
        "        row = DATA_MATRIX[node_idx % len(DATA_MATRIX)]\n",
        "        dim = candidate_dims[node_idx]\n",
        "        if len(row) >= dim:\n",
        "            sampled = row[:dim]\n",
        "        else:\n",
        "            sampled = np.pad(row, (0, dim - len(row)), constant_values=0.5)\n",
        "        targets.append({'target': sampled})\n",
        "    return targets\n",
        "\n",
        "synthetic_targets = generate_targets_per_node(DATA_MATRIX, candidate_dims, D_graph)\n",
        "\n",
        "# Test sizes\n",
        "for i, t in enumerate(synthetic_targets):\n",
        "    print(f\"Node {i} target size: {len(t['target'])}\")\n",
        "candidate_dims = [[7], [6], [6], [7], [6]]  # updated target dimensions for 5 nodes\n"
      ],
      "metadata": {
        "id": "n-6nAVTXGRn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# ---------------- FEATURE KEYS ----------------\n",
        "FEATURE_KEYS = ['Distance', 'Speed', 'Load', 'Capacity', 'TempRequirement', 'Containerization']\n",
        "\n",
        "FEATURE_TARGET = [[1]*len(FEATURE_KEYS) for _ in range(10)]  # 10 nodes, all features active\n",
        "\n",
        "def default_feature_values():\n",
        "    return {k: 0.5 for k in FEATURE_KEYS}\n",
        "\n",
        "\n",
        "METRIC_KEYS = ['Wait', 'Utilization', 'Throughput', 'Patience','Energy']\n",
        "METRIC_TARGET = [\n",
        "    [1,1,1,1,1],\n",
        "    [1,1,1,1,1],\n",
        "    [1,1,1,1,1],\n",
        "    [1,1,1,1,1],\n",
        "    [1,1,1,1,1],\n",
        "    [1,1,1,1,1],\n",
        "    [1,1,1,1,1],\n",
        "    [1,1,1,1,1],\n",
        "    [1,1,1,1,1],\n",
        "    [1,1,1,1,1],\n",
        "]  # extend as needed\n",
        "\n",
        "# metric formulas\n",
        "METRIC_FORMULAS = [\n",
        "    lambda x: np.tanh(x),\n",
        "    lambda x: np.sqrt(x + 0.1),\n",
        "    lambda x: np.exp(-0.5 * x),\n",
        "    lambda x: 1 / (1 + x),\n",
        "    lambda x: x * 0.7\n",
        "]\n",
        "\n",
        "# ---------------- GRID SEARCH OPTIMIZER ----------------\n",
        "class GridSearchOptimizer:\n",
        "    \"\"\"Exact, non-recursive optimizer using candidate evaluation over a fixed grid.\"\"\"\n",
        "    def __init__(self, num_candidates=10, minimize=True, value_range=(0.0, 5.0)):\n",
        "        self.num_candidates = num_candidates\n",
        "        self.minimize = minimize\n",
        "        self.value_range = value_range  # (min_value, max_value)\n",
        "\n",
        "    def optimize(self, features, y=None, metric_mask=None):\n",
        "        y = np.zeros(3) if y is None else np.array(y[:3])\n",
        "        base = np.mean(list(features.values())) + np.mean(y)\n",
        "\n",
        "        # create an evenly spaced grid of candidates around base\n",
        "        min_val, max_val = self.value_range\n",
        "        candidates = np.linspace(min_val + base, max_val + base, self.num_candidates)\n",
        "\n",
        "        # evaluate each candidate\n",
        "        best_score = -np.inf if not self.minimize else np.inf\n",
        "        best_value = candidates[0]\n",
        "\n",
        "        for val in candidates:\n",
        "            mv = [f(val) if m else 0.0 for f, m in zip(METRIC_FORMULAS, metric_mask)]\n",
        "            score = sum(mv)\n",
        "\n",
        "            if (self.minimize and score < best_score) or (not self.minimize and score > best_score):\n",
        "                best_score = score\n",
        "                best_value = val\n",
        "\n",
        "        return best_value\n",
        "\n",
        "\n",
        "# ---------------- METRICS EVALUATOR ----------------\n",
        "class MetricsEvaluator:\n",
        "    \"\"\"Compute node metrics with feature and metric masks, using GridSearch optimizer.\"\"\"\n",
        "    def __init__(self, data_matrix, num_candidates=10, minimize=False, value_range=(0.0,5.0)):\n",
        "        self.data_matrix = data_matrix\n",
        "        self.num_nodes = data_matrix.shape[0]\n",
        "        self.optimizer = GridSearchOptimizer(num_candidates=num_candidates, minimize=minimize, value_range=value_range)\n",
        "\n",
        "    def extract_features(self, node_idx):\n",
        "        node_data = self.data_matrix[node_idx, :]\n",
        "        defaults = default_feature_values()\n",
        "        features = {}\n",
        "        for i, key in enumerate(FEATURE_KEYS):\n",
        "            features[key] = node_data[i] if i < len(node_data) else defaults[key]\n",
        "        # normalize roughly\n",
        "        features = {k: v / (v + 1e-8) if v > 0 else defaults[k] for k,v in features.items()}\n",
        "        # apply feature mask\n",
        "        mask = FEATURE_TARGET[node_idx]\n",
        "        features = {k: v for k, v, m in zip(FEATURE_KEYS, features.values(), mask) if m}\n",
        "        return features\n",
        "\n",
        "    def compute_node_metrics(self, node_idx, y=None):\n",
        "        features = self.extract_features(node_idx)\n",
        "        metric_mask = METRIC_TARGET[node_idx]\n",
        "        opt_value = self.optimizer.optimize(features, y=y, metric_mask=metric_mask)\n",
        "\n",
        "        # generate metrics safely\n",
        "        metric_values = {}\n",
        "        for key, formula, mask in zip(METRIC_KEYS, METRIC_FORMULAS, metric_mask):\n",
        "            metric_values[key] = formula(opt_value) if mask else 0.0\n",
        "\n",
        "        metric_values['score'] = sum(metric_values.values())\n",
        "        return metric_values\n"
      ],
      "metadata": {
        "id": "A-9mrcWxELvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class BBOptimizer:\n",
        "    \"\"\"\n",
        "    Branch-and-Bound optimizer for 1D continuous problems.\n",
        "    Finds the maximum or minimum of a function over an interval.\n",
        "    \"\"\"\n",
        "    def __init__(self, minimize=False, tol=1e-3, max_depth=10, value_range=(0.0, 5.0)):\n",
        "        self.minimize = minimize\n",
        "        self.tol = tol\n",
        "        self.max_depth = max_depth\n",
        "        self.value_range = value_range\n",
        "\n",
        "    def optimize(self, func):\n",
        "        \"\"\"\n",
        "        func: callable f(x) -> score\n",
        "        returns x_opt (best input) and f_opt (best score)\n",
        "        \"\"\"\n",
        "        a, b = self.value_range\n",
        "        best_x, best_f = self._branch(func, a, b, depth=0)\n",
        "        return best_x\n",
        "\n",
        "    def _branch(self, func, a, b, depth):\n",
        "        if depth >= self.max_depth or (b - a) < self.tol:\n",
        "            mid = (a + b) / 2\n",
        "            f_mid = func(mid)\n",
        "            return mid, f_mid\n",
        "\n",
        "        # evaluate endpoints\n",
        "        f_a, f_b = func(a), func(b)\n",
        "        if self.minimize:\n",
        "            best_f, best_x = (f_a, a) if f_a < f_b else (f_b, b)\n",
        "        else:\n",
        "            best_f, best_x = (f_a, a) if f_a > f_b else (f_b, b)\n",
        "\n",
        "        # evaluate midpoint\n",
        "        mid = (a + b) / 2\n",
        "        f_mid = func(mid)\n",
        "        if (self.minimize and f_mid < best_f) or (not self.minimize and f_mid > best_f):\n",
        "            best_f, best_x = f_mid, mid\n",
        "\n",
        "        # recursively branch left and right\n",
        "        left_x, left_f = self._branch(func, a, mid, depth + 1)\n",
        "        right_x, right_f = self._branch(func, mid, b, depth + 1)\n",
        "\n",
        "        # select best among left, right, and midpoint\n",
        "        candidates = [(best_x, best_f), (left_x, left_f), (right_x, right_f)]\n",
        "        if self.minimize:\n",
        "            best_x, best_f = min(candidates, key=lambda t: t[1])\n",
        "        else:\n",
        "            best_x, best_f = max(candidates, key=lambda t: t[1])\n",
        "\n",
        "        return best_x, best_f\n",
        "class MetricsEvaluator:\n",
        "    \"\"\"Compute node metrics with feature and metric masks, using BB optimizer.\"\"\"\n",
        "    def __init__(self, data_matrix, minimize=False, value_range=(0.0,5.0), tol=1e-3):\n",
        "        self.data_matrix = data_matrix\n",
        "        self.num_nodes = data_matrix.shape[0]\n",
        "        self.optimizer = BBOptimizer(minimize=minimize, value_range=value_range, tol=tol)\n",
        "\n",
        "    def extract_features(self, node_idx):\n",
        "        node_data = self.data_matrix[node_idx, :]\n",
        "        defaults = default_feature_values()\n",
        "        features = {}\n",
        "        for i, key in enumerate(FEATURE_KEYS):\n",
        "            features[key] = node_data[i] if i < len(node_data) else defaults[key]\n",
        "        # normalize roughly\n",
        "        features = {k: v / (v + 1e-8) if v > 0 else defaults[k] for k,v in features.items()}\n",
        "        # apply feature mask\n",
        "        mask = FEATURE_TARGET[node_idx]\n",
        "        features = {k: v for k, v, m in zip(FEATURE_KEYS, features.values(), mask) if m}\n",
        "        return features\n",
        "\n",
        "    def compute_node_metrics(self, node_idx, y=None):\n",
        "        features = self.extract_features(node_idx)\n",
        "        metric_mask = METRIC_TARGET[node_idx]\n",
        "\n",
        "        # define score function for optimizer\n",
        "        def score_func(x):\n",
        "            mv = [f(x) if m else 0.0 for f, m in zip(METRIC_FORMULAS, metric_mask)]\n",
        "            return sum(mv)\n",
        "\n",
        "        opt_value = self.optimizer.optimize(score_func)\n",
        "\n",
        "        metric_values = {}\n",
        "        for key, formula, mask in zip(METRIC_KEYS, METRIC_FORMULAS, metric_mask):\n",
        "            metric_values[key] = formula(opt_value) if mask else 0.0\n",
        "\n",
        "        metric_values['score'] = sum(metric_values.values())\n",
        "        return metric_values\n"
      ],
      "metadata": {
        "id": "xtYHdxsCQTd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "\n",
        "# ---------------- CONFIG ----------------nsion\n",
        "\n",
        "inner_archive_size = 80\n",
        "inner_offspring = 40\n",
        "outer_archive_size = 40\n",
        "outer_offspring = 40\n",
        "inner_iters_per_outer = 50\n",
        "outer_generations = 10\n",
        "outer_cost_limit = 10000\n",
        "inner_learning = 0.1\n",
        "gamma_interlayer = 1\n",
        "top_k = 21\n",
        "seed = np.random.seed()\n",
        "\n",
        "new_DATA_MATRIX = np.random.rand(D_graph, DATA_MATRIX.shape[1])\n",
        "\n",
        "\n",
        "\n",
        "class InterLayer:\n",
        "    def __init__(self, D_graph, max_inner_dim, inter_dim=None, edge_threshold=0.02, gamma=1.0, seed=42):\n",
        "        np.random.seed(seed)\n",
        "        self.D_graph = D_graph\n",
        "        self.edge_threshold = edge_threshold\n",
        "        self.gamma = gamma\n",
        "        self.inter_dim = inter_dim[0] if isinstance(inter_dim, list) else (inter_dim if inter_dim is not None else max_inner_dim[0] if isinstance(max_inner_dim, list) else max_inner_dim)\n",
        "        self.max_input = 2 * (max_inner_dim[0] if isinstance(max_inner_dim, list) else max_inner_dim)\n",
        "                    # Initialize weights proportional to synthetic correlation between nodes\n",
        "        self.weights = {}\n",
        "        self.bias = {}\n",
        "        for i in range(D_graph):\n",
        "            for j in range(D_graph):\n",
        "                if i != j:\n",
        "                    # small random + slight bias towards correlation\n",
        "                    w_init = np.random.uniform(-0.1, 0.1, (self.inter_dim, self.max_input))\n",
        "                    self.weights[(i,j)] = w_init\n",
        "                    self.bias[(i,j)] = np.zeros(self.inter_dim)\n",
        "\n",
        "    def compute_edge_activation(self, i, j, nested_reps):\n",
        "        concat = np.concatenate([nested_reps[i], nested_reps[j]])\n",
        "        concat = np.pad(concat, (0, max(0, self.max_input - len(concat))))[:self.max_input]\n",
        "\n",
        "        # Normalize input to improve correlation\n",
        "        concat = (concat - np.mean(concat)) / (np.std(concat) + 1e-12)\n",
        "\n",
        "        # Compute activation\n",
        "        v = self.weights[(i,j)].dot(concat) + self.bias[(i,j)]\n",
        "\n",
        "        # Scale by correlation strength with input signals\n",
        "        input_strength = np.clip(np.mean(np.abs(concat)), 0, 1)\n",
        "        v = v * input_strength\n",
        "\n",
        "        return 1 / (1 + np.exp(-v))\n",
        "\n",
        "    def build_activations(self, Gmat, nested_reps):\n",
        "        acts = {}\n",
        "        for i in range(self.D_graph):\n",
        "            for j in range(self.D_graph):\n",
        "                if i == j:\n",
        "                    continue\n",
        "                if abs(Gmat[i,j]) > self.edge_threshold:\n",
        "                    acts[(i,j)] = self.compute_edge_activation(i, j, nested_reps)\n",
        "        return acts\n",
        "\n",
        "    @staticmethod\n",
        "    def pairwise_squared_corr(acts):\n",
        "        if len(acts) < 2:\n",
        "            return 0.0\n",
        "        A = np.stack(list(acts.values()))\n",
        "        A_centered = A - A.mean(axis=1, keepdims=True)\n",
        "        stds = np.sqrt(np.sum(A_centered**2, axis=1) / (A.shape[1]-1) + 1e-12)\n",
        "        cov = A_centered @ A_centered.T / (A.shape[1]-1)\n",
        "        corr = cov / (np.outer(stds, stds) + 1e-12)\n",
        "        np.fill_diagonal(corr, 0)\n",
        "        return float((corr**2).sum())\n",
        "\n",
        "    def mi_for_graph(self, Gmat, nested_reps):\n",
        "        acts = self.build_activations(Gmat, nested_reps)\n",
        "        if not acts:\n",
        "            return 0.0\n",
        "        return self.gamma * self.pairwise_squared_corr(acts)\n",
        "\n",
        "    def correlate_shrink_interlayer(self, fmt_bounds=None, interaction_tensor=None, metrics_keys=None, verbose=True):\n",
        "        \"\"\"\n",
        "        Compute Pearson correlation per node & metric between:\n",
        "            - shrink factor (adaptive FMT)\n",
        "            - mean outgoing inter-layer activations\n",
        "        Returns: {node_idx: {metric: {'r':..., 'p':...}}}\n",
        "        \"\"\"\n",
        "        from scipy.stats import pearsonr\n",
        "\n",
        "        if metrics_keys is None:\n",
        "            metrics_keys =self.MK\n",
        "\n",
        "        D = self.D_graph\n",
        "\n",
        "        # 1. Compute FMT bounds if not given\n",
        "        if fmt_bounds is None:\n",
        "            fmt_bounds = self.compute_fmt_with_bounds_adaptive(top_k=top_k)\n",
        "\n",
        "        # 2. Get inter-layer activations if not provided\n",
        "        if interaction_tensor is None:\n",
        "            interaction_tensor = self.print_interactions(return_tensor=True, verbose=False)\n",
        "\n",
        "        inter_mean = interaction_tensor.mean(axis=2)  # (D,D)\n",
        "        shrink_factors = self.compute_fmt_shrink_factor(fmt_bounds, metrics_keys)  # (D, num_metrics)\n",
        "\n",
        "        correlations = {}\n",
        "\n",
        "        for i in range(D):\n",
        "            correlations[i] = {}\n",
        "            for k, key in enumerate(metrics_keys):\n",
        "                # FMT shrink for node i (broadcasted across outgoing edges)\n",
        "                shrink_vec = shrink_factors[i, k] * np.ones(D)\n",
        "                # Outgoing inter-layer activations from node i\n",
        "                inter_vec = inter_mean[i, :]\n",
        "                # Remove self-loop\n",
        "                mask = np.arange(D) != i\n",
        "                shrink_vec = shrink_vec[mask]\n",
        "                inter_vec = inter_vec[mask]\n",
        "\n",
        "                # Compute Pearson correlation\n",
        "                if np.std(inter_vec) > 1e-8:  # valid correlation\n",
        "                    r, p = pearsonr(shrink_vec, inter_vec)\n",
        "                else:\n",
        "                    r, p = 0.0, 1.0  # no variability\n",
        "\n",
        "                correlations[i][key] = {'r': r, 'p': p}\n",
        "                if verbose:\n",
        "                    print(f\"Node {i} | {key} shrink vs inter-layer: r={r:.3f}, p={p:.3e}\")\n",
        "\n",
        "        return correlations\n",
        "\n",
        "\n",
        "\n",
        "# ---------------- UNIFIED ACOR MULTIPLEX ----------------\n",
        "class Fuzzy_Hierarchical_Multiplex:\n",
        "    def __init__(self, candidate_dims, D_graph, inner_archive_size, inner_offspring,\n",
        "                 outer_archive_size, outer_offspring, synthetic_targets, inner_learning,\n",
        "                 gamma_interlayer=1.0, causal_flag=True,metrics=METRIC_KEYS):\n",
        "        self.candidate_dims = candidate_dims\n",
        "        self.D_graph = D_graph\n",
        "        self.inner_archive_size = inner_archive_size\n",
        "        self.inner_offspring = inner_offspring\n",
        "        self.outer_archive_size = outer_archive_size\n",
        "        self.outer_offspring = outer_offspring\n",
        "        self.synthetic_targets = synthetic_targets\n",
        "        self.inner_learning = inner_learning\n",
        "        self.causal_flag = causal_flag\n",
        "        self.best_dim_per_node = [len(t)-1 for t in synthetic_targets]  # last element as best dim\n",
        "\n",
        "        self.MK = metrics\n",
        "        self.MKI = metrics+['score']\n",
        "        self.nested_reps = [np.zeros(c[0]) for c in candidate_dims]\n",
        "      #  self.best_dim_per_node = [candidate_dims[0] for _ in range(D_graph)]\n",
        "        self.inter_layer = InterLayer(D_graph, max_inner_dim=max(candidate_dims), gamma=gamma_interlayer)\n",
        "        self.chosen_Gmat = np.random.uniform(-0.5,0.5,(D_graph,D_graph))\n",
        "        np.fill_diagonal(self.chosen_Gmat,0)\n",
        "        self.l2_before, self.l2_after = [], []\n",
        "\n",
        "    # ---------- INNER LOOP (FCM) ----------\n",
        "    def run_inner(self, node_idx, target, D_fcm,\n",
        "              steps=100, lr_x=0.001, lr_y=0.001, lr_W=0.001,\n",
        "              decorrelate_metrics=True):\n",
        "\n",
        "        # --- Initialize activations ---\n",
        "        x = target.copy()\n",
        "        y = np.random.uniform(-0.6, 0.6, D_fcm)\n",
        "\n",
        "        # Pad target for L2 computation\n",
        "        target_padded = np.pad(target, (0, len(self.nested_reps[node_idx]) - len(target)),\n",
        "                            mode='constant', constant_values=0.5)\n",
        "        self.l2_before.append(np.linalg.norm(self.nested_reps[node_idx] - target_padded))\n",
        "\n",
        "        # --- FCM updates ---\n",
        "        W = np.random.uniform(-0.6, 0.6, (D_fcm, D_fcm))\n",
        "        np.fill_diagonal(W, 0)\n",
        "\n",
        "        for _ in range(steps):\n",
        "            z = y.dot(W) + x\n",
        "            Theta_grad_z = z - target\n",
        "            Theta_grad_x = Theta_grad_z\n",
        "            Theta_grad_y = Theta_grad_z.dot(W.T)\n",
        "            Theta_grad_W = np.outer(y, Theta_grad_z)\n",
        "\n",
        "            x -= lr_x * np.clip(Theta_grad_x, -0.05, 0.05)\n",
        "            y -= lr_y * np.clip(Theta_grad_y, -0.05, 0.05)\n",
        "            W -= lr_W * np.clip(Theta_grad_W, -0.01, 0.01)\n",
        "\n",
        "            x = np.clip(x, 0, 1)\n",
        "            y = np.clip(y, 0, 1)\n",
        "            np.fill_diagonal(W, 0)\n",
        "            W = np.clip(W, -1, 1)\n",
        "\n",
        "        # --- Pad FCM output to nested representation ---\n",
        "        x_padded = np.pad(x, (0, len(self.nested_reps[node_idx]) - len(x)),\n",
        "                        mode='constant', constant_values=0.5)\n",
        "        self.nested_reps[node_idx] = x_padded\n",
        "        self.l2_after.append(np.linalg.norm(x_padded - target_padded))\n",
        "\n",
        "        # --- Extract node features ---\n",
        "        metrics_evaluator = MetricsEvaluator(data_matrix=DATA_MATRIX)\n",
        "        features = metrics_evaluator.extract_features(node_idx)\n",
        "        feat_vals = np.array(list(features.values()))\n",
        "\n",
        "        # --- Compute metrics scaled by activations + features ---\n",
        "        metric_mask = METRIC_TARGET[node_idx]\n",
        "        metric_values = {}\n",
        "\n",
        "        for key, formula, mask in zip(METRIC_KEYS, METRIC_FORMULAS, metric_mask):\n",
        "            if mask:\n",
        "                # Match activations to features\n",
        "                act_vals = x[:len(feat_vals)]\n",
        "                # --- Inner weighted sum ---\n",
        "                weighted_input = np.mean(act_vals * feat_vals)\n",
        "\n",
        "                # --- Apply outer scale if available ---\n",
        "                outer_scale = self.best_node_weights[node_idx] if hasattr(self, 'best_node_weights') else 1.0\n",
        "                weighted_input *= outer_scale\n",
        "\n",
        "                # Optional: small node bias for uniqueness\n",
        "                weighted_input += 0.05 * node_idx\n",
        "\n",
        "                # Compute metric\n",
        "                metric_values[key] = formula(weighted_input)\n",
        "            else:\n",
        "                metric_values[key] = 0.0\n",
        "\n",
        "        # --- Total score ---\n",
        "        metric_values['score'] = sum(metric_values.values())\n",
        "\n",
        "        # --- Store inner activation for outer loop ---\n",
        "        metric_values['x'] = x_padded.copy()        # store padded activation\n",
        "        metric_values['feat_vals'] = feat_vals.copy()  # store features\n",
        "\n",
        "        # --- Compute inter-layer MI ---\n",
        "        mi_score = self.inter_layer.mi_for_graph(self.chosen_Gmat, self.nested_reps)\n",
        "\n",
        "        return x, y, W, mi_score, metric_values\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def run_outer(self, outer_cost_limit=1000):\n",
        "        from scipy.optimize import minimize\n",
        "\n",
        "        node_metrics_list = self.capped_node_metrics\n",
        "        D = self.D_graph\n",
        "        gamma = self.inter_layer.gamma\n",
        "        lambda_reg = 0.05\n",
        "\n",
        "        # --- Compute raw scores using inner activations ---\n",
        "        raw_scores = np.zeros(D)\n",
        "        for node_idx, metrics in enumerate(node_metrics_list):\n",
        "            act_vals = metrics['x'][:len(metrics['feat_vals'])]\n",
        "            feat_vals = metrics['feat_vals']\n",
        "\n",
        "            weighted_input = np.mean(act_vals * feat_vals)\n",
        "\n",
        "            # Optional node bias\n",
        "            weighted_input += 0.05 * node_idx\n",
        "\n",
        "            # --- Apply outer node weighting if already present ---\n",
        "            outer_scale = self.best_node_weights[node_idx] if hasattr(self, 'best_node_weights') else 1.0\n",
        "            weighted_input *= outer_scale\n",
        "\n",
        "            raw_scores[node_idx] = weighted_input\n",
        "\n",
        "        # --- Apply cap if needed ---\n",
        "        total_raw = raw_scores.sum()\n",
        "        if total_raw > outer_cost_limit:\n",
        "            scale_factor = outer_cost_limit / total_raw\n",
        "            raw_scores *= scale_factor\n",
        "\n",
        "        # --- Compute Fuzzy Metric Tensor ---\n",
        "        fuzzy_tensor = self.compute_fuzzy_metric_tensor(normalize=True)\n",
        "        fmt_node_sums = np.array([fuzzy_tensor[i,:,:].sum() - fuzzy_tensor[i,i,:].sum() for i in range(D)])\n",
        "\n",
        "        # --- Node contribution optimization ---\n",
        "        def objective(weights):\n",
        "            node_contrib = weights * (raw_scores + gamma * fmt_node_sums)\n",
        "            reg_penalty = lambda_reg * np.sum((weights - 1.0/D)**2)\n",
        "            return - (node_contrib.sum() - reg_penalty)\n",
        "\n",
        "        bounds = [(0.2, 0.4)] * D\n",
        "        cons = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1.0}\n",
        "        x0 = np.ones(D)/D\n",
        "\n",
        "        result = minimize(objective, x0=x0, bounds=bounds, constraints=cons, method='SLSQP')\n",
        "        node_weights = result.x\n",
        "\n",
        "        # --- Compute node contributions ---\n",
        "        node_contributions = (raw_scores + gamma * fmt_node_sums) * node_weights\n",
        "\n",
        "        # --- Weighted FMT ---\n",
        "        weighted_fmt = fuzzy_tensor.copy()\n",
        "        for i in range(D):\n",
        "            weighted_fmt[i,:,:] *= node_weights[i]\n",
        "\n",
        "        # Normalize for visualization\n",
        "        weighted_fmt = (weighted_fmt - weighted_fmt.min()) / (weighted_fmt.max() - weighted_fmt.min() + 1e-12)\n",
        "\n",
        "        # --- Correlation penalty ---\n",
        "        interaction_tensor = self.print_interactions(return_tensor=True, verbose=False)\n",
        "        fmt_mean = fuzzy_tensor.mean(axis=2)\n",
        "        inter_mean = interaction_tensor.mean(axis=2)\n",
        "        corr_penalty = 0.0\n",
        "        for i in range(D):\n",
        "            fmt_vec = fmt_mean[i,:] * node_weights[i]\n",
        "            inter_vec = inter_mean[i,:] * node_weights[i]\n",
        "            if np.std(fmt_vec) > 1e-8 and np.std(inter_vec) > 1e-8:\n",
        "                corr_penalty += abs(np.corrcoef(fmt_vec, inter_vec)[0,1])**2\n",
        "        corr_penalty /= D\n",
        "\n",
        "        combined_score = node_contributions.sum() - corr_penalty\n",
        "\n",
        "        # --- Save attributes ---\n",
        "        self.node_score_contributions = node_contributions\n",
        "        self.correlation_penalty = corr_penalty\n",
        "        self.weighted_fmt = weighted_fmt\n",
        "        self.best_node_weights = node_weights\n",
        "\n",
        "        return node_metrics_list, combined_score, node_contributions\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def run(self, outer_generations=outer_generations):\n",
        "        best_score = -np.inf\n",
        "\n",
        "        for gen in range(outer_generations):\n",
        "            node_metrics_list = []\n",
        "\n",
        "            for node_idx in range(self.D_graph):\n",
        "                full_target = self.synthetic_targets[node_idx]['target']\n",
        "                D_fcm = self.candidate_dims[node_idx][0]\n",
        "                target = full_target[:D_fcm]\n",
        "\n",
        "                _, _, _, _, metrics = self.run_inner(node_idx, target, D_fcm)\n",
        "                node_metrics_list.append(metrics)\n",
        "\n",
        "            # Outer loop\n",
        "            self.capped_node_metrics = node_metrics_list\n",
        "            _, capped_score, node_contributions = self.run_outer()\n",
        "\n",
        "            if capped_score > best_score:\n",
        "                best_score = capped_score\n",
        "            else:\n",
        "                print(f\"\\n--- Generation {gen} Metrics (NEW BEST) ---\")\n",
        "                for i, m in enumerate(node_metrics_list):\n",
        "                    out_str = []\n",
        "                    for k, v in m.items():\n",
        "                        if np.isscalar(v):\n",
        "                            out_str.append(f\"{k}: {v:.2f}\")\n",
        "                        elif isinstance(v, np.ndarray):\n",
        "                            out_str.append(f\"{k}: mean {v.mean():.2f}, shape {v.shape}\")\n",
        "                        else:\n",
        "                            out_str.append(f\"{k}: {v}\")\n",
        "                    print(f\"Node {i} | \" + \" | \".join(out_str))\n",
        "\n",
        "                print(f\"\\n--- Generation {gen} Node Contributions (NEW BEST) ---\")\n",
        "                for i, c in enumerate(node_contributions):\n",
        "                    print(f\"Node {i}: Contribution = {c:.4f}\")\n",
        "\n",
        "                print(f\"Outer Score (capped): {capped_score:.3f} <-- NEW BEST\")\n",
        "\n",
        "        return best_score\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # ---------- VISUALIZATIONS ----------\n",
        "    # ---------- VISUALIZATIONS ----------\n",
        "    def plot_pointwise_minmax_elite(self, top_k=21):\n",
        "        plt.figure(figsize=(14,3))\n",
        "        for i in range(self.D_graph):\n",
        "            # Node's actual dimension\n",
        "            dim_i = self.candidate_dims[i][0]  # ✅ integer\n",
        "            base = self.nested_reps[i][:dim_i]  # slice to candidate dim\n",
        "            reps = np.clip(base + np.random.normal(0,0.05,(top_k,len(base))),0,1)\n",
        "            y_min, y_max = reps.min(axis=0), reps.max(axis=0)\n",
        "            y_sel = base\n",
        "\n",
        "            # True target for this node, sliced to candidate dim\n",
        "            y_true = self.synthetic_targets[i]['target'][:len(y_sel)]\n",
        "            if len(y_true) < len(y_sel):\n",
        "                y_true = np.pad(y_true, (0, len(y_sel)-len(y_true)), \"constant\")\n",
        "            else:\n",
        "                y_true = y_true[:len(y_sel)]\n",
        "\n",
        "            plt.subplot(1,self.D_graph,i+1)\n",
        "            plt.fill_between(range(len(y_min)),y_min,y_max,color='skyblue',alpha=0.4,label='Elite Interval')\n",
        "            plt.plot(y_sel,'k-',lw=2,label='Estimated')\n",
        "            plt.plot(y_true,'r--',lw=2,label='True')\n",
        "            plt.ylim(0,1.05)\n",
        "            plt.title(f\"Node {i+1}\")\n",
        "            if i==0: plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def plot_nested_activations(self):\n",
        "        plt.figure(figsize=(12,3))\n",
        "        for i,rep in enumerate(self.nested_reps):\n",
        "            dim_i = self.candidate_dims[i][0]\n",
        "            rep_i = rep[:dim_i]  # slice to candidate dim\n",
        "            plt.subplot(1,self.D_graph,i+1)\n",
        "            plt.bar(range(len(rep_i)), rep_i, color=plt.cm.plasma(rep_i))\n",
        "            plt.ylim(0,1)\n",
        "            plt.title(f\"Node {i+1}\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def plot_outer_fuzzy_graph(self):\n",
        "        G = nx.DiGraph()\n",
        "        for i in range(self.D_graph): G.add_node(i)\n",
        "        for i in range(self.D_graph):\n",
        "            for j in range(self.D_graph):\n",
        "                if i!=j and abs(self.chosen_Gmat[i,j])>0.02:\n",
        "                    G.add_edge(i,j,weight=self.chosen_Gmat[i,j])\n",
        "        node_sizes = [self.best_dim_per_node[i]*200 for i in range(self.D_graph)]\n",
        "        edge_colors = ['green' if d['weight']>0 else 'red' for _,_,d in G.edges(data=True)]\n",
        "        edge_widths = [abs(d['weight'])*3 for _,_,d in G.edges(data=True)]\n",
        "        pos = nx.circular_layout(G)\n",
        "        plt.figure(figsize=(6,6))\n",
        "        nx.draw(G,pos,node_size=node_sizes,node_color='skyblue',\n",
        "                edge_color=edge_colors,width=edge_widths,arrows=True,with_labels=True)\n",
        "        plt.title(\"Outer Fuzzy Multiplex Graph\")\n",
        "        plt.show()\n",
        "# ---------------- INTERACTIONS INSPECTOR ----------------\n",
        "\n",
        "    def print_interactions(self, return_tensor=True, verbose=True):\n",
        "            D_graph = self.D_graph\n",
        "            inter_dim = self.inter_layer.inter_dim\n",
        "            inter_tensor = np.zeros((D_graph, D_graph, inter_dim))\n",
        "\n",
        "            acts = self.inter_layer.build_activations(self.chosen_Gmat, self.nested_reps)\n",
        "            if not acts:\n",
        "                if verbose:\n",
        "                    print(\"No active edges above threshold.\")\n",
        "                return inter_tensor if return_tensor else None\n",
        "\n",
        "            for (i, j), vec in acts.items():\n",
        "                inter_tensor[i, j, :] = vec\n",
        "                if verbose:\n",
        "                    act_str = \", \".join([f\"{v:.3f}\" for v in vec])\n",
        "                    print(f\"Node {i} -> Node {j}: [{act_str}]\")\n",
        "            return inter_tensor if return_tensor else None\n",
        "\n",
        "        # Move these outside of print_interactions (class-level)\n",
        "    def print_l2_summary(self):\n",
        "            print(\"\\nL2 Distances to Target per Node:\")\n",
        "            for idx, (before, after) in enumerate(zip(self.l2_before, self.l2_after)):\n",
        "                print(f\"Node {idx}: Before={before:.4f}, After={after:.4f}\")\n",
        "\n",
        "    def compute_fuzzy_metric_tensor(self, normalize=True, verbose=False):\n",
        "            \"\"\"\n",
        "            Computes a Fuzzy Metric Tensor (D_graph x D_graph x num_metrics)\n",
        "            using current nested reps and node metrics.\n",
        "            Each slice [i,j,:] represents metrics of node j (optionally weighted by Gmat[i,j])\n",
        "            \"\"\"\n",
        "            metrics_keys =self.MK\n",
        "            D = self.D_graph\n",
        "            num_metrics = len(metrics_keys)\n",
        "            tensor = np.zeros((D, D, num_metrics))\n",
        "\n",
        "            metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "\n",
        "            node_metrics = []\n",
        "            for i, rep in enumerate(self.nested_reps):\n",
        "                metrics = metrics_evaluator.compute_node_metrics(i, y=rep)\n",
        "                node_metrics.append(np.array([metrics[k] for k in metrics_keys]))\n",
        "            node_metrics = np.array(node_metrics)  # (D, num_metrics)\n",
        "\n",
        "            for i in range(D):\n",
        "                for j in range(D):\n",
        "                    if i==j:\n",
        "                        tensor[i,j,:] = node_metrics[j]\n",
        "                    else:\n",
        "                        weight = np.clip(abs(self.chosen_Gmat[i,j]), 0, 1)\n",
        "                        tensor[i,j,:] = weight * node_metrics[j]\n",
        "\n",
        "            if normalize:\n",
        "                tensor = (tensor - tensor.min()) / (tensor.max() - tensor.min() + 1e-12)\n",
        "\n",
        "            if verbose:\n",
        "                print(\"Fuzzy Metric Tensor shape:\", tensor.shape)\n",
        "\n",
        "            return tensor\n",
        "\n",
        "\n",
        "\n",
        "    def compute_fmt_shrink_factor(self, fmt_bounds, metrics_keys=None):\n",
        "        \"\"\"\n",
        "        Returns shrink factor per node and metric.\n",
        "        shrink_factor = 1 - (current_interval / original_interval)\n",
        "        \"\"\"\n",
        "        if metrics_keys is None:\n",
        "            metrics_keys = self.MK\n",
        "\n",
        "        D = self.D_graph\n",
        "        num_metrics = len(metrics_keys)\n",
        "        shrink_factors = np.zeros((D, num_metrics))\n",
        "\n",
        "        for i in range(D):\n",
        "            for k in range(num_metrics):\n",
        "                lower, upper = fmt_bounds[i, i, k, 0], fmt_bounds[i, i, k, 1]  # self-node interval\n",
        "                interval_width = upper - lower + 1e-12  # normalized [0,1]\n",
        "                shrink_factors[i, k] = 1 - interval_width  # more shrink = higher value\n",
        "\n",
        "        return shrink_factors\n",
        "\n",
        "    def compute_fmt_with_bounds_adaptive(self, top_k=21, max_shrink=0.5, metrics_keys=None):\n",
        "        \"\"\"\n",
        "        Computes FMT bounds using pointwise min/max across elite solutions,\n",
        "        and applies dynamic adaptive shrinking where variability is low.\n",
        "        Returns tensor shape (D,D,num_metrics,2) [lower, upper].\n",
        "        \"\"\"\n",
        "        if metrics_keys is None:\n",
        "            metrics_keys =self.MK\n",
        "\n",
        "        D = self.D_graph\n",
        "        num_metrics = len(metrics_keys)\n",
        "        tensor_bounds = np.zeros((D, D, num_metrics, 2))\n",
        "        metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "\n",
        "        variability = np.zeros((D, num_metrics))\n",
        "\n",
        "        # Step 1: compute bounds from perturbations\n",
        "        for i in range(D):\n",
        "            base = self.nested_reps[i]\n",
        "            reps = np.clip(base + np.random.normal(0, 0.05, (top_k, len(base))), 0, 1)\n",
        "            metrics_matrix = np.zeros((top_k, num_metrics))\n",
        "            for idx, rep in enumerate(reps):\n",
        "                m = metrics_evaluator.compute_node_metrics(i, y=rep)\n",
        "                metrics_matrix[idx, :] = [m[k] for k in metrics_keys]\n",
        "\n",
        "            lower_i = metrics_matrix.min(axis=0)\n",
        "            upper_i = metrics_matrix.max(axis=0)\n",
        "            tensor_bounds[i, :, :, 0] = lower_i[np.newaxis, :]  # broadcast to all j\n",
        "            tensor_bounds[i, :, :, 1] = upper_i[np.newaxis, :]\n",
        "            variability[i, :] = metrics_matrix.std(axis=0)\n",
        "\n",
        "        # Step 2: adaptive shrinking\n",
        "        for i in range(D):\n",
        "            for j in range(D):\n",
        "                for k in range(num_metrics):\n",
        "                    lower, upper = tensor_bounds[i,j,k,0], tensor_bounds[i,j,k,1]\n",
        "                    mean = (lower + upper)/2\n",
        "                    var_norm = min(1.0, variability[i,k]/(upper-lower + 1e-12))\n",
        "                    shrink_factor = max_shrink * (1 - var_norm)\n",
        "                    tensor_bounds[i,j,k,0] = mean - shrink_factor*(mean - lower)\n",
        "                    tensor_bounds[i,j,k,1] = mean + shrink_factor*(upper - mean)\n",
        "\n",
        "        return tensor_bounds\n",
        "\n",
        "\n",
        "\n",
        "    def plot_fuzzy_metric_tensor_heatmaps(self, fuzzy_tensor=None, metrics_keys=['wait','throughput','util','patience']):\n",
        "        \"\"\"\n",
        "        Plot a heatmap panel for each metric in the FMT.\n",
        "        Rows: source node i\n",
        "        Columns: target node j\n",
        "        \"\"\"\n",
        "        if fuzzy_tensor is None:\n",
        "            fuzzy_tensor = self.compute_fuzzy_metric_tensor(normalize=True)\n",
        "\n",
        "        D = self.D_graph\n",
        "        num_metrics = len(metrics_keys)\n",
        "\n",
        "        fig, axes = plt.subplots(1, num_metrics, figsize=(4*num_metrics,4))\n",
        "        if num_metrics == 1: axes = [axes]\n",
        "\n",
        "        for k, key in enumerate(metrics_keys):\n",
        "            data = fuzzy_tensor[:,:,k]\n",
        "            im = axes[k].imshow(data, cmap='viridis', vmin=0, vmax=1)\n",
        "            for i in range(D):\n",
        "                for j in range(D):\n",
        "                    axes[k].text(j,i,f\"{data[i,j]:.2f}\",ha='center',va='center',color='white',fontsize=9)\n",
        "            axes[k].set_xticks(range(D))\n",
        "            axes[k].set_yticks(range(D))\n",
        "            axes[k].set_xticklabels([f'Node {j}' for j in range(D)])\n",
        "            axes[k].set_yticklabels([f'Node {i}' for i in range(D)])\n",
        "            axes[k].set_title(f'FMT - {key}')\n",
        "\n",
        "        fig.colorbar(im, ax=axes, orientation='vertical', fraction=0.025, pad=0.04, label='Normalized Metric Value')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def compute_fmt_with_elite_bounds(self, top_k=21):\n",
        "        \"\"\"\n",
        "        Computes FMT bounds using pointwise min/max across elite solutions.\n",
        "        Returns tensor shape (D,D,num_metrics,2) [lower, upper].\n",
        "        \"\"\"\n",
        "        metrics_keys = self.MK\n",
        "        D = self.D_graph\n",
        "        num_metrics = len(metrics_keys)\n",
        "        tensor_bounds = np.zeros((D,D,num_metrics,2))\n",
        "\n",
        "        metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "\n",
        "        for i in range(D):\n",
        "            # Generate top_k perturbations around current nested_rep (like in plot_pointwise_minmax_elite)\n",
        "            base = self.nested_reps[i]\n",
        "            reps = np.clip(base + np.random.normal(0,0.05,(top_k,len(base))),0,1)\n",
        "\n",
        "            # Compute node metrics for each perturbed solution\n",
        "            metrics_matrix = np.zeros((top_k, num_metrics))\n",
        "            for idx, rep in enumerate(reps):\n",
        "                m = metrics_evaluator.compute_node_metrics(i, y=rep)\n",
        "                metrics_matrix[idx,:] = [m[k] for k in metrics_keys]\n",
        "\n",
        "            # Compute pointwise min/max across elite solutions\n",
        "            lower_i = metrics_matrix.min(axis=0)\n",
        "            upper_i = metrics_matrix.max(axis=0)\n",
        "\n",
        "            # Fill bounds tensor for all source nodes (i->j)\n",
        "            for j in range(D):\n",
        "                tensor_bounds[i,j,:,0] = lower_i\n",
        "                tensor_bounds[i,j,:,1] = upper_i\n",
        "\n",
        "        return tensor_bounds\n",
        "\n",
        "\n",
        "    def plot_fmt_with_bounds(self, fmt_tensor_bounds):\n",
        "        \"\"\"\n",
        "        Plot the FMT with lower/upper bounds, applying outer-loop weights if available.\n",
        "        Single row: mean across all source nodes\n",
        "        Columns: metrics (mean across target nodes)\n",
        "        \"\"\"\n",
        "        D = self.D_graph\n",
        "        metrics_keys = self.MK\n",
        "        M_actual = len(metrics_keys)\n",
        "\n",
        "        # Compute mean value per metric across all target nodes\n",
        "        mean_vals = (fmt_tensor_bounds[:, :, :, 0] + fmt_tensor_bounds[:, :, :, 1]) / 2  # (D, D, M_actual)\n",
        "        mean_vals = mean_vals.mean(axis=1)  # mean across targets -> (D, M_actual)\n",
        "\n",
        "        # Take mean across nodes to reduce to 1xM\n",
        "        mean_vals = mean_vals.mean(axis=0, keepdims=True)  # shape (1, M_actual)\n",
        "\n",
        "        # Apply outer-loop weights if available (optional: mean weight)\n",
        "        if hasattr(self, 'best_alpha') and hasattr(self, 'best_w_contrib'):\n",
        "            mean_weight = (self.best_alpha * self.best_w_contrib).mean()\n",
        "            mean_vals = mean_vals * mean_weight\n",
        "\n",
        "        # Normalize for heatmap clarity\n",
        "        mean_vals_norm = (mean_vals - mean_vals.min()) / (mean_vals.max() - mean_vals.min() + 1e-12)\n",
        "\n",
        "        # Plot\n",
        "        fig, ax = plt.subplots(figsize=(1.2*M_actual + 4, 2))\n",
        "        im = ax.imshow(mean_vals_norm, cmap='viridis', aspect='auto', vmin=0, vmax=1)\n",
        "\n",
        "        # Annotate cells\n",
        "        for i in range(mean_vals.shape[0]):  # only 1 row\n",
        "            for k in range(M_actual):\n",
        "                ax.text(k, i, f\"{mean_vals[0,k]:.2f}\", ha='center', va='center', color='white', fontsize=8)\n",
        "\n",
        "        ax.set_xticks(range(M_actual))\n",
        "        ax.set_xticklabels(metrics_keys[:M_actual], rotation=45, ha='right')\n",
        "        ax.set_yticks([0])\n",
        "        ax.set_yticklabels(['Mean across nodes'])\n",
        "        ax.set_title(\"Weighted FMT with Bounds (Collapsed to 1 Row)\")\n",
        "        fig.colorbar(im, ax=ax, label='Weighted Mean Metric Value')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def plot_node_score_contribution(self, metrics_keys=METRIC_KEYS):\n",
        "        \"\"\"\n",
        "        Plot per-node total score contribution:\n",
        "            - 3 panels: Raw, FMT (interaction), Total\n",
        "            - Diagonal represents raw contributions\n",
        "            - FMT scaled by best_weights if available\n",
        "            - Annotated cells\n",
        "            - Normalized across metrics for consistent visualization\n",
        "        \"\"\"\n",
        "        D = self.D_graph\n",
        "        node_contributions = np.array(self.node_score_contributions)\n",
        "\n",
        "        # --- FMT contribution ---\n",
        "        if hasattr(self, 'weighted_fmt'):\n",
        "            fuzzy_tensor = np.array(self.weighted_fmt)\n",
        "        else:\n",
        "            fuzzy_tensor = self.compute_fuzzy_metric_tensor(normalize=True)\n",
        "\n",
        "        # Normalize across entire tensor for plotting\n",
        "        fuzzy_tensor_norm = (fuzzy_tensor - fuzzy_tensor.min()) / (fuzzy_tensor.max() - fuzzy_tensor.min() + 1e-12)\n",
        "        fmt_matrix = fuzzy_tensor_norm.sum(axis=2)  # sum over metrics\n",
        "        np.fill_diagonal(fmt_matrix, 0)\n",
        "\n",
        "        # --- Raw matrix on diagonal ---\n",
        "        raw_matrix = np.zeros((D,D))\n",
        "        np.fill_diagonal(raw_matrix, node_contributions)\n",
        "\n",
        "        # --- Total contribution ---\n",
        "        total_matrix = raw_matrix + fmt_matrix\n",
        "\n",
        "        # --- Global min/max for color scale ---\n",
        "        global_min, global_max = total_matrix.min(), total_matrix.max()\n",
        "\n",
        "        # --- Plot ---\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "        matrices = [raw_matrix, fmt_matrix, total_matrix]\n",
        "        titles = [\"Raw Node Contribution\", \"Normalized FMT Contribution\", \"Total Contribution\"]\n",
        "\n",
        "        for ax, mat, title in zip(axes, matrices, titles):\n",
        "            im = ax.imshow(mat, cmap='viridis', vmin=0, vmax=1)  # normalized\n",
        "            for i in range(D):\n",
        "                for j in range(D):\n",
        "                    ax.text(j, i, f\"{mat[i,j]:.2f}\", ha='center', va='center', color='white', fontsize=8)\n",
        "            ax.set_title(title)\n",
        "            ax.set_xticks(range(D))\n",
        "            ax.set_xticklabels([f\"Node {i+1}\" for i in range(D)])\n",
        "            ax.set_yticks(range(D))\n",
        "            ax.set_yticklabels([f\"Node {i+1}\" for i in range(D)])\n",
        "\n",
        "        fig.colorbar(im, ax=axes, orientation='vertical', fraction=0.025, pad=0.04, label='Contribution Value')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def correlate_fmt_interactions_per_node(self, fmt_bounds=None, interaction_tensor=None, verbose=True):\n",
        "        \"\"\"\n",
        "        Correlate the FMT bounds with inter-layer interactions per node and per metric.\n",
        "        Returns a dict of shape: {node_idx: {metric: {'r':..., 'p':...}}}.\n",
        "        \"\"\"\n",
        "        from scipy.stats import pearsonr\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        metrics_keys = self.MK\n",
        "        D = self.D_graph\n",
        "\n",
        "        # Compute tensors if not provided\n",
        "        if fmt_bounds is None:\n",
        "            fmt_bounds = self.compute_fmt_with_elite_bounds(top_k=21)\n",
        "        if interaction_tensor is None:\n",
        "            interaction_tensor = self.print_interactions(return_tensor=True, verbose=False)\n",
        "\n",
        "        # Reduce interaction tensor along inter_dim\n",
        "        inter_mean = interaction_tensor.mean(axis=2)  # (D,D)\n",
        "\n",
        "        node_correlations = {}\n",
        "\n",
        "        for i in range(D):\n",
        "            node_correlations[i] = {}\n",
        "            for k, key in enumerate(metrics_keys):\n",
        "                # FMT bounds for target node j from source i (mean of lower/upper)\n",
        "                fmt_mean = fmt_bounds[i,:,k,:].mean(axis=1)  # shape (D,)\n",
        "                # Interaction tensor for edges from node i to j\n",
        "                inter_vec = inter_mean[i,:]  # shape (D,)\n",
        "                # Pearson correlation\n",
        "                corr, pval = pearsonr(fmt_mean, inter_vec)\n",
        "                node_correlations[i][key] = {'r': corr, 'p': pval}\n",
        "\n",
        "                if verbose:\n",
        "                    print(f\"Node {i} | {key}: r = {corr:.3f}, p = {pval:.3e}\")\n",
        "                    plt.figure(figsize=(4,3))\n",
        "                    plt.scatter(fmt_mean, inter_vec, alpha=0.7, edgecolor='k', color='skyblue')\n",
        "                    plt.xlabel(f\"FMT {key} (Node {i} -> others)\")\n",
        "                    plt.ylabel(f\"Interaction mean (Node {i} -> others)\")\n",
        "                    plt.title(f\"Node {i} | {key} correlation: r={corr:.3f}\")\n",
        "                    plt.grid(True)\n",
        "                    plt.show()\n",
        "\n",
        "        return node_correlations\n",
        "\n",
        "    def plot_fmt_with_run_metrics(self, metrics_keys=None):\n",
        "        \"\"\"\n",
        "        Plot FMT heatmaps using the actual weighted FMT metrics from the last run_outer.\n",
        "        Rows: source nodes\n",
        "        Columns: metrics (mean across target nodes)\n",
        "        \"\"\"\n",
        "        if metrics_keys is None:\n",
        "            metrics_keys = self.MK\n",
        "\n",
        "        D = self.D_graph\n",
        "        M_actual = len(metrics_keys)\n",
        "\n",
        "        # Use the weighted FMT from the last run_outer\n",
        "        if not hasattr(self, 'weighted_fmt'):\n",
        "            raise ValueError(\"Weighted FMT not available. Run run_outer() first.\")\n",
        "\n",
        "        weighted_fmt = np.array(self.weighted_fmt)  # shape (D, D, M_actual)\n",
        "        mean_vals = weighted_fmt.mean(axis=1)       # mean across target nodes -> (D, M_actual)\n",
        "\n",
        "        # Normalize for visualization\n",
        "        mean_vals_norm = (mean_vals - mean_vals.min()) / (mean_vals.max() - mean_vals.min() + 1e-12)\n",
        "\n",
        "        # Plot\n",
        "        fig, ax = plt.subplots(figsize=(1.2*M_actual + 4, 0.35*D + 4))\n",
        "        im = ax.imshow(mean_vals_norm, cmap='viridis', aspect='auto', vmin=0, vmax=1)\n",
        "\n",
        "        # Annotate cells\n",
        "        for i in range(D):\n",
        "            for k in range(M_actual):\n",
        "                ax.text(k, i, f\"{mean_vals[i,k]:.2f}\", ha='center', va='center', color='white', fontsize=8)\n",
        "\n",
        "        ax.set_xticks(range(M_actual))\n",
        "        ax.set_xticklabels(metrics_keys[:M_actual], rotation=45, ha='right')\n",
        "        ax.set_yticks(range(D))\n",
        "        ax.set_yticklabels([f\"Node {i}\" for i in range(D)])\n",
        "        ax.set_title(\"Weighted FMT Metrics (Run Output)\")\n",
        "        fig.colorbar(im, ax=ax, label='Weighted Metric Value')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def correlation_penalty(self, fmt_bounds=None, interaction_tensor=None):\n",
        "        \"\"\"\n",
        "        Computes a penalty term that is high if per-node FMT metrics correlate with interactions.\n",
        "        Returns total penalty to subtract from the outer score.\n",
        "        \"\"\"\n",
        "        from scipy.stats import pearsonr\n",
        "\n",
        "        D = self.D_graph\n",
        "        metrics_keys = self.MK\n",
        "        if fmt_bounds is None:\n",
        "            fmt_bounds = self.compute_fmt_with_elite_bounds(top_k=top_k)\n",
        "        if interaction_tensor is None:\n",
        "            interaction_tensor = self.print_interactions(return_tensor=True, verbose=False)\n",
        "\n",
        "        inter_mean = interaction_tensor.mean(axis=2)\n",
        "        total_penalty = 0.0\n",
        "\n",
        "        for i in range(D):\n",
        "            for k in range(len(metrics_keys)):\n",
        "                fmt_mean = fmt_bounds[i,:,k,:].mean(axis=1)\n",
        "                inter_vec = inter_mean[i,:]\n",
        "                if np.std(fmt_mean) > 1e-8 and np.std(inter_vec) > 1e-8:\n",
        "                    corr, _ = pearsonr(fmt_mean, inter_vec)\n",
        "                    total_penalty += abs(corr)  # penalize high correlation\n",
        "\n",
        "        # normalize by number of nodes × metrics\n",
        "        total_penalty /= (D * len(metrics_keys))**2\n",
        "        return total_penalty\n",
        "\n",
        "\n",
        "# ---------------- USAGE ----------------\n",
        "if __name__ == \"__main__\":\n",
        "    optimizer = Fuzzy_Hierarchical_Multiplex(\n",
        "        candidate_dims, D_graph,\n",
        "        inner_archive_size, inner_offspring,\n",
        "        outer_archive_size, outer_offspring,\n",
        "        synthetic_targets,\n",
        "        inner_learning, gamma_interlayer=1,\n",
        "        causal_flag=False\n",
        "    )\n",
        "    metrics_list = optimizer.run()\n",
        "    optimizer.plot_pointwise_minmax_elite()\n",
        "    optimizer.plot_nested_activations()\n",
        "    # Compute FMT with elite bounds\n",
        "    fmt_elite_bounds = optimizer.compute_fmt_with_elite_bounds(top_k=top_k+10)\n",
        "\n",
        "# Plot as heatmaps\n",
        "    optimizer.plot_fmt_with_run_metrics()\n",
        "\n",
        "    # Compute fuzzy multiplex tensor\n",
        "    fmt_tensor = optimizer.compute_fuzzy_metric_tensor(normalize=True)\n",
        "    optimizer.plot_fuzzy_metric_tensor_heatmaps(fmt_tensor)\n",
        "\n",
        "    # Compute FMT with bounds (minimax elite intervals)\n",
        "    optimizer.plot_node_score_contribution()\n",
        "    optimizer.plot_outer_fuzzy_graph()\n",
        "  #  optimizer.print_interactions()\n",
        "    tensor = optimizer.print_interactions()\n",
        "\n",
        "    print(\"Tensor shape:\", tensor.shape,'\\n',tensor)\n",
        "    # Compute tensors first\n",
        "\n",
        "    fmt_elite_bounds = optimizer.compute_fmt_with_elite_bounds(top_k=top_k)\n",
        "    interaction_tensor = optimizer.print_interactions(return_tensor=True, verbose=False)\n",
        "    optimizer.plot_fmt_with_bounds(fmt_elite_bounds)\n",
        "\n",
        "    #interaction_tensor = optimizer.print_interactions(return_tensor=True, verbose=False)\n",
        "\n",
        "    # Get per-node, per-metric correlations\n",
        "    #node_metric_corrs = optimizer.correlate_fmt_interactions_per_node(\n",
        "     #   fmt_bounds=fmt_elite_bounds,\n",
        "      #  interaction_tensor=interaction_tensor\n",
        "   # )"
      ],
      "metadata": {
        "id": "Wat40fe53ytG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LIOQkpi5QKPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f4aDMyys3-iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Enn9kbCY8tYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G1MA4JIj9GXr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}