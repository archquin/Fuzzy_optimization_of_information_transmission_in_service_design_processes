{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data generation for service evaluation\n",
        "### TP <--> Data --> Metrics --> DSM\n",
        "\n",
        "__This is a fully contained script that generates a DSM, initializes it throught metrics of the system adn then optimizes it. Eventually, it is a service design optimizer that compares results to baseline. The case is a last mile delivery service and the results are at the end. Optimization of single TP and TP ascociation are done to derive the best DSM and then provided.__\n",
        "\n"
      ],
      "metadata": {
        "id": "YvBjVLjnNI0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "\n",
        "np.random.seed()\n",
        "\n",
        "# =====================================================\n",
        "# 1. SERVICE TOUCHPOINTS (Bai nodes)\n",
        "# =====================================================\n",
        "TOUCHPOINTS = [\n",
        "    \"GenerateDemand\",\n",
        "    \"NavigateRoute\",\n",
        "    \"TrafficAssessment\",\n",
        "    \"LoadCheck\",\n",
        "    \"PackageHandling\",\n",
        "    \"DeliveryExecution\",\n",
        "    \"PaymentConfirmation\",\n",
        "    \"Feedback\"\n",
        "]\n",
        "\n",
        "# =====================================================\n",
        "# 2. PRE-OPTIMIZED (AS-IS) PROCESS ORDER\n",
        "# =====================================================\n",
        "PRE_OPTIMIZED_ORDER = TOUCHPOINTS.copy()\n",
        "\n",
        "# =====================================================\n",
        "# 3. BAI-STYLE DSM (FUZZY EXTENSION)\n",
        "# =====================================================\n",
        "DSM = np.array([\n",
        "    [0,   1,   0,   0,   0,   0,   0,   0],\n",
        "    [0,   0,   0.7, 0,   0,   1,   0,   0],\n",
        "    [0,   0.6, 0,   0.4, 0,   1,   0,   0],\n",
        "    [0,   0,   0.5, 0,   0.8, 1,   0,   0],\n",
        "    [0,   0,   0,   0.6, 0,   1,   0,   0],\n",
        "    [0,   0,   0.4, 0,   0,   0,   1,   0],\n",
        "    [0,   0,   0,   0,   0,   0,   0,   1],\n",
        "    [0,   0,   0,   0,   0,   0,   0,   0],\n",
        "])\n",
        "\n",
        "# =====================================================\n",
        "# 4. BUILD DSM GRAPH\n",
        "# =====================================================\n",
        "G = nx.DiGraph()\n",
        "for i, u in enumerate(TOUCHPOINTS):\n",
        "    for j, v in enumerate(TOUCHPOINTS):\n",
        "        if DSM[i, j] > 0:\n",
        "            G.add_edge(u, v, weight=DSM[i, j])\n",
        "\n",
        "# =====================================================\n",
        "# 5. STRONGLY CONNECTED COMPONENTS (COUPLED MODULES)\n",
        "# =====================================================\n",
        "SCCS = list(nx.strongly_connected_components(G))\n",
        "\n",
        "# =====================================================\n",
        "# 6. REDUCED GRAPH + LEVELING (BAI DEFINITION 5)\n",
        "# =====================================================\n",
        "module_map = {}\n",
        "for idx, comp in enumerate(SCCS):\n",
        "    for node in comp:\n",
        "        module_map[node] = idx\n",
        "\n",
        "MG = nx.DiGraph()\n",
        "for u, v in G.edges():\n",
        "    mu, mv = module_map[u], module_map[v]\n",
        "    if mu != mv:\n",
        "        MG.add_edge(mu, mv)\n",
        "\n",
        "LEVELS = list(nx.topological_generations(MG))\n",
        "\n",
        "# =====================================================\n",
        "# 7. TOUCHPOINT DATA GENERATORS\n",
        "# =====================================================\n",
        "def gen_generate_demand():\n",
        "    return {\"demand_volume\": np.random.randint(1, 50), \"urgency\": np.random.rand()}\n",
        "\n",
        "def gen_navigate_route():\n",
        "    return {\"distance_km\": np.random.uniform(1, 200), \"planned_speed\": np.random.uniform(30, 80)}\n",
        "\n",
        "def gen_traffic_assessment():\n",
        "    return {\"congestion\": np.random.rand(), \"weather_risk\": np.random.rand()}\n",
        "\n",
        "def gen_load_check():\n",
        "    return {\"capacity_ratio\": np.random.uniform(0.2, 1.0), \"current_load\": np.random.randint(1, 200)}\n",
        "\n",
        "def gen_package_handling():\n",
        "    return {\"handling_time_min\": np.random.uniform(5, 60), \"fragility\": np.random.rand()}\n",
        "\n",
        "def gen_delivery_execution():\n",
        "    return {\"actual_speed\": np.random.uniform(20, 90), \"delay_minutes\": np.random.uniform(0, 120)}\n",
        "\n",
        "def gen_payment_confirmation():\n",
        "    return {\"payment_success\": np.random.choice([0, 1]), \"processing_time_sec\": np.random.uniform(5, 60)}\n",
        "\n",
        "def gen_feedback():\n",
        "    return {\"satisfaction\": np.random.uniform(0, 5), \"complaint\": np.random.choice([0, 1], p=[0.85, 0.15])}\n",
        "\n",
        "GENERATOR_MAP = {\n",
        "    \"GenerateDemand\": gen_generate_demand,\n",
        "    \"NavigateRoute\": gen_navigate_route,\n",
        "    \"TrafficAssessment\": gen_traffic_assessment,\n",
        "    \"LoadCheck\": gen_load_check,\n",
        "    \"PackageHandling\": gen_package_handling,\n",
        "    \"DeliveryExecution\": gen_delivery_execution,\n",
        "    \"PaymentConfirmation\": gen_payment_confirmation,\n",
        "    \"Feedback\": gen_feedback,\n",
        "}\n",
        "\n",
        "# =====================================================\n",
        "# 8. EXECUTION FUNCTIONS (LONG-FORM, NO NaNs)\n",
        "# =====================================================\n",
        "def run_pre_optimized(order):\n",
        "    rows = []\n",
        "    for step, tp in enumerate(order, 1):\n",
        "        for k, v in GENERATOR_MAP[tp]().items():\n",
        "            rows.append({\n",
        "                \"process_state\": \"pre_optimized\",\n",
        "                \"touchpoint\": tp,\n",
        "                \"step\": step,\n",
        "                \"variable\": k,\n",
        "                \"value\": v\n",
        "            })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "def run_post_optimized(levels, sccs):\n",
        "    rows = []\n",
        "    for lvl, modules in enumerate(levels, 1):\n",
        "        for m in modules:\n",
        "            for tp in sccs[m]:\n",
        "                for k, v in GENERATOR_MAP[tp]().items():\n",
        "                    rows.append({\n",
        "                        \"process_state\": \"post_optimized\",\n",
        "                        \"touchpoint\": tp,\n",
        "                        \"level\": lvl,\n",
        "                        \"module\": m,\n",
        "                        \"variable\": k,\n",
        "                        \"value\": v\n",
        "                    })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# =====================================================\n",
        "# 9. RUN EVERYTHING\n",
        "# =====================================================\n",
        "df_pre = run_pre_optimized(PRE_OPTIMIZED_ORDER)\n",
        "df_post = run_post_optimized(LEVELS, SCCS)\n",
        "\n",
        "df_all = pd.concat([df_pre, df_post], ignore_index=True)\n",
        "\n",
        "# =====================================================\n",
        "# 10. OUTPUT SUMMARY\n",
        "# =====================================================\n",
        "print(\"\\n=== PRE-OPTIMIZED SAMPLE ===\")\n",
        "print(df_pre.head())\n",
        "\n",
        "print(\"\\n=== POST-OPTIMIZED SAMPLE ===\")\n",
        "print(df_post.head())\n",
        "\n",
        "print(\"\\n=== HIERARCHICAL LEVELS (BAI) ===\")\n",
        "for i, lvl in enumerate(LEVELS, 1):\n",
        "    print(f\"Level {i}: {[list(SCCS[m]) for m in lvl]}\")\n",
        "\n",
        "D_graph = 8\n",
        "candidate_dims = [[2],[2],[2],[2],[2],[2],[2],[2]]#,[1],]\n"
      ],
      "metadata": {
        "id": "DtNuv6KRNL1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Dummy DSM optimization"
      ],
      "metadata": {
        "id": "pnChe6lmjNof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ==========================================\n",
        "# 1. SETUP & DATA GENERATION (From Input)\n",
        "# ==========================================\n",
        "np.random.seed(42) # Fixed seed for reproducibility\n",
        "N = 1000  # Reduced N slightly for speed in demonstration\n",
        "\n",
        "# --- Domain 1: Route & Navigation ---\n",
        "route_nav = pd.DataFrame({\n",
        "    'distance_km': np.random.uniform(1, 200, N),\n",
        "    'planned_speed_kmh': np.random.uniform(20, 80, N),\n",
        "    'actual_speed_kmh': np.random.uniform(15, 90, N),\n",
        "    'eta_minutes': np.random.uniform(10, 300, N),\n",
        "    'route_risk_score': np.random.uniform(0, 1, N),\n",
        "    'fuel_capacity_l': np.random.uniform(20, 200, N),\n",
        "    'fuel_used_l': np.random.uniform(5, 180, N),\n",
        "})\n",
        "route_nav['speed_variance'] = route_nav['planned_speed_kmh'] - route_nav['actual_speed_kmh']\n",
        "\n",
        "# --- Domain 2: Traffic & Weather ---\n",
        "traffic_weather = pd.DataFrame({\n",
        "    'traffic_congestion': np.random.uniform(0, 1, N),\n",
        "    'road_condition_score': np.random.uniform(0, 1, N),\n",
        "    'visibility_km': np.random.uniform(0.5, 10, N),\n",
        "    'storm_probability': np.random.uniform(0, 1, N),\n",
        "    'precipitation_mm': np.random.uniform(0, 50, N),\n",
        "})\n",
        "\n",
        "# --- Domain 3: Delivery Load ---\n",
        "delivery_load = pd.DataFrame({\n",
        "    'max_packages': np.random.randint(10, 200, N),\n",
        "    'current_packages': np.random.randint(1, 200, N),\n",
        "    'package_weight_kg': np.random.uniform(0.5, 50, N),\n",
        "})\n",
        "delivery_load['load_ratio'] = delivery_load['current_packages'] / delivery_load['max_packages']\n",
        "\n",
        "# --- Domain 4: Package Handling ---\n",
        "package_handling = pd.DataFrame({\n",
        "    'package_type': np.random.choice(['standard', 'fragile', 'perishable', 'oversize'], N),\n",
        "    'handling_speed_pph': np.random.uniform(5, 50, N),\n",
        "    'storage_temp_req_C': np.random.uniform(-5, 25, N),\n",
        "})\n",
        "# Numeric encoding for matrix\n",
        "package_handling['package_type_id'] = pd.factorize(package_handling['package_type'])[0]\n",
        "package_handling_numeric = package_handling.drop(columns=['package_type'])\n",
        "\n",
        "# --- Domain 5: Temporal & Hub ---\n",
        "temporal_hub = pd.DataFrame({\n",
        "    'pickup_hour': np.random.randint(0, 24, N),\n",
        "    'pickup_day': np.random.randint(0, 7, N),\n",
        "    'hub_congestion_level': np.random.uniform(0, 1, N),\n",
        "    'delivery_delay_minutes': np.random.uniform(0, 120, N),\n",
        "})\n",
        "\n",
        "# List of dataframes (Nodes)\n",
        "domains = [route_nav, traffic_weather, delivery_load, package_handling_numeric, temporal_hub]\n",
        "domain_names = [\"Route\", \"Traffic/Wx\", \"Load\", \"Handling\", \"Hub/Time\"]\n",
        "\n",
        "# Combine for global matrix\n",
        "DATA_MATRIX = np.hstack([df.values for df in domains])\n",
        "# Normalize\n",
        "DATA_MATRIX = (DATA_MATRIX - DATA_MATRIX.min(axis=0)) / (np.ptp(DATA_MATRIX, axis=0) + 1e-8)\n",
        "\n",
        "# ==========================================\n",
        "# 2. DSM CONSTRUCTION\n",
        "# ==========================================\n",
        "\n",
        "def calculate_dsm(domains, domain_names):\n",
        "    \"\"\"\n",
        "    Calculates the Design Structure Matrix (DSM) based on correlation\n",
        "    between the features of different domains.\n",
        "    \"\"\"\n",
        "    n_domains = len(domains)\n",
        "    dsm = np.zeros((n_domains, n_domains))\n",
        "\n",
        "    # 1. Global Correlation Matrix of all raw features\n",
        "    all_data = pd.concat(domains, axis=1)\n",
        "    # Ensure all data is numeric\n",
        "    all_data = all_data.select_dtypes(include=[np.number])\n",
        "    corr_matrix = all_data.corr().abs()\n",
        "\n",
        "    # 2. Block-wise aggregation to Node-level DSM\n",
        "    # We iterate through the columns belonging to domain i and domain j\n",
        "    # and average their interaction strength.\n",
        "\n",
        "    col_idx = 0\n",
        "    domain_ranges = []\n",
        "    for df in domains:\n",
        "        cols = df.select_dtypes(include=[np.number]).shape[1]\n",
        "        domain_ranges.append((col_idx, col_idx + cols))\n",
        "        col_idx += cols\n",
        "\n",
        "    for i in range(n_domains):\n",
        "        for j in range(n_domains):\n",
        "            if i == j:\n",
        "                dsm[i, j] = 1.0 # Self-dependency\n",
        "            else:\n",
        "                r_i = domain_ranges[i]\n",
        "                r_j = domain_ranges[j]\n",
        "\n",
        "                # Extract the sub-block of correlations between domain i and j\n",
        "                # We use iloc on the correlation matrix\n",
        "                sub_block = corr_matrix.iloc[r_i[0]:r_i[1], r_j[0]:r_j[1]]\n",
        "\n",
        "                # The DSM value is the mean interaction strength\n",
        "                dsm[i, j] = sub_block.values.mean()\n",
        "\n",
        "    return pd.DataFrame(dsm, index=domain_names, columns=domain_names)\n",
        "\n",
        "DSM = calculate_dsm(domains, domain_names)\n",
        "\n",
        "print(\"--- Design Structure Matrix (Interaction Density) ---\")\n",
        "print(DSM.round(3))\n",
        "print(\"\\n\")\n",
        "\n",
        "# ==========================================\n",
        "# 3. METRIC EVALUATION (Cost & Quality)\n",
        "# ==========================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "\n",
        "np.random.seed()\n",
        "\n",
        "# =====================================================\n",
        "# 1. SERVICE TOUCHPOINTS (BAI Nodes)\n",
        "# =====================================================\n",
        "TOUCHPOINTS = [\n",
        "    \"GenerateDemand\",\n",
        "    \"NavigateRoute\",\n",
        "    \"TrafficAssessment\",\n",
        "    \"LoadCheck\",\n",
        "    \"PackageHandling\",\n",
        "    \"DeliveryExecution\",\n",
        "    \"PaymentConfirmation\",\n",
        "    \"Feedback\"\n",
        "]\n",
        "\n",
        "# =====================================================\n",
        "# 2. PRE-OPTIMIZED (AS-IS) PROCESS ORDER\n",
        "# =====================================================\n",
        "PRE_OPTIMIZED_ORDER = TOUCHPOINTS.copy()\n",
        "\n",
        "# =====================================================\n",
        "# 3. BAI-STYLE DSM (FUZZY DEPENDENCIES)\n",
        "# =====================================================\n",
        "DSM_MATRIX = np.array([\n",
        "    [0,   1,   0,   0,   0,   0,   0,   0],\n",
        "    [0,   0,   0.7, 0,   0,   1,   0,   0],\n",
        "    [0,   0.6, 0,   0.4, 0,   1,   0,   0],\n",
        "    [0,   0,   0.5, 0,   0.8, 1,   0,   0],\n",
        "    [0,   0,   0,   0.6, 0,   1,   0,   0],\n",
        "    [0,   0,   0.4, 0,   0,   0,   1,   0],\n",
        "    [0,   0,   0,   0,   0,   0,   0,   1],\n",
        "    [0,   0,   0,   0,   0,   0,   0,   0],\n",
        "])\n",
        "\n",
        "DSM = pd.DataFrame(\n",
        "    DSM_MATRIX,\n",
        "    index=TOUCHPOINTS,\n",
        "    columns=TOUCHPOINTS\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 4. BUILD DSM GRAPH\n",
        "# =====================================================\n",
        "G = nx.DiGraph()\n",
        "for i, u in enumerate(TOUCHPOINTS):\n",
        "    for j, v in enumerate(TOUCHPOINTS):\n",
        "        if DSM_MATRIX[i, j] > 0:\n",
        "            G.add_edge(u, v, weight=DSM_MATRIX[i, j])\n",
        "\n",
        "# =====================================================\n",
        "# 5. STRONGLY CONNECTED COMPONENTS (COUPLED MODULES)\n",
        "# =====================================================\n",
        "SCCS = list(nx.strongly_connected_components(G))\n",
        "\n",
        "# =====================================================\n",
        "# 6. REDUCED GRAPH + LEVELING (BAI DEFINITION 5)\n",
        "# =====================================================\n",
        "module_map = {}\n",
        "for idx, comp in enumerate(SCCS):\n",
        "    for node in comp:\n",
        "        module_map[node] = idx\n",
        "\n",
        "MG = nx.DiGraph()\n",
        "for u, v in G.edges():\n",
        "    mu, mv = module_map[u], module_map[v]\n",
        "    if mu != mv:\n",
        "        MG.add_edge(mu, mv)\n",
        "\n",
        "LEVELS = list(nx.topological_generations(MG))\n",
        "\n",
        "# =====================================================\n",
        "# 7. TOUCHPOINT DATA GENERATORS\n",
        "# =====================================================\n",
        "def gen_generate_demand():\n",
        "    return {\"demand_volume\": np.random.randint(1, 50), \"urgency\": np.random.rand()}\n",
        "\n",
        "def gen_navigate_route():\n",
        "    return {\"distance_km\": np.random.uniform(1, 200), \"planned_speed\": np.random.uniform(30, 80)}\n",
        "\n",
        "def gen_traffic_assessment():\n",
        "    return {\"congestion\": np.random.rand(), \"weather_risk\": np.random.rand()}\n",
        "\n",
        "def gen_load_check():\n",
        "    return {\"capacity_ratio\": np.random.uniform(0.2, 1.0), \"current_load\": np.random.randint(1, 200)}\n",
        "\n",
        "def gen_package_handling():\n",
        "    return {\"handling_time_min\": np.random.uniform(5, 60), \"fragility\": np.random.rand()}\n",
        "\n",
        "def gen_delivery_execution():\n",
        "    return {\"actual_speed\": np.random.uniform(20, 90), \"delay_minutes\": np.random.uniform(0, 120)}\n",
        "\n",
        "def gen_payment_confirmation():\n",
        "    return {\"payment_success\": np.random.choice([0, 1]), \"processing_time_sec\": np.random.uniform(5, 60)}\n",
        "\n",
        "def gen_feedback():\n",
        "    return {\"satisfaction\": np.random.uniform(0, 5), \"complaint\": np.random.choice([0, 1], p=[0.85, 0.15])}\n",
        "\n",
        "GENERATOR_MAP = {\n",
        "    \"GenerateDemand\": gen_generate_demand,\n",
        "    \"NavigateRoute\": gen_navigate_route,\n",
        "    \"TrafficAssessment\": gen_traffic_assessment,\n",
        "    \"LoadCheck\": gen_load_check,\n",
        "    \"PackageHandling\": gen_package_handling,\n",
        "    \"DeliveryExecution\": gen_delivery_execution,\n",
        "    \"PaymentConfirmation\": gen_payment_confirmation,\n",
        "    \"Feedback\": gen_feedback,\n",
        "}\n",
        "\n",
        "# =====================================================\n",
        "# 8. EXECUTION FUNCTIONS\n",
        "# =====================================================\n",
        "def run_pre_optimized(order):\n",
        "    rows = []\n",
        "    for step, tp in enumerate(order, 1):\n",
        "        for k, v in GENERATOR_MAP[tp]().items():\n",
        "            rows.append({\n",
        "                \"process_state\": \"pre_optimized\",\n",
        "                \"touchpoint\": tp,\n",
        "                \"step\": step,\n",
        "                \"variable\": k,\n",
        "                \"value\": v\n",
        "            })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "def run_post_optimized(levels, sccs):\n",
        "    rows = []\n",
        "    for lvl, modules in enumerate(levels, 1):\n",
        "        for m in modules:\n",
        "            for tp in sccs[m]:\n",
        "                for k, v in GENERATOR_MAP[tp]().items():\n",
        "                    rows.append({\n",
        "                        \"process_state\": \"post_optimized\",\n",
        "                        \"touchpoint\": tp,\n",
        "                        \"level\": lvl,\n",
        "                        \"module\": m,\n",
        "                        \"variable\": k,\n",
        "                        \"value\": v\n",
        "                    })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# =====================================================\n",
        "# 9. RUN\n",
        "# =====================================================\n",
        "df_pre = run_pre_optimized(PRE_OPTIMIZED_ORDER)\n",
        "df_post = run_post_optimized(LEVELS, SCCS)\n",
        "\n",
        "df_all = pd.concat([df_pre, df_post], ignore_index=True)\n",
        "\n",
        "class MetricsEvaluator:\n",
        "    def __init__(self):\n",
        "        self.optimizer = BranchBoundOptimizer(minimize=False)\n",
        "\n",
        "        self.metric_feature_map = {\n",
        "            'DeliveryTime': ['Distance', 'Speed', 'Load'],\n",
        "            'TravelDistance': ['Distance', 'PackageSize'],\n",
        "            'OperationalCost': ['Distance', 'Load', 'Capacity'],\n",
        "            'EnergyUse': ['Distance', 'Speed'],\n",
        "            'CustomerSatisfaction': ['DeliveryTime']\n",
        "        }\n",
        "\n",
        "    def compute_system_metrics(self, df_exec):\n",
        "        features = extract_features_from_execution(df_exec)\n",
        "\n",
        "        # --- Operational Cost ---\n",
        "        val_cost = self.optimizer.optimize(\n",
        "            features={k: features[k] for k in self.metric_feature_map['OperationalCost']},\n",
        "            metric_mask=[0, 0, 1, 0, 0]\n",
        "        )\n",
        "        operational_cost = 0.2 * val_cost + 0.1\n",
        "\n",
        "        # --- Quality (Time + Satisfaction) ---\n",
        "        val_quality = self.optimizer.optimize(\n",
        "            features=features,\n",
        "            metric_mask=[1, 0, 0, 0, 1]\n",
        "        )\n",
        "        quality = (\n",
        "            METRIC_FORMULAS[0](val_quality) +\n",
        "            METRIC_FORMULAS[4](val_quality)\n",
        "        )\n",
        "\n",
        "        return operational_cost, quality\n",
        "METRIC_FORMULAS = [\n",
        "    lambda x: np.exp(-x / 120),   # DeliveryTime\n",
        "    lambda x: np.exp(-x / 100),   # TravelDistance\n",
        "    lambda x: 0.2 * x + 0.1,      # OperationalCost\n",
        "    lambda x: x / 50,             # EnergyUse\n",
        "    lambda x: np.exp(-x / 60),    # CustomerSatisfaction\n",
        "]\n",
        "class BranchBoundOptimizer:\n",
        "    def __init__(self, tol=1e-3, max_depth=20, minimize=True, value_range=(0.0, 5.0)):\n",
        "        self.tol = tol\n",
        "        self.max_depth = max_depth\n",
        "        self.minimize = minimize\n",
        "        self.value_range = value_range\n",
        "\n",
        "    def optimize(self, features, y=None, metric_mask=None):\n",
        "        base = np.mean(list(features.values())) if features else 0.5\n",
        "        a0, b0 = self.value_range\n",
        "        a0 += base\n",
        "        b0 += base\n",
        "\n",
        "        work = [(a0, b0, 0)]\n",
        "        best_x = None\n",
        "        best_score = np.inf if self.minimize else -np.inf\n",
        "\n",
        "        while work:\n",
        "            a, b, depth = work.pop()\n",
        "            mid = 0.5 * (a + b)\n",
        "\n",
        "            values = [\n",
        "                f(mid) if m else 0.0\n",
        "                for f, m in zip(METRIC_FORMULAS, metric_mask)\n",
        "            ]\n",
        "            score = sum(values)\n",
        "\n",
        "            if best_x is None or (\n",
        "                score < best_score if self.minimize else score > best_score\n",
        "            ):\n",
        "                best_x = mid\n",
        "                best_score = score\n",
        "\n",
        "            if depth < self.max_depth and (b - a) > self.tol:\n",
        "                work.append((a, mid, depth + 1))\n",
        "                work.append((mid, b, depth + 1))\n",
        "\n",
        "        return best_x\n",
        "def extract_features_from_execution(df):\n",
        "    features = {}\n",
        "\n",
        "    # Route & execution\n",
        "    features[\"Distance\"] = df.query(\"variable == 'distance_km'\")[\"value\"].mean()\n",
        "    features[\"Speed\"] = df.query(\"variable == 'actual_speed'\")[\"value\"].mean()\n",
        "\n",
        "    # Load & capacity\n",
        "    features[\"Load\"] = df.query(\"variable == 'current_load'\")[\"value\"].mean()\n",
        "    features[\"Capacity\"] = df.query(\"variable == 'capacity_ratio'\")[\"value\"].mean()\n",
        "\n",
        "    # Package attributes\n",
        "    features[\"PackageSize\"] = df.query(\"variable == 'fragility'\")[\"value\"].mean()\n",
        "    features[\"TempRequirement\"] = df.query(\"variable == 'handling_time_min'\")[\"value\"].mean()\n",
        "\n",
        "    # Delivery time proxy\n",
        "    features[\"DeliveryTime\"] = df.query(\"variable == 'delay_minutes'\")[\"value\"].mean()\n",
        "\n",
        "    # Fill missing values safely\n",
        "    for k, v in features.items():\n",
        "        if np.isnan(v):\n",
        "            features[k] = 0.5\n",
        "\n",
        "    return features\n",
        "evaluator = MetricsEvaluator()\n",
        "avg_op_cost, avg_quality = evaluator.compute_system_metrics(df_post)\n",
        "\n",
        "# Structural cost from BAI DSM (not correlation DSM)\n",
        "structural_cost = DSM_MATRIX.sum()\n",
        "\n",
        "print(\"\\n=== CALCULATION RESULTS ===\")\n",
        "print(f\"1. Structural Complexity Cost (DSM Coupling): {structural_cost:.4f}\")\n",
        "print(f\"2. Operational Cost Index:                  {avg_op_cost:.4f}\")\n",
        "print(f\"3. System Quality Score:                    {avg_quality:.4f}\")\n"
      ],
      "metadata": {
        "id": "eA78BmO1bVr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Metric evaluator and Optimizer"
      ],
      "metadata": {
        "id": "JgMF6YlmjTmf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiAi9jPvLSnp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# ======================================================\n",
        "# METRIC KEYS (Node-level metrics)\n",
        "# ======================================================\n",
        "METRIC_KEYS = [\n",
        "    \"GenerateDemand\",\n",
        "    \"NavigateRoute\",\n",
        "    \"TrafficAssessment\",\n",
        "    \"LoadCheck\",\n",
        "    \"PackageHandling\",\n",
        "    \"DeliveryExecution\",\n",
        "    \"PaymentConfirmation\",\n",
        "    \"Feedback\",\n",
        "    \"Aggregate\"\n",
        "]\n",
        "FEATURE_KEYS = [\n",
        "    'demand_volume',\n",
        "    'urgency',\n",
        "    'distance_km',\n",
        "    'planned_speed',\n",
        "    'congestion',\n",
        "    'weather_risk',\n",
        "    'capacity_ratio',\n",
        "    'current_load',\n",
        "    'handling_time_min',\n",
        "    'fragility',\n",
        "    'actual_speed',\n",
        "    'delay_minutes',\n",
        "    'payment_success',\n",
        "    'processing_time_sec',\n",
        "    'satisfaction',\n",
        "    'complaint'\n",
        "]\n",
        "# ======================================================\n",
        "# METRIC TARGET\n",
        "# Rows = nodes, Columns = metrics\n",
        "# ======================================================\n",
        "METRIC_TARGET = [\n",
        "    # GD NR TA LC PH DE PC FB AG\n",
        "    [1, 0, 0, 0, 0, 0, 0, 0, 1],  # GenerateDemand\n",
        "    [0, 1, 0, 0, 0, 0, 0, 0, 1],  # NavigateRoute\n",
        "    [0, 0, 1, 0, 0, 0, 0, 0, 1],  # TrafficAssessment\n",
        "    [0, 0, 0, 1, 0, 0, 0, 0, 1],  # LoadCheck\n",
        "    [0, 0, 0, 0, 1, 0, 0, 0, 1],  # PackageHandling\n",
        "    [0, 0, 0, 0, 0, 1, 0, 0, 1],  # DeliveryExecution\n",
        "    [0, 0, 0, 0, 0, 0, 1, 0, 1],  # PaymentConfirmation\n",
        "    [0, 0, 0, 0, 0, 0, 0, 1, 1],  # Feedback\n",
        "    [0, 0, 0, 0, 0, 0, 0, 0, 1],  # Aggregate\n",
        "]\n",
        "\n",
        "# ======================================================\n",
        "# METRIC FORMULAS\n",
        "# ======================================================\n",
        "METRIC_FORMULAS = [\n",
        "    lambda x: np.exp(-x),          # GenerateDemand\n",
        "    lambda x: np.exp(-x),          # NavigateRoute\n",
        "    lambda x: 0.2 * x + 0.1,       # TrafficAssessment\n",
        "    lambda x: x,                   # LoadCheck\n",
        "    lambda x: x,                   # PackageHandling\n",
        "    lambda x: np.exp(-x),          # DeliveryExecution\n",
        "    lambda x: np.exp(-x),          # PaymentConfirmation\n",
        "    lambda x: x,                   # Feedback\n",
        "    lambda x: np.mean(x),          # Aggregate\n",
        "]\n",
        "\n",
        "# ======================================================\n",
        "# METRIC INVERSES\n",
        "# ======================================================\n",
        "METRIC_INVERSES = [\n",
        "    lambda y: -np.log(y),          # GenerateDemand\n",
        "    lambda y: -np.log(y),          # NavigateRoute\n",
        "    lambda y: (y - 0.1) / 0.2,     # TrafficAssessment\n",
        "    lambda y: y,                   # LoadCheck\n",
        "    lambda y: y,                   # PackageHandling\n",
        "    lambda y: -np.log(y),          # DeliveryExecution\n",
        "    lambda y: -np.log(y),          # PaymentConfirmation\n",
        "    lambda y: y,                   # Feedback\n",
        "    lambda y: y,                   # Aggregate\n",
        "]\n",
        "\n",
        "# ======================================================\n",
        "# METRIC → FEATURE MAP\n",
        "# ======================================================\n",
        "metric_feature_map = {\n",
        "    \"GenerateDemand\": ['demand_volume', 'urgency'],\n",
        "    \"NavigateRoute\": ['distance_km', 'planned_speed'],\n",
        "    \"TrafficAssessment\": ['congestion', 'weather_risk'],\n",
        "    \"LoadCheck\": ['capacity_ratio', 'current_load'],\n",
        "    \"PackageHandling\": ['handling_time_min', 'fragility'],\n",
        "    \"DeliveryExecution\": ['actual_speed', 'delay_minutes'],\n",
        "    \"PaymentConfirmation\": ['payment_success', 'processing_time_sec'],\n",
        "    \"Feedback\": ['satisfaction', 'complaint'],\n",
        "    \"Aggregate\": 'ALL'  # special case: aggregate over all node metrics\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "class BranchBoundOptimizer:\n",
        "    \"\"\"\n",
        "    Exact, non-recursive Branch & Bound optimizer for 1D continuous problems.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        tol=1e-3,\n",
        "        max_depth=20,\n",
        "        minimize=True,\n",
        "        value_range=(0.0, 5.0)\n",
        "    ):\n",
        "        self.tol = tol\n",
        "        self.max_depth = max_depth\n",
        "        self.minimize = minimize\n",
        "        self.value_range = value_range\n",
        "\n",
        "    def optimize(self, features, y=None, metric_mask=None):\n",
        "        y = np.zeros(3) if y is None else np.array(y[:3])\n",
        "        base = np.mean(list(features.values())) + np.mean(y)\n",
        "\n",
        "        # Initial interval (shifted by base)\n",
        "        a0, b0 = self.value_range\n",
        "        a0 += base\n",
        "        b0 += base\n",
        "\n",
        "        # Work list: (a, b, depth)\n",
        "        work = [(a0, b0, 0)]\n",
        "\n",
        "        best_x = None\n",
        "        best_score = np.inf if self.minimize else -np.inf\n",
        "\n",
        "        def better(s1, s2):\n",
        "            return s1 < s2 if self.minimize else s1 > s2\n",
        "\n",
        "        while work:\n",
        "            a, b, depth = work.pop()\n",
        "\n",
        "            mid = 0.5 * (a + b)\n",
        "\n",
        "            # Evaluate midpoint\n",
        "            mv = [f(mid) if m else 0.0 for f, m in zip(METRIC_FORMULAS, metric_mask)]\n",
        "            score = sum(mv)\n",
        "\n",
        "            if best_x is None or better(score, best_score):\n",
        "                best_x = mid\n",
        "                best_score = score\n",
        "\n",
        "            # Termination condition\n",
        "            if depth >= self.max_depth or (b - a) < self.tol:\n",
        "                continue\n",
        "\n",
        "            # Branch (NO pruning — exact)\n",
        "            work.append((a, mid, depth + 1))\n",
        "            work.append((mid, b, depth + 1))\n",
        "\n",
        "        return best_x\n",
        "\n",
        "\n",
        "class MetricsEvaluator:\n",
        "    \"\"\"Compute node metrics using feature subsets and exact B&B optimizer per metric.\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_matrix,\n",
        "        metric_formulas=METRIC_FORMULAS,\n",
        "        metric_feature_map=metric_feature_map,\n",
        "        feature_keys=FEATURE_KEYS,\n",
        "        feature_target=None,\n",
        "        metric_target=None,\n",
        "        tol=1e-3,\n",
        "        max_depth=20,\n",
        "        minimize=False,\n",
        "        value_range=(0.0, 5.0)\n",
        "    ):\n",
        "        self.data_matrix = data_matrix\n",
        "        self.metric_formulas = metric_formulas\n",
        "        self.metric_feature_map = metric_feature_map\n",
        "        self.feature_keys = feature_keys\n",
        "        self.feature_target = feature_target or [[1]*len(feature_keys) for _ in range(data_matrix.shape[0])]\n",
        "        self.metric_target = metric_target or [[1]*len(metric_formulas) for _ in range(data_matrix.shape[0])]\n",
        "        self.num_nodes = data_matrix.shape[0]\n",
        "\n",
        "        # Create a B&B optimizer instance\n",
        "        self.optimizer = BranchBoundOptimizer(\n",
        "            tol=tol,\n",
        "            max_depth=max_depth,\n",
        "            minimize=minimize,\n",
        "            value_range=value_range\n",
        "        )\n",
        "\n",
        "    def extract_features(self, node_idx):\n",
        "        \"\"\"Return active features for a node as {feature_key: value}.\"\"\"\n",
        "        row = self.data_matrix[node_idx]\n",
        "        mask = self.feature_target[node_idx]\n",
        "        features = {k: v for k, v, m in zip(self.feature_keys, row, mask) if m}\n",
        "        return features\n",
        "\n",
        "    def compute_node_metrics(self, node_idx, y=None):\n",
        "        \"\"\"\n",
        "        Compute metrics for a single node using:\n",
        "        - relevant features from metric_feature_map\n",
        "        - B&B optimizer per metric\n",
        "        \"\"\"\n",
        "        features = self.extract_features(node_idx)\n",
        "        metric_mask = self.metric_target[node_idx]\n",
        "\n",
        "        metric_values = {}\n",
        "\n",
        "        for key, formula, mask in zip(METRIC_KEYS, self.metric_formulas, metric_mask):\n",
        "            if mask:\n",
        "                # Pick only the relevant features for this metric\n",
        "                relevant_features = [features[f] for f in self.metric_feature_map[key] if f in features]\n",
        "                x = np.mean(relevant_features) if relevant_features else 0.0\n",
        "\n",
        "                # Optimize the scalar using B&B\n",
        "                opt_value = self.optimizer.optimize(features={key: x}, y=y, metric_mask=[1])\n",
        "                metric_values[key] = formula(opt_value)\n",
        "            else:\n",
        "                metric_values[key] = 0.0\n",
        "\n",
        "        metric_values['score'] = sum(metric_values.values())\n",
        "        return metric_values\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#METRIC_TARGET=[[1, 1, 1, 1, 1,1], [1, 1, 1, 1, 1,1],[1, 1, 1, 1, 1,1], [1, 1, 1, 1, 1,1],[1, 1, 1, 1, 1,1],[1, 1, 1, 1, 1,1], [1, 1, 1, 1, 0]]\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed()\n",
        "\n",
        "# Number of synthetic samples per node\n",
        "n_samples = 3\n",
        "\n",
        "# Number of nodes (including Aggregate)\n",
        "nodes = list(GENERATOR_MAP.keys()) + [\"Aggregate\"]\n",
        "\n",
        "# Candidate dimensions per node (example)\n",
        "candidate_dims = [2, 2, 2, 2, 2, 2, 2, 2, 3]  # last one for Aggregate\n",
        "\n",
        "def generate_synthetic_targets_per_node(generator_map, nodes, candidate_dims, n_samples=3):\n",
        "    synthetic_targets = []\n",
        "\n",
        "    for node_idx, node in enumerate(nodes):\n",
        "        dim = candidate_dims[node_idx]\n",
        "        for sample_id in range(n_samples):\n",
        "            if node != \"Aggregate\":\n",
        "                features = list(generator_map[node]().values())\n",
        "                # Pad or truncate to match dim\n",
        "                if len(features) >= dim:\n",
        "                    features = features[:dim]\n",
        "                else:\n",
        "                    features = np.pad(features, (0, dim - len(features)), constant_values=0.5)\n",
        "            else:\n",
        "                # Aggregate node: just a vector of 0.5s\n",
        "                features = np.full(dim, 0.5)\n",
        "\n",
        "            synthetic_targets.append({\n",
        "                \"node\": node,\n",
        "                \"sample_id\": sample_id + 1,\n",
        "                \"target\": np.array(features)\n",
        "            })\n",
        "\n",
        "    return synthetic_targets\n",
        "\n",
        "# Generate targets\n",
        "synthetic_targets = generate_synthetic_targets_per_node(GENERATOR_MAP, nodes, candidate_dims, n_samples)\n",
        "\n",
        "# Check results\n",
        "for t in synthetic_targets:\n",
        "    print(f\"Node {t['node']} Sample {t['sample_id']} Target: {t['target']}\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# ---------------- GENERATE RAW DATA MATRIX ----------------\n",
        "# For example, 100 samples\n",
        "num_samples = 100\n",
        "feature_list = []\n",
        "\n",
        "# Collect features from each generator\n",
        "for tp, gen in GENERATOR_MAP.items():\n",
        "    samples = [gen() for _ in range(num_samples)]\n",
        "    df = pd.DataFrame(samples)\n",
        "    feature_list.append(df)\n",
        "\n",
        "# Concatenate all columns horizontally\n",
        "DATA_MATRIX = pd.concat(feature_list, axis=1).to_numpy()\n",
        "\n",
        "# Normalize to 0-1\n",
        "DATA_MATRIX = (DATA_MATRIX - DATA_MATRIX.min(axis=0)) / (np.ptp(DATA_MATRIX, axis=0) + 1e-8)\n",
        "\n",
        "# ---------------- SYNTHETIC TARGET GENERATOR ----------------\n",
        "def generate_synthetic_targets_per_node(DATA_MATRIX, candidate_dims):\n",
        "    \"\"\"\n",
        "    Generate per-node synthetic targets from DATA_MATRIX and candidate_dims.\n",
        "    \"\"\"\n",
        "    num_nodes = len(candidate_dims)\n",
        "    num_rows = DATA_MATRIX.shape[0]\n",
        "\n",
        "    # Split features roughly equally per node\n",
        "    feature_blocks = np.array_split(np.arange(DATA_MATRIX.shape[1]), num_nodes)\n",
        "\n",
        "    targets = []\n",
        "    for node_idx in range(num_nodes):\n",
        "        row = DATA_MATRIX[node_idx % num_rows]\n",
        "        features = row[feature_blocks[node_idx]]\n",
        "        dim = candidate_dims[node_idx]\n",
        "\n",
        "        # Pad or truncate to match candidate_dims\n",
        "        if len(features) >= dim:\n",
        "            sampled = features[:dim]\n",
        "        else:\n",
        "            sampled = np.pad(features, (0, dim - len(features)), constant_values=0.5)\n",
        "\n",
        "        targets.append({'node_id': node_idx, 'target': sampled})\n",
        "\n",
        "    return targets\n",
        "\n",
        "# ---------------- EXAMPLE USAGE ----------------\n",
        "D_graph = 8\n",
        "candidate_dims = [2, 2, 2, 2, 2, 2, 2, 2, ]  # 9 nodes\n",
        "synthetic_targets = generate_synthetic_targets_per_node(DATA_MATRIX, candidate_dims)\n",
        "\n",
        "# ---------------- CHECK OUTPUT ----------------\n",
        "for t in synthetic_targets:\n",
        "    print(f\"Node {t['node_id']} target size: {len(t['target'])}, values: {t['target']}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "xHWu5PqJZsox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "synthetic_targets"
      ],
      "metadata": {
        "id": "bj-C_AigSMkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T-7oScgTpJTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Fuzzy Hierarchical Multiplex Model"
      ],
      "metadata": {
        "id": "zkzkHh2YNMNy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A fuzzy hierarchical multiplex that is optimized to given data targets for given outer objectives according to some metrics. It then provides diagnostic plots and prints the formula for each node metric formula. Here is a pseudo coded method.\n",
        "\n",
        "    -input\n",
        "        -data targets\n",
        "        -candidate dimension\n",
        "        -data learning rates\n",
        "        -metric mask\n",
        "    -processes\n",
        "        -Solves run inner optimal metrics\n",
        "        -uses mask to discenr metrics\n",
        "        -Evaluates metrics\n",
        "        -trains an SVM per node according to dimensions\n",
        "        -returns weights to outer layer\n",
        "        -solves for the objective of outer layer\n",
        "        -calculates outer contribution of metrics\n",
        "        -calculates overal outer metrics\n",
        "    -outputs\n",
        "        -solves optimizaiotn of data problem\n",
        "        -solves outer objective problem\n",
        "        -produces an SVM per node for metric reversal to datapoint\n",
        "        -plots diagnostics\n"
      ],
      "metadata": {
        "id": "h3zVe7LPNPIn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t7CWcH1LnssQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the DSM based on Behavioural random walks.\n",
        "\n",
        "### In this instance instead of assuming a DSM the metrics were taken as they were, couplings were implemented as in Bai paper, and a clever program was devised. The idea was simple, lets create a desing based on metrics that adheres to behavioural logic, which can be implemented using random walks. __data-->metrics-->RW-->DSM__... And so, like this we can optimize the desing based on metrics and anything that one can imagine"
      ],
      "metadata": {
        "id": "dwy0q9dsntRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "# Target Mask (which metrics apply to which node)\n",
        "candidate_dims = [[2],[2],[2],[2],[2],[2],[2],[2],]#[1],]\n",
        "\n",
        "def top_k_masked_probs(weights, k):\n",
        "    \"\"\"\n",
        "    Keep only top-k weights, zero out the rest, renormalize.\n",
        "    \"\"\"\n",
        "    if k >= len(weights):\n",
        "        return weights / (weights.sum() + 1e-12)\n",
        "\n",
        "    idx = np.argpartition(weights, -k)[-k:]\n",
        "    mask = np.zeros_like(weights)\n",
        "    mask[idx] = weights[idx]\n",
        "\n",
        "    s = mask.sum()\n",
        "    if s > 0:\n",
        "        mask /= s\n",
        "    return mask\n",
        "def get_all_paths(G, C, node_types, start=0, end=None):\n",
        "    \"\"\"\n",
        "    Return all simple paths that respect coupling constraints.\n",
        "    \"\"\"\n",
        "    if end is None:\n",
        "        end = G.shape[0] - 1\n",
        "\n",
        "    D = G.shape[0]\n",
        "    paths = []\n",
        "\n",
        "    def coupling_ok(i, j):\n",
        "        label = C[i, j]\n",
        "\n",
        "        # Default direct coupling\n",
        "        if \"->\" in label and \"->C\" not in label:\n",
        "            src, dst = label.split(\"->\")\n",
        "            return src == node_types[i] and dst == node_types[j]\n",
        "\n",
        "        # Escalation case (B->B->C)\n",
        "        if label == \"B->B->C\":\n",
        "            return node_types[i] == \"B\" and node_types[j] == \"B\"\n",
        "\n",
        "        return False\n",
        "\n",
        "    def dfs(node, path, visited):\n",
        "        if node == end:\n",
        "            paths.append(path.copy())\n",
        "            return\n",
        "\n",
        "        for nxt in range(D):\n",
        "            if G[node, nxt] > 0 and nxt not in visited:\n",
        "                if not coupling_ok(node, nxt):\n",
        "                    continue\n",
        "\n",
        "                visited.add(nxt)\n",
        "                dfs(nxt, path + [nxt], visited)\n",
        "                visited.remove(nxt)\n",
        "\n",
        "    dfs(start, [start], {start})\n",
        "    return paths\n",
        "\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# EXTERNAL DEPENDENCIES & CONFIGURATION\n",
        "# =============================================================================\n",
        "# These variables are referenced in the original code but not defined.\n",
        "# Assumed to be present in the execution environment.\n",
        "# -----------------------------------------------------------------------------\n",
        "# D_graph = ...\n",
        "# DATA_MATRIX = ...\n",
        "# METRIC_KEYS = [...]\n",
        "# METRIC_TARGET = [...]\n",
        "# METRIC_FORMULAS = [...]\n",
        "# METRIC_INVERSES = {...}\n",
        "# synthetic_targets = ...\n",
        "# MetricsEvaluator = ... (Class)\n",
        "# -----------------------------------------------------------------------------\n",
        "# =============================================================================\n",
        "# NEW: DETERMINISTIC DFS & PATH EVALUATOR\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "def evaluate_fixed_paths(paths, node_metrics, beta=0.3):\n",
        "    \"\"\"\n",
        "    Replaces collect_and_select_best_walks.\n",
        "    Scores the specific paths found by DFS.\n",
        "    \"\"\"\n",
        "    walks = []\n",
        "    best = None\n",
        "\n",
        "    for path in paths:\n",
        "        cost = 0.0\n",
        "        quality = 0.0\n",
        "\n",
        "        # Calculate Path Metrics\n",
        "        for node_idx in path:\n",
        "            m = node_metrics[node_idx]\n",
        "            cost += m.get(\"cost\", 0.0)\n",
        "            quality += m.get(\"quality\", 0.0)\n",
        "\n",
        "        score = quality - 1.0 * cost # using default lambda_cost=1.0\n",
        "\n",
        "        # DFS paths are unique, so count is always 1\n",
        "        count = 1\n",
        "        adjusted_score = score / (1 + beta * count)\n",
        "\n",
        "        record = {\n",
        "            \"path\": path,\n",
        "            \"score\": score,\n",
        "            \"adjusted_score\": adjusted_score,\n",
        "            \"cost\": cost,\n",
        "            \"quality\": quality,\n",
        "            \"count\": count\n",
        "        }\n",
        "        walks.append(record)\n",
        "\n",
        "        if best is None or adjusted_score > best[\"adjusted_score\"]:\n",
        "            best = record\n",
        "\n",
        "    return walks, best\n",
        "\n",
        "def dsm_walk(G, node_metrics, start=0, max_steps=50, lambda_cost=1.0):\n",
        "    D = G.shape[0]\n",
        "    target_node = D - 1  # Define Nlast\n",
        "\n",
        "    # Force start at 0 if not provided\n",
        "    current = start\n",
        "    path = [current]\n",
        "    visited = {current}\n",
        "\n",
        "    cost = 0.0\n",
        "    quality = 0.0\n",
        "\n",
        "    # Metrics for the start node\n",
        "    m_start = node_metrics[current]\n",
        "    cost += m_start.get(\"cost\", 0.0)\n",
        "    quality += m_start.get(\"quality\", 0.0)\n",
        "\n",
        "    reached_target = False\n",
        "\n",
        "    for _ in range(max_steps):\n",
        "        # If we reached the last node, stop successfully\n",
        "        if current == target_node:\n",
        "            reached_target = True\n",
        "            break\n",
        "\n",
        "        weights = np.zeros(D)\n",
        "\n",
        "        for j in range(D):\n",
        "            # Allow visiting the target even if visited (though unlikely to revisit in DAG)\n",
        "            # But generally prevent cycles\n",
        "            if j == 0:\n",
        "                continue\n",
        "            if j in visited:\n",
        "                continue\n",
        "\n",
        "            q = node_metrics[j].get(\"quality\", 0.0)\n",
        "            c = node_metrics[j].get(\"cost\", 0.0)\n",
        "\n",
        "            # Standard probability weight\n",
        "            weights[j] = G[current, j] * (q / (1 + c))\n",
        "\n",
        "        if weights.sum() == 0:\n",
        "            break\n",
        "\n",
        "        weights /= weights.sum()\n",
        "        nxt = np.random.choice(D, p=weights)\n",
        "\n",
        "        m = node_metrics[nxt]\n",
        "        cost += m.get(\"cost\", 0.0)\n",
        "        quality += m.get(\"quality\", 0.0)\n",
        "\n",
        "        path.append(nxt)\n",
        "        visited.add(nxt)\n",
        "        current = nxt\n",
        "\n",
        "    # Recalculate score based on success\n",
        "    score = quality - lambda_cost * cost\n",
        "\n",
        "    # Heavy penalty if the walk did not reach Nlast\n",
        "    if not reached_target:\n",
        "        score = -1e9\n",
        "\n",
        "    return path, score, cost, quality\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "def collect_and_select_best_walks(G, node_metrics, n_walks=300, beta=0.3):\n",
        "    walks = []\n",
        "    best = None\n",
        "    path_counts = defaultdict(int)\n",
        "    D = G.shape[0]\n",
        "\n",
        "    for _ in range(n_walks):\n",
        "        # Force start at 0\n",
        "        start = 0\n",
        "        path, score, cost, quality = dsm_walk(\n",
        "            G, node_metrics, start\n",
        "        )\n",
        "\n",
        "        # Only record if it successfully reached the last node\n",
        "        # (Score is -1e9 if it failed, per updated dsm_walk)\n",
        "        if path[-1] != (D - 1):\n",
        "            continue\n",
        "\n",
        "        key = tuple(path)\n",
        "        path_counts[key] += 1\n",
        "\n",
        "        # novelty penalty\n",
        "        adjusted_score = score / (1 + beta * path_counts[key])\n",
        "\n",
        "        record = {\n",
        "            \"path\": path,\n",
        "            \"score\": score,\n",
        "            \"adjusted_score\": adjusted_score,\n",
        "            \"cost\": cost,\n",
        "            \"quality\": quality,\n",
        "            \"count\": path_counts[key]\n",
        "        }\n",
        "        walks.append(record)\n",
        "\n",
        "        if best is None or adjusted_score > best[\"adjusted_score\"]:\n",
        "            best = record\n",
        "\n",
        "    return walks, best\n",
        "\n",
        "class CouplingState:\n",
        "    \"\"\"\n",
        "    One-step coupling automaton.\n",
        "    C = transient escalation after B->B->C\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.state = \"NORMAL\"\n",
        "\n",
        "    def update(self, coupling):\n",
        "        if coupling == \"B->B->C\":\n",
        "            self.state = \"C\"\n",
        "        else:\n",
        "            self.state = \"NORMAL\"\n",
        "\n",
        "    def allows(self, next_node_type):\n",
        "        \"\"\"\n",
        "        Rule:\n",
        "        C → random, but NOT B\n",
        "        \"\"\"\n",
        "        if self.state == \"C\" and next_node_type == \"B\":\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "def infer_node_types(node_metrics):\n",
        "    \"\"\"\n",
        "    A = quality-dominant\n",
        "    B = cost-dominant\n",
        "    \"\"\"\n",
        "    node_types = []\n",
        "    for m in node_metrics:\n",
        "        q = m.get(\"quality\", 0.0)\n",
        "        c = m.get(\"cost\", 0.0)\n",
        "        node_types.append(\"A\" if q >= c else \"B\")\n",
        "    return node_types\n",
        "\n",
        "def build_coupling_matrix(node_types):\n",
        "    \"\"\"\n",
        "    Builds a D x D coupling-label matrix.\n",
        "    C exists only as a coupling escalation (B->B->C).\n",
        "    \"\"\"\n",
        "    D = len(node_types)\n",
        "    C = np.empty((D, D), dtype=object)\n",
        "\n",
        "    for i in range(D):\n",
        "        for j in range(D):\n",
        "            src = node_types[i]\n",
        "            dst = node_types[j]\n",
        "\n",
        "            if src == \"B\" and dst == \"B\":\n",
        "                C[i, j] = \"B->B->C\"\n",
        "            else:\n",
        "                C[i, j] = f\"{src}->{dst}\"\n",
        "\n",
        "    return C\n",
        "\n",
        "\n",
        "def coupling_weight(coupling, metrics):\n",
        "    cost = metrics.get(\"cost\", 0.0)\n",
        "    quality = metrics.get(\"quality\", 0.0)\n",
        "\n",
        "    if coupling == \"A->A\":          # Quality → Quality\n",
        "        return quality\n",
        "\n",
        "    if coupling == \"A->B\":          # Quality → Cost\n",
        "        return quality\n",
        "\n",
        "    if coupling == \"B->A\":          # Cost → Quality\n",
        "        return quality\n",
        "\n",
        "    if coupling == \"B->B->C\":       # Cost escalation signal\n",
        "        return cost\n",
        "\n",
        "    return 0.0\n",
        "\n",
        "# Configuration\n",
        "#candidate_dims = [[2], [2], [2], [2], [2], [2], [2], [2], [1]]\n",
        "outer_generations = 1\n",
        "inner_learning = 0.1\n",
        "gamma_interlayer = 1\n",
        "top_k = 21\n",
        "\n",
        "# Initialize random state\n",
        "np.random.seed()\n",
        "seed = None  # Placeholder as per original logic\n",
        "\n",
        "# Placeholder for Data Matrix generation (from original snippet)\n",
        "# new_DATA_MATRIX = np.random.rand(D_graph, DATA_MATRIX.shape[1])\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# HELPER CLASSES\n",
        "# =============================================================================\n",
        "class DSM_Tracker:\n",
        "    \"\"\"\n",
        "    Tracks a DSM (Design Structure Matrix) layer and its residual.\n",
        "    \"\"\"\n",
        "    def __init__(self, multiplex_layer):\n",
        "        self.layer = multiplex_layer\n",
        "        self.primary_dsm = None\n",
        "        self.residual_dsm = None\n",
        "        self.update_dsms()\n",
        "\n",
        "    def update_dsms(self):\n",
        "        if self.primary_dsm is None:\n",
        "            # First time: store current DSM as reference\n",
        "            self.primary_dsm = self.layer.chosen_Gmat.copy()\n",
        "            self.residual_dsm = np.zeros_like(self.primary_dsm)\n",
        "        else:\n",
        "            # Update residual: current DSM minus primary\n",
        "            current = self.layer.chosen_Gmat\n",
        "            self.residual_dsm = current - self.primary_dsm\n",
        "\n",
        "    def get_matrices(self):\n",
        "        return self.primary_dsm, self.residual_dsm\n",
        "\n",
        "    def print_matrices(self):\n",
        "        print(\"\\n--- Primary DSM ---\")\n",
        "        print(self.primary_dsm)\n",
        "        print(\"\\n--- Residual DSM ---\")\n",
        "        print(self.residual_dsm)\n",
        "\n",
        "\n",
        "class DSM_Layer_Decomposer:\n",
        "    \"\"\"\n",
        "    Manages the additive decomposition of DSM layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, baseline_matrix, mode='additive'):\n",
        "        self.baseline_matrix = baseline_matrix.copy()\n",
        "        self.current_total = baseline_matrix.copy()\n",
        "        self.mode = mode\n",
        "        self.layers = []\n",
        "        self.residuals = []\n",
        "\n",
        "    def add_snapshot(self, new_total_matrix):\n",
        "        \"\"\"\n",
        "        Calculates the DELTA (change) between the new state and the previous state,\n",
        "        stores that delta as a layer.\n",
        "        \"\"\"\n",
        "        delta = new_total_matrix - self.current_total\n",
        "        self.layers.append(delta.copy())\n",
        "\n",
        "        # Update current tracker\n",
        "        self.current_total = new_total_matrix.copy()\n",
        "\n",
        "        # Calculate residual (Difference from the baseline)\n",
        "        residual = self.current_total - self.baseline_matrix\n",
        "        self.residuals.append(residual)\n",
        "\n",
        "        layer_id = len(self.layers) - 1\n",
        "        print(f\"\\n=== DSM LAYER {layer_id} CAPTURED ===\")\n",
        "        print(f\"Layer Contribution (Delta):\\n{np.round(delta, 3)}\")\n",
        "\n",
        "        return delta\n",
        "\n",
        "    def get_reconstruction(self):\n",
        "        return np.sum(self.layers, axis=0)\n",
        "\n",
        "\n",
        "class SVM:\n",
        "    \"\"\"\n",
        "    Metric-inverse multi-output SVM with epsilon-insensitive loss.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, output_dim=None, metric_keys=None, lr=0.001, epsilon=0.1):\n",
        "        self.input_dim = input_dim\n",
        "        self.metric_keys = metric_keys\n",
        "\n",
        "        if metric_keys is not None:\n",
        "            self.output_dim = len(metric_keys)\n",
        "        elif output_dim is not None:\n",
        "            self.output_dim = output_dim\n",
        "        else:\n",
        "            self.output_dim = 1\n",
        "\n",
        "        self.lr = lr\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        # Lazy initialization\n",
        "        self.X_train = None\n",
        "        self.y_train = None\n",
        "        self.alpha = None\n",
        "        self.b = None\n",
        "\n",
        "    def train_step(self, X, y_true):\n",
        "        X = np.array(X)\n",
        "        y_true = np.array(y_true)\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        if self.X_train is None:\n",
        "            self.X_train = X.copy()\n",
        "            self.y_train = y_true.copy()\n",
        "            self.alpha = np.zeros((X.shape[1], self.output_dim))\n",
        "            self.b = np.zeros(self.output_dim)\n",
        "\n",
        "        # Linear kernel\n",
        "        y_pred = X.dot(self.alpha) + self.b\n",
        "\n",
        "        # Epsilon-insensitive loss\n",
        "        diff = y_pred - y_true\n",
        "        mask = np.abs(diff) > self.epsilon\n",
        "        diff *= mask\n",
        "\n",
        "        grad_alpha = X.T.dot(diff) / n_samples\n",
        "        grad_b = diff.mean(axis=0)\n",
        "\n",
        "        self.alpha -= self.lr * grad_alpha\n",
        "        self.b -= self.lr * grad_b\n",
        "\n",
        "        loss = np.mean(np.maximum(0, np.abs(y_pred - y_true) - self.epsilon))\n",
        "        return loss\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.array(X)\n",
        "        y_pred = X.dot(self.alpha) + self.b\n",
        "\n",
        "        if self.metric_keys is None:\n",
        "            return y_pred\n",
        "\n",
        "        # Apply metric inverses\n",
        "        y_transformed = np.zeros_like(y_pred)\n",
        "        for i, key in enumerate(self.metric_keys):\n",
        "            inverse_fn = METRIC_INVERSES[key]\n",
        "            # Handle potential list return from inverse_fn\n",
        "            val_func = lambda y: inverse_fn(y)[0] if isinstance(inverse_fn(y), list) else inverse_fn(y)\n",
        "            y_transformed[:, i] = np.array([val_func(y) for y in y_pred[:, i]])\n",
        "\n",
        "        return y_transformed\n",
        "\n",
        "\n",
        "class InterLayer:\n",
        "    def __init__(self, D_graph, max_inner_dim, inter_dim=None, edge_threshold=0.02, gamma=1.0, seed=42):\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "\n",
        "        self.D_graph = D_graph\n",
        "        self.edge_threshold = edge_threshold\n",
        "        self.gamma = gamma\n",
        "\n",
        "        # Handle list vs int inputs for dimensions\n",
        "        m_dim = max_inner_dim[0] if isinstance(max_inner_dim, list) else max_inner_dim\n",
        "\n",
        "        if inter_dim is not None:\n",
        "            self.inter_dim = inter_dim[0] if isinstance(inter_dim, list) else inter_dim\n",
        "        else:\n",
        "            self.inter_dim = m_dim\n",
        "\n",
        "        self.max_input = 2 * m_dim\n",
        "\n",
        "        # Initialize weights\n",
        "        self.weights = {}\n",
        "        self.bias = {}\n",
        "        for i in range(D_graph):\n",
        "            for j in range(D_graph):\n",
        "                if i != j:\n",
        "                    w_init = np.random.uniform(-0.1, 0.1, (self.inter_dim, self.max_input))\n",
        "                    self.weights[(i, j)] = w_init\n",
        "                    self.bias[(i, j)] = np.zeros(self.inter_dim)\n",
        "\n",
        "    def compute_edge_activation(self, i, j, nested_reps):\n",
        "        concat = np.concatenate([nested_reps[i], nested_reps[j]])\n",
        "        # Pad or truncation\n",
        "        if len(concat) < self.max_input:\n",
        "            concat = np.pad(concat, (0, self.max_input - len(concat)))\n",
        "        else:\n",
        "            concat = concat[:self.max_input]\n",
        "\n",
        "        # Normalize\n",
        "        concat = (concat - np.mean(concat)) / (np.std(concat) + 1e-12)\n",
        "\n",
        "        # Activation\n",
        "        v = self.weights[(i, j)].dot(concat) + self.bias[(i, j)]\n",
        "        input_strength = np.clip(np.mean(np.abs(concat)), 0, 1)\n",
        "        v = v * input_strength\n",
        "\n",
        "        return 1 / (1 + np.exp(-v))\n",
        "\n",
        "    def build_activations(self, Gmat, nested_reps):\n",
        "        acts = {}\n",
        "        for i in range(self.D_graph):\n",
        "            for j in range(self.D_graph):\n",
        "                if i == j:\n",
        "                    continue\n",
        "                if abs(Gmat[i, j]) > self.edge_threshold:\n",
        "                    acts[(i, j)] = self.compute_edge_activation(i, j, nested_reps)\n",
        "        return acts\n",
        "\n",
        "    @staticmethod\n",
        "    def pairwise_squared_corr(acts):\n",
        "        if len(acts) < 2:\n",
        "            return 0.0\n",
        "        A = np.stack(list(acts.values()))\n",
        "        A_centered = A - A.mean(axis=1, keepdims=True)\n",
        "        stds = np.sqrt(np.sum(A_centered**2, axis=1) / (A.shape[1]-1) + 1e-12)\n",
        "        cov = A_centered @ A_centered.T / (A.shape[1]-1)\n",
        "        corr = cov / (np.outer(stds, stds) + 1e-12)\n",
        "        np.fill_diagonal(corr, 0)\n",
        "        return float((corr**2).sum())\n",
        "\n",
        "    def mi_for_graph(self, Gmat, nested_reps):\n",
        "        acts = self.build_activations(Gmat, nested_reps)\n",
        "        if not acts:\n",
        "            return 0.0\n",
        "        return self.gamma * self.pairwise_squared_corr(acts)\n",
        "\n",
        "def build_dsm_from_walks(D, paths):\n",
        "    \"\"\"\n",
        "    Constructs a DSM where entry [i,j] is the probability\n",
        "    that a successful process moves from i to j.\n",
        "    \"\"\"\n",
        "    flow = np.zeros((D, D))\n",
        "\n",
        "    # Count transitions\n",
        "    for path in paths:\n",
        "        for k in range(len(path) - 1):\n",
        "            u, v = path[k], path[k+1]\n",
        "            flow[u, v] += 1\n",
        "\n",
        "    # Normalize by the total number of successful walks.\n",
        "    # This prevents 'saturation'—if an edge is rarely used, it stays small.\n",
        "    n_paths = len(paths)\n",
        "    if n_paths > 0:\n",
        "        flow = flow / n_paths\n",
        "\n",
        "    np.fill_diagonal(flow, 0.0)\n",
        "    return flow\n",
        "# =============================================================================\n",
        "# MAIN OPTIMIZER CLASS\n",
        "# =============================================================================\n",
        "class MetricDrivenRandomWalk:\n",
        "    def __init__(self, coupling_matrix, node_metrics, max_steps=50):\n",
        "        self.C = coupling_matrix\n",
        "        self.node_metrics = node_metrics\n",
        "        self.D = coupling_matrix.shape[0]\n",
        "        self.max_steps = max_steps\n",
        "\n",
        "    def transition_probs(self, i, k=2):\n",
        "        w = np.zeros(self.D)\n",
        "\n",
        "        for j in range(self.D):\n",
        "            if i == j:\n",
        "                continue\n",
        "\n",
        "            weight = coupling_weight(self.C[i, j], self.node_metrics[j])\n",
        "\n",
        "            # Directional heuristic (keep yours)\n",
        "            if j > i:\n",
        "                weight *= 1.2\n",
        "            elif j < i:\n",
        "                weight *= 0.8\n",
        "\n",
        "            w[j] = weight\n",
        "\n",
        "        # If nothing viable, fallback to uniform\n",
        "        if w.sum() == 0:\n",
        "            return np.ones(self.D) / self.D\n",
        "\n",
        "        # Emphasize strong edges\n",
        "        w = w ** 2\n",
        "\n",
        "        # 🔒 HARD SPARSITY CONSTRAINT (TOP-K)\n",
        "        w = top_k_masked_probs(w, k)\n",
        "\n",
        "        return w\n",
        "\n",
        "\n",
        "    def run(self, start=0):\n",
        "        path = [start]\n",
        "        current = start\n",
        "        target_node = self.D - 1\n",
        "\n",
        "        for _ in range(self.max_steps):\n",
        "            if current == target_node:\n",
        "                break\n",
        "\n",
        "            probs = self.transition_probs(current)\n",
        "\n",
        "            # Move\n",
        "            current = np.random.choice(self.D, p=probs)\n",
        "            path.append(current)\n",
        "\n",
        "            if current == target_node:\n",
        "                break\n",
        "\n",
        "        return path\n",
        "\n",
        "class Fuzzy_Hierarchical_Multiplex:\n",
        "    def __init__(self, candidate_dims, D_graph,\n",
        "                 synthetic_targets, inner_learning,\n",
        "                 gamma_interlayer=1.0, causal_flag=True,\n",
        "                 metrics=METRIC_KEYS, metric_mask=METRIC_TARGET):\n",
        "\n",
        "        self.candidate_dims = candidate_dims\n",
        "        self.D_graph = D_graph\n",
        "        self.synthetic_targets = synthetic_targets\n",
        "        self.inner_learning = inner_learning\n",
        "        self.causal_flag = causal_flag\n",
        "        self.best_dim_per_node = [len(t)-1 for t in synthetic_targets]\n",
        "        self.MM = metric_mask\n",
        "        self.MK = metrics\n",
        "        self.MKI = metrics + ['score']\n",
        "\n",
        "        self.PLM = [[] for _ in range(self.D_graph)]\n",
        "        self.PLMS = [[] for _ in range(self.D_graph)]\n",
        "        self.nested_reps = [np.zeros(c[0]) for c in candidate_dims]\n",
        "\n",
        "        # Inter-layer setup\n",
        "        self.inter_layer = InterLayer(D_graph, max_inner_dim=max(candidate_dims), gamma=gamma_interlayer)\n",
        "        self.chosen_Gmat = np.random.uniform(0.0, 0.3, (D_graph, D_graph))\n",
        "        np.fill_diagonal(self.chosen_Gmat, 0)\n",
        "\n",
        "        self.l2_before, self.l2_after = [], []\n",
        "        self.max_target_len = max(len(t['target']) for t in synthetic_targets)\n",
        "        self.svm_lr = 0.01\n",
        "\n",
        "        self.metric_traces = {k: [] for k in metrics}\n",
        "        self.metric_traces_per_node = [{} for _ in range(self.D_graph)]\n",
        "\n",
        "        # DSM optimization hyperparameters\n",
        "        self.dsm_lr = 0.1\n",
        "        self.dsm_l1 = 0.02\n",
        "        self.dsm_clip = 1.0\n",
        "        self.dsm_history = []\n",
        "        self.dsm_cost_weight = 0.05\n",
        "\n",
        "    def print_dsm_basic(self):\n",
        "        D = self.D_graph\n",
        "        print(\"\\n=== DESIGN STRUCTURE MATRIX (DSM) : Gmat ===\")\n",
        "        header = \"     \" + \" \".join([f\"N{j:>4}\" for j in range(D)])\n",
        "        print(header)\n",
        "        for i in range(D):\n",
        "            row = \"N{:>2} | \".format(i)\n",
        "            for j in range(D):\n",
        "                row += f\"{self.chosen_Gmat[i, j]:>5.2f} \"\n",
        "            print(row)\n",
        "\n",
        "    # ---------- INNER LOOP (FCM) ----------\n",
        "    def run_inner(self, node_idx, target, D_fcm,\n",
        "                  steps=1000, lr_x=1, lr_y=0.010, lr_W=1,\n",
        "                  decorrelate_metrics=False):\n",
        "\n",
        "        # --- Initialize activations ---\n",
        "        x = np.random.uniform(-0.6, 0.6, D_fcm)\n",
        "        y = np.random.uniform(-0.1, 0.1, D_fcm)\n",
        "\n",
        "        # L2 tracking\n",
        "        self.l2_before.append(np.linalg.norm(self.nested_reps[node_idx][:len(target)] - target))\n",
        "\n",
        "        # --- FCM updates ---\n",
        "        W = np.random.uniform(-0.6, 0.6, (D_fcm, D_fcm))\n",
        "        np.fill_diagonal(W, 0)\n",
        "\n",
        "        for _ in range(steps):\n",
        "            z = y.dot(W) + x\n",
        "            Theta_grad_z = z - target\n",
        "            Theta_grad_x = Theta_grad_z\n",
        "            Theta_grad_y = Theta_grad_z.dot(W.T)\n",
        "            Theta_grad_W = np.outer(y, Theta_grad_z)\n",
        "\n",
        "            x -= lr_x * np.clip(Theta_grad_x, -0.05, 0.05)\n",
        "            y -= lr_y * np.clip(Theta_grad_y, -0.05, 0.05)\n",
        "            W -= lr_W * np.clip(Theta_grad_W, -0.01, 0.01)\n",
        "\n",
        "            x = np.clip(x, 0, 1)\n",
        "            y = np.clip(y, 0, 1)\n",
        "            np.fill_diagonal(W, 0)\n",
        "            W = np.clip(W, -1, 1)\n",
        "\n",
        "        # --- Update nested representation ---\n",
        "        self.nested_reps[node_idx][:len(x)] = x\n",
        "        self.l2_after.append(np.linalg.norm(x - target))\n",
        "\n",
        "        # --- Extract node features ---\n",
        "        # Assuming MetricsEvaluator is a global or imported class\n",
        "        metrics_evaluator = MetricsEvaluator(data_matrix=DATA_MATRIX)\n",
        "        features = metrics_evaluator.extract_features(node_idx)\n",
        "        feat_vals = np.array(list(features.values()))\n",
        "\n",
        "        # --- Compute metrics scaled by activations + features ---\n",
        "        metric_mask = METRIC_TARGET[node_idx]\n",
        "        metric_values = {}\n",
        "\n",
        "        for key, formula, mask in zip(METRIC_KEYS, METRIC_FORMULAS, metric_mask):\n",
        "            if mask:\n",
        "                weighted_input = np.mean(feat_vals)\n",
        "                # Outer scale check\n",
        "                outer_scale = getattr(self, 'best_node_weights', {}).get(node_idx, 1.0)\n",
        "                if isinstance(outer_scale, (list, np.ndarray)):\n",
        "                     # fallback if it was stored incorrectly in previous context\n",
        "                    outer_scale = 1.0\n",
        "\n",
        "                weighted_input *= outer_scale\n",
        "                metric_val = formula(weighted_input)\n",
        "                metric_values[key] = metric_val\n",
        "\n",
        "                # STORE DATAPOINT\n",
        "                self.metric_traces[key].append((weighted_input, metric_val))\n",
        "            else:\n",
        "                metric_values[key] = 0.0\n",
        "\n",
        "        # --- Total score ---\n",
        "        metric_values['score'] = sum(metric_values.values())\n",
        "\n",
        "        # --- Build SVM Training Data ---\n",
        "        metric_output_vals = np.array(\n",
        "            [v for k, v in metric_values.items() if k not in ['score', 'x', 'feat_vals']]\n",
        "        )\n",
        "\n",
        "        # Lazy init per-node SVM\n",
        "        if not hasattr(self, \"node_svms\"):\n",
        "            self.node_svms = {}\n",
        "\n",
        "        if node_idx not in self.node_svms:\n",
        "            self.node_svms[node_idx] = SVM(\n",
        "                input_dim=len(self.MK),\n",
        "                output_dim=self.candidate_dims[node_idx][0],\n",
        "                lr=self.svm_lr\n",
        "            )\n",
        "\n",
        "        svm = self.node_svms[node_idx]\n",
        "\n",
        "        # Build SVM Input/Output\n",
        "        x_in_full = np.zeros(len(self.MK))\n",
        "        x_in_full[:len(metric_output_vals)] = metric_output_vals\n",
        "        x_in = x_in_full.reshape(1, -1)\n",
        "\n",
        "        y_out_full = np.zeros(self.candidate_dims[node_idx][0])\n",
        "        y_out_full[:len(x)] = x\n",
        "        y_out = y_out_full.reshape(1, -1)\n",
        "\n",
        "        # Train SVM\n",
        "        _ = svm.train_step(x_in, y_out)\n",
        "\n",
        "        # --- Store PLMS trace ---\n",
        "        self.PLMS[node_idx].append((float(weighted_input), metric_output_vals))\n",
        "\n",
        "        if len(self.PLMS[node_idx]) % 100 == 0:\n",
        "            print(f\"Node {node_idx}, samples learned:\", len(self.PLMS[node_idx]))\n",
        "\n",
        "        # --- Compute inter-layer MI ---\n",
        "        mi_score = self.inter_layer.mi_for_graph(self.chosen_Gmat, self.nested_reps)\n",
        "\n",
        "        return x, y, W, mi_score, metric_values\n",
        "\n",
        "    # ---------- OUTER LOOP (Topology Optimization) ----------\n",
        "    def run_outer(self, outer_cost_limit=1000, alpha=0.0, additive_rate=0.5):\n",
        "        \"\"\"\n",
        "        Generates an additive DSM layer.\n",
        "        CRITICAL: Enforces strict constraints regardless of layer state.\n",
        "\n",
        "        Constraints:\n",
        "        1. Start Node == 0\n",
        "        2. End Node == D-1\n",
        "        3. Hamiltonian (Length == D, visits all nodes exactly once)\n",
        "        4. CouplingState (Respects A/B/C transition rules)\n",
        "        \"\"\"\n",
        "        node_metrics_list = self.capped_node_metrics\n",
        "        D = self.D_graph\n",
        "\n",
        "        # =========================================================\n",
        "        # 1. METRIC SCORING & TENSOR CALCULATIONS\n",
        "        # =========================================================\n",
        "        raw_scores = np.array([m['score'] for m in node_metrics_list])\n",
        "        total_raw = raw_scores.sum()\n",
        "        if total_raw > outer_cost_limit:\n",
        "            scale_factor = outer_cost_limit / total_raw\n",
        "            for metrics in node_metrics_list:\n",
        "                for key in self.MKI:\n",
        "                    metrics[key] *= scale_factor\n",
        "            raw_scores *= scale_factor\n",
        "\n",
        "        fuzzy_tensor = self.compute_fuzzy_metric_tensor(normalize=False)\n",
        "        self.weighted_fmt = fuzzy_tensor.copy()\n",
        "\n",
        "        # Calculate Contributions\n",
        "        node_contributions = np.zeros(D)\n",
        "        for i in range(D):\n",
        "            own_score = raw_scores[i]\n",
        "            fmt_contrib = fuzzy_tensor[i, :, :].sum() - fuzzy_tensor[i, i, :].sum()\n",
        "            node_contributions[i] = own_score + self.inter_layer.gamma * fmt_contrib\n",
        "        self.node_score_contributions = node_contributions\n",
        "\n",
        "        # Correlation Penalty\n",
        "        interaction_tensor = self.print_interactions(return_tensor=True, verbose=False)\n",
        "        if interaction_tensor is not None:\n",
        "            fmt_mean = fuzzy_tensor.mean(axis=2)\n",
        "            inter_mean = interaction_tensor.mean(axis=2)\n",
        "            corr_penalty = 0.0\n",
        "            for i in range(D):\n",
        "                fmt_vec = fmt_mean[i, :]\n",
        "                inter_vec = inter_mean[i, :]\n",
        "                if np.std(fmt_vec) > 1e-8 and np.std(inter_vec) > 1e-8:\n",
        "                    corr_penalty += abs(np.corrcoef(fmt_vec, inter_vec)[0, 1])**2\n",
        "            corr_penalty /= D\n",
        "        else:\n",
        "            corr_penalty = 0.0\n",
        "        self.correlation_penalty = corr_penalty\n",
        "\n",
        "        # =========================================================\n",
        "        # 2. STRICT PATHFINDING LOGIC\n",
        "        # =========================================================\n",
        "\n",
        "        node_metrics = self.capped_node_metrics\n",
        "        node_types = infer_node_types(node_metrics)\n",
        "\n",
        "        # --- INTERNAL VALIDATOR FUNCTION ---\n",
        "        def is_strictly_valid(path):\n",
        "            # 1. Topology Constraints\n",
        "            if path[0] != 0: return False\n",
        "            if path[-1] != (D - 1): return False\n",
        "            if len(path) != D: return False # Must be Hamiltonian\n",
        "            if len(set(path)) != D: return False # Unique nodes only\n",
        "\n",
        "            # 2. Coupling Constraints (Simulation)\n",
        "            state_machine = CouplingState()\n",
        "\n",
        "            for k in range(len(path) - 1):\n",
        "                u, v = path[k], path[k+1]\n",
        "                type_u = node_types[u]\n",
        "                type_v = node_types[v]\n",
        "\n",
        "                # Check if transition is allowed by current state\n",
        "                if not state_machine.allows(type_v):\n",
        "                    return False\n",
        "\n",
        "                # Determine coupling label for update\n",
        "                if type_u == \"B\" and type_v == \"B\":\n",
        "                    coupling = \"B->B->C\"\n",
        "                else:\n",
        "                    coupling = f\"{type_u}->{type_v}\"\n",
        "\n",
        "                state_machine.update(coupling)\n",
        "\n",
        "            return True\n",
        "\n",
        "        # --- A. SEARCH EXISTING TOPOLOGY ---\n",
        "        C_matrix = build_coupling_matrix(node_types)\n",
        "\n",
        "        # We start DFS specifically at 0\n",
        "        raw_paths = get_all_paths(\n",
        "            self.chosen_Gmat,\n",
        "            C_matrix,\n",
        "            node_types,\n",
        "            start=0,\n",
        "            end=D - 1\n",
        "        )\n",
        "\n",
        "        valid_paths = [p for p in raw_paths if is_strictly_valid(p)]\n",
        "\n",
        "        # --- B. SYNTHESIS FALLBACK (If topology is sparse/invalid) ---\n",
        "        # If no valid paths exist in the current weights, we MUST synthesize one\n",
        "        # to ensure the layer does not break the logic.\n",
        "        if len(valid_paths) == 0:\n",
        "            print(f\" [Constraint Enforcement] No natural paths found. Synthesizing valid Hamiltonian permutations...\")\n",
        "\n",
        "            middle_nodes = [n for n in range(D) if n != 0 and n != (D-1)]\n",
        "            attempts = 0\n",
        "\n",
        "            # Try random permutations until we find one that satisfies CouplingState\n",
        "            while len(valid_paths) < 3 and attempts < 1000:\n",
        "                np.random.shuffle(middle_nodes)\n",
        "                candidate = [0] + middle_nodes + [D-1]\n",
        "\n",
        "                if is_strictly_valid(candidate):\n",
        "                    valid_paths.append(candidate)\n",
        "\n",
        "                attempts += 1\n",
        "\n",
        "            if len(valid_paths) == 0:\n",
        "                # Should be extremely rare unless constraints are impossible (e.g. all B nodes)\n",
        "                print(\"CRITICAL WARNING: Constraints are too tight. No valid path exists.\")\n",
        "                # We do not fallback to invalid paths. The layer will be empty (Identity).\n",
        "\n",
        "        # =========================================================\n",
        "        # 3. LAYER CONSTRUCTION\n",
        "        # =========================================================\n",
        "\n",
        "        # Score the Valid Paths\n",
        "        def path_sum_score(path, G):\n",
        "            return sum(G[path[i], path[i+1]] for i in range(len(path) - 1))\n",
        "\n",
        "        scored_paths = [\n",
        "            (path, path_sum_score(path, self.chosen_Gmat))\n",
        "            for path in valid_paths\n",
        "        ]\n",
        "\n",
        "        # Sort and Select Top-K\n",
        "        K_paths = 3\n",
        "        scored_paths.sort(key=lambda x: x[1], reverse=True)\n",
        "        top_paths = [p for p, _ in scored_paths[:K_paths]]\n",
        "\n",
        "        # Build Matrix from Paths\n",
        "        G_layer = build_dsm_from_walks(D, top_paths)\n",
        "\n",
        "        # Sparsity: Top-1 outgoing edge per node for this layer\n",
        "        K_sparse = 2\n",
        "        G_layer_sparse = np.zeros_like(G_layer)\n",
        "        for i in range(D - 1):\n",
        "            row = G_layer[i].copy()\n",
        "            row[i] = 0.0\n",
        "            if row.sum() == 0: continue\n",
        "            idx = np.argsort(row)[-K_sparse:]\n",
        "            G_layer_sparse[i, idx] = row[idx]\n",
        "            s = G_layer_sparse[i].sum()\n",
        "            if s > 0: G_layer_sparse[i] /= s\n",
        "\n",
        "        # =========================================================\n",
        "        # 4. REVIEW & UPDATE\n",
        "        # =========================================================\n",
        "        print(f\"\\n--- REVIEWING STRICT LAYER ({len(valid_paths)} valid paths) ---\")\n",
        "        for idx, (p, s) in enumerate(scored_paths[:K_paths]):\n",
        "            path_str = \" -> \".join([f\"N{node}\" for node in p])\n",
        "            # Double check validation just for log assurance\n",
        "            status = \"VALID\" if is_strictly_valid(p) else \"INVALID\"\n",
        "            print(f\"Path {idx+1} [{status}]: {path_str} (Score: {s:.3f})\")\n",
        "\n",
        "        # Apply Additive Update\n",
        "        self.dsm_history.append(G_layer_sparse.copy())\n",
        "        self.chosen_Gmat = self.chosen_Gmat + (additive_rate * G_layer_sparse)\n",
        "\n",
        "        # Normalize\n",
        "        max_val = np.max(self.chosen_Gmat)\n",
        "        if max_val > 1.0:\n",
        "            self.chosen_Gmat /= max_val\n",
        "\n",
        "        # Collect final statistics (for visualization)\n",
        "        self.walks, self.best_walk = collect_and_select_best_walks(\n",
        "            self.chosen_Gmat,\n",
        "            self.capped_node_metrics\n",
        "        )\n",
        "\n",
        "        dsm_cost = np.sum(np.abs(self.chosen_Gmat))\n",
        "        combined_score = node_contributions.sum() - corr_penalty - (self.dsm_cost_weight * dsm_cost)\n",
        "\n",
        "        self.print_dsm_basic()\n",
        "\n",
        "        if not hasattr(self, \"_node_contributions_history\"):\n",
        "            self._node_contributions_history = []\n",
        "        self._node_contributions_history.append(node_contributions.copy())\n",
        "\n",
        "        return node_metrics_list, combined_score, node_contributions\n",
        "\n",
        "    def run(self, outer_generations=1, num_dsm_layers=5):\n",
        "        best_score = -np.inf\n",
        "\n",
        "        # Fix: Initialize Decomposer with a zero matrix\n",
        "        baseline = np.zeros_like(self.chosen_Gmat)\n",
        "        dsm_decomposer = DSM_Layer_Decomposer(baseline, mode='compounded')\n",
        "        dsm_decomposer.current_total = self.chosen_Gmat.copy()\n",
        "\n",
        "        print(f\"Starting Optimization: {num_dsm_layers} Layers x {outer_generations} Gens\")\n",
        "\n",
        "        for layer_idx in range(num_dsm_layers):\n",
        "            print(f\"\\n>>> STARTING LAYER {layer_idx + 1} / {num_dsm_layers} <<<\")\n",
        "\n",
        "            for gen in range(outer_generations):\n",
        "                # 1. Inner Loop (Simulation)\n",
        "                node_metrics_list = []\n",
        "                for node_idx in range(self.D_graph):\n",
        "                    full_target = self.synthetic_targets[node_idx]['target']\n",
        "                    D_fcm = self.candidate_dims[node_idx][0]\n",
        "                    target = full_target[:D_fcm]\n",
        "\n",
        "                    _, _, _, _, metrics = self.run_inner(node_idx, target, D_fcm)\n",
        "                    node_metrics_list.append(metrics)\n",
        "\n",
        "                self.capped_node_metrics = node_metrics_list\n",
        "\n",
        "                # 2. Outer Loop (Topology Optimization)\n",
        "                _, capped_score, _ = self.run_outer()\n",
        "                best_score = max(best_score, capped_score)\n",
        "\n",
        "                # --- PROGRESS PRINT ---\n",
        "                print(f\" [Layer {layer_idx+1}] Gen {gen+1}/{outer_generations} | Score: {capped_score:.4f}\", end='\\r')\n",
        "\n",
        "            print(\"\") # Newline after generation loop finishes\n",
        "\n",
        "            # Snapshot the state\n",
        "            dsm_decomposer.add_snapshot(self.chosen_Gmat)\n",
        "\n",
        "        self.dsm_layers = dsm_decomposer.layers\n",
        "        print(\"\\nOptimization Complete.\")\n",
        "        return best_score\n",
        "\n",
        "    # ---------- VISUALIZATIONS & ANALYSIS ----------\n",
        "\n",
        "    def plot_pointwise_minmax_elite(self, top_k=21):\n",
        "        plt.figure(figsize=(14, 3))\n",
        "        for i in range(self.D_graph):\n",
        "            dim_i = self.candidate_dims[i][0]\n",
        "            base = self.nested_reps[i][:dim_i]\n",
        "            reps = np.clip(base + np.random.normal(0, 0.05, (top_k, len(base))), 0, 1)\n",
        "            y_min, y_max = reps.min(axis=0), reps.max(axis=0)\n",
        "            y_sel = base\n",
        "\n",
        "            y_true = self.synthetic_targets[i]['target'][:len(y_sel)]\n",
        "            if len(y_true) < len(y_sel):\n",
        "                y_true = np.pad(y_true, (0, len(y_sel) - len(y_true)), \"constant\")\n",
        "            else:\n",
        "                y_true = y_true[:len(y_sel)]\n",
        "\n",
        "            plt.subplot(1, self.D_graph, i + 1)\n",
        "            plt.fill_between(range(len(y_min)), y_min, y_max, color='skyblue', alpha=0.4, label='Elite Interval')\n",
        "            plt.plot(y_sel, 'k-', lw=2, label='Estimated')\n",
        "            plt.plot(y_true, 'r--', lw=2, label='True')\n",
        "            plt.ylim(0, 1.05)\n",
        "            plt.title(f\"Node {i + 1}\")\n",
        "            if i == 0: plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def plot_nested_activations(self):\n",
        "        plt.figure(figsize=(12, 3))\n",
        "        for i, rep in enumerate(self.nested_reps):\n",
        "            dim_i = self.candidate_dims[i][0]\n",
        "            rep_i = rep[:dim_i]\n",
        "            plt.subplot(1, self.D_graph, i + 1)\n",
        "            plt.bar(range(len(rep_i)), rep_i, color=plt.cm.plasma(rep_i))\n",
        "            plt.ylim(0, 1)\n",
        "            plt.title(f\"Node {i + 1}\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def plot_outer_fuzzy_graph(self):\n",
        "        G = nx.DiGraph()\n",
        "        for i in range(self.D_graph): G.add_node(i)\n",
        "        for i in range(self.D_graph):\n",
        "            for j in range(self.D_graph):\n",
        "                if i != j and abs(self.chosen_Gmat[i, j]) > 0.02:\n",
        "                    G.add_edge(i, j, weight=self.chosen_Gmat[i, j])\n",
        "\n",
        "        node_sizes = [self.best_dim_per_node[i] * 200 for i in range(self.D_graph)]\n",
        "        edge_colors = ['green' if d['weight'] > 0 else 'red' for _, _, d in G.edges(data=True)]\n",
        "        edge_widths = [abs(d['weight']) * 3 for _, _, d in G.edges(data=True)]\n",
        "\n",
        "        pos = nx.spring_layout(G)\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        nx.draw(G, pos, node_size=node_sizes, node_color='skyblue',\n",
        "                edge_color=edge_colors, width=edge_widths, arrows=True, with_labels=True)\n",
        "        plt.title(\"Outer Fuzzy Multiplex Graph\")\n",
        "        plt.show()\n",
        "\n",
        "    def print_interactions(self, return_tensor=True, verbose=True):\n",
        "        D_graph = self.D_graph\n",
        "        inter_dim = self.inter_layer.inter_dim\n",
        "        inter_tensor = np.zeros((D_graph, D_graph, inter_dim))\n",
        "\n",
        "        acts = self.inter_layer.build_activations(self.chosen_Gmat, self.nested_reps)\n",
        "        if not acts:\n",
        "            if verbose:\n",
        "                print(\"No active edges above threshold.\")\n",
        "            return inter_tensor if return_tensor else None\n",
        "\n",
        "        for (i, j), vec in acts.items():\n",
        "            inter_tensor[i, j, :] = vec\n",
        "            if verbose:\n",
        "                act_str = \", \".join([f\"{v:.3f}\" for v in vec])\n",
        "                print(f\"Node {i} -> Node {j}: [{act_str}]\")\n",
        "        return inter_tensor if return_tensor else None\n",
        "\n",
        "    def compute_fuzzy_metric_tensor(self, normalize=True, verbose=False):\n",
        "        metrics_keys = self.MK\n",
        "        D = self.D_graph\n",
        "        num_metrics = len(metrics_keys)\n",
        "        tensor = np.zeros((D, D, num_metrics))\n",
        "\n",
        "        metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "\n",
        "        node_metrics = []\n",
        "        for i, rep in enumerate(self.nested_reps):\n",
        "            metrics = metrics_evaluator.compute_node_metrics(i, y=rep)\n",
        "            node_metrics.append(np.array([metrics[k] for k in metrics_keys]))\n",
        "        node_metrics = np.array(node_metrics)\n",
        "\n",
        "        for i in range(D):\n",
        "            for j in range(D):\n",
        "                if i == j:\n",
        "                    tensor[i, j, :] = node_metrics[j]\n",
        "                else:\n",
        "                    weight = np.clip(abs(self.chosen_Gmat[i, j]), 0, 1)\n",
        "                    tensor[i, j, :] = weight * node_metrics[j]\n",
        "\n",
        "        if normalize:\n",
        "            tensor = (tensor - tensor.min()) / (tensor.max() - tensor.min() + 1e-12)\n",
        "\n",
        "        if verbose:\n",
        "            print(\"Fuzzy Metric Tensor shape:\", tensor.shape)\n",
        "\n",
        "        return tensor\n",
        "\n",
        "    def plot_fuzzy_metric_tensor_heatmaps(self, fuzzy_tensor=None, metrics_keys=None):\n",
        "        if metrics_keys is None:\n",
        "            metrics_keys = self.MK\n",
        "        if fuzzy_tensor is None:\n",
        "            fuzzy_tensor = self.compute_fuzzy_metric_tensor(normalize=True)\n",
        "\n",
        "        D = self.D_graph\n",
        "        num_metrics = len(metrics_keys)\n",
        "\n",
        "        fig, axes = plt.subplots(1, num_metrics, figsize=(4 * num_metrics, 4))\n",
        "        if num_metrics == 1: axes = [axes]\n",
        "\n",
        "        im = None\n",
        "        for k, key in enumerate(metrics_keys):\n",
        "            data = fuzzy_tensor[:, :, k]\n",
        "            im = axes[k].imshow(data, cmap='viridis', vmin=0, vmax=1)\n",
        "            for i in range(D):\n",
        "                for j in range(D):\n",
        "                    axes[k].text(j, i, f\"{data[i, j]:.2f}\", ha='center', va='center', color='white', fontsize=9)\n",
        "            axes[k].set_xticks(range(D))\n",
        "            axes[k].set_yticks(range(D))\n",
        "            axes[k].set_xticklabels([f'Node {j}' for j in range(D)])\n",
        "            axes[k].set_yticklabels([f'Node {i}' for i in range(D)])\n",
        "            axes[k].set_title(f'FMT - {key}')\n",
        "\n",
        "        fig.colorbar(im, ax=axes, orientation='vertical', fraction=0.025, pad=0.04, label='Normalized Metric Value')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def compute_fmt_with_elite_bounds(self, top_k=21):\n",
        "        metrics_keys = self.MK\n",
        "        D = self.D_graph\n",
        "        num_metrics = len(metrics_keys)\n",
        "        tensor_bounds = np.zeros((D, D, num_metrics, 2))\n",
        "\n",
        "        metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "\n",
        "        for i in range(D):\n",
        "            base = self.nested_reps[i]\n",
        "            reps = np.clip(base + np.random.normal(0, 0.05, (top_k, len(base))), 0, 1)\n",
        "\n",
        "            metrics_matrix = np.zeros((top_k, num_metrics))\n",
        "            for idx, rep in enumerate(reps):\n",
        "                m = metrics_evaluator.compute_node_metrics(i, y=rep)\n",
        "                metrics_matrix[idx, :] = [m[k] for k in metrics_keys]\n",
        "\n",
        "            lower_i = metrics_matrix.min(axis=0)\n",
        "            upper_i = metrics_matrix.max(axis=0)\n",
        "\n",
        "            for j in range(D):\n",
        "                tensor_bounds[i, j, :, 0] = lower_i\n",
        "                tensor_bounds[i, j, :, 1] = upper_i\n",
        "\n",
        "        return tensor_bounds\n",
        "\n",
        "    def plot_fmt_with_bounds(self, fmt_tensor_bounds):\n",
        "        D = self.D_graph\n",
        "        metrics_keys = self.MK\n",
        "        M_actual = len(metrics_keys)\n",
        "\n",
        "        mean_vals = (fmt_tensor_bounds[:, :, :, 0] + fmt_tensor_bounds[:, :, :, 1]) / 2\n",
        "        mean_vals = mean_vals.mean(axis=1)  # mean across targets\n",
        "        mean_vals = mean_vals.mean(axis=0, keepdims=True)  # mean across nodes\n",
        "\n",
        "        if hasattr(self, 'best_alpha') and hasattr(self, 'best_w_contrib'):\n",
        "            mean_weight = (self.best_alpha * self.best_w_contrib).mean()\n",
        "            mean_vals = mean_vals * mean_weight\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(1.2 * M_actual + 4, 2))\n",
        "        im = ax.imshow(mean_vals, cmap='viridis', aspect='auto')\n",
        "\n",
        "        vmin, vmax = mean_vals.min(), mean_vals.max()\n",
        "        for i in range(mean_vals.shape[0]):\n",
        "            for k in range(M_actual):\n",
        "                val = mean_vals[i, k]\n",
        "                color = 'white' if val < (vmin + 0.5 * (vmax - vmin)) else 'black'\n",
        "                ax.text(k, i, f\"{val:.2f}\", ha='center', va='center', color=color, fontsize=8)\n",
        "\n",
        "        ax.set_xticks(range(M_actual))\n",
        "        ax.set_xticklabels(metrics_keys[:M_actual], rotation=45, ha='right')\n",
        "        ax.set_yticks([0])\n",
        "        ax.set_yticklabels(['Mean across nodes'])\n",
        "        ax.set_title(\"Weighted FMT with Bounds\")\n",
        "        fig.colorbar(im, ax=ax, label='Weighted Mean Metric Value')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def plot_node_score_contribution(self, metrics_keys=None):\n",
        "        if metrics_keys is None:\n",
        "            metrics_keys = self.MK\n",
        "        D = self.D_graph\n",
        "        node_contributions = np.array(self.node_score_contributions)\n",
        "\n",
        "        if hasattr(self, 'weighted_fmt'):\n",
        "            fuzzy_tensor = np.array(self.weighted_fmt)\n",
        "        else:\n",
        "            fuzzy_tensor = self.compute_fuzzy_metric_tensor(normalize=True)\n",
        "\n",
        "        fuzzy_tensor_norm = (fuzzy_tensor - fuzzy_tensor.min()) / (fuzzy_tensor.max() - fuzzy_tensor.min() + 1e-12)\n",
        "        fmt_matrix = fuzzy_tensor_norm.sum(axis=2)\n",
        "        np.fill_diagonal(fmt_matrix, 0)\n",
        "\n",
        "        raw_matrix = np.zeros((D, D))\n",
        "        np.fill_diagonal(raw_matrix, node_contributions)\n",
        "\n",
        "        total_matrix = raw_matrix + fmt_matrix\n",
        "\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "        matrices = [raw_matrix, fmt_matrix, total_matrix]\n",
        "        titles = [\"Raw Node Contribution\", \"Normalized FMT Contribution\", \"Total Contribution\"]\n",
        "\n",
        "        im = None\n",
        "        for ax, mat, title in zip(axes, matrices, titles):\n",
        "            im = ax.imshow(mat, cmap='viridis', vmin=0, vmax=1)\n",
        "            for i in range(D):\n",
        "                for j in range(D):\n",
        "                    ax.text(j, i, f\"{mat[i, j]:.2f}\", ha='center', va='center', color='white', fontsize=8)\n",
        "            ax.set_title(title)\n",
        "            ax.set_xticks(range(D))\n",
        "            ax.set_xticklabels([f\"Node {i + 1}\" for i in range(D)])\n",
        "            ax.set_yticks(range(D))\n",
        "            ax.set_yticklabels([f\"Node {i + 1}\" for i in range(D)])\n",
        "\n",
        "        fig.colorbar(im, ax=axes, orientation='vertical', fraction=0.025, pad=0.04, label='Contribution Value')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def plot_fmt_with_run_metrics(self, metrics_keys=None):\n",
        "        if metrics_keys is None:\n",
        "            metrics_keys = self.MK\n",
        "        D = self.D_graph\n",
        "        M_actual = len(metrics_keys)\n",
        "\n",
        "        if not hasattr(self, 'capped_node_metrics'):\n",
        "            raise ValueError(\"No node metrics available. Run run_outer() first.\")\n",
        "\n",
        "        weighted_fmt = np.zeros((D, D, M_actual))\n",
        "        for i in range(D):\n",
        "            for j in range(D):\n",
        "                for k, key in enumerate(metrics_keys):\n",
        "                    val = self.capped_node_metrics[j][key]\n",
        "                    if i != j:\n",
        "                        val *= np.clip(abs(self.chosen_Gmat[i, j]), 0, 1)\n",
        "                    weighted_fmt[i, j, k] = val\n",
        "\n",
        "        for i in range(D):\n",
        "            for k in range(M_actual):\n",
        "                if not METRIC_TARGET[i][k]:\n",
        "                    weighted_fmt[i, :, k] = 0.0\n",
        "\n",
        "        mean_vals = weighted_fmt.mean(axis=1)\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(1.2 * M_actual + 4, 0.35 * D + 4))\n",
        "        im = ax.imshow(mean_vals, cmap='viridis', aspect='auto')\n",
        "\n",
        "        vmin, vmax = mean_vals.min(), mean_vals.max()\n",
        "        for i in range(D):\n",
        "            for k in range(M_actual):\n",
        "                val = mean_vals[i, k]\n",
        "                color = 'white' if val < (vmin + 0.5 * (vmax - vmin)) else 'black'\n",
        "                ax.text(k, i, f\"{val:.2f}\", ha='center', va='center', color=color, fontsize=8)\n",
        "\n",
        "        ax.set_xticks(range(M_actual))\n",
        "        ax.set_xticklabels(metrics_keys[:M_actual], rotation=45, ha='right')\n",
        "        ax.set_yticks(range(D))\n",
        "        ax.set_yticklabels([f\"Node {i}\" for i in range(D)])\n",
        "        ax.set_title(\"Weighted FMT Metrics (Actual Run Output)\")\n",
        "        fig.colorbar(im, ax=ax, label='Weighted Metric Value')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def collect_fmt_datapoints(self):\n",
        "        self.fmt_datapoints = {k: [] for k in self.MK}\n",
        "        for node_idx in range(self.D_graph):\n",
        "            if len(self.PLMS[node_idx]) == 0:\n",
        "                continue\n",
        "            for weighted_input_val, metric_vals in self.PLMS[node_idx]:\n",
        "                for m, key in enumerate(self.MK):\n",
        "                    if m < len(metric_vals):\n",
        "                        self.fmt_datapoints[key].append((weighted_input_val, metric_vals[m]))\n",
        "\n",
        "    def collect_metric_traces_per_node(self):\n",
        "        self.metric_traces_per_node = [{} for _ in range(self.D_graph)]\n",
        "        for node_idx in range(self.D_graph):\n",
        "            self.metric_traces_per_node[node_idx] = {k: [] for k in self.MK}\n",
        "            if len(self.PLMS[node_idx]) == 0:\n",
        "                continue\n",
        "            for weighted_input_val, metric_vals in self.PLMS[node_idx]:\n",
        "                for m, key in enumerate(self.MK):\n",
        "                    if m < len(metric_vals):\n",
        "                        self.metric_traces_per_node[node_idx][key].append((weighted_input_val, metric_vals[m]))\n",
        "\n",
        "    def plot_fmt_per_datapoint(self, top_k=21, span=0.3, grid_size=100):\n",
        "        if not hasattr(self, 'fmt_datapoints'):\n",
        "            self.collect_fmt_datapoints()\n",
        "\n",
        "        for key, formula in zip(self.MK, METRIC_FORMULAS):\n",
        "            if key not in self.fmt_datapoints or len(self.fmt_datapoints[key]) == 0:\n",
        "                continue\n",
        "\n",
        "            data = np.array(self.fmt_datapoints[key])\n",
        "            x_data, y_data = data[:, 0], data[:, 1]\n",
        "\n",
        "            x_curve = np.linspace(x_data.min() - span, x_data.max() + span, grid_size)\n",
        "            y_curve = np.array([formula(x) for x in x_curve])\n",
        "\n",
        "            plt.figure(figsize=(6, 4))\n",
        "            plt.scatter(x_data, y_data, alpha=0.6, label=\"FMT datapoints\")\n",
        "            plt.plot(x_curve, y_curve, 'r', lw=2, label=\"Metric equation\")\n",
        "            plt.xlabel(\"Weighted Input\")\n",
        "            plt.ylabel(f\"{key} (FMT)\")\n",
        "            plt.title(f\"FMT per datapoint - Metric: {key}\")\n",
        "            plt.legend()\n",
        "            plt.grid(alpha=0.3)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "    def plot_metric_equations_per_node(self, grid_size=100, span=0.3):\n",
        "        if not hasattr(self, 'metric_traces_per_node'):\n",
        "            self.collect_metric_traces_per_node()\n",
        "\n",
        "        for node_idx in range(self.D_graph):\n",
        "            node_traces = self.metric_traces_per_node[node_idx]\n",
        "            if all(len(v) == 0 for v in node_traces.values()):\n",
        "                continue\n",
        "\n",
        "            plt.figure(figsize=(6, 4))\n",
        "            for key, formula in zip(self.MK, METRIC_FORMULAS):\n",
        "                if key not in node_traces or len(node_traces[key]) == 0:\n",
        "                    continue\n",
        "                data = np.array(node_traces[key])\n",
        "                x_data, y_data = data[:, 0], data[:, 1]\n",
        "\n",
        "                x_curve = np.linspace(x_data.min() - span, x_data.max() + span, grid_size)\n",
        "                y_curve = np.array([formula(x) for x in x_curve])\n",
        "\n",
        "                plt.scatter(x_data, y_data, alpha=0.6, label=f\"{key} datapoints\")\n",
        "                plt.plot(x_curve, y_curve, 'r', lw=2, label=f\"{key} equation\")\n",
        "\n",
        "            plt.xlabel(\"Weighted Input\")\n",
        "            plt.ylabel(\"Metric Value\")\n",
        "            plt.title(f\"Node {node_idx} - Metric Equations\")\n",
        "            plt.legend()\n",
        "            plt.grid(alpha=0.3)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN EXECUTION BLOCK\n",
        "# =============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Ensure necessary globals exist before running; otherwise this block is illustrative\n",
        "    try:\n",
        "        optimizer = Fuzzy_Hierarchical_Multiplex(\n",
        "            candidate_dims, D_graph,\n",
        "            synthetic_targets,\n",
        "            inner_learning, gamma_interlayer=0,\n",
        "            causal_flag=False\n",
        "        )\n",
        "\n",
        "        # Run Optimization\n",
        "        metrics_list = optimizer.run()\n",
        "\n",
        "        # Visualizations\n",
        "        optimizer.plot_pointwise_minmax_elite()\n",
        "        optimizer.plot_nested_activations()\n",
        "\n",
        "        # Compute FMT with elite bounds\n",
        "        #fmt_elite_bounds = optimizer.compute_fmt_with_elite_bounds(top_k=top_k + 10)\n",
        "\n",
        "        # Plot as heatmaps\n",
        "        #optimizer.plot_fmt_with_run_metrics()\n",
        "\n",
        "        # Compute fuzzy multiplex tensor\n",
        "        #fmt_tensor = optimizer.compute_fuzzy_metric_tensor(normalize=False)\n",
        "        #optimizer.plot_fuzzy_metric_tensor_heatmaps(fmt_tensor)\n",
        "\n",
        "        # Plot Contributions & Graph\n",
        "        optimizer.plot_node_score_contribution()\n",
        "        optimizer.plot_outer_fuzzy_graph()\n",
        "\n",
        "        # Interactions\n",
        "       # tensor = optimizer.print_interactions()\n",
        "        #print(\"Tensor shape:\", tensor.shape, '\\n', tensor)\n",
        "\n",
        "        # Datapoints & Equations\n",
        "        optimizer.collect_fmt_datapoints()\n",
        "        optimizer.plot_fmt_per_datapoint()\n",
        "        optimizer.collect_metric_traces_per_node()\n",
        "        optimizer.plot_metric_equations_per_node()\n",
        "\n",
        "        # DSM Tracking Demo\n",
        "        dsm_tracker = DSM_Tracker(optimizer)\n",
        "\n",
        "        # Run extra DSM update\n",
        "        optimizer.run_outer()\n",
        "        dsm_tracker.update_dsms()\n",
        "\n",
        "        # Retrieve matrices\n",
        "        primary, residual = dsm_tracker.get_matrices()\n",
        "        print(\"Primary DSM:\\n\", primary)\n",
        "        print(\"Residual DSM:\\n\", residual)\n",
        "\n",
        "    except NameError as e:\n",
        "        print(f\"Error: Missing external dependency definition. \\n{e}\")\n",
        "        print(\"Please ensure D_graph, DATA_MATRIX, METRIC_KEYS, etc. are defined.\")"
      ],
      "metadata": {
        "id": "5svhAWO7NO1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Comparison of dummy, initial DSM and optimized DSM"
      ],
      "metadata": {
        "id": "KJyg3qT9jbJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_dsm_three_way(optimizer,\n",
        "                          baseline_dsm,\n",
        "                          outer_steps=10,\n",
        "                          threshold=0.05):\n",
        "    \"\"\"\n",
        "    Computes baseline DSM comparison with initial and optimized DSMs.\n",
        "    Automatically aligns matrices by size to avoid broadcasting errors.\n",
        "\n",
        "    Parameters:\n",
        "        optimizer: object with .chosen_Gmat and .run_outer()\n",
        "        baseline_dsm: np.array or pd.DataFrame (baseline DSM)\n",
        "        outer_steps: int, number of optimization iterations\n",
        "        threshold: float, edge threshold for graph drawing\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    import networkx as nx\n",
        "\n",
        "    # Convert to numpy if DataFrame\n",
        "    if isinstance(baseline_dsm, pd.DataFrame):\n",
        "        baseline_dsm = baseline_dsm.values.copy()\n",
        "\n",
        "    D = baseline_dsm.shape[0]\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 1. INITIAL MODEL DSM\n",
        "    # --------------------------------------------------\n",
        "    initial_dsm = optimizer.chosen_Gmat.copy()\n",
        "\n",
        "    # Ensure shapes match\n",
        "    if initial_dsm.shape != baseline_dsm.shape:\n",
        "        raise ValueError(f\"Shape mismatch: initial DSM {initial_dsm.shape} vs baseline DSM {baseline_dsm.shape}\")\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 2. RUN DSM OPTIMIZATION\n",
        "    # --------------------------------------------------\n",
        "    for _ in range(outer_steps):\n",
        "        optimizer.run_outer()\n",
        "\n",
        "    optimized_dsm = optimizer.chosen_Gmat.copy()\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 3. DELTAS\n",
        "    # --------------------------------------------------\n",
        "    delta_init_vs_base = initial_dsm - baseline_dsm\n",
        "    delta_opt_vs_base = optimized_dsm - baseline_dsm\n",
        "    delta_opt_vs_init = optimized_dsm - initial_dsm\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 4. NUMERICAL SUMMARY\n",
        "    # --------------------------------------------------\n",
        "    def summarize(label, delta):\n",
        "        return {\n",
        "            \"label\": label,\n",
        "            \"L1\": np.sum(np.abs(delta)),\n",
        "            \"L2\": np.linalg.norm(delta),\n",
        "            \"max\": np.max(np.abs(delta))\n",
        "        }\n",
        "\n",
        "    summaries = [\n",
        "        summarize(\"Initial − Baseline\", delta_init_vs_base),\n",
        "        summarize(\"Optimized − Baseline\", delta_opt_vs_base),\n",
        "        summarize(\"Optimized − Initial\", delta_opt_vs_init),\n",
        "    ]\n",
        "\n",
        "    print(\"\\n=== TRIPLE DSM COMPARISON ===\")\n",
        "    for s in summaries:\n",
        "        print(f\"{s['label']:<22} | \"\n",
        "              f\"L1={s['L1']:.4f}  \"\n",
        "              f\"L2={s['L2']:.4f}  \"\n",
        "              f\"MaxΔ={s['max']:.4f}\")\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 5. HEATMAPS\n",
        "    # --------------------------------------------------\n",
        "    mats = [\n",
        "        baseline_dsm,\n",
        "        initial_dsm,\n",
        "        optimized_dsm,\n",
        "        delta_opt_vs_base\n",
        "    ]\n",
        "    titles = [\n",
        "        \"Baseline DSM\",\n",
        "        \"Initial DSM\",\n",
        "        \"Optimized DSM\",\n",
        "        \"Optimized − Baseline\"\n",
        "    ]\n",
        "\n",
        "    fig, axes = plt.subplots(1, 4, figsize=(20, 4))\n",
        "    for ax, mat, title in zip(axes, mats, titles):\n",
        "        im = ax.imshow(mat, cmap=\"coolwarm\")\n",
        "        ax.set_title(title)\n",
        "        ax.set_xticks(range(D))\n",
        "        ax.set_yticks(range(D))\n",
        "    fig.colorbar(im, ax=axes, fraction=0.025, pad=0.04)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 6. GRAPH COMPARISON\n",
        "    # --------------------------------------------------\n",
        "    def draw_graph(Gmat, title):\n",
        "        G = nx.DiGraph()\n",
        "        for i in range(Gmat.shape[0]):\n",
        "            for j in range(Gmat.shape[1]):\n",
        "                if i != j and abs(Gmat[i, j]) > threshold:\n",
        "                    G.add_edge(i, j, weight=Gmat[i, j])\n",
        "\n",
        "        pos = nx.shell_layout(G)\n",
        "        edge_colors = [\"green\" if d[\"weight\"] > 0 else \"red\"\n",
        "                       for _, _, d in G.edges(data=True)]\n",
        "        widths = [abs(d[\"weight\"]) * 3 for _, _, d in G.edges(data=True)]\n",
        "\n",
        "        plt.figure(figsize=(7, 7))\n",
        "        nx.draw(G, pos,\n",
        "                node_color=\"lightblue\",\n",
        "                node_size=600,\n",
        "                edge_color=edge_colors,\n",
        "                width=widths,\n",
        "                arrows=True,\n",
        "                with_labels=True)\n",
        "        plt.title(title)\n",
        "        plt.show()\n",
        "\n",
        "    draw_graph(baseline_dsm, \"Baseline DSM\")\n",
        "    draw_graph(initial_dsm, \"Initial DSM\")\n",
        "    draw_graph(optimized_dsm, \"Optimized DSM\")\n",
        "\n",
        "    return {\n",
        "        \"baseline\": baseline_dsm,\n",
        "        \"initial\": initial_dsm,\n",
        "        \"optimized\": optimized_dsm,\n",
        "        \"delta_init_vs_base\": delta_init_vs_base,\n",
        "        \"delta_opt_vs_base\": delta_opt_vs_base,\n",
        "        \"delta_opt_vs_init\": delta_opt_vs_init,\n",
        "        \"summaries\": summaries\n",
        "    }\n"
      ],
      "metadata": {
        "id": "3IAT5ZW3o_jZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = compare_dsm_three_way(\n",
        "    optimizer=optimizer,\n",
        "    baseline_dsm=DSM_MATRIX,  # 8×8\n",
        "    outer_steps=0\n",
        ")\n"
      ],
      "metadata": {
        "id": "VpAvD9JiZy7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Evaluation of the results to a canonical basis"
      ],
      "metadata": {
        "id": "1akv67QujpYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# ALL-IN-ONE DSM + Cost/Quality Evaluation\n",
        "# ==========================================\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---- Helper: Evaluate cost/quality ----\n",
        "def evaluate_cost_quality_for_dsm(evaluator, dsm_matrix):\n",
        "    \"\"\"\n",
        "    Apply DSM effect (e.g., as a weight) to data_matrix and compute cost & quality.\n",
        "    Currently uses evaluator as-is. In a real scenario, DSM interactions could modulate features.\n",
        "    \"\"\"\n",
        "    return evaluator.compute_system_metrics(df_post)  # using post-optimized execution data\n",
        "\n",
        "# ---- Run DSM comparison & get matrices ----\n",
        "dsm_results = compare_dsm_three_way(\n",
        "    optimizer=optimizer,\n",
        "    baseline_dsm=DSM_MATRIX,  # 8×8 BAI DSM\n",
        "    outer_steps=1\n",
        ")\n",
        "\n",
        "baseline_dsm  = dsm_results[\"baseline\"]\n",
        "initial_dsm   = dsm_results[\"initial\"]\n",
        "optimized_dsm = dsm_results[\"optimized\"]\n",
        "\n",
        "# ---- Evaluate cost & quality for each DSM ----\n",
        "baseline_cost, baseline_quality = evaluate_cost_quality_for_dsm(evaluator, baseline_dsm)\n",
        "init_cost, init_quality         = evaluate_cost_quality_for_dsm(evaluator, initial_dsm)\n",
        "opt_cost, opt_quality           = evaluate_cost_quality_for_dsm(evaluator, optimized_dsm)\n",
        "\n",
        "# ---- Compute DSM deltas ----\n",
        "def dsm_delta_metrics(A, B):\n",
        "    delta = A - B\n",
        "    return np.sum(np.abs(delta)), np.linalg.norm(delta), np.max(np.abs(delta))\n",
        "\n",
        "deltas = {\n",
        "    \"Initial − Baseline\"   : dsm_delta_metrics(initial_dsm, baseline_dsm),\n",
        "    \"Optimized − Baseline\" : dsm_delta_metrics(optimized_dsm, baseline_dsm),\n",
        "    \"Optimized − Initial\"  : dsm_delta_metrics(optimized_dsm, initial_dsm)\n",
        "}\n",
        "\n",
        "# ---- Print cost/quality comparison ----\n",
        "print(\"\\n=== COST / QUALITY COMPARISON ===\")\n",
        "print(f\"Baseline   | Cost: {baseline_cost:.4f} | Quality: {baseline_quality:.4f}\")\n",
        "print(f\"Initial    | Cost: {init_cost:.4f} | Quality: {init_quality:.4f}\")\n",
        "print(f\"Optimized  | Cost: {opt_cost:.4f} | Quality: {opt_quality:.4f}\")\n",
        "\n",
        "# ---- Optional: Heatmaps ----\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "for ax, mat, title in zip(axes,\n",
        "                          [baseline_dsm, initial_dsm, optimized_dsm],\n",
        "                          [\"Baseline DSM\", \"Initial DSM\", \"Optimized DSM\"]):\n",
        "    im = ax.imshow(mat, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
        "    ax.set_title(title)\n",
        "plt.colorbar(im, ax=axes, fraction=0.03, pad=0.04)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-q9GNUcsj1UY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"=== DUMMY RESULTS ===\")\n",
        "print(f\"1. Structural Complexity Cost (DSM Coupling): {structural_cost:.4f}\")\n",
        "#print(f\"   (High value = High integration effort between Route, Traffic, Load, etc.)\")\n",
        "print(f\"2. Operational Cost Index (Avg per delivery):  {avg_op_cost:.4f}\")\n",
        "print(f\"3. System Quality Score (Time & Satisfaction): {avg_quality:.4f}\")\n",
        "\n",
        "\n",
        "print(\"\\n=== COST / QUALITY COMPARISON ===\")\n",
        "print(f\"Dummy Opt  | Cost: {avg_op_cost:.4f} | Quality: {avg_quality:.4f}\")\n",
        "print(f\"Baseline   | Cost: {baseline_cost:.4f} | Quality: {baseline_quality:.4f}\")\n",
        "print(f\"Initial    | Cost: {init_cost:.4f} | Quality: {init_quality:.4f}\")\n",
        "print(f\"Optimized  | Cost: {opt_cost:.4f} | Quality: {opt_quality:.4f}\")\n"
      ],
      "metadata": {
        "id": "IgEmOPLhYfbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AB9NL8AIpMmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### The cost and the Quality are never intened for optimizaton, however they could be.\n",
        "\n",
        "# 7. Pareto Front Generation"
      ],
      "metadata": {
        "id": "UJ9OrOv4g5lG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_hKkhLIrizkz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}