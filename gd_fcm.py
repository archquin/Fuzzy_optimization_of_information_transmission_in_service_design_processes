# -*- coding: utf-8 -*-
"""GD_FCM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fR7CFGNiVmxcC8JbBptoHzaO1wvJNFI_

# Abstract

---

GDFCM stands for gradient descent fuzzy cognition multiplex, and it is a holistic approach to analyze and optimize service design processes. The idea is to determine which *new* data points are relevant for the processes and to what degree; to do so, one needs to define metrics and use the input data for which these metrics are applied. This way, a deductive net is constructed that measures the processes' contribution as well as the services' expenses in a way that the design process has defined. However, there is no new data in this construct as it is strictly supervised and it is meant to determine the constituent process activation at each node; that is achievable by a simple FCM to. Moreover, a tensor is formed with **deep node** interactions which are then used to create process layers for trait/metric activation. Finally, it is possible to deduce whether a new data point fits in the model, to what degree, and for which layer; that is according to inputs and the output is the solution of a process design function, according to metrics.

---
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import networkx as nx

# ---------------- Synthetic dataset ----------------
np.random.seed(42)
N = 1000

route_planning = pd.DataFrame({
    'origin_x': np.random.uniform(0, 100, N),
    'origin_y': np.random.uniform(0, 100, N),
    'dest_x': np.random.uniform(0, 100, N),
    'dest_y': np.random.uniform(0, 100, N),
    'traffic_density': np.random.uniform(0, 1, N),
    'road_type': np.random.choice([1, 2, 3], N),
})
route_planning['distance'] = np.sqrt((route_planning['dest_x'] - route_planning['origin_x'])**2 +
                                     (route_planning['dest_y'] - route_planning['origin_y'])**2)
speed_base = {1:50, 2:40, 3:30}
route_planning['speed'] = route_planning['road_type'].map(speed_base) * np.random.uniform(0.8,1.2,N)
route_planning['travel_time'] = (route_planning['distance']/route_planning['speed'])*60*\
                                (1+route_planning['traffic_density']*np.random.uniform(0.1,0.5,N))

vehicle_assignment = pd.DataFrame({
    'vehicle_capacity': np.random.randint(50,200,N),
    'battery_level': np.random.uniform(0.3,1.0,N),
    'delivery_size': np.random.randint(5,50,N),
    'vehicle_type': np.random.choice([1,2],N),
    'speed_factor': np.random.uniform(0.9,1.1,N),
})
vehicle_assignment['assigned_speed'] = route_planning['speed']*vehicle_assignment['speed_factor']
vehicle_assignment['load_utilization'] = vehicle_assignment['delivery_size']/vehicle_assignment['vehicle_capacity']

time_scheduling = pd.DataFrame({
    'requested_time': np.random.randint(8,20,N),
    'delivery_priority': np.random.randint(1,5,N),
    'customer_patience': np.random.uniform(0,1,N),
})
time_scheduling['delay_probability'] = np.clip(
    (route_planning['travel_time']/60)*(1+vehicle_assignment['load_utilization']*0.5)*np.random.uniform(0.8,1.2,N),
    0,1
)

dynamic_rerouting = pd.DataFrame({
    'current_x': np.random.uniform(0,100,N),
    'current_y': np.random.uniform(0,100,N),
    'traffic_updates': np.random.uniform(0,1,N),
    'new_delivery_requests': np.random.randint(0,3,N),
    'vehicle_status': np.random.choice([0,1],N),
    'weather': np.random.choice([0,1],N),
})
dynamic_rerouting['congestion_score'] = dynamic_rerouting['traffic_updates'] + \
                                       dynamic_rerouting['new_delivery_requests']*0.5 + \
                                       dynamic_rerouting['weather']*0.5 + \
                                       (route_planning['travel_time']/route_planning['travel_time'].max())*0.5

# ---------------- Combine and normalize ----------------
datasets = [route_planning, vehicle_assignment, time_scheduling, dynamic_rerouting]
dataset_dims = [df.shape[1] for df in datasets]
max_dim = max(dataset_dims)

padded_data = []
for df in datasets:
    arr = df.values
    if arr.shape[1] < max_dim:
        arr = np.hstack([arr, np.zeros((arr.shape[0], max_dim - arr.shape[1]))])
    padded_data.append(arr)

DATA_MATRIX = np.hstack(padded_data)
DATA_MATRIX = (DATA_MATRIX - DATA_MATRIX.min(axis=0)) / (np.ptp(DATA_MATRIX, axis=0)+1e-8)

def generate_targets(DATA_MATRIX, candidate_dims, D_graph):
    targets = []
    for node_idx in range(D_graph):
        row = DATA_MATRIX[node_idx % len(DATA_MATRIX)]
        node_targets = {}
        for dim in candidate_dims:
            if len(row) >= dim:
                sampled = row[:dim]
            else:
                sampled = np.pad(row, (0, dim - len(row)), constant_values=0.5)
            node_targets[dim] = sampled
        targets.append(node_targets)
    return targets
candidate_dims = [6,6,6,6,6,6]#,6,6,6,6,6,6,6,6,6,6,6,6,]
D_graph = 5
synthetic_targets = generate_targets(DATA_MATRIX, candidate_dims, D_graph)

# ---------------- Targets ----------------
#candidate_dims = [6,6,6,6,6,6,6]#,6,6,6,6,6,6,6,6,6,6,6,6,]
#D_graph = 4
inner_archive_size = 120
inner_offspring = 80
outer_archive_size = 80
outer_offspring = 80
inner_iters_per_outer = 15
outer_generations = 500
outer_cost_limit = 1350
inner_learning = 0.9
seed = 42
np.random.seed(seed)

# ---------------- Metrics Evaluator ----------------
class MetricsEvaluator:
    def __init__(self, data_matrix):
        self.data_matrix = data_matrix
        self.num_features = data_matrix.shape[1]
        self.W0 = 10.0
        self.T0 = 100.0
        self.U0 = 1.0

    def compute_node_metrics(self, node_idx, y=None):
        row = self.data_matrix[node_idx % self.data_matrix.shape[0]]
        k = self.num_features
        base = k // 3 if k >= 3 else 1
        wait_cols = list(range(0, base))
        thr_cols = list(range(base, 2*base))
        util_cols = list(range(2*base, k))
        wait_signal = np.mean(row[wait_cols])
        throughput_signal = np.mean(row[thr_cols])
        util_signal = np.mean(row[util_cols])
        if y is None: y = np.array([0.5,0.5,0.5])
        else: y = np.array(y[:3]) if len(y)>=3 else np.pad(y,(0,3-len(y)),constant_values=0.5)
        wait = self.W0*(1+1.2*wait_signal +0.8*y[0])
        throughput = self.T0*(1+1.1*throughput_signal +0.6*y[1]-0.4*wait_signal)
        util = self.U0 + 0.8*util_signal + 0.6*y[2]
        wait = float(np.clip(wait,0,100))
        throughput = float(np.clip(throughput,0,150))
        util = float(np.clip(util,0,1))
        score = -wait + throughput + util
        return {'wait': wait, 'throughput': throughput, 'util': util, 'score': score}

metrics_evaluator = MetricsEvaluator(DATA_MATRIX)
# ---------------- InterLayer (FULL) ----------------
class InterLayer:
    def __init__(self, D_graph, max_inner_dim, edge_threshold=0.02, seed=42):
        np.random.seed(seed)
        self.D_graph = D_graph
        self.max_input = 2*max_inner_dim
        self.weights = {}
        self.bias = {}
        for i in range(D_graph):
            for j in range(D_graph):
                if i==j: continue
                self.weights[(i,j)] = np.random.uniform(-0.6,0.6,(max_inner_dim, self.max_input))
                self.bias[(i,j)] = np.random.uniform(-0.3,0.3,max_inner_dim)
        self.edge_threshold = edge_threshold

    def compute_edge_activation(self, i,j,nested_reps):
        concat = np.concatenate([nested_reps[i], nested_reps[j]])
        if len(concat)<self.max_input:
            concat = np.pad(concat,(0,self.max_input-len(concat)))
        else:
            concat = concat[:self.max_input]
        v = self.weights[(i,j)].dot(concat) + self.bias[(i,j)]
        return 1/(1+np.exp(-v))

    def build_inter_activations(self,Gmat,nested_reps):
        acts = {}
        for i in range(self.D_graph):
            for j in range(self.D_graph):
                if i==j: continue
                if abs(Gmat[i,j])>self.edge_threshold:
                    acts[(i,j)] = self.compute_edge_activation(i,j,nested_reps)
        return acts

    @staticmethod
    def pairwise_squared_corr(acts):
        if len(acts)<2: return 0.0
        A = np.stack(list(acts.values()))
        A_cent = A - A.mean(axis=1, keepdims=True)
        stds = np.sqrt((A_cent**2).sum(axis=1)/(A.shape[1]-1)+1e-12)
        cov = A_cent.dot(A_cent.T)/(A.shape[1]-1)
        denom = np.outer(stds,stds)+1e-12
        corr = cov/denom
        np.fill_diagonal(corr,0)
        return (corr**2).sum()

    def mi_for_graph(self,Gmat,nested_reps):
        acts = self.build_inter_activations(Gmat,nested_reps)
        if len(acts)==0: return 0.0
        return float(self.pairwise_squared_corr(acts))


# ---------------- UnifiedACORMultiplex ----------------
class UnifiedACORMultiplex:
    def __init__(self, candidate_dims,D_graph,inner_archive_size,inner_offspring,
                 outer_archive_size,outer_offspring,synthetic_targets,inner_learning,causal_flag=True):
        self.candidate_dims=candidate_dims
        self.D_graph=D_graph
        self.inner_archive_size=inner_archive_size
        self.inner_offspring=inner_offspring
        self.outer_archive_size=outer_archive_size
        self.outer_offspring=outer_offspring
        self.synthetic_targets=synthetic_targets
        self.inner_learning=inner_learning
        self.causal_flag=causal_flag

        self.nested_reps=[np.zeros(max(candidate_dims)) for _ in range(D_graph)]
        self.best_dim_per_node=[candidate_dims[0] for _ in range(D_graph)]
        self.inter_layer=InterLayer(D_graph,max_inner_dim=max(candidate_dims))
        self.chosen_Gmat=np.random.uniform(-0.5,0.5,(D_graph,D_graph))
        np.fill_diagonal(self.chosen_Gmat,0)
        self.l2_before=[]
        self.l2_after=[]

    @staticmethod
    def fcm_propagate(x,W,steps=30):
        y=x.copy()
        for _ in range(steps):
            y=1/(1+np.exp(- (W.dot(y)+x)))
        return y

    @staticmethod
    def behavioral_update(W,y,alpha=0.6,lr=0.1,decay=0.01,causal_mask=None,eps=1e-6):
        C=y.copy()
        D=np.abs(y-np.mean(y))
        S=alpha*C+(1-alpha)*D
        RC=(y-np.mean(y))/(np.std(y)+eps)
        RI=(S-np.mean(S))/(np.std(S)+eps)
        delta=lr*np.outer(RC,RI)-decay*W
        np.fill_diagonal(delta,0)
        W_new=W+delta
        if causal_mask is not None:
            W_new=np.sign(causal_mask)*np.abs(W_new)
        np.clip(W_new,-1,1)
        np.fill_diagonal(W_new,0)
        return W_new

    def run_inner(self, node_idx, target, D_fcm):
        self.l2_before.append(np.linalg.norm(self.nested_reps[node_idx] - target))
        x = target.copy()
        W = np.random.uniform(-0.6, 0.6, (D_fcm, D_fcm))
        np.fill_diagonal(W, 0)

        for it in range(inner_iters_per_outer):
            y = self.fcm_propagate(x, W)
            W = self.behavioral_update(W, y)
            x += self.inner_learning * (target - y)
            x = np.clip(x, 0, 1)

        self.nested_reps[node_idx] = y
        self.l2_after.append(np.linalg.norm(y - target))

        # --- INTER-LAYER MI COMPUTATION ---
        mi_score = self.inter_layer.mi_for_graph(self.chosen_Gmat, self.nested_reps)
        # Optionally use mi_score to adjust x or W
        # print(f"Node {node_idx} MI score: {mi_score:.4f}")

        return x, W, y, mi_score

    def run_outer(self):
        node_metrics_list=[]
        raw_scores=[]
        for i,y in enumerate(self.nested_reps):
            metrics=metrics_evaluator.compute_node_metrics(i,y=y)
            node_metrics_list.append(metrics)
            raw_scores.append(metrics['score'])
        raw_scores=np.array(raw_scores)
        total_raw=raw_scores.sum()
        if total_raw>outer_cost_limit:
            scale_factor=outer_cost_limit/total_raw
            scaled_scores=raw_scores*scale_factor
            for i,s in enumerate(scaled_scores):
                node_metrics_list[i]['score']=s
            total_capped=scaled_scores.sum()
        else:
            total_capped=total_raw
        return node_metrics_list,total_capped

    def run(self, outer_generations=outer_generations):
        final_metrics_list = None  # store last generation metrics
        for gen in range(outer_generations):
            mi_scores=[]
            for node_idx in range(self.D_graph):
                dim = self.best_dim_per_node[node_idx]
                target = self.synthetic_targets[node_idx][dim]
                _, _, _, mi_score = self.run_inner(node_idx, target, dim)
                mi_scores.append(mi_score)
            metrics_list, capped_score = self.run_outer()

            print(f"\n--- Generation {gen} Metrics ---")
            for i, m in enumerate(metrics_list):
                metric_str = " | ".join([f"{k}: {v:.2f}" for k, v in m.items()])
                print(f"Node {i} | {metric_str}")
            print(f"Outer Score (global capped): {capped_score:.3f}")

            final_metrics_list = metrics_list  # save last generation metrics

        return final_metrics_list  # <-- return metrics for comparison

           # print(f"Inter-layer MI (sum over edges): {sum
    # ---------------- PLOTTING ----------------
    def plot_nested_activations(self):
        plt.figure(figsize=(12,3))
        for i,rep in enumerate(self.nested_reps):
            plt.subplot(1,self.D_graph,i+1)
            plt.bar(range(len(rep)),rep,color=plt.cm.plasma(rep))
            plt.ylim(0,1)
            plt.title(f"Node {i+1} Activations")
        plt.tight_layout()
        plt.show()

    def plot_outer_fuzzy_graph(self):
        G=nx.DiGraph()
        for i in range(self.D_graph): G.add_node(i)
        for i in range(self.D_graph):
            for j in range(self.D_graph):
                if i!=j and abs(self.chosen_Gmat[i,j])>0.02:
                    G.add_edge(i,j,weight=self.chosen_Gmat[i,j])
        node_sizes=[self.best_dim_per_node[i]*200 for i in range(self.D_graph)]
        edge_colors=['green' if d['weight']>0 else 'red' for _,_,d in G.edges(data=True)]
        edge_widths=[abs(d['weight'])*3 for _,_,d in G.edges(data=True)]
        pos=nx.circular_layout(G)
        plt.figure(figsize=(6,6))
        nx.draw(G,pos,node_size=node_sizes,node_color='skyblue',edge_color=edge_colors,
                width=edge_widths,arrows=True,with_labels=True)
        plt.title("Outer Fuzzy Multiplex Graph")
        plt.show()

    def plot_nested_vs_target(self):
        plt.figure(figsize=(12,4))
        for i in range(self.D_graph):
            best_dim=self.best_dim_per_node[i]
            y_actual=self.nested_reps[i]
            y_target=self.synthetic_targets[i][best_dim]
            if len(y_target)<len(y_actual):
                y_target=np.pad(y_target,(0,len(y_actual)-len(y_target)),"constant")
            elif len(y_target)>len(y_actual):
                y_target=y_target[:len(y_actual)]
            plt.subplot(1,self.D_graph,i+1)
            plt.plot(range(len(y_actual)),y_actual,'o-',label='FCM Output')
            plt.plot(range(len(y_target)),y_target,'x--',label='Target')
            plt.ylim(0,1.1)
            plt.title(f"Node {i+1} | Dim {best_dim}")
            if i==0: plt.legend()
        plt.tight_layout()
        plt.show()

    def collect_pointwise_minmax_elite(self,node_idx,dim,top_k=10):
        # simulate top_k elite samples (here using nested_reps with small noise)
        reps=[]
        base=self.nested_reps[node_idx]
        for _ in range(top_k):
            reps.append(np.clip(base + np.random.normal(0,0.05,len(base)),0,1))
        reps=np.array(reps)
        return reps.min(axis=0), reps.max(axis=0)

    def plot_pointwise_minmax_elite(self,top_k=21):
        plt.figure(figsize=(14,3))
        for i in range(self.D_graph):
            dim=self.best_dim_per_node[i]
            y_min,y_max=self.collect_pointwise_minmax_elite(i,dim,top_k)
            y_sel=self.nested_reps[i]
            y_true=self.synthetic_targets[i][dim]
            if len(y_true)<len(y_sel):
                y_true=np.pad(y_true,(0,len(y_sel)-len(y_true)),"constant")
            elif len(y_true)>len(y_sel):
                y_true=y_true[:len(y_sel)]
            x=np.arange(len(y_min))
            plt.subplot(1,self.D_graph,i+1)
            plt.fill_between(x,y_min,y_max,color='skyblue',alpha=0.4,label='Elite Interval')
            plt.plot(x,y_sel,'k-',lw=2,label='Estimated Activation')
            plt.plot(x,y_true,'r--',lw=2,label='True Activation')
            plt.ylim(0,1.05)
            plt.title(f"Node {i+1} | Dim {dim}")
            if i==0: plt.legend()
        plt.tight_layout()
        plt.show()

    def print_l2_summary(self):
        print("\nL2 Distances to Target per Node:")
        #for i,(b,a) in enumerate(zip(self.l2_before,self.l2_after)):
         #   print(f"Node {i+1}: Before={b:.4f} | After={a:.4f} | Improvement={b-a:.4f}")


# ---------------- RUN ----------------
if __name__=="__main__":
    optimizer=UnifiedACORMultiplex(candidate_dims,D_graph,
                                    inner_archive_size,inner_offspring,
                                    outer_archive_size,outer_offspring,
                                    synthetic_targets,
                                    inner_learning,causal_flag=False)
    metrics_list = optimizer.run()
#optimizer.run()
    optimizer.plot_pointwise_minmax_elite(top_k=21)
    optimizer.plot_nested_activations()
    optimizer.plot_outer_fuzzy_graph()
    optimizer.plot_nested_vs_target()
    optimizer.print_l2_summary()

"""# Gradient Descent Fuzzy Cognition Multiplex (GDFCM-ODE)
**Note**

To determine the data on the multiplex one has to fit the input data to targets, and one way to achieve that is by minimizing the L2 norm.Initially, minimizing the norm and creating fuzzy intervals was done by adjusting hyperparameters arbitrarily. Moreover, it seemed that by maximizing the fit, a realistic objective minimization was achieved, which is the activations' antagonizing objective. There was neither a condition nor any unsupervised approach for this ,however, forcing the multiplex to minimize L2 norm.

In other words, imitate the input data by solving for the metrics and using the gradient descent. This way, the service was assumed to work optimally according to its original design. Furthermore to simplify the process, as the data had to fit a multiplex with three variable target, and not a single target FCM, an ordinary differential equation was devised based on back propagation through time; this ODE substituted the sigmoid of x,y,W for z,u,V and provided always an exact solution.


---
We define a scalar potential function:

$$
\Theta(u, v, z) = (u + 1)(v + 1) + 2 z - L
$$

where:  
- $u, v$ are state variables (node activations)  
- $z = W \cdot x$ is the pre-activation (weighted sum)  
- $L$ is the target vector  

The continuous-time dynamics are given by the **gradient flow** of $\Theta$:

$$
\frac{dx}{dt} = - \frac{\partial \Theta}{\partial x}, \quad
\frac{dW}{dt} = - \frac{\partial \Theta}{\partial W}
$$

Using the chain rule:

$$
\frac{\partial \Theta}{\partial z} = 2 z - L
$$

$$
\frac{\partial \Theta}{\partial x} = (2 z - L) W + (u+1)(v+1)
$$

$$
\frac{\partial \Theta}{\partial W} = (2 z - L) \otimes x
$$

The **ODE updates** are discretized using an Euler step:

$$
x \gets x - \eta_x \frac{\partial \Theta}{\partial x}, \quad
W \gets W - \eta_W \frac{\partial \Theta}{\partial W}
$$

where $\eta_x, \eta_W$ are small step sizes controlling stability.


---


To determine the particular sub-process activation, or that is activations per data point a fuzzy cognition multiplex was formed using the input data. The ode that was described above is designed to fit the data on any basis to the multiplex structure; this way a tensor that points to nodes and input relations of the multiplex is constructed. The tensor is later manipulated and use in a simple and elegant manner and node-metric based activations are formed.


## A transversal through the multiplex dimmension now follows
"""

import numpy as np
import matplotlib.pyplot as plt
import networkx as nx

# ---------------- CONFIG ----------------
candidate_dims = [6] * 3
D_graph = 5
inner_archive_size = 80
inner_offspring = 40
outer_archive_size = 40
outer_offspring = 40
inner_iters_per_outer = 50
outer_generations = 101
outer_cost_limit = 1000
inner_learning = 0.1
gamma_interlayer = 0.1
seed = 42
np.random.seed(seed)
class MetricsEvaluator:
    def __init__(self, data_matrix):
        self.data_matrix = data_matrix
        self.num_features = data_matrix.shape[1]
        self.W0, self.T0, self.U0 = 10.0, 100.0, 1.0
        self.P0 = 1.0  # baseline for patience

    def compute_node_metrics(self, node_idx, y=None):
        row = self.data_matrix[node_idx % self.data_matrix.shape[0]]
        k = self.num_features
        base = max(1, k // 3)
        wait_cols = slice(0, base)
        thr_cols = slice(base, 2*base)
        util_cols = slice(2*base, k)

        wait_signal = np.mean(row[wait_cols])
        throughput_signal = np.mean(row[thr_cols])
        util_signal = np.mean(row[util_cols])

        if y is None:
            y = np.array([0.5, 0.5, 0.5])
        else:
            y = np.array(y[:3]) if len(y) >= 3 else np.pad(y, (0, 3-len(y)), constant_values=0.5)

        # Original metrics
        wait = np.clip(self.W0*(1 + 1.2*wait_signal + 0.8*y[0]), 0, 100)
        throughput = np.clip(self.T0*(1 + 1.1*throughput_signal + 0.6*y[1] - 0.4*wait_signal), 0, 150)
        util = np.clip(self.U0 + 0.8*util_signal + 0.6*y[2], 0, 1)

        # --- New metric: patience ---
        patience_signal = np.mean(row)  # simple average as proxy
        patience = np.clip(self.P0 * (1 + 0.5*patience_signal + 0.5*y[0]), 0, 2)  # scaled like others

        # Combine into score
        score = -wait + throughput + util + patience  # add patience like others

        return {'wait': wait, 'throughput': throughput, 'util': util,
                'patience': patience, 'score': score}

# ---------------- INTER-LAYER MUTUAL INFORMATION ----------------
class InterLayer:
    def __init__(self, D_graph, max_inner_dim, inter_dim=None, edge_threshold=0.02, gamma=1.0, seed=42):
        np.random.seed(seed)
        self.D_graph = D_graph
        self.max_input = 2*max_inner_dim
        self.edge_threshold = edge_threshold
        self.gamma = gamma
        self.inter_dim = inter_dim if inter_dim is not None else max_inner_dim
        self.weights = {(i,j): np.random.uniform(-0.6,0.6,(self.inter_dim,self.max_input))
                        for i in range(D_graph) for j in range(D_graph) if i!=j}
        self.bias = {(i,j): np.random.uniform(-0.3,0.3,self.inter_dim)
                     for i in range(D_graph) for j in range(D_graph) if i!=j}

    def compute_edge_activation(self, i, j, nested_reps):
        concat = np.concatenate([nested_reps[i], nested_reps[j]])
        concat = np.pad(concat, (0, max(0,self.max_input-len(concat))))[:self.max_input]
        v = self.weights[(i,j)].dot(concat) + self.bias[(i,j)]
        return 1/(1+np.exp(-v))

    def build_activations(self, Gmat, nested_reps):
        acts = {}
        for i in range(self.D_graph):
            for j in range(self.D_graph):
                if i==j: continue
                if abs(Gmat[i,j])>self.edge_threshold:
                    acts[(i,j)] = self.compute_edge_activation(i,j,nested_reps)
        return acts

    @staticmethod
    def pairwise_squared_corr(acts):
        if len(acts)<2: return 0.0
        A = np.stack(list(acts.values()))
        A_centered = A - A.mean(axis=1,keepdims=True)
        stds = np.sqrt(np.sum(A_centered**2,axis=1)/(A.shape[1]-1) + 1e-12)
        cov = A_centered @ A_centered.T / (A.shape[1]-1)
        corr = cov / (np.outer(stds,stds)+1e-12)
        np.fill_diagonal(corr,0)
        return float((corr**2).sum())

    def mi_for_graph(self, Gmat, nested_reps):
        acts = self.build_activations(Gmat,nested_reps)
        if not acts: return 0.0
        return self.gamma * self.pairwise_squared_corr(acts)

# ---------------- UNIFIED ACOR MULTIPLEX ----------------
class GDFCM:
    def __init__(self, candidate_dims, D_graph, inner_archive_size, inner_offspring,
                 outer_archive_size, outer_offspring, synthetic_targets, inner_learning,
                 gamma_interlayer=1.0, causal_flag=True):
        self.candidate_dims = candidate_dims
        self.D_graph = D_graph
        self.inner_archive_size = inner_archive_size
        self.inner_offspring = inner_offspring
        self.outer_archive_size = outer_archive_size
        self.outer_offspring = outer_offspring
        self.synthetic_targets = synthetic_targets
        self.inner_learning = inner_learning
        self.causal_flag = causal_flag

        self.nested_reps = [np.zeros(max(candidate_dims)) for _ in range(D_graph)]
        self.best_dim_per_node = [candidate_dims[0] for _ in range(D_graph)]
        self.inter_layer = InterLayer(D_graph, max_inner_dim=max(candidate_dims), gamma=gamma_interlayer)
        self.chosen_Gmat = np.random.uniform(-0.5,0.5,(D_graph,D_graph))
        np.fill_diagonal(self.chosen_Gmat,0)
        self.l2_before, self.l2_after = [], []

    # ---------- INNER LOOP (FCM) ----------
    def run_inner(self, node_idx, target, D_fcm, steps=100, lr_x=0.001, lr_W=0.001):
        x = target.copy()
        W = np.random.uniform(-0.6,0.6,(D_fcm,D_fcm))
        np.fill_diagonal(W,0)
        self.l2_before.append(np.linalg.norm(self.nested_reps[node_idx]-target))

        for _ in range(steps):
            z = W.dot(x)
            Theta_grad_z = 2*z - target
            Theta_grad_x = Theta_grad_z @ W + 0.5*(x+1)**2
            Theta_grad_W = np.outer(Theta_grad_z,x)

            x -= lr_x * np.clip(Theta_grad_x,-0.05,0.05)
            x = np.clip(x,0,1)
            W -= lr_W * np.clip(Theta_grad_W,-0.01,0.01)
            np.fill_diagonal(W,0)
            W = np.clip(W,-1,1)

        self.nested_reps[node_idx] = x
        self.l2_after.append(np.linalg.norm(x-target))
        mi_score = self.inter_layer.mi_for_graph(self.chosen_Gmat,self.nested_reps)
        return x, W, x, mi_score

    # ---------- OUTER LOOP ----------
    def run_outer(self):
        metrics_evaluator = MetricsEvaluator(DATA_MATRIX)
        node_metrics_list = []
        raw_scores = []

        for i,y in enumerate(self.nested_reps):
            metrics = metrics_evaluator.compute_node_metrics(i,y=y)
            node_metrics_list.append(metrics)
            raw_scores.append(metrics['score'])

        raw_scores = np.array(raw_scores)
        total_raw = raw_scores.sum()
        if total_raw>outer_cost_limit:
            scale_factor = outer_cost_limit/total_raw
            for i,s in enumerate(raw_scores*scale_factor):
                node_metrics_list[i]['score']=s
            total_capped = (raw_scores*scale_factor).sum()
        else:
            total_capped = total_raw
        return node_metrics_list, total_capped

    # ---------- FULL RUN ----------
    def run(self, outer_generations=outer_generations):
        final_metrics = None
        for gen in range(outer_generations):
            mi_scores = []
            for node_idx in range(self.D_graph):
                dim = self.best_dim_per_node[node_idx]
                target = self.synthetic_targets[node_idx][dim]
                _, _, _, mi_score = self.run_inner(node_idx,target,dim)
                mi_scores.append(mi_score)

            metrics_list, capped_score = self.run_outer()
            print(f"\n--- Generation {gen} Metrics ---")
            for i,m in enumerate(metrics_list):
                print(f"Node {i} | " + " | ".join([f"{k}: {v:.2f}" for k,v in m.items()]))
            print(f"Outer Score (capped): {capped_score:.3f}")
            final_metrics = metrics_list
        return final_metrics

    # ---------- VISUALIZATIONS ----------
    def plot_pointwise_minmax_elite(self, top_k=21):
        plt.figure(figsize=(14,3))
        for i in range(self.D_graph):
            base = self.nested_reps[i]
            reps = np.clip(base + np.random.normal(0,0.05,(top_k,len(base))),0,1)
            y_min, y_max = reps.min(axis=0), reps.max(axis=0)
            y_sel = base
            y_true = self.synthetic_targets[i][self.best_dim_per_node[i]]
            if len(y_true)<len(y_sel):
                y_true = np.pad(y_true,(0,len(y_sel)-len(y_true)),"constant")
            else:
                y_true = y_true[:len(y_sel)]

            plt.subplot(1,self.D_graph,i+1)
            plt.fill_between(range(len(y_min)),y_min,y_max,color='skyblue',alpha=0.4,label='Elite Interval')
            plt.plot(y_sel,'k-',lw=2,label='Estimated')
            plt.plot(y_true,'r--',lw=2,label='True')
            plt.ylim(0,1.05)
            plt.title(f"Node {i+1}")
            if i==0: plt.legend()
        plt.tight_layout()
        plt.show()

    def plot_nested_activations(self):
        plt.figure(figsize=(12,3))
        for i,rep in enumerate(self.nested_reps):
            plt.subplot(1,self.D_graph,i+1)
            plt.bar(range(len(rep)),rep,color=plt.cm.plasma(rep))
            plt.ylim(0,1)
            plt.title(f"Node {i+1}")
        plt.tight_layout()
        plt.show()

    def plot_outer_fuzzy_graph(self):
        G = nx.DiGraph()
        for i in range(self.D_graph): G.add_node(i)
        for i in range(self.D_graph):
            for j in range(self.D_graph):
                if i!=j and abs(self.chosen_Gmat[i,j])>0.02:
                    G.add_edge(i,j,weight=self.chosen_Gmat[i,j])
        node_sizes = [self.best_dim_per_node[i]*200 for i in range(self.D_graph)]
        edge_colors = ['green' if d['weight']>0 else 'red' for _,_,d in G.edges(data=True)]
        edge_widths = [abs(d['weight'])*3 for _,_,d in G.edges(data=True)]
        pos = nx.circular_layout(G)
        plt.figure(figsize=(6,6))
        nx.draw(G,pos,node_size=node_sizes,node_color='skyblue',
                edge_color=edge_colors,width=edge_widths,arrows=True,with_labels=True)
        plt.title("Outer Fuzzy Multiplex Graph")
        plt.show()
# ---------------- INTERACTIONS INSPECTOR ----------------
    def print_interactions(self, return_tensor=True, verbose=True):
        """
        Build inter-layer activations tensor and optionally print them.

        Returns:
            inter_tensor: np.ndarray of shape (D_graph, D_graph, inter_dim)
                        inter_tensor[i,j,:] contains activations from node i to node j,
                        zeros if no edge exists.
        """
        D_graph = self.D_graph
        inter_dim = self.inter_layer.inter_dim
        inter_tensor = np.zeros((D_graph, D_graph, inter_dim))

        acts = self.inter_layer.build_activations(self.chosen_Gmat, self.nested_reps)

        if not acts:
            if verbose:
                print("No active edges above threshold.")
            return inter_tensor if return_tensor else None

        for (i, j), vec in acts.items():
            inter_tensor[i, j, :] = vec
            if verbose:
                act_str = ", ".join([f"{v:.3f}" for v in vec])
                print(f"Node {i} -> Node {j}: [{act_str}]")

        return inter_tensor if return_tensor else None


        def print_l2_summary(self):
            print("\nL2 Distances to Target per Node:")

# ---------------- USAGE ----------------
if __name__ == "__main__":
    optimizer = GDFCM(
        candidate_dims, D_graph,
        inner_archive_size, inner_offspring,
        outer_archive_size, outer_offspring,
        synthetic_targets,
        inner_learning, gamma_interlayer=gamma_interlayer,
        causal_flag=False
    )
    metrics_list = optimizer.run()
    optimizer.plot_pointwise_minmax_elite()
    optimizer.plot_nested_activations()
    optimizer.plot_outer_fuzzy_graph()
  #  optimizer.print_interactions()
    tensor = optimizer.print_interactions()
    print("Tensor shape:", tensor.shape,'\n',tensor)
    import matplotlib.pyplot as plt
    import networkx as nx
    import numpy as np
    import matplotlib.pyplot as plt
    from mpl_toolkits.mplot3d import Axes3D
D_graph = len(optimizer.nested_reps)
tensor = optimizer.print_interactions(return_tensor=True, verbose=False)

# ---------------- Outer nodes (hubs) ----------------
G_outer = nx.DiGraph()
for i in range(D_graph):
    G_outer.add_node(i)
for i in range(D_graph):
    for j in range(D_graph):
        if i != j and np.any(tensor[i,j,:] != 0):
            # Shift to signed weights: 0.5 -> 0, <0.5 negative, >0.5 positive
            mean_weight = 2 * (np.mean(tensor[i,j,:]) - 0.5)
            G_outer.add_edge(i, j, weight=mean_weight)

# Outer spring layout
pos_outer_2d = nx.circular_layout(G_outer, scale=5)
pos_outer = np.array([[x, y, 0] for x, y in pos_outer_2d.values()])

fig = plt.figure(figsize=(10,8))
ax = fig.add_subplot(111, projection='3d')

# Plot outer nodes
for i in range(D_graph):
    ax.scatter(*pos_outer[i], s=300, color='skyblue')
    ax.text(*pos_outer[i], f'Node {i}', color='black')

# Plot outer edges with positive/negative colors

for i, j, data in G_outer.edges(data=True):
    x_vals = [pos_outer[i,0], pos_outer[j,0]]
    y_vals = [pos_outer[i,1], pos_outer[j,1]]
    z_vals = [pos_outer[i,2], pos_outer[j,2]]

    # Positive = bright green, Negative = bright red
    color = 'green' if data['weight'] > 0 else 'red'
    linewidth = 2 + 4*abs(data['weight'])  # scale width by magnitude
    ax.plot(x_vals, y_vals, z_vals, color=color, linewidth=linewidth)
# ---------------- Inner FCMs (small circular around hub) ----------------
for i, rep in enumerate(optimizer.nested_reps):
    dims = len(rep)
    angle = np.linspace(0, 2*np.pi, dims, endpoint=False)
    radius = 0.8  # small circle
    xs = pos_outer[i,0] + radius * np.cos(angle)
    ys = pos_outer[i,1] + radius * np.sin(angle)
    zs = pos_outer[i,2] + rep  # activation as height

    # Plot inner nodes
    ax.scatter(xs, ys, zs, c=rep, cmap='plasma', s=50)

    # Connect inner nodes in circle
    for k in range(dims):
        ax.plot([xs[k], xs[(k+1)%dims]], [ys[k], ys[(k+1)%dims]], [zs[k], zs[(k+1)%dims]], color='gray', alpha=0.5)

ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Activation')
ax.set_title('Outer Nodes with Inner FCMs (Signed correlations)')
plt.show()

"""# Reverse Activation Optimization

Actually the tensor side is used to construct a supervised fcm based on graphs, isnt it ? Yet, that would not require a full multiplex, an FCM can also do it; The advantage of the multiplex is its inherent cabability to organize the data activations accordingly!!!! Given these activations that you cannot use for the data or ascociate them, cause with these it is already a supervised system, you nonetheless have a new system. Given that you now have six instances of data and one tensor that describes how this data interacted implicitly at a local level there are 4 ways or rather 4-1 = 3 ways that any new metric of data will have a meaningfull activation at a corresponding layer. An example with 5 graph nodes is illustrated.
"""

import numpy as np
import networkx as nx
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Assuming 'tensor' is your inter-layer activation tensor: shape (4,4,6)
D_graph = tensor.shape[0]
num_layers = tensor.shape[2]

# ---------------- Build 6 FCM layers ----------------
FCM_layers = []
for k in range(num_layers):
    G = nx.DiGraph()
    for i in range(D_graph):
        G.add_node(i)
    for i in range(D_graph):
        for j in range(D_graph):
            if i != j:
                # Take k-th dimension of tensor[i,j,:] and shift to signed [-1,1]
                weight = 2 * (tensor[i,j,k] - 0.5)
                if abs(weight) > 1e-3:  # threshold small values
                    G.add_edge(i, j, weight=weight)
    FCM_layers.append(G)

# ---------------- 3D Visualization ----------------
fig = plt.figure(figsize=(12,10))
ax = fig.add_subplot(111, projection='3d')

# Outer nodes positions (hub layout)
pos_outer_2d = nx.circular_layout(range(D_graph), scale=5)
pos_outer = np.array([[x, y, 0] for x, y in pos_outer_2d.values()])

# Plot outer nodes
for i in range(D_graph):
    ax.scatter(*pos_outer[i], s=300, color='skyblue')
    ax.text(*pos_outer[i], f'Node {i}', color='black', fontsize=10)

# Plot FCM layers (stacked vertically per node)
layer_height = 1.5
colors = plt.cm.plasma(np.linspace(0,1,num_layers))

for k, G in enumerate(FCM_layers):
    z_offset = (k+1) * layer_height  # stack layers above hubs
    for i,j,data in G.edges(data=True):
        x_vals = [pos_outer[i,0], pos_outer[j,0]]
        y_vals = [pos_outer[i,1], pos_outer[j,1]]
        z_vals = [z_offset, z_offset]
        color = 'green' if data['weight'] > 0 else 'red'
        linewidth = 1 + 3*abs(data['weight'])
        ax.plot(x_vals, y_vals, z_vals, color=color, linewidth=linewidth, alpha=0.7)
    # Plot layer nodes
    for i in range(D_graph):
        ax.scatter(pos_outer[i,0], pos_outer[i,1], z_offset, s=100, color=colors[k], alpha=0.8)

ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('FCM Layer / Activation')
ax.set_title('Multiplex FCM: 6 Layers of Inner Activations')
plt.show()

import numpy as np
import matplotlib.pyplot as plt

# ---------------- Step 0: Prepare ----------------
D_graph = len(optimizer.nested_reps)          # number of outer nodes
inter_dim = optimizer.inter_layer.inter_dim  # number of layers

# Generate new random sample for nodes
new_sample = [np.random.rand(inter_dim) for _ in range(D_graph)]
optimizer.nested_reps = new_sample

# Compute interactions tensor
tensor = optimizer.print_interactions(return_tensor=True, verbose=False)

# ---------------- Step 1: Layer-by-layer bar plots ----------------
colors = plt.cm.plasma(np.linspace(0,1,D_graph))
fig, axes = plt.subplots(inter_dim, 1, figsize=(12, 3*inter_dim))
if inter_dim == 1:
    axes = [axes]

for layer_idx in range(inter_dim):
    ax = axes[layer_idx]
    ax.set_title(f'Plex Level {layer_idx+1} - Inter-layer Activations')

    for i in range(D_graph):
        inner_vals = [tensor[i, j, layer_idx] for j in range(D_graph) if i != j]
        x_pos = np.arange(len(inner_vals)) + i*(len(inner_vals)+0.5)
        ax.bar(x_pos, inner_vals, color=colors[:len(inner_vals)], alpha=0.8, label=f'Node {i}' if layer_idx==0 else "")

        for j, val in enumerate(inner_vals):
            print(f"Plex Level {layer_idx+1} | Node {i} -> Node {j if j<i else j+1}: {val:.6f}")

    ax.set_xlabel('Inner Node Targets (per outer node)')
    ax.set_ylabel('Activation')
    ax.set_ylim(0,1)
    ax.set_xticks([len(inner_vals)/2 + n*(len(inner_vals)+0.5) for n in range(D_graph)])
    ax.set_xticklabels([f'Node {n}' for n in range(D_graph)])
    if layer_idx == 0:
        ax.legend()
plt.tight_layout()
plt.show()

# ---------------- Step 2: Update nested representations ----------------
alpha = 0.1
updated_reps = []

for i in range(D_graph):
    base_activation = np.array(optimizer.nested_reps[i])
    secondary_influence = np.zeros_like(base_activation)

    for j in range(D_graph):
        if j == i:
            continue
        for k in range(inter_dim):
            secondary_influence[k] += tensor[j, i, k] * optimizer.nested_reps[j][k]

    combined = base_activation + alpha * secondary_influence
    combined = np.clip(combined, 0, 1)
    updated_reps.append(combined)

optimizer.nested_reps = updated_reps

# ---------------- Step 3: Evaluate metrics per layer ----------------
data_matrix = np.array(optimizer.nested_reps)
metrics = MetricsEvaluator(data_matrix)

print("\nMetrics Evaluation Results with Surplus Contribution (Per Layer):\n")

for layer_idx in range(inter_dim):
    print(f"--- Layer {layer_idx+1} ---")
    layer_surplus_info = []

    for node_idx in range(D_graph):
        # Use only the layer-specific value for metrics
        node_val = [data_matrix[node_idx][layer_idx]]
        node_metrics = metrics.compute_node_metrics(node_idx, y=node_val)

        # Surplus = positive contributions
        contributions = {
            'wait_surplus': max(0, -node_metrics['wait']),
            'throughput_surplus': max(0, node_metrics['throughput']),
            'util_surplus': max(0, node_metrics['util']),
            'patience_surplus': max(0, node_metrics['patience'])
        }
        total_surplus = sum(contributions.values())

        layer_surplus_info.append({
            'node': node_idx,
            'metrics': node_metrics,
            'contributions': contributions,
            'total_surplus': total_surplus
        })

        print(f"\nNode {node_idx} Total Surplus: {total_surplus:.4f}")
        for m, val in contributions.items():
            print(f"  {m}: {val:.4f}")

    # Identify the most valuable node/metric for this layer
    best_node = max(layer_surplus_info, key=lambda x: x['total_surplus'])
    best_metric = max(best_node['contributions'], key=lambda k: best_node['contributions'][k])

    print(f"\nMost Valuable Node in Layer {layer_idx+1}: {best_node['node']}")
    print(f"Most Valuable Metric: {best_metric} with contribution {best_node['contributions'][best_metric]:.4f}\n")

import numpy as np
import matplotlib.pyplot as plt

# ---------------- Step 0: Prepare ----------------
D_graph = len(optimizer.nested_reps)          # number of outer nodes
inter_dim = optimizer.inter_layer.inter_dim  # number of layers

# Generate new random sample for nodes
new_sample = [np.random.rand(inter_dim) for _ in range(D_graph)]
optimizer.nested_reps = new_sample

# Compute interactions tensor
tensor = optimizer.print_interactions(return_tensor=True, verbose=False)

# ---------------- Step 1: Layer-by-layer bar plots ----------------
colors = plt.cm.plasma(np.linspace(0,1,D_graph))
fig, axes = plt.subplots(inter_dim, 1, figsize=(12, 3*inter_dim))
if inter_dim == 1:
    axes = [axes]

for layer_idx in range(inter_dim):
    ax = axes[layer_idx]
    ax.set_title(f'Plex Level {layer_idx+1} - Inter-layer Activations')

    for i in range(D_graph):
        inner_vals = [tensor[i, j, layer_idx] for j in range(D_graph) if i != j]
        x_pos = np.arange(len(inner_vals)) + i*(len(inner_vals)+0.5)
        ax.bar(x_pos, inner_vals, color=colors[:len(inner_vals)], alpha=0.8, label=f'Node {i}' if layer_idx==0 else "")

        for j, val in enumerate(inner_vals):
            print(f"Plex Level {layer_idx+1} | Node {i} -> Node {j if j<i else j+1}: {val:.6f}")

    ax.set_xlabel('Inner Node Targets (per outer node)')
    ax.set_ylabel('Activation')
    ax.set_ylim(0,1)
    ax.set_xticks([len(inner_vals)/2 + n*(len(inner_vals)+0.5) for n in range(D_graph)])
    ax.set_xticklabels([f'Node {n}' for n in range(D_graph)])
    if layer_idx == 0:
        ax.legend()
plt.tight_layout()
plt.show()

# ---------------- Step 2: Update nested representations ----------------
alpha = 0.1
updated_reps = []

for i in range(D_graph):
    base_activation = np.array(optimizer.nested_reps[i])
    secondary_influence = np.zeros_like(base_activation)

    for j in range(D_graph):
        if j == i:
            continue
        for k in range(inter_dim):
            secondary_influence[k] += tensor[j, i, k] * optimizer.nested_reps[j][k]

    combined = base_activation + alpha * secondary_influence
    combined = np.clip(combined, 0, 1)
    updated_reps.append(combined)

optimizer.nested_reps = updated_reps

# ---------------- Step 3: Evaluate metrics per layer ----------------
data_matrix = np.array(optimizer.nested_reps)
metrics = MetricsEvaluator(data_matrix)

print("\nMetrics Evaluation Results with Surplus Contribution (Per Layer):\n")

for layer_idx in range(inter_dim):
    print(f"--- Layer {layer_idx+1} ---")
    layer_surplus_info = []

    for node_idx in range(D_graph):
        # Use only the layer-specific value for metrics
        node_val = [data_matrix[node_idx][layer_idx]]
        node_metrics = metrics.compute_node_metrics(node_idx, y=node_val)

        # Surplus = positive contributions
        contributions = {
            'wait_surplus': max(0, -node_metrics['wait']),
            'throughput_surplus': max(0, node_metrics['throughput']),
            'util_surplus': max(0, node_metrics['util']),
            'patience_surplus': max(0, node_metrics['patience'])
        }
        total_surplus = sum(contributions.values())

        layer_surplus_info.append({
            'node': node_idx,
            'metrics': node_metrics,
            'contributions': contributions,
            'total_surplus': total_surplus
        })

        print(f"\nNode {node_idx} Total Surplus: {total_surplus:.4f}")
        for m, val in contributions.items():
            print(f"  {m}: {val:.4f}")

    # Identify the most valuable node/metric for this layer
    best_node = max(layer_surplus_info, key=lambda x: x['total_surplus'])
    best_metric = max(best_node['contributions'], key=lambda k: best_node['contributions'][k])

    print(f"\nMost Valuable Node in Layer {layer_idx+1}: {best_node['node']}")
    print(f"Most Valuable Metric: {best_metric} with contribution {best_node['contributions'][best_metric]:.4f}\n")

"""### Here is an example of data handling based on the same metrics as before, **illustration**"""

import numpy as np
import matplotlib.pyplot as plt

# ---------------- Step 0: Prepare ----------------
D_graph = len(optimizer.nested_reps)          # number of outer nodes
inter_dim = optimizer.inter_layer.inter_dim  # number of layers

# Generate new random sample for nodes
new_sample = [np.random.rand(inter_dim) for _ in range(D_graph)]
optimizer.nested_reps = new_sample

# Compute interactions tensor
tensor = optimizer.print_interactions(return_tensor=True, verbose=False)

# ---------------- Step 1: Layer-by-layer bar plots ----------------
colors = plt.cm.plasma(np.linspace(0,1,D_graph))
fig, axes = plt.subplots(inter_dim, 1, figsize=(12, 3*inter_dim))
if inter_dim == 1:
    axes = [axes]

for layer_idx in range(inter_dim):
    ax = axes[layer_idx]
    ax.set_title(f'Plex Level {layer_idx+1} - Inter-layer Activations')

    for i in range(D_graph):
        inner_vals = [tensor[i, j, layer_idx] for j in range(D_graph) if i != j]
        x_pos = np.arange(len(inner_vals)) + i*(len(inner_vals)+0.5)
        ax.bar(x_pos, inner_vals, color=colors[:len(inner_vals)], alpha=0.8, label=f'Node {i}' if layer_idx==0 else "")

        for j, val in enumerate(inner_vals):
            print(f"Plex Level {layer_idx+1} | Node {i} -> Node {j if j<i else j+1}: {val:.6f}")

    ax.set_xlabel('Inner Node Targets (per outer node)')
    ax.set_ylabel('Activation')
    ax.set_ylim(0,1)
    ax.set_xticks([len(inner_vals)/2 + n*(len(inner_vals)+0.5) for n in range(D_graph)])
    ax.set_xticklabels([f'Node {n}' for n in range(D_graph)])
    if layer_idx == 0:
        ax.legend()
plt.tight_layout()
plt.show()

# ---------------- Step 2: Update nested representations ----------------
alpha = 0.1
updated_reps = []

for i in range(D_graph):
    base_activation = np.array(optimizer.nested_reps[i])
    secondary_influence = np.zeros_like(base_activation)

    for j in range(D_graph):
        if j == i:
            continue
        for k in range(inter_dim):
            secondary_influence[k] += tensor[j, i, k] * optimizer.nested_reps[j][k]

    combined = base_activation + alpha * secondary_influence
    combined = np.clip(combined, 0, 1)
    updated_reps.append(combined)

optimizer.nested_reps = updated_reps

# ---------------- Step 3: Evaluate metrics layer by layer ----------------
data_matrix = np.array(optimizer.nested_reps)
metrics = MetricsEvaluator(data_matrix)

bandwidths = {
    'wait': (0, 50),
    'throughput': (50, 120),
    'util': (0.4, 0.8),
    'patience': (0.8, 1.5)
}

print("\nMetrics Evaluation Within Bandwidth (Per Layer):\n")

for layer_idx in range(inter_dim):
    print(f"--- Layer {layer_idx+1} ---")
    layer_band_info = []

    for node_idx in range(D_graph):
        # Only consider the current layer value as input for metrics
        node_val = [data_matrix[node_idx][layer_idx]]  # single value per layer
        node_metrics = metrics.compute_node_metrics(node_idx, y=node_val)

        within_band = {}
        for metric, val in node_metrics.items():
            if metric not in bandwidths:
                continue
            low, high = bandwidths[metric]
            within_band[metric] = low <= val <= high

        layer_band_info.append({
            'node': node_idx,
            'metrics': node_metrics,
            'within_band': within_band
        })

        print(f"\nNode {node_idx}:")
        for metric, in_band in within_band.items():
            status = "✔️ within band" if in_band else "❌ out of band"
            print(f"  {metric}: {node_metrics[metric]:.3f} -> {status}")

    # Identify the most compliant node in this layer
    best_node = max(layer_band_info, key=lambda x: sum(x['within_band'].values()))
    best_metrics = [m for m, val in best_node['within_band'].items() if val]

    print(f"\nMost Compliant Node in Layer {layer_idx+1}: {best_node['node']}")
    print(f"Metrics within bandwidth: {best_metrics}\n")

"""Lastly, here are the processes and for each sub-process an activation function is formed according to metrics, old or new, and the output is calculated. Operations can be for overflow, underflow, or anything else for each metric; so the system can deduce things about new unknown data based on prior knowledge of procedural activations.





---
# Baseline comparison to an FCM fit

**There is no internal structure, just black box math for the FCM**
"""

import numpy as np
import matplotlib.pyplot as plt
import networkx as nx

np.random.seed(42)

D_graph = 4
candidate_dims = [6] * D_graph
dim_per_node = candidate_dims

# Flatten synthetic targets into one vector
total_dim = sum(dim_per_node)
x_targets = np.zeros(total_dim)
ptr = 0
for i, dim in enumerate(dim_per_node):
    t = np.array(synthetic_targets[i][dim])
    if len(t) < dim:
        t = np.pad(t, (0, dim - len(t)), constant_values=0.5)
    x_targets[ptr:ptr+dim] = t
    ptr += dim

# ------------------------
# Supervised W learning
# ------------------------
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Initialize W small
W = np.random.uniform(-0.1, 0.1, (total_dim, total_dim))
np.fill_diagonal(W, 0)

# Learning rate
lr = 0.5
steps = 500

# Supervised Hebbian-like learning
for _ in range(steps):
    y = sigmoid(W @ x_targets)
    delta = np.outer(x_targets - y, x_targets)
    W += lr * delta
    np.fill_diagonal(W, 0)

# ------------------------
# Verify fixed-point
# ------------------------
x_init = x_targets.copy()  # start at target
for _ in range(100):
    x_init = sigmoid(W @ x_init)
    x_init = np.clip(x_init, 0, 1)

# Split per-node activations
activations_global = []
ptr = 0
for dim in dim_per_node:
    activations_global.append(x_init[ptr:ptr+dim].copy())
    ptr += dim

# ------------------------
# Plot
# ------------------------
plt.figure(figsize=(12,3))
for i, act in enumerate(activations_global):
    plt.subplot(1,D_graph,i+1)
    plt.plot(act,'o-',label='FCM Output')
    target = np.array(synthetic_targets[i][dim_per_node[i]])
    if len(target) < len(act):
        target = np.pad(target,(0,len(act)-len(target)),constant_values=0.5)
    elif len(target) > len(act):
        target = target[:len(act)]
    plt.plot(target,'x--',label='Target')
    plt.ylim(0,1.05)
    if i==0: plt.legend()
    plt.title(f"Node {i} | dim {len(act)}")
plt.suptitle("Supervised FCM: Activations vs Targets")
plt.tight_layout()
plt.show()

# ------------------------
# Compute L2 distances
# ------------------------
l2s = [np.linalg.norm(act - np.array(synthetic_targets[i][dim_per_node[i]]))
       for i, act in enumerate(activations_global)]
print("L2 distances to targets per node:", l2s)

for i, l2 in enumerate(l2s):
    print(f"Node {i} L2 distance to target: {l2:.4f}")

# Quick activations plot
plt.figure(figsize=(12,3))
for i, act in enumerate(activations_global):
    plt.subplot(1, D_graph, i+1)
    plt.plot(act, 'o-', label='Global FCM')
    target = np.array(synthetic_targets[i][dim_per_node[i]])
    if len(target) < len(act):
        target = np.pad(target, (0,len(act)-len(target)), constant_values=0.5)
    elif len(target) > len(act):
        target = target[:len(act)]
    plt.plot(target, 'x--', label='Target')
    plt.ylim(0,1.05)
    if i==0: plt.legend()
    plt.title(f"Node {i}")
plt.suptitle("Global FCM Activations vs Targets")
plt.tight_layout()
plt.show()

# ------------------------
# Compute metrics for each node
# ------------------------
fcms_metrics = []
for i, act in enumerate(activations_global):
    metrics = metrics_evaluator.compute_node_metrics(i, y=act)
    fcms_metrics.append(metrics)

# Print metrics
print("\n=== Supervised FCM Metrics ===")
for i, m in enumerate(fcms_metrics):
    print(f"Node {i}: wait={m['wait']:.2f}, throughput={m['throughput']:.2f}, util={m['util']:.2f}, score={m['score']:.2f}")

# ------------------------
# Compute system-level summaries
# ------------------------
total_score = sum(m['score'] for m in fcms_metrics)
avg_wait = np.mean([m['wait'] for m in fcms_metrics])
avg_thrpt = np.mean([m['throughput'] for m in fcms_metrics])
avg_util = np.mean([m['util'] for m in fcms_metrics])

print(f"\nSystem-level summary:")
print(f"Total score: {total_score:.2f}")
print(f"Average wait: {avg_wait:.2f}")
print(f"Average throughput: {avg_thrpt:.2f}")
print(f"Average utilization: {avg_util:.2f}")

# Map global W into per-node interactions
ptr_i = 0
print("\n=== Pairwise Node Interactions ===")
for i, dim_i in enumerate(dim_per_node):
    ptr_j = 0
    for j, dim_j in enumerate(dim_per_node):
        block = W[ptr_i:ptr_i+dim_i, ptr_j:ptr_j+dim_j]
        mean_strength = np.mean(np.abs(block))
        print(f"Node {i} -> Node {j} | mean weight magnitude: {mean_strength:.3f}")
        ptr_j += dim_j
    ptr_i += dim_i
interaction_tensor = np.zeros((D_graph, D_graph, max(dim_per_node), max(dim_per_node)))
ptr_i = 0
for i, dim_i in enumerate(dim_per_node):
    ptr_j = 0
    for j, dim_j in enumerate(dim_per_node):
        block = W[ptr_i:ptr_i+dim_i, ptr_j:ptr_j+dim_j]
        interaction_tensor[i, j, :dim_i, :dim_j] = block
        ptr_j += dim_j
    ptr_i += dim_i
interaction_tensor = interaction_tensor.mean(axis=2)  # shape -> (4, 4, 6)

print("Interaction tensor shape:", interaction_tensor.shape)
print()
print('Tensor',interaction_tensor)







