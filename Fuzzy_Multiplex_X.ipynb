{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4QC-b9qcBeJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFVu6itBcCEt"
      },
      "source": [
        "# Data Gen\n",
        "\n",
        "here is a multi problem multi objective optimization. Problems might as well immitate a courier service each of which has its own unique attributes. Furthermore, data and targets are also encoded and there are multi-dimensional in nature. Moreover, the data is then passed through multiplex optimizers for specific metrics of interests and FCMS to. There are five different systems under evaluation or 3 entities. Metaheuristics, such as acor and ga, FCMs and nested FCMs, and neural evaluators. Lastly there are other evaluation metrics which include\n",
        "- MAE\n",
        "- RMSE\n",
        "- R2\n",
        "- Cosine sim\n",
        "\n",
        "\n",
        "of the predictions for the data. Each method more or less aligns to these metrics in its own accord.\n",
        "\n",
        "ACO Multiplex works best\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVqlRh8fd5CA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "np.random.seed()\n",
        "N = 1000  # super large number of samples\n",
        "\n",
        "# --------------- Domain 1: Route Planning ---------------\n",
        "route_planning = pd.DataFrame({\n",
        "    'origin_x': np.random.uniform(0, 100, N),\n",
        "    'origin_y': np.random.uniform(0, 100, N),\n",
        "    'dest_x': np.random.uniform(0, 100, N),\n",
        "    'dest_y': np.random.uniform(0, 100, N),\n",
        "    'traffic_density': np.random.uniform(0, 1, N),\n",
        "    'road_type': np.random.choice([1, 2, 3, 4, 5], N),\n",
        "})\n",
        "route_planning['distance'] = np.sqrt(\n",
        "    (route_planning['dest_x'] - route_planning['origin_x'])**2 +\n",
        "    (route_planning['dest_y'] - route_planning['origin_y'])**2\n",
        ")\n",
        "speed_base = {1:60, 2:50, 3:40, 4:30, 5:20}\n",
        "route_planning['speed'] = route_planning['road_type'].map(speed_base) * np.random.uniform(0.7, 1.3, N)\n",
        "route_planning['travel_time'] = (route_planning['distance']/route_planning['speed'])*60*\\\n",
        "                                (1 + route_planning['traffic_density']*np.random.uniform(0.05,0.7,N))\n",
        "\n",
        "# --------------- Domain 2: Vehicle Assignment ---------------\n",
        "vehicle_assignment = pd.DataFrame({\n",
        "    'vehicle_capacity': np.random.randint(50,500,N),\n",
        "    'battery_level': np.random.uniform(0.2,1.0,N),\n",
        "    'delivery_size': np.random.randint(5,150,N),\n",
        "    'vehicle_type': np.random.choice([1,2,3],N),\n",
        "    'speed_factor': np.random.uniform(0.7,1.2,N),\n",
        "})\n",
        "vehicle_assignment['assigned_speed'] = route_planning['speed'] * vehicle_assignment['speed_factor']\n",
        "vehicle_assignment['load_utilization'] = vehicle_assignment['delivery_size']/vehicle_assignment['vehicle_capacity']\n",
        "\n",
        "# --------------- Domain 3: Time Scheduling ---------------\n",
        "time_scheduling = pd.DataFrame({\n",
        "    'requested_time': np.random.randint(0,24,N),\n",
        "    'delivery_priority': np.random.randint(1,10,N),\n",
        "    'customer_patience': np.random.uniform(0,1,N),\n",
        "})\n",
        "time_scheduling['delay_probability'] = np.clip(\n",
        "    (route_planning['travel_time']/60)*(1+vehicle_assignment['load_utilization']*0.7)*np.random.uniform(0.5,1.5,N),\n",
        "    0,1\n",
        ")\n",
        "\n",
        "# --------------- Domain 4: Dynamic Rerouting & Traffic ---------------\n",
        "dynamic_rerouting = pd.DataFrame({\n",
        "    'current_x': np.random.uniform(0,100,N),\n",
        "    'current_y': np.random.uniform(0,100,N),\n",
        "    'traffic_updates': np.random.uniform(0,1,N),\n",
        "    'new_delivery_requests': np.random.randint(0,5,N),\n",
        "    'vehicle_status': np.random.choice([0,1],N),\n",
        "    'weather': np.random.choice([0,1,2],N),  # 0=clear,1=rain,2=storm\n",
        "})\n",
        "dynamic_rerouting['congestion_score'] = dynamic_rerouting['traffic_updates'] + \\\n",
        "                                       dynamic_rerouting['new_delivery_requests']*0.5 + \\\n",
        "                                       dynamic_rerouting['weather']*0.7 + \\\n",
        "                                       (route_planning['travel_time']/route_planning['travel_time'].max())*0.5\n",
        "\n",
        "# --------------- Domain 5: Energy & Emissions ---------------\n",
        "energy_emissions = pd.DataFrame({\n",
        "    'fuel_level': np.random.uniform(0,1,N),\n",
        "    'emission_rate': np.random.uniform(0,1,N),\n",
        "    'energy_consumed': vehicle_assignment['delivery_size']*np.random.uniform(0.1,0.5,N),\n",
        "    'regeneration_rate': np.random.uniform(0,0.2,N)\n",
        "})\n",
        "energy_emissions['net_energy'] = energy_emissions['fuel_level'] - energy_emissions['energy_consumed'] + energy_emissions['regeneration_rate']\n",
        "\n",
        "# --------------- Combine and Normalize ---------------\n",
        "datasets = [route_planning, vehicle_assignment, time_scheduling, dynamic_rerouting, energy_emissions]\n",
        "DATA_MATRIX = np.hstack([df.values for df in datasets])\n",
        "DATA_MATRIX = (DATA_MATRIX - DATA_MATRIX.min(axis=0)) / (np.ptp(DATA_MATRIX, axis=0)+1e-8)\n",
        "\n",
        "# --------------- Graph-based Multi-node Targets ---------------\n",
        "D_graph = 5  # number of nodes=\n",
        "candidate_dims =candidate_dims = [8, 6, 4, 3, 6]  # D_graph=5\n",
        " # each node has its own target dimension\n",
        "\n",
        "\n",
        "# ---------------- Combine and normalize ----------------\n",
        "datasets = [route_planning, vehicle_assignment, time_scheduling, dynamic_rerouting]\n",
        "dataset_dims = [df.shape[1] for df in datasets]\n",
        "max_dim = max(dataset_dims)\n",
        "padded_data = [df.values for df in datasets]\n",
        "\n",
        "DATA_MATRIX = np.hstack(padded_data)\n",
        "DATA_MATRIX = (DATA_MATRIX - DATA_MATRIX.min(axis=0)) / (np.ptp(DATA_MATRIX, axis=0)+1e-8)\n",
        "def generate_targets_per_node(DATA_MATRIX, candidate_dims, D_graph):\n",
        "    \"\"\"\n",
        "    Generate target vectors for each node with its own candidate dimension.\n",
        "\n",
        "    Parameters:\n",
        "    - DATA_MATRIX: normalized data array\n",
        "    - candidate_dims: list of ints, length = D_graph, each specifying the target vector size per node\n",
        "    - D_graph: number of graph nodes\n",
        "\n",
        "    Returns:\n",
        "    - List of dicts: each dict contains the node's target vector under key 'target'\n",
        "    \"\"\"\n",
        "    targets = []\n",
        "    for node_idx in range(D_graph):\n",
        "        row = DATA_MATRIX[node_idx % len(DATA_MATRIX)]\n",
        "        dim = candidate_dims[node_idx]  # get the dim for this node\n",
        "        if len(row) >= dim:\n",
        "            sampled = row[:dim]\n",
        "        else:\n",
        "            sampled = np.pad(row, (0, dim - len(row)), constant_values=0.5)\n",
        "        targets.append({'target': sampled})\n",
        "    return targets\n",
        "\n",
        "# Example: each node has its own target dimension\n",
        "candidate_dims = [8, 6, 4, 3,6]  # D_graph=5\n",
        "D_graph = len(candidate_dims)\n",
        "\n",
        "synthetic_targets = generate_targets_per_node(DATA_MATRIX, candidate_dims, D_graph)\n",
        "\n",
        "# check\n",
        "for i, t in enumerate(synthetic_targets):\n",
        "    print(f\"Node {i} target size: {len(t['target'])}\")\n",
        "\n",
        "class MetricsEvaluator:\n",
        "    def __init__(self, data_matrix):\n",
        "        self.data_matrix = data_matrix\n",
        "        self.num_features = data_matrix.shape[1]\n",
        "        self.W0, self.T0, self.U0 = 15.0, 100.0, 10.0\n",
        "        self.P0 = 10.0  # baseline for patience\n",
        "\n",
        "    def compute_node_metrics(self, node_idx, y=None, node_size=None):\n",
        "        \"\"\"\n",
        "        node_size: number of features in this node (optional)\n",
        "        \"\"\"\n",
        "        # Default to 4 features if node_size not given\n",
        "        node_size = node_size or 4\n",
        "\n",
        "        # Slice only the node's columns\n",
        "        start_col = sum(self.data_matrix.shape[1] // self.data_matrix.shape[0] for _ in range(node_idx))  # optional\n",
        "        node_cols = slice(start_col, start_col + node_size)\n",
        "        node_data = self.data_matrix[:, node_cols]\n",
        "\n",
        "        # Compute signals safely for available features\n",
        "        signals_full = node_data.mean(axis=0)\n",
        "        signals = np.zeros(5)\n",
        "        for i in range(min(5, len(signals_full))):\n",
        "            signals[i] = signals_full[i]\n",
        "        # signals[i] = 0 for missing features\n",
        "\n",
        "        # Handle y: use only available entries\n",
        "        if y is None:\n",
        "            y = np.zeros(3)\n",
        "        else:\n",
        "            y = np.array(y)\n",
        "            if len(y) < 3:\n",
        "                y = np.pad(y, (0, 3-len(y)), constant_values=0.5)\n",
        "            else:\n",
        "                y = y[:3]\n",
        "\n",
        "        wait = np.clip(self.W0*(1 + 1.2*signals[0] + 1*y[0]), 0, 150)\n",
        "        throughput = np.clip(self.T0*(1 + 1.1*signals[1] + 0.6*y[1] - 0.4*signals[0]), 0, 150)\n",
        "        util = np.clip(self.U0 + 1.2*signals[2] + 1.5*y[2], 0, 150)\n",
        "        patience = np.clip(self.P0*(1 + 0.5*signals[3] + 0.5*y[0]), 0, 200)\n",
        "\n",
        "        score = wait + throughput + util + patience\n",
        "\n",
        "        return {'wait': wait, 'throughput': throughput, 'util': util,\n",
        "                'patience': patience, 'score': score}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HL13e5s3fDQe"
      },
      "outputs": [],
      "source": [
        "DATA_MATRIX.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASvvvgItuZSS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1zk9Z_kWMMV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rc1B84IgYxsG"
      },
      "source": [
        "# Fuzzy Multiplex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjlEQiV9WMMV",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class MetricsEvaluator:\n",
        "    def __init__(self, data_matrix):\n",
        "        self.data_matrix = data_matrix\n",
        "        self.num_features = data_matrix.shape[1]\n",
        "        self.num_nodes = data_matrix.shape[0]\n",
        "\n",
        "        # Baseline parameters\n",
        "        self.W0, self.T0, self.U0 = 15.0, 100.0, 10.0\n",
        "        self.P0 = 10.0\n",
        "\n",
        "        # GA-like perturbation parameters\n",
        "        self.ga_population = 20\n",
        "        self.ga_generations = 50\n",
        "        self.mutation = 0.05\n",
        "\n",
        "    def compute_node_metrics(self, node_idx, y=None, node_size=4):\n",
        "        \"\"\"\n",
        "        Compute synthetic metrics for a given node.\n",
        "\n",
        "        node_idx: index of the node\n",
        "        y: optional external input array\n",
        "        node_size: number of features per node\n",
        "        \"\"\"\n",
        "\n",
        "        # --- Slice node's columns safely ---\n",
        "        cols_per_node = self.num_features // self.num_nodes\n",
        "        start_col = node_idx * cols_per_node\n",
        "        node_cols = slice(start_col, start_col + node_size)\n",
        "        node_data = self.data_matrix[:, node_cols]\n",
        "\n",
        "        # --- Compute signals ---\n",
        "        signals_full = node_data.mean(axis=0)\n",
        "        signals = np.zeros(4)\n",
        "        for i in range(min(4, len(signals_full))):\n",
        "            signals[i] = signals_full[i]\n",
        "\n",
        "        # --- Handle y safely ---\n",
        "        if y is None:\n",
        "            y = np.zeros(3)\n",
        "        else:\n",
        "            y = np.array(y)\n",
        "            if len(y) < 3:\n",
        "                y = np.pad(y, (0, 3 - len(y)), constant_values=0.5)\n",
        "            else:\n",
        "                y = y[:3]\n",
        "\n",
        "        # --- Extract relevant node features ---\n",
        "        travel_time = node_data[0, 0] if node_data.shape[1] > 0 else 0.5\n",
        "        load_util = node_data[1, 0] if node_data.shape[0] > 1 else 0.5\n",
        "        priority = node_data[2, 0] if node_data.shape[0] > 2 else 0.5\n",
        "        delay_prob = node_data[3, 0] if node_data.shape[0] > 3 else 0.1\n",
        "        congestion = node_data[4, 0] if node_data.shape[0] > 4 else 0.2\n",
        "        energy_level = node_data[5, 0] if node_data.shape[0] > 5 else 0.5\n",
        "\n",
        "        # --- Internal GA-like perturbation function ---\n",
        "        def ga_metric(base, population=self.ga_population, generations=self.ga_generations, mutation=self.mutation):\n",
        "            pop = base + 0.1 * np.random.randn(population)\n",
        "            for _ in range(generations):\n",
        "                fitness = np.exp(-pop) + 0.1 * np.random.randn(population)  # synthetic fitness\n",
        "                top_idx = np.argmax(fitness)\n",
        "                best = pop[top_idx]\n",
        "                pop = best + mutation * np.random.randn(population)\n",
        "            return best\n",
        "\n",
        "        # --- Compute synthetic metrics ---\n",
        "        wait = 1 / (1 + ga_metric(travel_time * (1 + congestion*0.5)))\n",
        "        throughput = ga_metric(load_util * np.sqrt(energy_level + 0.1))\n",
        "        util = ga_metric(priority * (1 - delay_prob) + 0.1 * energy_level)\n",
        "        patience = 1 / (1 + ga_metric(congestion * (1 + delay_prob)))\n",
        "\n",
        "        # Add small noise to avoid deterministic outputs\n",
        "        wait += np.random.normal(0, 0.01)\n",
        "        throughput += np.random.normal(0, 0.01)\n",
        "        util += np.random.normal(0, 0.01)\n",
        "        patience += np.random.normal(0, 0.01)\n",
        "\n",
        "        # Clip metrics to [0, 1]\n",
        "        # Aggregate score (synthetic)\n",
        "        score = wait + throughput + util + patience\n",
        "\n",
        "        return {\n",
        "            'wait': wait,\n",
        "            'throughput': throughput,\n",
        "            'util': util,\n",
        "            'patience': patience,\n",
        "            'score': score\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebagrGJPeAgE"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "# ---------------- CONFIG ----------------nsion\n",
        "\n",
        "inner_archive_size = 80\n",
        "inner_offspring = 40\n",
        "outer_archive_size = 40\n",
        "outer_offspring = 40\n",
        "inner_iters_per_outer = 50\n",
        "outer_generations = 10\n",
        "outer_cost_limit = 10000\n",
        "inner_learning = 0.1\n",
        "gamma_interlayer = 1\n",
        "top_k = 263\n",
        "np.random.seed()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class InterLayer:\n",
        "    def __init__(self, D_graph, max_inner_dim, inter_dim=None, edge_threshold=0.02, gamma=1.0, seed=42):\n",
        "        np.random.seed(seed)\n",
        "        self.D_graph = D_graph\n",
        "        self.max_input = 2 * max_inner_dim\n",
        "        self.edge_threshold = edge_threshold\n",
        "        self.gamma = gamma\n",
        "        self.inter_dim = inter_dim if inter_dim is not None else max_inner_dim\n",
        "\n",
        "        # Initialize weights proportional to synthetic correlation between nodes\n",
        "        self.weights = {}\n",
        "        self.bias = {}\n",
        "        for i in range(D_graph):\n",
        "            for j in range(D_graph):\n",
        "                if i != j:\n",
        "                    # small random + slight bias towards correlation\n",
        "                    w_init = np.random.uniform(-0.1, 0.1, (self.inter_dim, self.max_input))\n",
        "                    self.weights[(i,j)] = w_init\n",
        "                    self.bias[(i,j)] = np.zeros(self.inter_dim)\n",
        "\n",
        "    def compute_edge_activation(self, i, j, nested_reps):\n",
        "        concat = np.concatenate([nested_reps[i], nested_reps[j]])\n",
        "        concat = np.pad(concat, (0, max(0, self.max_input - len(concat))))[:self.max_input]\n",
        "\n",
        "        # Normalize input to improve correlation\n",
        "        concat = (concat - np.mean(concat)) / (np.std(concat) + 1e-12)\n",
        "\n",
        "        # Compute activation\n",
        "        v = self.weights[(i,j)].dot(concat) + self.bias[(i,j)]\n",
        "\n",
        "        # Scale by correlation strength with input signals\n",
        "        input_strength = np.clip(np.mean(np.abs(concat)), 0, 1)\n",
        "        v = v * input_strength\n",
        "\n",
        "        return 1 / (1 + np.exp(-v))\n",
        "\n",
        "    def build_activations(self, Gmat, nested_reps):\n",
        "        acts = {}\n",
        "        for i in range(self.D_graph):\n",
        "            for j in range(self.D_graph):\n",
        "                if i == j:\n",
        "                    continue\n",
        "                if abs(Gmat[i,j]) > self.edge_threshold:\n",
        "                    acts[(i,j)] = self.compute_edge_activation(i, j, nested_reps)\n",
        "        return acts\n",
        "\n",
        "    @staticmethod\n",
        "    def pairwise_squared_corr(acts):\n",
        "        if len(acts) < 2:\n",
        "            return 0.0\n",
        "        A = np.stack(list(acts.values()))\n",
        "        A_centered = A - A.mean(axis=1, keepdims=True)\n",
        "        stds = np.sqrt(np.sum(A_centered**2, axis=1) / (A.shape[1]-1) + 1e-12)\n",
        "        cov = A_centered @ A_centered.T / (A.shape[1]-1)\n",
        "        corr = cov / (np.outer(stds, stds) + 1e-12)\n",
        "        np.fill_diagonal(corr, 0)\n",
        "        return float((corr**2).sum())\n",
        "\n",
        "    def mi_for_graph(self, Gmat, nested_reps):\n",
        "        acts = self.build_activations(Gmat, nested_reps)\n",
        "        if not acts:\n",
        "            return 0.0\n",
        "        return self.gamma * self.pairwise_squared_corr(acts)\n",
        "\n",
        "    def correlate_shrink_interlayer(self, fmt_bounds=None, interaction_tensor=None, metrics_keys=None, verbose=True):\n",
        "        \"\"\"\n",
        "        Compute Pearson correlation per node & metric between:\n",
        "            - shrink factor (adaptive FMT)\n",
        "            - mean outgoing inter-layer activations\n",
        "        Returns: {node_idx: {metric: {'r':..., 'p':...}}}\n",
        "        \"\"\"\n",
        "        from scipy.stats import pearsonr\n",
        "\n",
        "        if metrics_keys is None:\n",
        "            metrics_keys = ['wait','throughput','util','patience']\n",
        "\n",
        "        D = self.D_graph\n",
        "\n",
        "        # 1. Compute FMT bounds if not given\n",
        "        if fmt_bounds is None:\n",
        "            fmt_bounds = self.compute_fmt_with_bounds_adaptive(top_k=top_k)\n",
        "\n",
        "        # 2. Get inter-layer activations if not provided\n",
        "        if interaction_tensor is None:\n",
        "            interaction_tensor = self.print_interactions(return_tensor=True, verbose=False)\n",
        "\n",
        "        inter_mean = interaction_tensor.mean(axis=2)  # (D,D)\n",
        "        shrink_factors = self.compute_fmt_shrink_factor(fmt_bounds, metrics_keys)  # (D, num_metrics)\n",
        "\n",
        "        correlations = {}\n",
        "\n",
        "        for i in range(D):\n",
        "            correlations[i] = {}\n",
        "            for k, key in enumerate(metrics_keys):\n",
        "                # FMT shrink for node i (broadcasted across outgoing edges)\n",
        "                shrink_vec = shrink_factors[i, k] * np.ones(D)\n",
        "                # Outgoing inter-layer activations from node i\n",
        "                inter_vec = inter_mean[i, :]\n",
        "                # Remove self-loop\n",
        "                mask = np.arange(D) != i\n",
        "                shrink_vec = shrink_vec[mask]\n",
        "                inter_vec = inter_vec[mask]\n",
        "\n",
        "                # Compute Pearson correlation\n",
        "                if np.std(inter_vec) > 1e-8:  # valid correlation\n",
        "                    r, p = pearsonr(shrink_vec, inter_vec)\n",
        "                else:\n",
        "                    r, p = 0.0, 1.0  # no variability\n",
        "\n",
        "                correlations[i][key] = {'r': r, 'p': p}\n",
        "                if verbose:\n",
        "                    print(f\"Node {i} | {key} shrink vs inter-layer: r={r:.3f}, p={p:.3e}\")\n",
        "\n",
        "        return correlations\n",
        "\n",
        "\n",
        "\n",
        "# ---------------- UNIFIED ACOR MULTIPLEX ----------------\n",
        "class GDFCM:\n",
        "    def __init__(self, candidate_dims, D_graph, inner_archive_size, inner_offspring,\n",
        "                 outer_archive_size, outer_offspring, synthetic_targets, inner_learning,\n",
        "                 gamma_interlayer=1.0, causal_flag=True):\n",
        "        self.candidate_dims = candidate_dims\n",
        "        self.D_graph = D_graph\n",
        "        self.inner_archive_size = inner_archive_size\n",
        "        self.inner_offspring = inner_offspring\n",
        "        self.outer_archive_size = outer_archive_size\n",
        "        self.outer_offspring = outer_offspring\n",
        "        self.synthetic_targets = synthetic_targets\n",
        "        self.inner_learning = inner_learning\n",
        "        self.causal_flag = causal_flag\n",
        "        self.best_dim_per_node = [len(t)-1 for t in synthetic_targets]  # last element as best dim\n",
        "\n",
        "        self.nested_reps = [np.zeros(max(candidate_dims)) for _ in range(D_graph)]\n",
        "      #  self.best_dim_per_node = [candidate_dims[0] for _ in range(D_graph)]\n",
        "        self.inter_layer = InterLayer(D_graph, max_inner_dim=max(candidate_dims), gamma=gamma_interlayer)\n",
        "        self.chosen_Gmat = np.random.uniform(-0.5,0.5,(D_graph,D_graph))\n",
        "        np.fill_diagonal(self.chosen_Gmat,0)\n",
        "        self.l2_before, self.l2_after = [], []\n",
        "\n",
        "    # ---------- INNER LOOP (FCM) ----------\n",
        "    def run_inner(self, node_idx, target, D_fcm,\n",
        "              steps=100, lr_x=0.001, lr_y=0.001, lr_W=0.001,\n",
        "              decorrelate_metrics=True):\n",
        "\n",
        "      # --- Initialize activations ---\n",
        "        # Inside run_inner\n",
        "        x = target.copy()\n",
        "        y = target.copy()\n",
        "\n",
        "        # Pad target for L2 computation\n",
        "        target_padded = np.pad(target, (0, len(self.nested_reps[node_idx]) - len(target)),\n",
        "                            mode='constant', constant_values=0.5)\n",
        "        self.l2_before.append(np.linalg.norm(self.nested_reps[node_idx] - target_padded))\n",
        "\n",
        "        # FCM updates\n",
        "        W = np.random.uniform(-0.6, 0.6, (D_fcm, D_fcm))\n",
        "        np.fill_diagonal(W, 0)\n",
        "\n",
        "        for _ in range(steps):\n",
        "            z = W.dot(x)\n",
        "            Theta_grad_z = 2*z - target\n",
        "            Theta_grad_x = Theta_grad_z @ W + (y+1)\n",
        "            Theta_grad_y = x + 1\n",
        "            Theta_grad_W = np.outer(Theta_grad_z, x)\n",
        "\n",
        "            x -= lr_x * np.clip(Theta_grad_x, -0.05, 0.05)\n",
        "            y -= lr_y * np.clip(Theta_grad_y, -0.05, 0.05)\n",
        "            x = np.clip(x, 0, 1)\n",
        "            y = np.clip(y, 0, 1)\n",
        "            W -= lr_W * np.clip(Theta_grad_W, -0.01, 0.01)\n",
        "            np.fill_diagonal(W, 0)\n",
        "            W = np.clip(W, -1, 1)\n",
        "\n",
        "        # Pad FCM output to max dim for nested_reps\n",
        "        x_padded = np.pad(x, (0, len(self.nested_reps[node_idx]) - len(x)),\n",
        "                        mode='constant', constant_values=0.5)\n",
        "        self.nested_reps[node_idx] = x_padded\n",
        "        self.l2_after.append(np.linalg.norm(x_padded - target_padded))\n",
        "\n",
        "\n",
        "      # --- Metric decoupling ---\n",
        "        metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "\n",
        "        if decorrelate_metrics:\n",
        "            # Neutralized input\n",
        "            neutral_y = np.full_like(x, 0.5)\n",
        "            metrics = metrics_evaluator.compute_node_metrics(node_idx, y=neutral_y)\n",
        "\n",
        "            # Optional: orthogonalize against inter-layer mean\n",
        "                    # Optional: orthogonalize against inter-layer mean\n",
        "        inter_tensor = self.inter_layer.build_activations(self.chosen_Gmat, self.nested_reps)\n",
        "        if inter_tensor:\n",
        "                D = self.D_graph\n",
        "                inter_mat = np.zeros((D, D))\n",
        "                for (i,j), vec in inter_tensor.items():\n",
        "                    inter_mat[i,j] = vec.mean()  # mean scalar\n",
        "\n",
        "                node_vec = np.array([metrics[k] for k in ['wait','throughput','util','patience']])\n",
        "                for i in range(D):\n",
        "                    f_scalar = inter_mat[i,:].mean()  # mean over row\n",
        "                    proj = f_scalar * (node_vec.mean() / (1e-12 + 1))  # scale by node_vec mean\n",
        "                    node_vec -= proj\n",
        "\n",
        "\n",
        "            # --- Fully decorrelated score ---\n",
        "                metrics['score'] = metrics['wait'] + metrics['throughput'] + metrics['util'] + metrics['patience']\n",
        "\n",
        "        else:\n",
        "            # Compute normally if no decorrelation\n",
        "            metrics = metrics_evaluator.compute_node_metrics(node_idx, y=x)\n",
        "\n",
        "        # --- Compute MI score for inter-layer ---\n",
        "        mi_score = self.inter_layer.mi_for_graph(self.chosen_Gmat, self.nested_reps)\n",
        "\n",
        "        return x, y, W, mi_score, metrics\n",
        "\n",
        "    # ---------- OUTER LOOP ----------\n",
        "    def run_outer(self, outer_cost_limit=1000):\n",
        "      metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "      node_metrics_list = []\n",
        "      raw_scores = []\n",
        "\n",
        "      # --- Compute node metrics per node ---\n",
        "      for i, y in enumerate(self.nested_reps):\n",
        "          metrics = metrics_evaluator.compute_node_metrics(i, y=y)\n",
        "          node_metrics_list.append(metrics)\n",
        "          raw_scores.append(metrics['score'])\n",
        "\n",
        "      raw_scores = np.array(raw_scores)\n",
        "      total_raw = raw_scores.sum()\n",
        "\n",
        "      # --- Apply cap to raw metrics ---\n",
        "      capped_total_raw = total_raw\n",
        "      if total_raw > outer_cost_limit:\n",
        "          scale_factor = outer_cost_limit / total_raw\n",
        "          for metrics in node_metrics_list:\n",
        "              for key in ['wait', 'throughput', 'util', 'patience', 'score']:\n",
        "                  metrics[key] *= scale_factor\n",
        "          raw_scores *= scale_factor\n",
        "          capped_total_raw = outer_cost_limit\n",
        "\n",
        "      # --- Compute Fuzzy Metric Tensor contribution ---\n",
        "      fuzzy_tensor = self.compute_fuzzy_metric_tensor(normalize=True)\n",
        "      D = self.D_graph\n",
        "\n",
        "      # Only consider off-diagonal entries for inter-node interactions\n",
        "      off_diag_mask = np.ones((D, D), dtype=bool)\n",
        "      np.fill_diagonal(off_diag_mask, 0)\n",
        "      fuzzy_score_offdiag = fuzzy_tensor[off_diag_mask].sum()\n",
        "\n",
        "      # --- Compute per-node contribution ---\n",
        "      node_contributions = np.zeros(D)\n",
        "      for i in range(D):\n",
        "          # Contribution from own metrics\n",
        "          own_score = raw_scores[i]\n",
        "\n",
        "          # Contribution from FMT interactions (row i -> others)\n",
        "          fmt_contrib = fuzzy_tensor[i, :, :].sum() - fuzzy_tensor[i, i, :].sum()  # exclude self\n",
        "          node_contributions[i] = own_score + self.inter_layer.gamma * fmt_contrib\n",
        "\n",
        "      # --- Strong decoupling: correlation penalty ---\n",
        "      inter_tensor = self.print_interactions(return_tensor=True, verbose=False)\n",
        "      if inter_tensor is None or inter_tensor.shape[2] == 0:\n",
        "          inter_mean = np.zeros((D, D))\n",
        "      else:\n",
        "          inter_mean = inter_tensor.mean(axis=2)  # shape (D,D)\n",
        "\n",
        "      fmt_mean = fuzzy_tensor.mean(axis=2)  # shape (D,D)\n",
        "\n",
        "      corr_penalty = 0.0\n",
        "      for i in range(D):\n",
        "          fmt_vec = fmt_mean[i, :]\n",
        "          inter_vec = inter_mean[i, :]\n",
        "          if np.std(fmt_vec) > 1e-8 and np.std(inter_vec) > 1e-8:\n",
        "              corr = np.corrcoef(fmt_vec, inter_vec)[0, 1]\n",
        "              corr_penalty += abs(corr) ** 2\n",
        "\n",
        "      corr_penalty /= D\n",
        "      combined_score = node_contributions.sum() - corr_penalty * 500  # strong decorrelation\n",
        "\n",
        "      # --- Store for plotting / further analysis ---\n",
        "      self.capped_node_metrics = node_metrics_list\n",
        "      self.node_score_contributions = node_contributions\n",
        "      self.correlation_penalty = corr_penalty\n",
        "\n",
        "      return node_metrics_list, combined_score, node_contributions\n",
        "\n",
        "\n",
        "\n",
        "    def run(self, outer_generations=outer_generations):\n",
        "        final_metrics = None\n",
        "\n",
        "        for gen in range(outer_generations):\n",
        "            mi_scores = []\n",
        "            node_metrics_list = []\n",
        "\n",
        "            for node_idx in range(self.D_graph):\n",
        "                # Full target vector for this node\n",
        "                full_target = self.synthetic_targets[node_idx]['target']\n",
        "\n",
        "                # Use the candidate dimension assigned to this node\n",
        "                D_fcm = self.candidate_dims[node_idx]\n",
        "                target = full_target[:D_fcm]  # slice according to candidate_dims\n",
        "\n",
        "                # Run inner FCM\n",
        "                _, _, _, mi_score, metrics = self.run_inner(node_idx, target, D_fcm)\n",
        "                mi_scores.append(mi_score)\n",
        "                node_metrics_list.append(metrics)\n",
        "\n",
        "\n",
        "            # --- Outer loop uses decorrelated metrics ---\n",
        "            self.capped_node_metrics = node_metrics_list\n",
        "            _, capped_score, node_contributions = self.run_outer()  # uses self.capped_node_metrics\n",
        "\n",
        "            print(f\"\\n--- Generation {gen} Metrics ---\")\n",
        "            for i, m in enumerate(node_metrics_list):\n",
        "                print(f\"Node {i} | \" + \" | \".join([f\"{k}: {v:.2f}\" for k,v in m.items()]))\n",
        "\n",
        "            print(f\"\\n--- Generation {gen} Node Contributions ---\")\n",
        "            for i, c in enumerate(node_contributions):\n",
        "                print(f\"Node {i}: Contribution = {c:.4f}\")\n",
        "\n",
        "            print(f\"Outer Score (capped): {capped_score:.3f}\")\n",
        "\n",
        "            final_metrics = node_metrics_list\n",
        "\n",
        "        return final_metrics\n",
        "\n",
        "\n",
        "    # ---------- VISUALIZATIONS ----------\n",
        "    # ---------- VISUALIZATIONS ----------\n",
        "    def plot_pointwise_minmax_elite(self, top_k=21):\n",
        "        plt.figure(figsize=(14,3))\n",
        "        for i in range(self.D_graph):\n",
        "            # Node's actual dimension\n",
        "            dim_i = self.candidate_dims[i]\n",
        "            base = self.nested_reps[i][:dim_i]  # slice to candidate dim\n",
        "            reps = np.clip(base + np.random.normal(0,0.05,(top_k,len(base))),0,1)\n",
        "            y_min, y_max = reps.min(axis=0), reps.max(axis=0)\n",
        "            y_sel = base\n",
        "\n",
        "            # True target for this node, sliced to candidate dim\n",
        "            y_true = self.synthetic_targets[i]['target'][:len(y_sel)]\n",
        "            if len(y_true) < len(y_sel):\n",
        "                y_true = np.pad(y_true, (0, len(y_sel)-len(y_true)), \"constant\")\n",
        "            else:\n",
        "                y_true = y_true[:len(y_sel)]\n",
        "\n",
        "            plt.subplot(1,self.D_graph,i+1)\n",
        "            plt.fill_between(range(len(y_min)),y_min,y_max,color='skyblue',alpha=0.4,label='Elite Interval')\n",
        "            plt.plot(y_sel,'k-',lw=2,label='Estimated')\n",
        "            plt.plot(y_true,'r--',lw=2,label='True')\n",
        "            plt.ylim(0,1.05)\n",
        "            plt.title(f\"Node {i+1}\")\n",
        "            if i==0: plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def plot_nested_activations(self):\n",
        "        plt.figure(figsize=(12,3))\n",
        "        for i,rep in enumerate(self.nested_reps):\n",
        "            dim_i = self.candidate_dims[i]\n",
        "            rep_i = rep[:dim_i]  # slice to candidate dim\n",
        "            plt.subplot(1,self.D_graph,i+1)\n",
        "            plt.bar(range(len(rep_i)), rep_i, color=plt.cm.plasma(rep_i))\n",
        "            plt.ylim(0,1)\n",
        "            plt.title(f\"Node {i+1}\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def plot_outer_fuzzy_graph(self):\n",
        "        G = nx.DiGraph()\n",
        "        for i in range(self.D_graph): G.add_node(i)\n",
        "        for i in range(self.D_graph):\n",
        "            for j in range(self.D_graph):\n",
        "                if i!=j and abs(self.chosen_Gmat[i,j])>0.02:\n",
        "                    G.add_edge(i,j,weight=self.chosen_Gmat[i,j])\n",
        "        node_sizes = [self.best_dim_per_node[i]*200 for i in range(self.D_graph)]\n",
        "        edge_colors = ['green' if d['weight']>0 else 'red' for _,_,d in G.edges(data=True)]\n",
        "        edge_widths = [abs(d['weight'])*3 for _,_,d in G.edges(data=True)]\n",
        "        pos = nx.circular_layout(G)\n",
        "        plt.figure(figsize=(6,6))\n",
        "        nx.draw(G,pos,node_size=node_sizes,node_color='skyblue',\n",
        "                edge_color=edge_colors,width=edge_widths,arrows=True,with_labels=True)\n",
        "        plt.title(\"Outer Fuzzy Multiplex Graph\")\n",
        "        plt.show()\n",
        "# ---------------- INTERACTIONS INSPECTOR ----------------\n",
        "\n",
        "    def print_interactions(self, return_tensor=True, verbose=True):\n",
        "            D_graph = self.D_graph\n",
        "            inter_dim = self.inter_layer.inter_dim\n",
        "            inter_tensor = np.zeros((D_graph, D_graph, inter_dim))\n",
        "\n",
        "            acts = self.inter_layer.build_activations(self.chosen_Gmat, self.nested_reps)\n",
        "            if not acts:\n",
        "                if verbose:\n",
        "                    print(\"No active edges above threshold.\")\n",
        "                return inter_tensor if return_tensor else None\n",
        "\n",
        "            for (i, j), vec in acts.items():\n",
        "                inter_tensor[i, j, :] = vec\n",
        "                if verbose:\n",
        "                    act_str = \", \".join([f\"{v:.3f}\" for v in vec])\n",
        "                    print(f\"Node {i} -> Node {j}: [{act_str}]\")\n",
        "            return inter_tensor if return_tensor else None\n",
        "\n",
        "        # Move these outside of print_interactions (class-level)\n",
        "    def print_l2_summary(self):\n",
        "            print(\"\\nL2 Distances to Target per Node:\")\n",
        "            for idx, (before, after) in enumerate(zip(self.l2_before, self.l2_after)):\n",
        "                print(f\"Node {idx}: Before={before:.4f}, After={after:.4f}\")\n",
        "\n",
        "    def compute_fuzzy_metric_tensor(self, normalize=True, verbose=False):\n",
        "            \"\"\"\n",
        "            Computes a Fuzzy Metric Tensor (D_graph x D_graph x num_metrics)\n",
        "            using current nested reps and node metrics.\n",
        "            Each slice [i,j,:] represents metrics of node j (optionally weighted by Gmat[i,j])\n",
        "            \"\"\"\n",
        "            metrics_keys = ['wait', 'throughput', 'util', 'patience']\n",
        "            D = self.D_graph\n",
        "            num_metrics = len(metrics_keys)\n",
        "            tensor = np.zeros((D, D, num_metrics))\n",
        "\n",
        "            metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "\n",
        "            node_metrics = []\n",
        "            for i, rep in enumerate(self.nested_reps):\n",
        "                metrics = metrics_evaluator.compute_node_metrics(i, y=rep)\n",
        "                node_metrics.append(np.array([metrics[k] for k in metrics_keys]))\n",
        "            node_metrics = np.array(node_metrics)  # (D, num_metrics)\n",
        "\n",
        "            for i in range(D):\n",
        "                for j in range(D):\n",
        "                    if i==j:\n",
        "                        tensor[i,j,:] = node_metrics[j]\n",
        "                    else:\n",
        "                        weight = np.clip(abs(self.chosen_Gmat[i,j]), 0, 1)\n",
        "                        tensor[i,j,:] = weight * node_metrics[j]\n",
        "\n",
        "            if normalize:\n",
        "                tensor = (tensor - tensor.min()) / (tensor.max() - tensor.min() + 1e-12)\n",
        "\n",
        "            if verbose:\n",
        "                print(\"Fuzzy Metric Tensor shape:\", tensor.shape)\n",
        "\n",
        "            return tensor\n",
        "\n",
        "\n",
        "\n",
        "    def compute_fmt_shrink_factor(self, fmt_bounds, metrics_keys=None):\n",
        "        \"\"\"\n",
        "        Returns shrink factor per node and metric.\n",
        "        shrink_factor = 1 - (current_interval / original_interval)\n",
        "        \"\"\"\n",
        "        if metrics_keys is None:\n",
        "            metrics_keys = ['wait','throughput','util','patience']\n",
        "\n",
        "        D = self.D_graph\n",
        "        num_metrics = len(metrics_keys)\n",
        "        shrink_factors = np.zeros((D, num_metrics))\n",
        "\n",
        "        for i in range(D):\n",
        "            for k in range(num_metrics):\n",
        "                lower, upper = fmt_bounds[i, i, k, 0], fmt_bounds[i, i, k, 1]  # self-node interval\n",
        "                interval_width = upper - lower + 1e-12  # normalized [0,1]\n",
        "                shrink_factors[i, k] = 1 - interval_width  # more shrink = higher value\n",
        "\n",
        "        return shrink_factors\n",
        "\n",
        "    def compute_fmt_with_bounds_adaptive(self, top_k=21, max_shrink=0.5, metrics_keys=None):\n",
        "        \"\"\"\n",
        "        Computes FMT bounds using pointwise min/max across elite solutions,\n",
        "        and applies dynamic adaptive shrinking where variability is low.\n",
        "        Returns tensor shape (D,D,num_metrics,2) [lower, upper].\n",
        "        \"\"\"\n",
        "        if metrics_keys is None:\n",
        "            metrics_keys = ['wait','throughput','util','patience']\n",
        "\n",
        "        D = self.D_graph\n",
        "        num_metrics = len(metrics_keys)\n",
        "        tensor_bounds = np.zeros((D, D, num_metrics, 2))\n",
        "        metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "\n",
        "        variability = np.zeros((D, num_metrics))\n",
        "\n",
        "        # Step 1: compute bounds from perturbations\n",
        "        for i in range(D):\n",
        "            base = self.nested_reps[i]\n",
        "            reps = np.clip(base + np.random.normal(0, 0.05, (top_k, len(base))), 0, 1)\n",
        "            metrics_matrix = np.zeros((top_k, num_metrics))\n",
        "            for idx, rep in enumerate(reps):\n",
        "                m = metrics_evaluator.compute_node_metrics(i, y=rep)\n",
        "                metrics_matrix[idx, :] = [m[k] for k in metrics_keys]\n",
        "\n",
        "            lower_i = metrics_matrix.min(axis=0)\n",
        "            upper_i = metrics_matrix.max(axis=0)\n",
        "            tensor_bounds[i, :, :, 0] = lower_i[np.newaxis, :]  # broadcast to all j\n",
        "            tensor_bounds[i, :, :, 1] = upper_i[np.newaxis, :]\n",
        "            variability[i, :] = metrics_matrix.std(axis=0)\n",
        "\n",
        "        # Step 2: adaptive shrinking\n",
        "        for i in range(D):\n",
        "            for j in range(D):\n",
        "                for k in range(num_metrics):\n",
        "                    lower, upper = tensor_bounds[i,j,k,0], tensor_bounds[i,j,k,1]\n",
        "                    mean = (lower + upper)/2\n",
        "                    var_norm = min(1.0, variability[i,k]/(upper-lower + 1e-12))\n",
        "                    shrink_factor = max_shrink * (1 - var_norm)\n",
        "                    tensor_bounds[i,j,k,0] = mean - shrink_factor*(mean - lower)\n",
        "                    tensor_bounds[i,j,k,1] = mean + shrink_factor*(upper - mean)\n",
        "\n",
        "        return tensor_bounds\n",
        "\n",
        "\n",
        "\n",
        "    def plot_fuzzy_metric_tensor_heatmaps(self, fuzzy_tensor=None, metrics_keys=['wait','throughput','util','patience']):\n",
        "        \"\"\"\n",
        "        Plot a heatmap panel for each metric in the FMT.\n",
        "        Rows: source node i\n",
        "        Columns: target node j\n",
        "        \"\"\"\n",
        "        if fuzzy_tensor is None:\n",
        "            fuzzy_tensor = self.compute_fuzzy_metric_tensor(normalize=True)\n",
        "\n",
        "        D = self.D_graph\n",
        "        num_metrics = len(metrics_keys)\n",
        "\n",
        "        fig, axes = plt.subplots(1, num_metrics, figsize=(4*num_metrics,4))\n",
        "        if num_metrics == 1: axes = [axes]\n",
        "\n",
        "        for k, key in enumerate(metrics_keys):\n",
        "            data = fuzzy_tensor[:,:,k]\n",
        "            im = axes[k].imshow(data, cmap='viridis', vmin=0, vmax=1)\n",
        "            for i in range(D):\n",
        "                for j in range(D):\n",
        "                    axes[k].text(j,i,f\"{data[i,j]:.2f}\",ha='center',va='center',color='white',fontsize=9)\n",
        "            axes[k].set_xticks(range(D))\n",
        "            axes[k].set_yticks(range(D))\n",
        "            axes[k].set_xticklabels([f'Node {j}' for j in range(D)])\n",
        "            axes[k].set_yticklabels([f'Node {i}' for i in range(D)])\n",
        "            axes[k].set_title(f'FMT - {key}')\n",
        "\n",
        "        fig.colorbar(im, ax=axes, orientation='vertical', fraction=0.025, pad=0.04, label='Normalized Metric Value')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def compute_fmt_with_elite_bounds(self, top_k=21):\n",
        "        \"\"\"\n",
        "        Computes FMT bounds using pointwise min/max across elite solutions.\n",
        "        Returns tensor shape (D,D,num_metrics,2) [lower, upper].\n",
        "        \"\"\"\n",
        "        metrics_keys = ['wait','throughput','util','patience']\n",
        "        D = self.D_graph\n",
        "        num_metrics = len(metrics_keys)\n",
        "        tensor_bounds = np.zeros((D,D,num_metrics,2))\n",
        "\n",
        "        metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "\n",
        "        for i in range(D):\n",
        "            # Generate top_k perturbations around current nested_rep (like in plot_pointwise_minmax_elite)\n",
        "            base = self.nested_reps[i]\n",
        "            reps = np.clip(base + np.random.normal(0,0.05,(top_k,len(base))),0,1)\n",
        "\n",
        "            # Compute node metrics for each perturbed solution\n",
        "            metrics_matrix = np.zeros((top_k, num_metrics))\n",
        "            for idx, rep in enumerate(reps):\n",
        "                m = metrics_evaluator.compute_node_metrics(i, y=rep)\n",
        "                metrics_matrix[idx,:] = [m[k] for k in metrics_keys]\n",
        "\n",
        "            # Compute pointwise min/max across elite solutions\n",
        "            lower_i = metrics_matrix.min(axis=0)\n",
        "            upper_i = metrics_matrix.max(axis=0)\n",
        "\n",
        "            # Fill bounds tensor for all source nodes (i->j)\n",
        "            for j in range(D):\n",
        "                tensor_bounds[i,j,:,0] = lower_i\n",
        "                tensor_bounds[i,j,:,1] = upper_i\n",
        "\n",
        "        return tensor_bounds\n",
        "\n",
        "\n",
        "    def plot_fmt_with_bounds(self, fmt_tensor_bounds):\n",
        "        D = self.D_graph\n",
        "        metrics_keys = ['wait','throughput','util','patience']\n",
        "        num_metrics = len(metrics_keys)\n",
        "\n",
        "        fig, axes = plt.subplots(1, num_metrics, figsize=(4*num_metrics,4))\n",
        "        if num_metrics == 1: axes = [axes]\n",
        "\n",
        "        for k, key in enumerate(metrics_keys):\n",
        "            lower = fmt_tensor_bounds[:,:,k,0]\n",
        "            upper = fmt_tensor_bounds[:,:,k,1]\n",
        "            mean_vals = (lower+upper)/2\n",
        "            range_vals = upper-lower\n",
        "            max_range = range_vals.max() if range_vals.max()>0 else 1.0\n",
        "            alphas = 0.2 + 0.8 * range_vals/max_range\n",
        "\n",
        "            im = axes[k].imshow(mean_vals, cmap='viridis', vmin=0, vmax=mean_vals.max())\n",
        "            for i in range(D):\n",
        "                for j in range(D):\n",
        "                    alpha_val = np.clip(1-alphas[i,j],0,1)\n",
        "                    rect = plt.Rectangle((j-0.5,i-0.5),1,1,color='white',alpha=alpha_val)\n",
        "                    axes[k].add_patch(rect)\n",
        "                    axes[k].text(j,i,f\"{lower[i,j]:.1f}\\n{upper[i,j]:.1f}\",ha='center',va='center',fontsize=8)\n",
        "            axes[k].set_title(f'FMT Bounds - {key}')\n",
        "            axes[k].set_xticks(range(D))\n",
        "            axes[k].set_yticks(range(D))\n",
        "            axes[k].set_xticklabels([f'Node {j}' for j in range(D)])\n",
        "            axes[k].set_yticklabels([f'Node {i}' for i in range(D)])\n",
        "\n",
        "        fig.colorbar(im, ax=axes, orientation='vertical', fraction=0.025, pad=0.04, label='Mean Metric Value')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    def plot_node_score_contribution(self, metrics_keys=['wait','throughput','util','patience']):\n",
        "        \"\"\"\n",
        "        Plot per-node total score contribution in the SAME STYLE as the FMT plots:\n",
        "            - uses imshow\n",
        "            - one panel for: raw, FMT, and total stacked\n",
        "            - diagonal masked\n",
        "            - annotated cells\n",
        "            - node contribution highlighted like your FMT code\n",
        "        \"\"\"\n",
        "        # ---------------------------------------------------------------------\n",
        "        # 1. Collect node contributions from run_outer()\n",
        "        # ---------------------------------------------------------------------\n",
        "        _, _, node_contributions = self.run_outer()\n",
        "        node_contributions = np.array(node_contributions)\n",
        "        D = len(node_contributions)\n",
        "\n",
        "        # ---------------------------------------------------------------------\n",
        "        # 2. Recompute FMT influence (same style as your FMT plots)\n",
        "        # ---------------------------------------------------------------------\n",
        "        fuzzy_tensor = self.compute_fuzzy_metric_tensor(normalize=True)\n",
        "        total_tensor = fuzzy_tensor.sum(axis=2)           # sum over metrics\n",
        "        fmt_tensor = total_tensor.copy()\n",
        "        np.fill_diagonal(fmt_tensor, 0)                   # mask diagonal\n",
        "\n",
        "        fmt_per_node = fmt_tensor.sum(axis=1)             # row sum\n",
        "        raw_per_node = node_contributions - fmt_per_node  # everything else\n",
        "\n",
        "        # Construct matrices for plotting (DD)\n",
        "        raw_matrix = np.zeros((D, D))\n",
        "        fmt_matrix = fmt_tensor\n",
        "        total_matrix = raw_matrix + fmt_matrix            # raw only on diagonal? no  distribute raw as row diag\n",
        "        np.fill_diagonal(raw_matrix, raw_per_node)\n",
        "        total_matrix = raw_matrix + fmt_matrix\n",
        "\n",
        "        # ---------------------------------------------------------------------\n",
        "        # 3. Plot - 3 subplots in SAME STYLE as FMT panels\n",
        "        # ---------------------------------------------------------------------\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "        matrices = [raw_matrix, fmt_matrix, total_matrix]\n",
        "        titles = [\"Raw Node Contribution\", \"FMT Interaction Contribution\", \"Total Contribution\"]\n",
        "\n",
        "        for ax, mat, title in zip(axes, matrices, titles):\n",
        "\n",
        "            im = ax.imshow(mat, cmap='viridis', vmin=np.min(mat), vmax=np.max(mat))\n",
        "\n",
        "            # annotate values\n",
        "            for i in range(D):\n",
        "                for j in range(D):\n",
        "                    val = mat[i, j]\n",
        "                    ax.text(j, i, f\"{val:.2f}\", ha='center',\n",
        "                            va='center', color='white', fontsize=8)\n",
        "\n",
        "            ax.set_title(title)\n",
        "            ax.set_xticks(range(D))\n",
        "            ax.set_xticklabels([f\"Node {i+1}\" for i in range(D)])\n",
        "            ax.set_yticks(range(D))\n",
        "            ax.set_yticklabels([f\"Node {i+1}\" for i in range(D)])\n",
        "\n",
        "        fig.colorbar(im, ax=axes, orientation='vertical', fraction=0.025, pad=0.04,\n",
        "                    label='Contribution Value')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def correlate_fmt_interactions_per_node(self, fmt_bounds=None, interaction_tensor=None, verbose=True):\n",
        "        \"\"\"\n",
        "        Correlate the FMT bounds with inter-layer interactions per node and per metric.\n",
        "        Returns a dict of shape: {node_idx: {metric: {'r':..., 'p':...}}}.\n",
        "        \"\"\"\n",
        "        from scipy.stats import pearsonr\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        metrics_keys = ['wait','throughput','util','patience']\n",
        "        D = self.D_graph\n",
        "\n",
        "        # Compute tensors if not provided\n",
        "        if fmt_bounds is None:\n",
        "            fmt_bounds = self.compute_fmt_with_elite_bounds(top_k=21)\n",
        "        if interaction_tensor is None:\n",
        "            interaction_tensor = self.print_interactions(return_tensor=True, verbose=False)\n",
        "\n",
        "        # Reduce interaction tensor along inter_dim\n",
        "        inter_mean = interaction_tensor.mean(axis=2)  # (D,D)\n",
        "\n",
        "        node_correlations = {}\n",
        "\n",
        "        for i in range(D):\n",
        "            node_correlations[i] = {}\n",
        "            for k, key in enumerate(metrics_keys):\n",
        "                # FMT bounds for target node j from source i (mean of lower/upper)\n",
        "                fmt_mean = fmt_bounds[i,:,k,:].mean(axis=1)  # shape (D,)\n",
        "                # Interaction tensor for edges from node i to j\n",
        "                inter_vec = inter_mean[i,:]  # shape (D,)\n",
        "                # Pearson correlation\n",
        "                corr, pval = pearsonr(fmt_mean, inter_vec)\n",
        "                node_correlations[i][key] = {'r': corr, 'p': pval}\n",
        "\n",
        "                if verbose:\n",
        "                    print(f\"Node {i} | {key}: r = {corr:.3f}, p = {pval:.3e}\")\n",
        "                    plt.figure(figsize=(4,3))\n",
        "                    plt.scatter(fmt_mean, inter_vec, alpha=0.7, edgecolor='k', color='skyblue')\n",
        "                    plt.xlabel(f\"FMT {key} (Node {i} -> others)\")\n",
        "                    plt.ylabel(f\"Interaction mean (Node {i} -> others)\")\n",
        "                    plt.title(f\"Node {i} | {key} correlation: r={corr:.3f}\")\n",
        "                    plt.grid(True)\n",
        "                    plt.show()\n",
        "\n",
        "        return node_correlations\n",
        "\n",
        "    def correlation_penalty(self, fmt_bounds=None, interaction_tensor=None):\n",
        "        \"\"\"\n",
        "        Computes a penalty term that is high if per-node FMT metrics correlate with interactions.\n",
        "        Returns total penalty to subtract from the outer score.\n",
        "        \"\"\"\n",
        "        from scipy.stats import pearsonr\n",
        "\n",
        "        D = self.D_graph\n",
        "        metrics_keys = ['wait','throughput','util','patience']\n",
        "\n",
        "        if fmt_bounds is None:\n",
        "            fmt_bounds = self.compute_fmt_with_elite_bounds(top_k=top_k)\n",
        "        if interaction_tensor is None:\n",
        "            interaction_tensor = self.print_interactions(return_tensor=True, verbose=False)\n",
        "\n",
        "        inter_mean = interaction_tensor.mean(axis=2)\n",
        "        total_penalty = 0.0\n",
        "\n",
        "        for i in range(D):\n",
        "            for k in range(len(metrics_keys)):\n",
        "                fmt_mean = fmt_bounds[i,:,k,:].mean(axis=1)\n",
        "                inter_vec = inter_mean[i,:]\n",
        "                if np.std(fmt_mean) > 1e-8 and np.std(inter_vec) > 1e-8:\n",
        "                    corr, _ = pearsonr(fmt_mean, inter_vec)\n",
        "                    total_penalty += abs(corr)  # penalize high correlation\n",
        "\n",
        "        # normalize by number of nodes  metrics\n",
        "        total_penalty /= (D * len(metrics_keys))**2\n",
        "        return total_penalty\n",
        "\n",
        "\n",
        "# ---------------- USAGE ----------------\n",
        "if __name__ == \"__main__\":\n",
        "    optimizer = GDFCM(\n",
        "        candidate_dims, D_graph,\n",
        "        inner_archive_size, inner_offspring,\n",
        "        outer_archive_size, outer_offspring,\n",
        "        synthetic_targets,\n",
        "        inner_learning, gamma_interlayer=1,\n",
        "        causal_flag=False\n",
        "    )\n",
        "    metrics_list = optimizer.run()\n",
        "    optimizer.plot_pointwise_minmax_elite()\n",
        "    optimizer.plot_nested_activations()\n",
        "    # Compute FMT with elite bounds\n",
        "    fmt_elite_bounds = optimizer.compute_fmt_with_elite_bounds(top_k=top_k+10)\n",
        "\n",
        "# Plot as heatmaps\n",
        "    optimizer.plot_fmt_with_bounds(fmt_elite_bounds)\n",
        "\n",
        "    # Compute fuzzy multiplex tensor\n",
        "    fmt_tensor = optimizer.compute_fuzzy_metric_tensor(normalize=True)\n",
        "    optimizer.plot_fuzzy_metric_tensor_heatmaps(fmt_tensor)\n",
        "\n",
        "    # Compute FMT with bounds (minimax elite intervals)\n",
        "    optimizer.plot_node_score_contribution()\n",
        "    optimizer.plot_outer_fuzzy_graph()\n",
        "  #  optimizer.print_interactions()\n",
        "    tensor = optimizer.print_interactions()\n",
        "\n",
        "    print(\"Tensor shape:\", tensor.shape,'\\n',tensor)\n",
        "    # Compute tensors first\n",
        "    fmt_elite_bounds = optimizer.compute_fmt_with_elite_bounds(top_k=top_k)\n",
        "    interaction_tensor = optimizer.print_interactions(return_tensor=True, verbose=False)\n",
        "\n",
        "\n",
        "    interaction_tensor = optimizer.print_interactions(return_tensor=True, verbose=False)\n",
        "\n",
        "    # Get per-node, per-metric correlations\n",
        "    #node_metric_corrs = optimizer.correlate_fmt_interactions_per_node(\n",
        "     #   fmt_bounds=fmt_elite_bounds,\n",
        "      #  interaction_tensor=interaction_tensor\n",
        "   # )\n",
        "    import networkx as nx\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    from mpl_toolkits.mplot3d import Axes3D\n",
        "D_graph = len(optimizer.nested_reps)\n",
        "tensor = optimizer.print_interactions(return_tensor=True, verbose=False)\n",
        "\n",
        "# ---------------- Outer nodes (hubs) ----------------\n",
        "G_outer = nx.DiGraph()\n",
        "for i in range(D_graph):\n",
        "    G_outer.add_node(i)\n",
        "for i in range(D_graph):\n",
        "    for j in range(D_graph):\n",
        "        if i != j and np.any(tensor[i,j,:] != 0):\n",
        "            # Shift to signed weights: 0.5 -> 0, <0.5 negative, >0.5 positive\n",
        "            mean_weight = 2 * (np.mean(tensor[i,j,:]) - 0.5)\n",
        "            G_outer.add_edge(i, j, weight=mean_weight)\n",
        "\n",
        "# Outer spring layout\n",
        "pos_outer_2d = nx.circular_layout(G_outer, scale=5)\n",
        "pos_outer = np.array([[x, y, 0] for x, y in pos_outer_2d.values()])\n",
        "\n",
        "fig = plt.figure(figsize=(10,8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Plot outer nodes\n",
        "for i in range(D_graph):\n",
        "    ax.scatter(*pos_outer[i], s=300, color='skyblue')\n",
        "    ax.text(*pos_outer[i], f'Node {i}', color='black')\n",
        "\n",
        "# Plot outer edges with positive/negative colors\n",
        "\n",
        "for i, j, data in G_outer.edges(data=True):\n",
        "    x_vals = [pos_outer[i,0], pos_outer[j,0]]\n",
        "    y_vals = [pos_outer[i,1], pos_outer[j,1]]\n",
        "    z_vals = [pos_outer[i,2], pos_outer[j,2]]\n",
        "\n",
        "    # Positive = bright green, Negative = bright red\n",
        "    color = 'green' if data['weight'] > 0 else 'red'\n",
        "    linewidth = 2 + 4*abs(data['weight'])  # scale width by magnitude\n",
        "    ax.plot(x_vals, y_vals, z_vals, color=color, linewidth=linewidth)\n",
        "# ---------------- Inner FCMs (small circular around hub) ----------------\n",
        "for i, rep in enumerate(optimizer.nested_reps):\n",
        "    dims = len(rep)\n",
        "    angle = np.linspace(0, 2*np.pi, dims, endpoint=False)\n",
        "    radius = 0.8  # small circle\n",
        "    xs = pos_outer[i,0] + radius * np.cos(angle)\n",
        "    ys = pos_outer[i,1] + radius * np.sin(angle)\n",
        "    zs = pos_outer[i,2] + rep  # activation as height\n",
        "\n",
        "    # Plot inner nodes\n",
        "    ax.scatter(xs, ys, zs, c=rep, cmap='plasma', s=50)\n",
        "\n",
        "    # Connect inner nodes in circle\n",
        "    for k in range(dims):\n",
        "        ax.plot([xs[k], xs[(k+1)%dims]], [ys[k], ys[(k+1)%dims]], [zs[k], zs[(k+1)%dims]], color='gray', alpha=0.5)\n",
        "\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Activation')\n",
        "ax.set_title('Outer Nodes with Inner FCMs (Signed correlations)')\n",
        "plt.show()\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class GDFCMPredictorForward:\n",
        "    \"\"\"\n",
        "    Forward predictor for GDFCM.\n",
        "    Can handle new input data (DATA_MATRIX) and compute:\n",
        "        - Node activations\n",
        "        - Node metrics\n",
        "        - Inter-layer MI scores\n",
        "        - Total contributions and outer score\n",
        "    \"\"\"\n",
        "    def __init__(self, trained_gdfcm: GDFCM):\n",
        "        self.gdfcm = trained_gdfcm\n",
        "        self.D_graph = trained_gdfcm.D_graph\n",
        "        self.nested_reps = trained_gdfcm.nested_reps\n",
        "        self.chosen_Gmat = trained_gdfcm.chosen_Gmat\n",
        "        self.inter_layer = trained_gdfcm.inter_layer\n",
        "\n",
        "    def predict_node(self, node_idx, data_row):\n",
        "        \"\"\"\n",
        "        Predict metrics and activations for a single node given a new data row.\n",
        "        \"\"\"\n",
        "        # Use stored nested_rep as starting point\n",
        "        x = self.nested_reps[node_idx].copy()\n",
        "        # Optional: you could combine with data_row to modify x\n",
        "        # For now, we just compute metrics using data_row as the input\n",
        "        metrics_evaluator = MetricsEvaluator(np.array([data_row]))\n",
        "        metrics = metrics_evaluator.compute_node_metrics(0, y=x)\n",
        "\n",
        "        # Inter-layer MI\n",
        "        mi_score = self.inter_layer.mi_for_graph(self.chosen_Gmat, self.nested_reps)\n",
        "\n",
        "        return {'activations': x, 'metrics': metrics, 'mi_score': mi_score}\n",
        "\n",
        "    def predict_all_nodes(self, new_data_matrix):\n",
        "        \"\"\"\n",
        "        Predict metrics and activations for all nodes using new data.\n",
        "        new_data_matrix: shape (D_graph, num_features)\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        for node_idx in range(self.D_graph):\n",
        "            data_row = new_data_matrix[node_idx % len(new_data_matrix)]\n",
        "            node_result = self.predict_node(node_idx, data_row)\n",
        "            results.append(node_result)\n",
        "        return results\n",
        "\n",
        "    def predict_scores(self):\n",
        "        \"\"\"\n",
        "        Compute per-node contributions and total outer score.\n",
        "        \"\"\"\n",
        "        _, total_score, node_contributions = self.gdfcm.run_outer()\n",
        "        return {'node_contributions': node_contributions, 'total_score': total_score}\n",
        "# Suppose optimizer is your trained GDFCM\n",
        "predictor = GDFCMPredictorForward(optimizer)\n",
        "\n",
        "# New data: same number of nodes, each with same num_features\n",
        "new_DATA_MATRIX = np.random.rand(D_graph, DATA_MATRIX.shape[1])\n",
        "\n",
        "# Predict all nodes\n",
        "predictions = predictor.predict_all_nodes(new_DATA_MATRIX)\n",
        "\n",
        "for idx, node_pred in enumerate(predictions):\n",
        "    print(f\"Node {idx} Metrics:\", node_pred['metrics'])\n",
        "    print(f\"Node {idx} Activations:\", node_pred['activations'])\n",
        "    print(f\"Node {idx} MI Score:\", node_pred['mi_score'])\n",
        "\n",
        "# Optional: compute total contributions\n",
        "score_info = predictor.predict_scores()\n",
        "print(\"Node Contributions:\", score_info['node_contributions'])\n",
        "print(\"Total Score:\", score_info['total_score'])\n",
        "\n",
        "class GDFCMPredictorAdaptive:\n",
        "    \"\"\"\n",
        "    Adaptive forward predictor for GDFCM.\n",
        "    - Accepts new data per node.\n",
        "    - Updates activations slightly using a mini inner-loop.\n",
        "    - Computes metrics, inter-layer MI, and outer score contributions.\n",
        "    \"\"\"\n",
        "    def __init__(self, trained_gdfcm: GDFCM, lr_x=0.01, lr_steps=10):\n",
        "        self.gdfcm = trained_gdfcm\n",
        "        self.D_graph = trained_gdfcm.D_graph\n",
        "        self.nested_reps = [rep.copy() for rep in trained_gdfcm.nested_reps]\n",
        "        self.chosen_Gmat = trained_gdfcm.chosen_Gmat\n",
        "        self.inter_layer = trained_gdfcm.inter_layer\n",
        "        self.lr_x = lr_x\n",
        "        self.lr_steps = lr_steps\n",
        "\n",
        "    def adapt_node(self, node_idx, data_row):\n",
        "        \"\"\"\n",
        "        Mini inner-loop: slightly update activations based on new data row.\n",
        "        Handles shape mismatch by projecting new data to node activation dim.\n",
        "        \"\"\"\n",
        "        x = self.nested_reps[node_idx].copy()\n",
        "        dim = len(x)\n",
        "\n",
        "        # Simple linear projection of new data to activation space\n",
        "        if len(data_row) != dim:\n",
        "            # Use mean pooling to reduce or slice if too large\n",
        "            if len(data_row) > dim:\n",
        "                target = data_row[:dim]\n",
        "            else:\n",
        "                # Pad with 0.5\n",
        "                target = np.pad(data_row, (0, dim - len(data_row)), 'constant', constant_values=0.5)\n",
        "        else:\n",
        "            target = data_row\n",
        "\n",
        "        target = np.clip(target, 0, 1)\n",
        "\n",
        "        # Mini inner-loop update\n",
        "        for _ in range(self.lr_steps):\n",
        "            grad = x - target\n",
        "            x -= self.lr_x * grad\n",
        "            x = np.clip(x, 0, 1)\n",
        "\n",
        "        self.nested_reps[node_idx] = x\n",
        "\n",
        "        # Compute metrics\n",
        "        metrics_evaluator = MetricsEvaluator(np.array([data_row]))\n",
        "        metrics = metrics_evaluator.compute_node_metrics(0, y=x)\n",
        "\n",
        "        # Inter-layer MI\n",
        "        mi_score = self.inter_layer.mi_for_graph(self.chosen_Gmat, self.nested_reps)\n",
        "\n",
        "        return {'activations': x, 'metrics': metrics, 'mi_score': mi_score}\n",
        "\n",
        "\n",
        "    def adapt_all_nodes(self, new_data_matrix):\n",
        "        \"\"\"\n",
        "        Update activations for all nodes given new data matrix.\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        for node_idx in range(self.D_graph):\n",
        "            data_row = new_data_matrix[node_idx % len(new_data_matrix)]\n",
        "            node_result = self.adapt_node(node_idx, data_row)\n",
        "            results.append(node_result)\n",
        "        return results\n",
        "\n",
        "    def compute_outer_score(self):\n",
        "        \"\"\"\n",
        "        Compute node contributions and total outer score using current nested_reps.\n",
        "        \"\"\"\n",
        "        _, total_score, node_contributions = self.gdfcm.run_outer()\n",
        "        return {'node_contributions': node_contributions, 'total_score': total_score}\n",
        "# Initialize adaptive predictor\n",
        "adaptive_predictor = GDFCMPredictorAdaptive(optimizer, lr_x=0.02, lr_steps=15)\n",
        "\n",
        "# New data\n",
        "new_DATA_MATRIX = np.random.rand(D_graph, DATA_MATRIX.shape[1])\n",
        "\n",
        "# Adapt activations to new data\n",
        "predictions = adaptive_predictor.adapt_all_nodes(new_DATA_MATRIX)\n",
        "\n",
        "for idx, node_pred in enumerate(predictions):\n",
        "    print(f\"Node {idx} Metrics:\", node_pred['metrics'])\n",
        "    print(f\"Node {idx} Activations:\", node_pred['activations'])\n",
        "    print(f\"Node {idx} MI Score:\", node_pred['mi_score'])\n",
        "\n",
        "# Compute outer node contributions after adaptation\n",
        "score_info = adaptive_predictor.compute_outer_score()\n",
        "print(\"Node Contributions:\", score_info['node_contributions'])\n",
        "print(\"Total Score:\", score_info['total_score'])\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "pred_metrics = []\n",
        "true_metrics = []\n",
        "for idx, data_row in enumerate(new_DATA_MATRIX):\n",
        "    pred = adaptive_predictor.adapt_node(idx, data_row)\n",
        "    pred_metrics.append([pred['metrics'][k] for k in ['wait','throughput','util','patience']])\n",
        "    true_metrics.append([MetricsEvaluator(new_DATA_MATRIX).compute_node_metrics(idx)[k]\n",
        "                         for k in ['wait','throughput','util','patience']])\n",
        "\n",
        "pred_metrics = np.array(pred_metrics)\n",
        "true_metrics = np.array(true_metrics)\n",
        "\n",
        "\n",
        "def evaluate_predictions(true_metrics, pred_metrics, metric_names=None):\n",
        "    \"\"\"\n",
        "    Evaluate multi-metric predictions per node and overall.\n",
        "\n",
        "    Args:\n",
        "        true_metrics: np.array, shape (num_nodes, num_metrics)\n",
        "        pred_metrics: np.array, same shape as true_metrics\n",
        "        metric_names: list of metric names (optional)\n",
        "\n",
        "    Returns:\n",
        "        dict with evaluation metrics\n",
        "    \"\"\"\n",
        "    true_metrics = np.array(true_metrics)\n",
        "    pred_metrics = np.array(pred_metrics)\n",
        "\n",
        "    if metric_names is None:\n",
        "        metric_names = [f'Metric {i}' for i in range(true_metrics.shape[1])]\n",
        "\n",
        "    num_nodes, num_metrics = true_metrics.shape\n",
        "\n",
        "    results = {'per_metric': {}, 'overall': {}}\n",
        "\n",
        "    # Per metric\n",
        "    for i, name in enumerate(metric_names):\n",
        "        t = true_metrics[:, i]\n",
        "        p = pred_metrics[:, i]\n",
        "        mae = mean_absolute_error(t, p)\n",
        "        rmse = np.sqrt(mean_squared_error(t, p))\n",
        "        r2 = r2_score(t, p)\n",
        "        corr = np.corrcoef(t, p)[0,1]\n",
        "        mape = np.mean(np.abs((t - p) / (t + 1e-12))) * 100\n",
        "        results['per_metric'][name] = {\n",
        "            'MAE': mae,\n",
        "            'RMSE': rmse,\n",
        "            'R2': r2,\n",
        "            'Pearson': corr,\n",
        "            'MAPE (%)': mape\n",
        "        }\n",
        "\n",
        "    # Overall\n",
        "    mae_overall = mean_absolute_error(true_metrics.flatten(), pred_metrics.flatten())\n",
        "    rmse_overall = np.sqrt(mean_squared_error(true_metrics.flatten(), pred_metrics.flatten()))\n",
        "    r2_overall = r2_score(true_metrics.flatten(), pred_metrics.flatten())\n",
        "\n",
        "    # Cosine similarity (averaged over nodes)\n",
        "    cos_sim = np.mean([cosine_similarity(t.reshape(1,-1), p.reshape(1,-1))[0,0]\n",
        "                       for t,p in zip(true_metrics, pred_metrics)])\n",
        "\n",
        "    results['overall'] = {\n",
        "        'MAE': mae_overall,\n",
        "        'RMSE': rmse_overall,\n",
        "        'R2': r2_overall,\n",
        "        'CosineSim': cos_sim\n",
        "    }\n",
        "\n",
        "    return results\n",
        "# Suppose true_metrics and pred_metrics have shape (D_graph, 4)\n",
        "metrics_eval = evaluate_predictions(true_metrics, pred_metrics, metric_names=['wait','throughput','util','patience'])\n",
        "\n",
        "# Print per-metric evaluation\n",
        "for metric, vals in metrics_eval['per_metric'].items():\n",
        "    print(f\"{metric}: {vals}\")\n",
        "\n",
        "# Print overall evaluation\n",
        "print(\"\\nOverall metrics:\", metrics_eval['overall'])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVSwaE1CMeI7"
      },
      "source": [
        "# ACO Multiplex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8aRYYYFMdeH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class MetricsEvaluator:\n",
        "    def __init__(self, data_matrix):\n",
        "        self.data_matrix = data_matrix\n",
        "        self.num_features = data_matrix.shape[1]\n",
        "        self.num_nodes = data_matrix.shape[0]\n",
        "\n",
        "        # Baseline parameters\n",
        "        self.W0, self.T0, self.U0 = 15.0, 100.0, 10.0\n",
        "        self.P0 = 10.0\n",
        "\n",
        "        # ACO-like parameters\n",
        "        self.aco_num_ants = 40\n",
        "        self.aco_iterations = 80\n",
        "        self.evaporation = 0.1\n",
        "        self.pheromone_init = 0.5\n",
        "\n",
        "    def compute_node_metrics(self, node_idx, y=None, node_size=4):\n",
        "        cols_per_node = self.num_features // self.num_nodes\n",
        "        start_col = node_idx * cols_per_node\n",
        "        node_cols = slice(start_col, start_col + node_size)\n",
        "        node_data = self.data_matrix[:, node_cols]\n",
        "\n",
        "        signals_full = node_data.mean(axis=0)\n",
        "        signals = np.zeros(4)\n",
        "        for i in range(min(4, len(signals_full))):\n",
        "            signals[i] = signals_full[i]\n",
        "\n",
        "        if y is None:\n",
        "            y = np.zeros(3)\n",
        "        else:\n",
        "            y = np.array(y)\n",
        "            if len(y) < 3:\n",
        "                y = np.pad(y, (0, 3 - len(y)), constant_values=0.5)\n",
        "            else:\n",
        "                y = y[:3]\n",
        "\n",
        "        travel_time = node_data[0, 0] if node_data.shape[1] > 0 else 0.5\n",
        "        load_util = node_data[1, 0] if node_data.shape[0] > 1 else 0.5\n",
        "        priority = node_data[2, 0] if node_data.shape[0] > 2 else 0.5\n",
        "        delay_prob = node_data[3, 0] if node_data.shape[0] > 3 else 0.1\n",
        "        congestion = node_data[4, 0] if node_data.shape[0] > 4 else 0.2\n",
        "        energy_level = node_data[5, 0] if node_data.shape[0] > 5 else 0.5\n",
        "\n",
        "        # --- ACO-like metric function ---\n",
        "        def aco_metric(base):\n",
        "            # Initialize pheromone values\n",
        "            pheromones = np.full(self.aco_num_ants, self.pheromone_init)\n",
        "            best_value = base\n",
        "\n",
        "            for _ in range(self.aco_iterations):\n",
        "                # Each ant chooses a candidate solution influenced by pheromones + randomness\n",
        "                candidates = base + pheromones + 0.1 * np.random.randn(self.aco_num_ants)\n",
        "                # Synthetic fitness (higher is better)\n",
        "                fitness = np.exp(-candidates) + 0.1 * np.random.randn(self.aco_num_ants)\n",
        "                # Update pheromones: evaporate and reinforce\n",
        "                pheromones = (1 - self.evaporation) * pheromones + self.evaporation * fitness\n",
        "                # Keep track of best candidate\n",
        "                best_value = candidates[np.argmax(fitness)]\n",
        "            return best_value\n",
        "\n",
        "        wait = 1 / (1 + aco_metric(travel_time * (1 + congestion*0.5)))\n",
        "        throughput = aco_metric(load_util * np.sqrt(energy_level + 0.1))\n",
        "        util = aco_metric(priority * (1 - delay_prob) + 0.1 * energy_level)\n",
        "        patience = 1 / (1 + aco_metric(congestion * (1 + delay_prob)))\n",
        "\n",
        "        wait += np.random.normal(0, 0.01)\n",
        "        throughput += np.random.normal(0, 0.01)\n",
        "        util += np.random.normal(0, 0.01)\n",
        "        patience += np.random.normal(0, 0.01)\n",
        "\n",
        "        score = wait + throughput + util + patience\n",
        "\n",
        "        return {\n",
        "            'wait': wait,\n",
        "            'throughput': throughput,\n",
        "            'util': util,\n",
        "            'patience': patience,\n",
        "            'score': score\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAJ_9zzcgtYh"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "# ---------------- CONFIG ----------------nsion\n",
        "\n",
        "inner_archive_size = 80\n",
        "inner_offspring = 40\n",
        "outer_archive_size = 40\n",
        "outer_offspring = 40\n",
        "inner_iters_per_outer = 50\n",
        "outer_generations = 10\n",
        "outer_cost_limit = 10000\n",
        "inner_learning = 0.1\n",
        "gamma_interlayer = 1\n",
        "top_k = 263\n",
        "np.random.seed()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class InterLayer:\n",
        "    def __init__(self, D_graph, max_inner_dim, inter_dim=None, edge_threshold=0.02, gamma=1.0, seed=42):\n",
        "        np.random.seed(seed)\n",
        "        self.D_graph = D_graph\n",
        "        self.max_input = 2 * max_inner_dim\n",
        "        self.edge_threshold = edge_threshold\n",
        "        self.gamma = gamma\n",
        "        self.inter_dim = inter_dim if inter_dim is not None else max_inner_dim\n",
        "\n",
        "        # Initialize weights proportional to synthetic correlation between nodes\n",
        "        self.weights = {}\n",
        "        self.bias = {}\n",
        "        for i in range(D_graph):\n",
        "            for j in range(D_graph):\n",
        "                if i != j:\n",
        "                    # small random + slight bias towards correlation\n",
        "                    w_init = np.random.uniform(-0.1, 0.1, (self.inter_dim, self.max_input))\n",
        "                    self.weights[(i,j)] = w_init\n",
        "                    self.bias[(i,j)] = np.zeros(self.inter_dim)\n",
        "\n",
        "    def compute_edge_activation(self, i, j, nested_reps):\n",
        "        concat = np.concatenate([nested_reps[i], nested_reps[j]])\n",
        "        concat = np.pad(concat, (0, max(0, self.max_input - len(concat))))[:self.max_input]\n",
        "\n",
        "        # Normalize input to improve correlation\n",
        "        concat = (concat - np.mean(concat)) / (np.std(concat) + 1e-12)\n",
        "\n",
        "        # Compute activation\n",
        "        v = self.weights[(i,j)].dot(concat) + self.bias[(i,j)]\n",
        "\n",
        "        # Scale by correlation strength with input signals\n",
        "        input_strength = np.clip(np.mean(np.abs(concat)), 0, 1)\n",
        "        v = v * input_strength\n",
        "\n",
        "        return 1 / (1 + np.exp(-v))\n",
        "\n",
        "    def build_activations(self, Gmat, nested_reps):\n",
        "        acts = {}\n",
        "        for i in range(self.D_graph):\n",
        "            for j in range(self.D_graph):\n",
        "                if i == j:\n",
        "                    continue\n",
        "                if abs(Gmat[i,j]) > self.edge_threshold:\n",
        "                    acts[(i,j)] = self.compute_edge_activation(i, j, nested_reps)\n",
        "        return acts\n",
        "\n",
        "    @staticmethod\n",
        "    def pairwise_squared_corr(acts):\n",
        "        if len(acts) < 2:\n",
        "            return 0.0\n",
        "        A = np.stack(list(acts.values()))\n",
        "        A_centered = A - A.mean(axis=1, keepdims=True)\n",
        "        stds = np.sqrt(np.sum(A_centered**2, axis=1) / (A.shape[1]-1) + 1e-12)\n",
        "        cov = A_centered @ A_centered.T / (A.shape[1]-1)\n",
        "        corr = cov / (np.outer(stds, stds) + 1e-12)\n",
        "        np.fill_diagonal(corr, 0)\n",
        "        return float((corr**2).sum())\n",
        "\n",
        "    def mi_for_graph(self, Gmat, nested_reps):\n",
        "        acts = self.build_activations(Gmat, nested_reps)\n",
        "        if not acts:\n",
        "            return 0.0\n",
        "        return self.gamma * self.pairwise_squared_corr(acts)\n",
        "\n",
        "    def correlate_shrink_interlayer(self, fmt_bounds=None, interaction_tensor=None, metrics_keys=None, verbose=True):\n",
        "        \"\"\"\n",
        "        Compute Pearson correlation per node & metric between:\n",
        "            - shrink factor (adaptive FMT)\n",
        "            - mean outgoing inter-layer activations\n",
        "        Returns: {node_idx: {metric: {'r':..., 'p':...}}}\n",
        "        \"\"\"\n",
        "        from scipy.stats import pearsonr\n",
        "\n",
        "        if metrics_keys is None:\n",
        "            metrics_keys = ['wait','throughput','util','patience']\n",
        "\n",
        "        D = self.D_graph\n",
        "\n",
        "        # 1. Compute FMT bounds if not given\n",
        "        if fmt_bounds is None:\n",
        "            fmt_bounds = self.compute_fmt_with_bounds_adaptive(top_k=top_k)\n",
        "\n",
        "        # 2. Get inter-layer activations if not provided\n",
        "        if interaction_tensor is None:\n",
        "            interaction_tensor = self.print_interactions(return_tensor=True, verbose=False)\n",
        "\n",
        "        inter_mean = interaction_tensor.mean(axis=2)  # (D,D)\n",
        "        shrink_factors = self.compute_fmt_shrink_factor(fmt_bounds, metrics_keys)  # (D, num_metrics)\n",
        "\n",
        "        correlations = {}\n",
        "\n",
        "        for i in range(D):\n",
        "            correlations[i] = {}\n",
        "            for k, key in enumerate(metrics_keys):\n",
        "                # FMT shrink for node i (broadcasted across outgoing edges)\n",
        "                shrink_vec = shrink_factors[i, k] * np.ones(D)\n",
        "                # Outgoing inter-layer activations from node i\n",
        "                inter_vec = inter_mean[i, :]\n",
        "                # Remove self-loop\n",
        "                mask = np.arange(D) != i\n",
        "                shrink_vec = shrink_vec[mask]\n",
        "                inter_vec = inter_vec[mask]\n",
        "\n",
        "                # Compute Pearson correlation\n",
        "                if np.std(inter_vec) > 1e-8:  # valid correlation\n",
        "                    r, p = pearsonr(shrink_vec, inter_vec)\n",
        "                else:\n",
        "                    r, p = 0.0, 1.0  # no variability\n",
        "\n",
        "                correlations[i][key] = {'r': r, 'p': p}\n",
        "                if verbose:\n",
        "                    print(f\"Node {i} | {key} shrink vs inter-layer: r={r:.3f}, p={p:.3e}\")\n",
        "\n",
        "        return correlations\n",
        "\n",
        "\n",
        "\n",
        "# ---------------- UNIFIED ACOR MULTIPLEX ----------------\n",
        "class GDFCM:\n",
        "    def __init__(self, candidate_dims, D_graph, inner_archive_size, inner_offspring,\n",
        "                 outer_archive_size, outer_offspring, synthetic_targets, inner_learning,\n",
        "                 gamma_interlayer=1.0, causal_flag=True):\n",
        "        self.candidate_dims = candidate_dims\n",
        "        self.D_graph = D_graph\n",
        "        self.inner_archive_size = inner_archive_size\n",
        "        self.inner_offspring = inner_offspring\n",
        "        self.outer_archive_size = outer_archive_size\n",
        "        self.outer_offspring = outer_offspring\n",
        "        self.synthetic_targets = synthetic_targets\n",
        "        self.inner_learning = inner_learning\n",
        "        self.causal_flag = causal_flag\n",
        "        self.best_dim_per_node = [len(t)-1 for t in synthetic_targets]  # last element as best dim\n",
        "\n",
        "        self.nested_reps = [np.zeros(max(candidate_dims)) for _ in range(D_graph)]\n",
        "      #  self.best_dim_per_node = [candidate_dims[0] for _ in range(D_graph)]\n",
        "        self.inter_layer = InterLayer(D_graph, max_inner_dim=max(candidate_dims), gamma=gamma_interlayer)\n",
        "        self.chosen_Gmat = np.random.uniform(-0.5,0.5,(D_graph,D_graph))\n",
        "        np.fill_diagonal(self.chosen_Gmat,0)\n",
        "        self.l2_before, self.l2_after = [], []\n",
        "\n",
        "    # ---------- INNER LOOP (FCM) ----------\n",
        "    def run_inner(self, node_idx, target, D_fcm,\n",
        "              steps=100, lr_x=0.001, lr_y=0.001, lr_W=0.001,\n",
        "              decorrelate_metrics=True):\n",
        "\n",
        "      # --- Initialize activations ---\n",
        "        # Inside run_inner\n",
        "        x = target.copy()\n",
        "        y = target.copy()\n",
        "\n",
        "        # Pad target for L2 computation\n",
        "        target_padded = np.pad(target, (0, len(self.nested_reps[node_idx]) - len(target)),\n",
        "                            mode='constant', constant_values=0.5)\n",
        "        self.l2_before.append(np.linalg.norm(self.nested_reps[node_idx] - target_padded))\n",
        "\n",
        "        # FCM updates\n",
        "        W = np.random.uniform(-0.6, 0.6, (D_fcm, D_fcm))\n",
        "        np.fill_diagonal(W, 0)\n",
        "\n",
        "        for _ in range(steps):\n",
        "            z = W.dot(x)\n",
        "            Theta_grad_z = 2*z - target\n",
        "            Theta_grad_x = Theta_grad_z @ W + (y+1)\n",
        "            Theta_grad_y = x + 1\n",
        "            Theta_grad_W = np.outer(Theta_grad_z, x)\n",
        "\n",
        "            x -= lr_x * np.clip(Theta_grad_x, -0.05, 0.05)\n",
        "            y -= lr_y * np.clip(Theta_grad_y, -0.05, 0.05)\n",
        "            x = np.clip(x, 0, 1)\n",
        "            y = np.clip(y, 0, 1)\n",
        "            W -= lr_W * np.clip(Theta_grad_W, -0.01, 0.01)\n",
        "            np.fill_diagonal(W, 0)\n",
        "            W = np.clip(W, -1, 1)\n",
        "\n",
        "        # Pad FCM output to max dim for nested_reps\n",
        "        x_padded = np.pad(x, (0, len(self.nested_reps[node_idx]) - len(x)),\n",
        "                        mode='constant', constant_values=0.5)\n",
        "        self.nested_reps[node_idx] = x_padded\n",
        "        self.l2_after.append(np.linalg.norm(x_padded - target_padded))\n",
        "\n",
        "\n",
        "      # --- Metric decoupling ---\n",
        "        metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "\n",
        "        if decorrelate_metrics:\n",
        "            # Neutralized input\n",
        "            neutral_y = np.full_like(x, 0.5)\n",
        "            metrics = metrics_evaluator.compute_node_metrics(node_idx, y=neutral_y)\n",
        "\n",
        "            # Optional: orthogonalize against inter-layer mean\n",
        "                    # Optional: orthogonalize against inter-layer mean\n",
        "        inter_tensor = self.inter_layer.build_activations(self.chosen_Gmat, self.nested_reps)\n",
        "        if inter_tensor:\n",
        "                D = self.D_graph\n",
        "                inter_mat = np.zeros((D, D))\n",
        "                for (i,j), vec in inter_tensor.items():\n",
        "                    inter_mat[i,j] = vec.mean()  # mean scalar\n",
        "\n",
        "                node_vec = np.array([metrics[k] for k in ['wait','throughput','util','patience']])\n",
        "                for i in range(D):\n",
        "                    f_scalar = inter_mat[i,:].mean()  # mean over row\n",
        "                    proj = f_scalar * (node_vec.mean() / (1e-12 + 1))  # scale by node_vec mean\n",
        "                    node_vec -= proj\n",
        "\n",
        "\n",
        "            # --- Fully decorrelated score ---\n",
        "                metrics['score'] = metrics['wait'] + metrics['throughput'] + metrics['util'] + metrics['patience']\n",
        "\n",
        "        else:\n",
        "            # Compute normally if no decorrelation\n",
        "            metrics = metrics_evaluator.compute_node_metrics(node_idx, y=x)\n",
        "\n",
        "        # --- Compute MI score for inter-layer ---\n",
        "        mi_score = self.inter_layer.mi_for_graph(self.chosen_Gmat, self.nested_reps)\n",
        "\n",
        "        return x, y, W, mi_score, metrics\n",
        "\n",
        "    # ---------- OUTER LOOP ----------\n",
        "    def run_outer(self, outer_cost_limit=1000):\n",
        "      metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "      node_metrics_list = []\n",
        "      raw_scores = []\n",
        "\n",
        "      # --- Compute node metrics per node ---\n",
        "      for i, y in enumerate(self.nested_reps):\n",
        "          metrics = metrics_evaluator.compute_node_metrics(i, y=y)\n",
        "          node_metrics_list.append(metrics)\n",
        "          raw_scores.append(metrics['score'])\n",
        "\n",
        "      raw_scores = np.array(raw_scores)\n",
        "      total_raw = raw_scores.sum()\n",
        "\n",
        "      # --- Apply cap to raw metrics ---\n",
        "      capped_total_raw = total_raw\n",
        "      if total_raw > outer_cost_limit:\n",
        "          scale_factor = outer_cost_limit / total_raw\n",
        "          for metrics in node_metrics_list:\n",
        "              for key in ['wait', 'throughput', 'util', 'patience', 'score']:\n",
        "                  metrics[key] *= scale_factor\n",
        "          raw_scores *= scale_factor\n",
        "          capped_total_raw = outer_cost_limit\n",
        "\n",
        "      # --- Compute Fuzzy Metric Tensor contribution ---\n",
        "      fuzzy_tensor = self.compute_fuzzy_metric_tensor(normalize=True)\n",
        "      D = self.D_graph\n",
        "\n",
        "      # Only consider off-diagonal entries for inter-node interactions\n",
        "      off_diag_mask = np.ones((D, D), dtype=bool)\n",
        "      np.fill_diagonal(off_diag_mask, 0)\n",
        "      fuzzy_score_offdiag = fuzzy_tensor[off_diag_mask].sum()\n",
        "\n",
        "      # --- Compute per-node contribution ---\n",
        "      node_contributions = np.zeros(D)\n",
        "      for i in range(D):\n",
        "          # Contribution from own metrics\n",
        "          own_score = raw_scores[i]\n",
        "\n",
        "          # Contribution from FMT interactions (row i -> others)\n",
        "          fmt_contrib = fuzzy_tensor[i, :, :].sum() - fuzzy_tensor[i, i, :].sum()  # exclude self\n",
        "          node_contributions[i] = own_score + self.inter_layer.gamma * fmt_contrib\n",
        "\n",
        "      # --- Strong decoupling: correlation penalty ---\n",
        "      inter_tensor = self.print_interactions(return_tensor=True, verbose=False)\n",
        "      if inter_tensor is None or inter_tensor.shape[2] == 0:\n",
        "          inter_mean = np.zeros((D, D))\n",
        "      else:\n",
        "          inter_mean = inter_tensor.mean(axis=2)  # shape (D,D)\n",
        "\n",
        "      fmt_mean = fuzzy_tensor.mean(axis=2)  # shape (D,D)\n",
        "\n",
        "      corr_penalty = 0.0\n",
        "      for i in range(D):\n",
        "          fmt_vec = fmt_mean[i, :]\n",
        "          inter_vec = inter_mean[i, :]\n",
        "          if np.std(fmt_vec) > 1e-8 and np.std(inter_vec) > 1e-8:\n",
        "              corr = np.corrcoef(fmt_vec, inter_vec)[0, 1]\n",
        "              corr_penalty += abs(corr) ** 2\n",
        "\n",
        "      corr_penalty /= D\n",
        "      combined_score = node_contributions.sum() - corr_penalty * 500  # strong decorrelation\n",
        "\n",
        "      # --- Store for plotting / further analysis ---\n",
        "      self.capped_node_metrics = node_metrics_list\n",
        "      self.node_score_contributions = node_contributions\n",
        "      self.correlation_penalty = corr_penalty\n",
        "\n",
        "      return node_metrics_list, combined_score, node_contributions\n",
        "\n",
        "\n",
        "\n",
        "    def run(self, outer_generations=outer_generations):\n",
        "        final_metrics = None\n",
        "\n",
        "        for gen in range(outer_generations):\n",
        "            mi_scores = []\n",
        "            node_metrics_list = []\n",
        "\n",
        "            for node_idx in range(self.D_graph):\n",
        "                # Full target vector for this node\n",
        "                full_target = self.synthetic_targets[node_idx]['target']\n",
        "\n",
        "                # Use the candidate dimension assigned to this node\n",
        "                D_fcm = self.candidate_dims[node_idx]\n",
        "                target = full_target[:D_fcm]  # slice according to candidate_dims\n",
        "\n",
        "                # Run inner FCM\n",
        "                _, _, _, mi_score, metrics = self.run_inner(node_idx, target, D_fcm)\n",
        "                mi_scores.append(mi_score)\n",
        "                node_metrics_list.append(metrics)\n",
        "\n",
        "\n",
        "            # --- Outer loop uses decorrelated metrics ---\n",
        "            self.capped_node_metrics = node_metrics_list\n",
        "            _, capped_score, node_contributions = self.run_outer()  # uses self.capped_node_metrics\n",
        "\n",
        "            print(f\"\\n--- Generation {gen} Metrics ---\")\n",
        "            for i, m in enumerate(node_metrics_list):\n",
        "                print(f\"Node {i} | \" + \" | \".join([f\"{k}: {v:.2f}\" for k,v in m.items()]))\n",
        "\n",
        "            print(f\"\\n--- Generation {gen} Node Contributions ---\")\n",
        "            for i, c in enumerate(node_contributions):\n",
        "                print(f\"Node {i}: Contribution = {c:.4f}\")\n",
        "\n",
        "            print(f\"Outer Score (capped): {capped_score:.3f}\")\n",
        "\n",
        "            final_metrics = node_metrics_list\n",
        "\n",
        "        return final_metrics\n",
        "\n",
        "\n",
        "    # ---------- VISUALIZATIONS ----------\n",
        "    # ---------- VISUALIZATIONS ----------\n",
        "    def plot_pointwise_minmax_elite(self, top_k=21):\n",
        "        plt.figure(figsize=(14,3))\n",
        "        for i in range(self.D_graph):\n",
        "            # Node's actual dimension\n",
        "            dim_i = self.candidate_dims[i]\n",
        "            base = self.nested_reps[i][:dim_i]  # slice to candidate dim\n",
        "            reps = np.clip(base + np.random.normal(0,0.05,(top_k,len(base))),0,1)\n",
        "            y_min, y_max = reps.min(axis=0), reps.max(axis=0)\n",
        "            y_sel = base\n",
        "\n",
        "            # True target for this node, sliced to candidate dim\n",
        "            y_true = self.synthetic_targets[i]['target'][:len(y_sel)]\n",
        "            if len(y_true) < len(y_sel):\n",
        "                y_true = np.pad(y_true, (0, len(y_sel)-len(y_true)), \"constant\")\n",
        "            else:\n",
        "                y_true = y_true[:len(y_sel)]\n",
        "\n",
        "            plt.subplot(1,self.D_graph,i+1)\n",
        "            plt.fill_between(range(len(y_min)),y_min,y_max,color='skyblue',alpha=0.4,label='Elite Interval')\n",
        "            plt.plot(y_sel,'k-',lw=2,label='Estimated')\n",
        "            plt.plot(y_true,'r--',lw=2,label='True')\n",
        "            plt.ylim(0,1.05)\n",
        "            plt.title(f\"Node {i+1}\")\n",
        "            if i==0: plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def plot_nested_activations(self):\n",
        "        plt.figure(figsize=(12,3))\n",
        "        for i,rep in enumerate(self.nested_reps):\n",
        "            dim_i = self.candidate_dims[i]\n",
        "            rep_i = rep[:dim_i]  # slice to candidate dim\n",
        "            plt.subplot(1,self.D_graph,i+1)\n",
        "            plt.bar(range(len(rep_i)), rep_i, color=plt.cm.plasma(rep_i))\n",
        "            plt.ylim(0,1)\n",
        "            plt.title(f\"Node {i+1}\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def plot_outer_fuzzy_graph(self):\n",
        "        G = nx.DiGraph()\n",
        "        for i in range(self.D_graph): G.add_node(i)\n",
        "        for i in range(self.D_graph):\n",
        "            for j in range(self.D_graph):\n",
        "                if i!=j and abs(self.chosen_Gmat[i,j])>0.02:\n",
        "                    G.add_edge(i,j,weight=self.chosen_Gmat[i,j])\n",
        "        node_sizes = [self.best_dim_per_node[i]*200 for i in range(self.D_graph)]\n",
        "        edge_colors = ['green' if d['weight']>0 else 'red' for _,_,d in G.edges(data=True)]\n",
        "        edge_widths = [abs(d['weight'])*3 for _,_,d in G.edges(data=True)]\n",
        "        pos = nx.circular_layout(G)\n",
        "        plt.figure(figsize=(6,6))\n",
        "        nx.draw(G,pos,node_size=node_sizes,node_color='skyblue',\n",
        "                edge_color=edge_colors,width=edge_widths,arrows=True,with_labels=True)\n",
        "        plt.title(\"Outer Fuzzy Multiplex Graph\")\n",
        "        plt.show()\n",
        "# ---------------- INTERACTIONS INSPECTOR ----------------\n",
        "\n",
        "    def print_interactions(self, return_tensor=True, verbose=True):\n",
        "            D_graph = self.D_graph\n",
        "            inter_dim = self.inter_layer.inter_dim\n",
        "            inter_tensor = np.zeros((D_graph, D_graph, inter_dim))\n",
        "\n",
        "            acts = self.inter_layer.build_activations(self.chosen_Gmat, self.nested_reps)\n",
        "            if not acts:\n",
        "                if verbose:\n",
        "                    print(\"No active edges above threshold.\")\n",
        "                return inter_tensor if return_tensor else None\n",
        "\n",
        "            for (i, j), vec in acts.items():\n",
        "                inter_tensor[i, j, :] = vec\n",
        "                if verbose:\n",
        "                    act_str = \", \".join([f\"{v:.3f}\" for v in vec])\n",
        "                    print(f\"Node {i} -> Node {j}: [{act_str}]\")\n",
        "            return inter_tensor if return_tensor else None\n",
        "\n",
        "        # Move these outside of print_interactions (class-level)\n",
        "    def print_l2_summary(self):\n",
        "            print(\"\\nL2 Distances to Target per Node:\")\n",
        "            for idx, (before, after) in enumerate(zip(self.l2_before, self.l2_after)):\n",
        "                print(f\"Node {idx}: Before={before:.4f}, After={after:.4f}\")\n",
        "\n",
        "    def compute_fuzzy_metric_tensor(self, normalize=True, verbose=False):\n",
        "            \"\"\"\n",
        "            Computes a Fuzzy Metric Tensor (D_graph x D_graph x num_metrics)\n",
        "            using current nested reps and node metrics.\n",
        "            Each slice [i,j,:] represents metrics of node j (optionally weighted by Gmat[i,j])\n",
        "            \"\"\"\n",
        "            metrics_keys = ['wait', 'throughput', 'util', 'patience']\n",
        "            D = self.D_graph\n",
        "            num_metrics = len(metrics_keys)\n",
        "            tensor = np.zeros((D, D, num_metrics))\n",
        "\n",
        "            metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "\n",
        "            node_metrics = []\n",
        "            for i, rep in enumerate(self.nested_reps):\n",
        "                metrics = metrics_evaluator.compute_node_metrics(i, y=rep)\n",
        "                node_metrics.append(np.array([metrics[k] for k in metrics_keys]))\n",
        "            node_metrics = np.array(node_metrics)  # (D, num_metrics)\n",
        "\n",
        "            for i in range(D):\n",
        "                for j in range(D):\n",
        "                    if i==j:\n",
        "                        tensor[i,j,:] = node_metrics[j]\n",
        "                    else:\n",
        "                        weight = np.clip(abs(self.chosen_Gmat[i,j]), 0, 1)\n",
        "                        tensor[i,j,:] = weight * node_metrics[j]\n",
        "\n",
        "            if normalize:\n",
        "                tensor = (tensor - tensor.min()) / (tensor.max() - tensor.min() + 1e-12)\n",
        "\n",
        "            if verbose:\n",
        "                print(\"Fuzzy Metric Tensor shape:\", tensor.shape)\n",
        "\n",
        "            return tensor\n",
        "\n",
        "\n",
        "\n",
        "    def compute_fmt_shrink_factor(self, fmt_bounds, metrics_keys=None):\n",
        "        \"\"\"\n",
        "        Returns shrink factor per node and metric.\n",
        "        shrink_factor = 1 - (current_interval / original_interval)\n",
        "        \"\"\"\n",
        "        if metrics_keys is None:\n",
        "            metrics_keys = ['wait','throughput','util','patience']\n",
        "\n",
        "        D = self.D_graph\n",
        "        num_metrics = len(metrics_keys)\n",
        "        shrink_factors = np.zeros((D, num_metrics))\n",
        "\n",
        "        for i in range(D):\n",
        "            for k in range(num_metrics):\n",
        "                lower, upper = fmt_bounds[i, i, k, 0], fmt_bounds[i, i, k, 1]  # self-node interval\n",
        "                interval_width = upper - lower + 1e-12  # normalized [0,1]\n",
        "                shrink_factors[i, k] = 1 - interval_width  # more shrink = higher value\n",
        "\n",
        "        return shrink_factors\n",
        "\n",
        "    def compute_fmt_with_bounds_adaptive(self, top_k=21, max_shrink=0.5, metrics_keys=None):\n",
        "        \"\"\"\n",
        "        Computes FMT bounds using pointwise min/max across elite solutions,\n",
        "        and applies dynamic adaptive shrinking where variability is low.\n",
        "        Returns tensor shape (D,D,num_metrics,2) [lower, upper].\n",
        "        \"\"\"\n",
        "        if metrics_keys is None:\n",
        "            metrics_keys = ['wait','throughput','util','patience']\n",
        "\n",
        "        D = self.D_graph\n",
        "        num_metrics = len(metrics_keys)\n",
        "        tensor_bounds = np.zeros((D, D, num_metrics, 2))\n",
        "        metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "\n",
        "        variability = np.zeros((D, num_metrics))\n",
        "\n",
        "        # Step 1: compute bounds from perturbations\n",
        "        for i in range(D):\n",
        "            base = self.nested_reps[i]\n",
        "            reps = np.clip(base + np.random.normal(0, 0.05, (top_k, len(base))), 0, 1)\n",
        "            metrics_matrix = np.zeros((top_k, num_metrics))\n",
        "            for idx, rep in enumerate(reps):\n",
        "                m = metrics_evaluator.compute_node_metrics(i, y=rep)\n",
        "                metrics_matrix[idx, :] = [m[k] for k in metrics_keys]\n",
        "\n",
        "            lower_i = metrics_matrix.min(axis=0)\n",
        "            upper_i = metrics_matrix.max(axis=0)\n",
        "            tensor_bounds[i, :, :, 0] = lower_i[np.newaxis, :]  # broadcast to all j\n",
        "            tensor_bounds[i, :, :, 1] = upper_i[np.newaxis, :]\n",
        "            variability[i, :] = metrics_matrix.std(axis=0)\n",
        "\n",
        "        # Step 2: adaptive shrinking\n",
        "        for i in range(D):\n",
        "            for j in range(D):\n",
        "                for k in range(num_metrics):\n",
        "                    lower, upper = tensor_bounds[i,j,k,0], tensor_bounds[i,j,k,1]\n",
        "                    mean = (lower + upper)/2\n",
        "                    var_norm = min(1.0, variability[i,k]/(upper-lower + 1e-12))\n",
        "                    shrink_factor = max_shrink * (1 - var_norm)\n",
        "                    tensor_bounds[i,j,k,0] = mean - shrink_factor*(mean - lower)\n",
        "                    tensor_bounds[i,j,k,1] = mean + shrink_factor*(upper - mean)\n",
        "\n",
        "        return tensor_bounds\n",
        "\n",
        "\n",
        "\n",
        "    def plot_fuzzy_metric_tensor_heatmaps(self, fuzzy_tensor=None, metrics_keys=['wait','throughput','util','patience']):\n",
        "        \"\"\"\n",
        "        Plot a heatmap panel for each metric in the FMT.\n",
        "        Rows: source node i\n",
        "        Columns: target node j\n",
        "        \"\"\"\n",
        "        if fuzzy_tensor is None:\n",
        "            fuzzy_tensor = self.compute_fuzzy_metric_tensor(normalize=True)\n",
        "\n",
        "        D = self.D_graph\n",
        "        num_metrics = len(metrics_keys)\n",
        "\n",
        "        fig, axes = plt.subplots(1, num_metrics, figsize=(4*num_metrics,4))\n",
        "        if num_metrics == 1: axes = [axes]\n",
        "\n",
        "        for k, key in enumerate(metrics_keys):\n",
        "            data = fuzzy_tensor[:,:,k]\n",
        "            im = axes[k].imshow(data, cmap='viridis', vmin=0, vmax=1)\n",
        "            for i in range(D):\n",
        "                for j in range(D):\n",
        "                    axes[k].text(j,i,f\"{data[i,j]:.2f}\",ha='center',va='center',color='white',fontsize=9)\n",
        "            axes[k].set_xticks(range(D))\n",
        "            axes[k].set_yticks(range(D))\n",
        "            axes[k].set_xticklabels([f'Node {j}' for j in range(D)])\n",
        "            axes[k].set_yticklabels([f'Node {i}' for i in range(D)])\n",
        "            axes[k].set_title(f'FMT - {key}')\n",
        "\n",
        "        fig.colorbar(im, ax=axes, orientation='vertical', fraction=0.025, pad=0.04, label='Normalized Metric Value')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def compute_fmt_with_elite_bounds(self, top_k=21):\n",
        "        \"\"\"\n",
        "        Computes FMT bounds using pointwise min/max across elite solutions.\n",
        "        Returns tensor shape (D,D,num_metrics,2) [lower, upper].\n",
        "        \"\"\"\n",
        "        metrics_keys = ['wait','throughput','util','patience']\n",
        "        D = self.D_graph\n",
        "        num_metrics = len(metrics_keys)\n",
        "        tensor_bounds = np.zeros((D,D,num_metrics,2))\n",
        "\n",
        "        metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "\n",
        "        for i in range(D):\n",
        "            # Generate top_k perturbations around current nested_rep (like in plot_pointwise_minmax_elite)\n",
        "            base = self.nested_reps[i]\n",
        "            reps = np.clip(base + np.random.normal(0,0.05,(top_k,len(base))),0,1)\n",
        "\n",
        "            # Compute node metrics for each perturbed solution\n",
        "            metrics_matrix = np.zeros((top_k, num_metrics))\n",
        "            for idx, rep in enumerate(reps):\n",
        "                m = metrics_evaluator.compute_node_metrics(i, y=rep)\n",
        "                metrics_matrix[idx,:] = [m[k] for k in metrics_keys]\n",
        "\n",
        "            # Compute pointwise min/max across elite solutions\n",
        "            lower_i = metrics_matrix.min(axis=0)\n",
        "            upper_i = metrics_matrix.max(axis=0)\n",
        "\n",
        "            # Fill bounds tensor for all source nodes (i->j)\n",
        "            for j in range(D):\n",
        "                tensor_bounds[i,j,:,0] = lower_i\n",
        "                tensor_bounds[i,j,:,1] = upper_i\n",
        "\n",
        "        return tensor_bounds\n",
        "\n",
        "\n",
        "    def plot_fmt_with_bounds(self, fmt_tensor_bounds):\n",
        "        D = self.D_graph\n",
        "        metrics_keys = ['wait','throughput','util','patience']\n",
        "        num_metrics = len(metrics_keys)\n",
        "\n",
        "        fig, axes = plt.subplots(1, num_metrics, figsize=(4*num_metrics,4))\n",
        "        if num_metrics == 1: axes = [axes]\n",
        "\n",
        "        for k, key in enumerate(metrics_keys):\n",
        "            lower = fmt_tensor_bounds[:,:,k,0]\n",
        "            upper = fmt_tensor_bounds[:,:,k,1]\n",
        "            mean_vals = (lower+upper)/2\n",
        "            range_vals = upper-lower\n",
        "            max_range = range_vals.max() if range_vals.max()>0 else 1.0\n",
        "            alphas = 0.2 + 0.8 * range_vals/max_range\n",
        "\n",
        "            im = axes[k].imshow(mean_vals, cmap='viridis', vmin=0, vmax=mean_vals.max())\n",
        "            for i in range(D):\n",
        "                for j in range(D):\n",
        "                    alpha_val = np.clip(1-alphas[i,j],0,1)\n",
        "                    rect = plt.Rectangle((j-0.5,i-0.5),1,1,color='white',alpha=alpha_val)\n",
        "                    axes[k].add_patch(rect)\n",
        "                    axes[k].text(j,i,f\"{lower[i,j]:.1f}\\n{upper[i,j]:.1f}\",ha='center',va='center',fontsize=8)\n",
        "            axes[k].set_title(f'FMT Bounds - {key}')\n",
        "            axes[k].set_xticks(range(D))\n",
        "            axes[k].set_yticks(range(D))\n",
        "            axes[k].set_xticklabels([f'Node {j}' for j in range(D)])\n",
        "            axes[k].set_yticklabels([f'Node {i}' for i in range(D)])\n",
        "\n",
        "        fig.colorbar(im, ax=axes, orientation='vertical', fraction=0.025, pad=0.04, label='Mean Metric Value')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    def plot_node_score_contribution(self, metrics_keys=['wait','throughput','util','patience']):\n",
        "        \"\"\"\n",
        "        Plot per-node total score contribution in the SAME STYLE as the FMT plots:\n",
        "            - uses imshow\n",
        "            - one panel for: raw, FMT, and total stacked\n",
        "            - diagonal masked\n",
        "            - annotated cells\n",
        "            - node contribution highlighted like your FMT code\n",
        "        \"\"\"\n",
        "        # ---------------------------------------------------------------------\n",
        "        # 1. Collect node contributions from run_outer()\n",
        "        # ---------------------------------------------------------------------\n",
        "        _, _, node_contributions = self.run_outer()\n",
        "        node_contributions = np.array(node_contributions)\n",
        "        D = len(node_contributions)\n",
        "\n",
        "        # ---------------------------------------------------------------------\n",
        "        # 2. Recompute FMT influence (same style as your FMT plots)\n",
        "        # ---------------------------------------------------------------------\n",
        "        fuzzy_tensor = self.compute_fuzzy_metric_tensor(normalize=True)\n",
        "        total_tensor = fuzzy_tensor.sum(axis=2)           # sum over metrics\n",
        "        fmt_tensor = total_tensor.copy()\n",
        "        np.fill_diagonal(fmt_tensor, 0)                   # mask diagonal\n",
        "\n",
        "        fmt_per_node = fmt_tensor.sum(axis=1)             # row sum\n",
        "        raw_per_node = node_contributions - fmt_per_node  # everything else\n",
        "\n",
        "        # Construct matrices for plotting (DD)\n",
        "        raw_matrix = np.zeros((D, D))\n",
        "        fmt_matrix = fmt_tensor\n",
        "        total_matrix = raw_matrix + fmt_matrix            # raw only on diagonal? no  distribute raw as row diag\n",
        "        np.fill_diagonal(raw_matrix, raw_per_node)\n",
        "        total_matrix = raw_matrix + fmt_matrix\n",
        "\n",
        "        # ---------------------------------------------------------------------\n",
        "        # 3. Plot - 3 subplots in SAME STYLE as FMT panels\n",
        "        # ---------------------------------------------------------------------\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "        matrices = [raw_matrix, fmt_matrix, total_matrix]\n",
        "        titles = [\"Raw Node Contribution\", \"FMT Interaction Contribution\", \"Total Contribution\"]\n",
        "\n",
        "        for ax, mat, title in zip(axes, matrices, titles):\n",
        "\n",
        "            im = ax.imshow(mat, cmap='viridis', vmin=np.min(mat), vmax=np.max(mat))\n",
        "\n",
        "            # annotate values\n",
        "            for i in range(D):\n",
        "                for j in range(D):\n",
        "                    val = mat[i, j]\n",
        "                    ax.text(j, i, f\"{val:.2f}\", ha='center',\n",
        "                            va='center', color='white', fontsize=8)\n",
        "\n",
        "            ax.set_title(title)\n",
        "            ax.set_xticks(range(D))\n",
        "            ax.set_xticklabels([f\"Node {i+1}\" for i in range(D)])\n",
        "            ax.set_yticks(range(D))\n",
        "            ax.set_yticklabels([f\"Node {i+1}\" for i in range(D)])\n",
        "\n",
        "        fig.colorbar(im, ax=axes, orientation='vertical', fraction=0.025, pad=0.04,\n",
        "                    label='Contribution Value')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def correlate_fmt_interactions_per_node(self, fmt_bounds=None, interaction_tensor=None, verbose=True):\n",
        "        \"\"\"\n",
        "        Correlate the FMT bounds with inter-layer interactions per node and per metric.\n",
        "        Returns a dict of shape: {node_idx: {metric: {'r':..., 'p':...}}}.\n",
        "        \"\"\"\n",
        "        from scipy.stats import pearsonr\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        metrics_keys = ['wait','throughput','util','patience']\n",
        "        D = self.D_graph\n",
        "\n",
        "        # Compute tensors if not provided\n",
        "        if fmt_bounds is None:\n",
        "            fmt_bounds = self.compute_fmt_with_elite_bounds(top_k=21)\n",
        "        if interaction_tensor is None:\n",
        "            interaction_tensor = self.print_interactions(return_tensor=True, verbose=False)\n",
        "\n",
        "        # Reduce interaction tensor along inter_dim\n",
        "        inter_mean = interaction_tensor.mean(axis=2)  # (D,D)\n",
        "\n",
        "        node_correlations = {}\n",
        "\n",
        "        for i in range(D):\n",
        "            node_correlations[i] = {}\n",
        "            for k, key in enumerate(metrics_keys):\n",
        "                # FMT bounds for target node j from source i (mean of lower/upper)\n",
        "                fmt_mean = fmt_bounds[i,:,k,:].mean(axis=1)  # shape (D,)\n",
        "                # Interaction tensor for edges from node i to j\n",
        "                inter_vec = inter_mean[i,:]  # shape (D,)\n",
        "                # Pearson correlation\n",
        "                corr, pval = pearsonr(fmt_mean, inter_vec)\n",
        "                node_correlations[i][key] = {'r': corr, 'p': pval}\n",
        "\n",
        "                if verbose:\n",
        "                    print(f\"Node {i} | {key}: r = {corr:.3f}, p = {pval:.3e}\")\n",
        "                    plt.figure(figsize=(4,3))\n",
        "                    plt.scatter(fmt_mean, inter_vec, alpha=0.7, edgecolor='k', color='skyblue')\n",
        "                    plt.xlabel(f\"FMT {key} (Node {i} -> others)\")\n",
        "                    plt.ylabel(f\"Interaction mean (Node {i} -> others)\")\n",
        "                    plt.title(f\"Node {i} | {key} correlation: r={corr:.3f}\")\n",
        "                    plt.grid(True)\n",
        "                    plt.show()\n",
        "\n",
        "        return node_correlations\n",
        "\n",
        "    def correlation_penalty(self, fmt_bounds=None, interaction_tensor=None):\n",
        "        \"\"\"\n",
        "        Computes a penalty term that is high if per-node FMT metrics correlate with interactions.\n",
        "        Returns total penalty to subtract from the outer score.\n",
        "        \"\"\"\n",
        "        from scipy.stats import pearsonr\n",
        "\n",
        "        D = self.D_graph\n",
        "        metrics_keys = ['wait','throughput','util','patience']\n",
        "\n",
        "        if fmt_bounds is None:\n",
        "            fmt_bounds = self.compute_fmt_with_elite_bounds(top_k=top_k)\n",
        "        if interaction_tensor is None:\n",
        "            interaction_tensor = self.print_interactions(return_tensor=True, verbose=False)\n",
        "\n",
        "        inter_mean = interaction_tensor.mean(axis=2)\n",
        "        total_penalty = 0.0\n",
        "\n",
        "        for i in range(D):\n",
        "            for k in range(len(metrics_keys)):\n",
        "                fmt_mean = fmt_bounds[i,:,k,:].mean(axis=1)\n",
        "                inter_vec = inter_mean[i,:]\n",
        "                if np.std(fmt_mean) > 1e-8 and np.std(inter_vec) > 1e-8:\n",
        "                    corr, _ = pearsonr(fmt_mean, inter_vec)\n",
        "                    total_penalty += abs(corr)  # penalize high correlation\n",
        "\n",
        "        # normalize by number of nodes  metrics\n",
        "        total_penalty /= (D * len(metrics_keys))**2\n",
        "        return total_penalty\n",
        "\n",
        "\n",
        "# ---------------- USAGE ----------------\n",
        "if __name__ == \"__main__\":\n",
        "    optimizer = GDFCM(\n",
        "        candidate_dims, D_graph,\n",
        "        inner_archive_size, inner_offspring,\n",
        "        outer_archive_size, outer_offspring,\n",
        "        synthetic_targets,\n",
        "        inner_learning, gamma_interlayer=1,\n",
        "        causal_flag=False\n",
        "    )\n",
        "    metrics_list = optimizer.run()\n",
        "    optimizer.plot_pointwise_minmax_elite()\n",
        "    optimizer.plot_nested_activations()\n",
        "    # Compute FMT with elite bounds\n",
        "    fmt_elite_bounds = optimizer.compute_fmt_with_elite_bounds(top_k=top_k+10)\n",
        "\n",
        "# Plot as heatmaps\n",
        "    optimizer.plot_fmt_with_bounds(fmt_elite_bounds)\n",
        "\n",
        "    # Compute fuzzy multiplex tensor\n",
        "    fmt_tensor = optimizer.compute_fuzzy_metric_tensor(normalize=True)\n",
        "    optimizer.plot_fuzzy_metric_tensor_heatmaps(fmt_tensor)\n",
        "\n",
        "    # Compute FMT with bounds (minimax elite intervals)\n",
        "    optimizer.plot_node_score_contribution()\n",
        "    optimizer.plot_outer_fuzzy_graph()\n",
        "  #  optimizer.print_interactions()\n",
        "    tensor = optimizer.print_interactions()\n",
        "\n",
        "    print(\"Tensor shape:\", tensor.shape,'\\n',tensor)\n",
        "    # Compute tensors first\n",
        "    fmt_elite_bounds = optimizer.compute_fmt_with_elite_bounds(top_k=top_k)\n",
        "    interaction_tensor = optimizer.print_interactions(return_tensor=True, verbose=False)\n",
        "\n",
        "\n",
        "    interaction_tensor = optimizer.print_interactions(return_tensor=True, verbose=False)\n",
        "\n",
        "    # Get per-node, per-metric correlations\n",
        "    #node_metric_corrs = optimizer.correlate_fmt_interactions_per_node(\n",
        "     #   fmt_bounds=fmt_elite_bounds,\n",
        "      #  interaction_tensor=interaction_tensor\n",
        "   # )\n",
        "    import networkx as nx\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    from mpl_toolkits.mplot3d import Axes3D\n",
        "D_graph = len(optimizer.nested_reps)\n",
        "tensor = optimizer.print_interactions(return_tensor=True, verbose=False)\n",
        "\n",
        "# ---------------- Outer nodes (hubs) ----------------\n",
        "G_outer = nx.DiGraph()\n",
        "for i in range(D_graph):\n",
        "    G_outer.add_node(i)\n",
        "for i in range(D_graph):\n",
        "    for j in range(D_graph):\n",
        "        if i != j and np.any(tensor[i,j,:] != 0):\n",
        "            # Shift to signed weights: 0.5 -> 0, <0.5 negative, >0.5 positive\n",
        "            mean_weight = 2 * (np.mean(tensor[i,j,:]) - 0.5)\n",
        "            G_outer.add_edge(i, j, weight=mean_weight)\n",
        "\n",
        "# Outer spring layout\n",
        "pos_outer_2d = nx.circular_layout(G_outer, scale=5)\n",
        "pos_outer = np.array([[x, y, 0] for x, y in pos_outer_2d.values()])\n",
        "\n",
        "fig = plt.figure(figsize=(10,8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Plot outer nodes\n",
        "for i in range(D_graph):\n",
        "    ax.scatter(*pos_outer[i], s=300, color='skyblue')\n",
        "    ax.text(*pos_outer[i], f'Node {i}', color='black')\n",
        "\n",
        "# Plot outer edges with positive/negative colors\n",
        "\n",
        "for i, j, data in G_outer.edges(data=True):\n",
        "    x_vals = [pos_outer[i,0], pos_outer[j,0]]\n",
        "    y_vals = [pos_outer[i,1], pos_outer[j,1]]\n",
        "    z_vals = [pos_outer[i,2], pos_outer[j,2]]\n",
        "\n",
        "    # Positive = bright green, Negative = bright red\n",
        "    color = 'green' if data['weight'] > 0 else 'red'\n",
        "    linewidth = 2 + 4*abs(data['weight'])  # scale width by magnitude\n",
        "    ax.plot(x_vals, y_vals, z_vals, color=color, linewidth=linewidth)\n",
        "# ---------------- Inner FCMs (small circular around hub) ----------------\n",
        "for i, rep in enumerate(optimizer.nested_reps):\n",
        "    dims = len(rep)\n",
        "    angle = np.linspace(0, 2*np.pi, dims, endpoint=False)\n",
        "    radius = 0.8  # small circle\n",
        "    xs = pos_outer[i,0] + radius * np.cos(angle)\n",
        "    ys = pos_outer[i,1] + radius * np.sin(angle)\n",
        "    zs = pos_outer[i,2] + rep  # activation as height\n",
        "\n",
        "    # Plot inner nodes\n",
        "    ax.scatter(xs, ys, zs, c=rep, cmap='plasma', s=50)\n",
        "\n",
        "    # Connect inner nodes in circle\n",
        "    for k in range(dims):\n",
        "        ax.plot([xs[k], xs[(k+1)%dims]], [ys[k], ys[(k+1)%dims]], [zs[k], zs[(k+1)%dims]], color='gray', alpha=0.5)\n",
        "\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Activation')\n",
        "ax.set_title('Outer Nodes with Inner FCMs (Signed correlations)')\n",
        "plt.show()\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class GDFCMPredictorForward:\n",
        "    \"\"\"\n",
        "    Forward predictor for GDFCM.\n",
        "    Can handle new input data (DATA_MATRIX) and compute:\n",
        "        - Node activations\n",
        "        - Node metrics\n",
        "        - Inter-layer MI scores\n",
        "        - Total contributions and outer score\n",
        "    \"\"\"\n",
        "    def __init__(self, trained_gdfcm: GDFCM):\n",
        "        self.gdfcm = trained_gdfcm\n",
        "        self.D_graph = trained_gdfcm.D_graph\n",
        "        self.nested_reps = trained_gdfcm.nested_reps\n",
        "        self.chosen_Gmat = trained_gdfcm.chosen_Gmat\n",
        "        self.inter_layer = trained_gdfcm.inter_layer\n",
        "\n",
        "    def predict_node(self, node_idx, data_row):\n",
        "        \"\"\"\n",
        "        Predict metrics and activations for a single node given a new data row.\n",
        "        \"\"\"\n",
        "        # Use stored nested_rep as starting point\n",
        "        x = self.nested_reps[node_idx].copy()\n",
        "        # Optional: you could combine with data_row to modify x\n",
        "        # For now, we just compute metrics using data_row as the input\n",
        "        metrics_evaluator = MetricsEvaluator(np.array([data_row]))\n",
        "        metrics = metrics_evaluator.compute_node_metrics(0, y=x)\n",
        "\n",
        "        # Inter-layer MI\n",
        "        mi_score = self.inter_layer.mi_for_graph(self.chosen_Gmat, self.nested_reps)\n",
        "\n",
        "        return {'activations': x, 'metrics': metrics, 'mi_score': mi_score}\n",
        "\n",
        "    def predict_all_nodes(self, new_data_matrix):\n",
        "        \"\"\"\n",
        "        Predict metrics and activations for all nodes using new data.\n",
        "        new_data_matrix: shape (D_graph, num_features)\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        for node_idx in range(self.D_graph):\n",
        "            data_row = new_data_matrix[node_idx % len(new_data_matrix)]\n",
        "            node_result = self.predict_node(node_idx, data_row)\n",
        "            results.append(node_result)\n",
        "        return results\n",
        "\n",
        "    def predict_scores(self):\n",
        "        \"\"\"\n",
        "        Compute per-node contributions and total outer score.\n",
        "        \"\"\"\n",
        "        _, total_score, node_contributions = self.gdfcm.run_outer()\n",
        "        return {'node_contributions': node_contributions, 'total_score': total_score}\n",
        "# Suppose optimizer is your trained GDFCM\n",
        "predictor = GDFCMPredictorForward(optimizer)\n",
        "\n",
        "# New data: same number of nodes, each with same num_features\n",
        "new_DATA_MATRIX = np.random.rand(D_graph, DATA_MATRIX.shape[1])\n",
        "\n",
        "# Predict all nodes\n",
        "predictions = predictor.predict_all_nodes(new_DATA_MATRIX)\n",
        "\n",
        "for idx, node_pred in enumerate(predictions):\n",
        "    print(f\"Node {idx} Metrics:\", node_pred['metrics'])\n",
        "    print(f\"Node {idx} Activations:\", node_pred['activations'])\n",
        "    print(f\"Node {idx} MI Score:\", node_pred['mi_score'])\n",
        "\n",
        "# Optional: compute total contributions\n",
        "score_info = predictor.predict_scores()\n",
        "print(\"Node Contributions:\", score_info['node_contributions'])\n",
        "print(\"Total Score:\", score_info['total_score'])\n",
        "\n",
        "class GDFCMPredictorAdaptive:\n",
        "    \"\"\"\n",
        "    Adaptive forward predictor for GDFCM.\n",
        "    - Accepts new data per node.\n",
        "    - Updates activations slightly using a mini inner-loop.\n",
        "    - Computes metrics, inter-layer MI, and outer score contributions.\n",
        "    \"\"\"\n",
        "    def __init__(self, trained_gdfcm: GDFCM, lr_x=0.01, lr_steps=10):\n",
        "        self.gdfcm = trained_gdfcm\n",
        "        self.D_graph = trained_gdfcm.D_graph\n",
        "        self.nested_reps = [rep.copy() for rep in trained_gdfcm.nested_reps]\n",
        "        self.chosen_Gmat = trained_gdfcm.chosen_Gmat\n",
        "        self.inter_layer = trained_gdfcm.inter_layer\n",
        "        self.lr_x = lr_x\n",
        "        self.lr_steps = lr_steps\n",
        "\n",
        "    def adapt_node(self, node_idx, data_row):\n",
        "        \"\"\"\n",
        "        Mini inner-loop: slightly update activations based on new data row.\n",
        "        Handles shape mismatch by projecting new data to node activation dim.\n",
        "        \"\"\"\n",
        "        x = self.nested_reps[node_idx].copy()\n",
        "        dim = len(x)\n",
        "\n",
        "        # Simple linear projection of new data to activation space\n",
        "        if len(data_row) != dim:\n",
        "            # Use mean pooling to reduce or slice if too large\n",
        "            if len(data_row) > dim:\n",
        "                target = data_row[:dim]\n",
        "            else:\n",
        "                # Pad with 0.5\n",
        "                target = np.pad(data_row, (0, dim - len(data_row)), 'constant', constant_values=0.5)\n",
        "        else:\n",
        "            target = data_row\n",
        "\n",
        "        target = np.clip(target, 0, 1)\n",
        "\n",
        "        # Mini inner-loop update\n",
        "        for _ in range(self.lr_steps):\n",
        "            grad = x - target\n",
        "            x -= self.lr_x * grad\n",
        "            x = np.clip(x, 0, 1)\n",
        "\n",
        "        self.nested_reps[node_idx] = x\n",
        "\n",
        "        # Compute metrics\n",
        "        metrics_evaluator = MetricsEvaluator(np.array([data_row]))\n",
        "        metrics = metrics_evaluator.compute_node_metrics(0, y=x)\n",
        "\n",
        "        # Inter-layer MI\n",
        "        mi_score = self.inter_layer.mi_for_graph(self.chosen_Gmat, self.nested_reps)\n",
        "\n",
        "        return {'activations': x, 'metrics': metrics, 'mi_score': mi_score}\n",
        "\n",
        "\n",
        "    def adapt_all_nodes(self, new_data_matrix):\n",
        "        \"\"\"\n",
        "        Update activations for all nodes given new data matrix.\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        for node_idx in range(self.D_graph):\n",
        "            data_row = new_data_matrix[node_idx % len(new_data_matrix)]\n",
        "            node_result = self.adapt_node(node_idx, data_row)\n",
        "            results.append(node_result)\n",
        "        return results\n",
        "\n",
        "    def compute_outer_score(self):\n",
        "        \"\"\"\n",
        "        Compute node contributions and total outer score using current nested_reps.\n",
        "        \"\"\"\n",
        "        _, total_score, node_contributions = self.gdfcm.run_outer()\n",
        "        return {'node_contributions': node_contributions, 'total_score': total_score}\n",
        "# Initialize adaptive predictor\n",
        "adaptive_predictor = GDFCMPredictorAdaptive(optimizer, lr_x=0.02, lr_steps=15)\n",
        "\n",
        "# New data\n",
        "new_DATA_MATRIX = np.random.rand(D_graph, DATA_MATRIX.shape[1])\n",
        "\n",
        "# Adapt activations to new data\n",
        "predictions = adaptive_predictor.adapt_all_nodes(new_DATA_MATRIX)\n",
        "\n",
        "for idx, node_pred in enumerate(predictions):\n",
        "    print(f\"Node {idx} Metrics:\", node_pred['metrics'])\n",
        "    print(f\"Node {idx} Activations:\", node_pred['activations'])\n",
        "    print(f\"Node {idx} MI Score:\", node_pred['mi_score'])\n",
        "\n",
        "# Compute outer node contributions after adaptation\n",
        "score_info = adaptive_predictor.compute_outer_score()\n",
        "print(\"Node Contributions:\", score_info['node_contributions'])\n",
        "print(\"Total Score:\", score_info['total_score'])\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "pred_metrics = []\n",
        "true_metrics = []\n",
        "for idx, data_row in enumerate(new_DATA_MATRIX):\n",
        "    pred = adaptive_predictor.adapt_node(idx, data_row)\n",
        "    pred_metrics.append([pred['metrics'][k] for k in ['wait','throughput','util','patience']])\n",
        "    true_metrics.append([MetricsEvaluator(new_DATA_MATRIX).compute_node_metrics(idx)[k]\n",
        "                         for k in ['wait','throughput','util','patience']])\n",
        "\n",
        "pred_metrics = np.array(pred_metrics)\n",
        "true_metrics = np.array(true_metrics)\n",
        "\n",
        "\n",
        "def evaluate_predictions(true_metrics, pred_metrics, metric_names=None):\n",
        "    \"\"\"\n",
        "    Evaluate multi-metric predictions per node and overall.\n",
        "\n",
        "    Args:\n",
        "        true_metrics: np.array, shape (num_nodes, num_metrics)\n",
        "        pred_metrics: np.array, same shape as true_metrics\n",
        "        metric_names: list of metric names (optional)\n",
        "\n",
        "    Returns:\n",
        "        dict with evaluation metrics\n",
        "    \"\"\"\n",
        "    true_metrics = np.array(true_metrics)\n",
        "    pred_metrics = np.array(pred_metrics)\n",
        "\n",
        "    if metric_names is None:\n",
        "        metric_names = [f'Metric {i}' for i in range(true_metrics.shape[1])]\n",
        "\n",
        "    num_nodes, num_metrics = true_metrics.shape\n",
        "\n",
        "    results = {'per_metric': {}, 'overall': {}}\n",
        "\n",
        "    # Per metric\n",
        "    for i, name in enumerate(metric_names):\n",
        "        t = true_metrics[:, i]\n",
        "        p = pred_metrics[:, i]\n",
        "        mae = mean_absolute_error(t, p)\n",
        "        rmse = np.sqrt(mean_squared_error(t, p))\n",
        "        r2 = r2_score(t, p)\n",
        "        corr = np.corrcoef(t, p)[0,1]\n",
        "        mape = np.mean(np.abs((t - p) / (t + 1e-12))) * 100\n",
        "        results['per_metric'][name] = {\n",
        "            'MAE': mae,\n",
        "            'RMSE': rmse,\n",
        "            'R2': r2,\n",
        "            'Pearson': corr,\n",
        "            'MAPE (%)': mape\n",
        "        }\n",
        "\n",
        "    # Overall\n",
        "    mae_overall = mean_absolute_error(true_metrics.flatten(), pred_metrics.flatten())\n",
        "    rmse_overall = np.sqrt(mean_squared_error(true_metrics.flatten(), pred_metrics.flatten()))\n",
        "    r2_overall = r2_score(true_metrics.flatten(), pred_metrics.flatten())\n",
        "\n",
        "    # Cosine similarity (averaged over nodes)\n",
        "    cos_sim = np.mean([cosine_similarity(t.reshape(1,-1), p.reshape(1,-1))[0,0]\n",
        "                       for t,p in zip(true_metrics, pred_metrics)])\n",
        "\n",
        "    results['overall'] = {\n",
        "        'MAE': mae_overall,\n",
        "        'RMSE': rmse_overall,\n",
        "        'R2': r2_overall,\n",
        "        'CosineSim': cos_sim\n",
        "    }\n",
        "\n",
        "    return results\n",
        "# Suppose true_metrics and pred_metrics have shape (D_graph, 4)\n",
        "metrics_eval = evaluate_predictions(true_metrics, pred_metrics, metric_names=['wait','throughput','util','patience'])\n",
        "\n",
        "# Print per-metric evaluation\n",
        "for metric, vals in metrics_eval['per_metric'].items():\n",
        "    print(f\"{metric}: {vals}\")\n",
        "\n",
        "# Print overall evaluation\n",
        "print(\"\\nOverall metrics:\", metrics_eval['overall'])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgqjhNTSMdBX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGZK8eV0Y1SJ"
      },
      "source": [
        "# FCM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gengtjhZwUR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# ================================================================\n",
        "#  SIMPLE FCM MODEL\n",
        "# ================================================================\n",
        "class SimpleFCM:\n",
        "    def __init__(self, n_nodes, lr=0.05, max_iter=200, seed=0):\n",
        "        self.n_nodes = n_nodes\n",
        "        self.lr = lr\n",
        "        self.max_iter = max_iter\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "        self.W = self.rng.normal(0, 0.3, size=(n_nodes, n_nodes))\n",
        "        np.fill_diagonal(self.W, 0.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def activation(x):\n",
        "        return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "    def propagate(self, state):\n",
        "        return self.activation(state @ self.W.T)\n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        for _ in range(self.max_iter):\n",
        "            idx = self.rng.integers(0, len(X))\n",
        "            x = X[idx]\n",
        "            y_true = Y[idx]\n",
        "            y_pred = self.propagate(x)\n",
        "            error = y_true - y_pred\n",
        "            deltaW = self.lr * np.outer(error, x)\n",
        "            np.fill_diagonal(deltaW, 0.0)\n",
        "            self.W += deltaW\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.activation(X @ self.W.T)\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "#  COMPUTE WAIT, THROUGHPUT, UTILIZATION, PATIENCE\n",
        "# ================================================================\n",
        "def compute_metrics(raw_data):\n",
        "    \"\"\"\n",
        "    raw_data: (T, D, F) raw inputs per node\n",
        "    Returns: (T, D, 4) metrics [wait, throughput, utilization, patience]\n",
        "    \"\"\"\n",
        "    T, D, F = raw_data.shape\n",
        "    metrics = np.zeros((T, D, 4))\n",
        "\n",
        "    for t in range(T):\n",
        "        for d in range(D):\n",
        "            node_data = raw_data[t, d, :]\n",
        "            # Example formulas (replace with your real formulas)\n",
        "            wait = node_data[0]\n",
        "            throughput = node_data[1]\n",
        "            utilization = node_data[2]\n",
        "            patience = node_data[3]\n",
        "\n",
        "            metrics[t, d, 0] = wait\n",
        "            metrics[t, d, 1] = throughput\n",
        "            metrics[t, d, 2] = utilization\n",
        "            metrics[t, d, 3] = patience\n",
        "    return metrics\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "#  EVALUATION FUNCTION\n",
        "# ================================================================\n",
        "def evaluate_predictions(true_metrics, pred_metrics, metric_names=None):\n",
        "    true_metrics = np.asarray(true_metrics)\n",
        "    pred_metrics = np.asarray(pred_metrics)\n",
        "    results = {'per_metric': {}, 'overall': {}}\n",
        "\n",
        "    if true_metrics.ndim == 1:\n",
        "        true_metrics = true_metrics[:, None]\n",
        "    if pred_metrics.ndim == 1:\n",
        "        pred_metrics = pred_metrics[:, None]\n",
        "\n",
        "    num_nodes = true_metrics.shape[1]\n",
        "    if metric_names is None:\n",
        "        metric_names = [f'Node {i}' for i in range(num_nodes)]\n",
        "\n",
        "    for i, name in enumerate(metric_names):\n",
        "        t = true_metrics[:, i]\n",
        "        p = pred_metrics[:, i]\n",
        "        mae = mean_absolute_error(t, p)\n",
        "        rmse = np.sqrt(mean_squared_error(t, p))\n",
        "        r2 = r2_score(t, p)\n",
        "        corr = np.corrcoef(t, p)[0, 1] if np.std(t) * np.std(p) > 0 else np.nan\n",
        "        mape = np.mean(np.abs((t - p) / (t + 1e-12))) * 100\n",
        "        results['per_metric'][name] = {'MAE': mae, 'RMSE': rmse, 'R2': r2, 'Pearson': corr, 'MAPE (%)': mape}\n",
        "\n",
        "    mae_overall = mean_absolute_error(true_metrics.flatten(), pred_metrics.flatten())\n",
        "    rmse_overall = np.sqrt(mean_squared_error(true_metrics.flatten(), pred_metrics.flatten()))\n",
        "    r2_overall = r2_score(true_metrics.flatten(), pred_metrics.flatten())\n",
        "    cos_sim = np.mean([cosine_similarity(t.reshape(1, -1), p.reshape(1, -1))[0, 0]\n",
        "                       for t, p in zip(true_metrics, pred_metrics)])\n",
        "    results['overall'] = {'MAE': mae_overall, 'RMSE': rmse_overall, 'R2': r2_overall, 'CosineSim': cos_sim}\n",
        "    return results\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "#  SIMPLE FCM PIPELINE\n",
        "# ================================================================\n",
        "def run_fcm_pipeline(raw_data, test_size=0.2, lr=0.05, max_iter=500):\n",
        "    \"\"\"\n",
        "    raw_data: (T, D, F) raw inputs per node\n",
        "    \"\"\"\n",
        "    # Compute metrics\n",
        "    metrics_data = compute_metrics(raw_data)\n",
        "    T, D, M = metrics_data.shape\n",
        "    N = D * M\n",
        "\n",
        "    # Flatten and normalize\n",
        "    DATA_MATRIX = metrics_data.reshape(T, N)\n",
        "    DATA_MATRIX = (DATA_MATRIX - DATA_MATRIX.min(axis=0)) / (np.ptp(DATA_MATRIX, axis=0) + 1e-8)\n",
        "\n",
        "    # Train/test split\n",
        "    X_train, X_test = train_test_split(DATA_MATRIX, test_size=test_size, random_state=42)\n",
        "    y_train, y_test = X_train.copy(), X_test.copy()\n",
        "\n",
        "    # Train FCM\n",
        "    fcm = SimpleFCM(n_nodes=N, lr=lr, max_iter=max_iter)\n",
        "    fcm.fit(X_train, y_train)\n",
        "\n",
        "    # Predict\n",
        "    y_pred = fcm.predict(X_test)\n",
        "\n",
        "    # Evaluate\n",
        "    metric_names = [f\"N{n}-M{m}\" for n in range(D) for m in range(M)]\n",
        "    metrics_eval = evaluate_predictions(y_test, y_pred, metric_names)\n",
        "    return fcm, metrics_eval\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "#  EXAMPLE USAGE\n",
        "# ================================================================\n",
        "# Suppose you have raw node data: shape (T, D, 4)\n",
        "T, D, F = 100, 5, 4\n",
        "raw_data = np.random.rand(T, D, F)  # replace with actual data\n",
        "\n",
        "fcm_model, eval_results = run_fcm_pipeline(raw_data, test_size=0.2, lr=0.05, max_iter=500)\n",
        "\n",
        "print(\"\\nOverall metrics:\")\n",
        "print(eval_results['overall'])\n",
        "\n",
        "print(\"\\nPer-node metrics (first few):\")\n",
        "for k, v in list(eval_results['per_metric'].items())[:D*F]:\n",
        "    print(k, v)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlT8EnXEZw63"
      },
      "source": [
        "# Neural Metric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# MetricsEvaluator External Config (Scaled Up)\n",
        "# -----------------------------\n",
        "\n",
        "metrics_evaluator_config = {\n",
        "    # -------------------------\n",
        "    # Data-related parameters\n",
        "    # -------------------------\n",
        "    \"data_matrix\": None,           # numpy array of shape (N_samples, N_features), required\n",
        "    \"adj_matrices\": None,          # list of adjacency matrices (optional)\n",
        "    \"mi_weights\": None,            # list of float weights for adjacency matrices\n",
        "    \"node_size\": 5,                # increase node features for richer representation\n",
        "\n",
        "    # -------------------------\n",
        "    # Network architecture\n",
        "    # -------------------------\n",
        "    \"hidden_dim\": 64,            # larger hidden dimension for subnets and MLPs\n",
        "    \"use_cnn\": True,               # enable CNN supernet\n",
        "    \"cnn_channels\": 1024,          # more channels for CNN supernet\n",
        "    \"kernel_size\": [3,5,7],        # multiple kernel sizes for multi-scale feature extraction\n",
        "    \"use_rnn\": False,               # use RNN supernet\n",
        "    \"rnn_hidden\": 1024,            # larger hidden dimension for GRU supernet\n",
        "    \"rnn_layers\": 3,               # stacked GRU layers for deeper modeling\n",
        "    \"dropout\": 0.2,                # dropout for CNN/RNN supernet\n",
        "    \"prop_factor\": 1.05,           # small graph propagation factor\n",
        "    \"num_metrics\": 4,              # number of learned objectives\n",
        "\n",
        "    # -------------------------\n",
        "    # Training hyperparameters\n",
        "    # -------------------------\n",
        "    \"epochs\": 2500,                # longer training for complex model\n",
        "    \"lr\": 0.0005,                  # slightly lower LR for stability\n",
        "\n",
        "    # Loss weighting\n",
        "    \"lambda_coord\": 0.15,          # consistency across nodes\n",
        "    \"lambda_act\": 0.02,            # activation penalty\n",
        "    \"lambda_corr\": 1.15,           # feature->metric correlation\n",
        "    \"lambda_deco\": 0.1,            # decorrelation across nodes\n",
        "    \"lambda_pred\": 0.3,            # feature->metric predictor match\n",
        "    \"lambda_rel\": -0.1,             # relative variance encouragement\n",
        "    \"lambda_feat\": 100,           # extra feature->metric loss\n",
        "    \"lambda_r2\": 1000,              # emphasize maximizing R between predictions and true objectives\n",
        "\n",
        "    # -------------------------\n",
        "    # Miscellaneous\n",
        "    # -------------------------\n",
        "    \"verbose\": True,               # whether to print progress\n",
        "}\n",
        "\n",
        "# -------------------------\n",
        "# Example instantiation\n",
        "# -------------------------\n",
        "# import numpy as np\n",
        "# from your_module import MetricsEvaluator\n",
        "#\n",
        "# data_matrix = np.random.rand(200, 64)  # example: 8 nodes with 8 features each\n",
        "# config = metrics_evaluator_config.copy()\n",
        "# config[\"data_matrix\"] = data_matrix\n",
        "#\n",
        "# model = MetricsEvaluator(**config)\n",
        "# model.train_from_system()\n"
      ],
      "metadata": {
        "id": "kHWl6WpJGtY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5y95mdZgBlQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class MetricsEvaluator(nn.Module):\n",
        "    def __init__(self, data_matrix, adj_matrices=None, mi_weights=None,\n",
        "                 node_size=None, hidden_dim=256,\n",
        "                 use_cnn=False, cnn_channels=64, kernel_size=3,\n",
        "                 use_rnn=True, rnn_hidden=256, rnn_layers=2, dropout=0.1,\n",
        "                 prop_factor=0.0, num_metrics=4):\n",
        "        \"\"\"\n",
        "        data_matrix : numpy array (N_samples x N_features)\n",
        "        adj_matrices : optional list of adjacency numpy arrays\n",
        "        mi_weights : optional list of float weights for adjacency layers\n",
        "        node_size : features per node\n",
        "        hidden_dim : hidden size for subnets/MLPs\n",
        "        use_cnn : whether to use CNN for supernet\n",
        "        cnn_channels : channels for CNN supernet\n",
        "        kernel_size : kernel size for CNN\n",
        "        use_rnn : whether to use RNN for supernet\n",
        "        rnn_hidden : hidden size for GRU supernet\n",
        "        rnn_layers : number of stacked GRU layers\n",
        "        dropout : dropout rate for supernet\n",
        "        prop_factor : graph propagation factor\n",
        "        num_metrics : number of learned objectives\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_metrics = num_metrics\n",
        "        self.data_matrix = data_matrix\n",
        "        self.node_size = node_size or 4\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.prev_metrics = torch.zeros(num_metrics)\n",
        "        self.adj_matrices = adj_matrices\n",
        "        self.mi_weights = mi_weights\n",
        "        self.prop_factor = prop_factor\n",
        "        self.use_cnn = use_cnn\n",
        "        self.use_rnn = use_rnn\n",
        "\n",
        "        # Subnet input size: node features + external y(3) + prev metrics\n",
        "        self.sub_input_size = self.node_size + 3 + 4\n",
        "\n",
        "        # --------------------------\n",
        "        # Subnets (4 objectives)\n",
        "        # --------------------------\n",
        "        def make_subnet(activation):\n",
        "            return nn.Sequential(\n",
        "                nn.Linear(self.sub_input_size, hidden_dim),\n",
        "                activation(),\n",
        "                nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "                activation(),\n",
        "                nn.Linear(hidden_dim // 2, 1),\n",
        "                nn.Softplus()\n",
        "            )\n",
        "\n",
        "        self.subnets = nn.ModuleDict({\n",
        "            \"wait\": make_subnet(nn.Tanh),\n",
        "            \"throughput\": make_subnet(nn.ReLU),\n",
        "            \"util\": make_subnet(nn.Sigmoid),\n",
        "            \"patience\": make_subnet(nn.ELU)\n",
        "        })\n",
        "\n",
        "        # --------------------------\n",
        "        # Supernet\n",
        "        # --------------------------\n",
        "        super_in = self.node_size + 4 + 4  # node_feats + raw_metrics + prev_metrics\n",
        "\n",
        "        if use_cnn:\n",
        "            # Deep CNN with BatchNorm + Dropout\n",
        "            self.supernet = nn.Sequential(\n",
        "                nn.Conv1d(1, cnn_channels, kernel_size, padding=kernel_size // 2),\n",
        "                nn.BatchNorm1d(cnn_channels),\n",
        "                nn.ReLU(),\n",
        "                nn.Conv1d(cnn_channels, cnn_channels, kernel_size, padding=kernel_size // 2),\n",
        "                nn.BatchNorm1d(cnn_channels),\n",
        "                nn.ReLU(),\n",
        "                nn.Conv1d(cnn_channels, 4, kernel_size=1),\n",
        "                nn.AdaptiveAvgPool1d(1),\n",
        "                nn.Flatten(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Softplus()\n",
        "            )\n",
        "        elif use_rnn:\n",
        "            # Stacked GRU with extra linear + dropout\n",
        "            self.supernet_rnn = nn.GRU(\n",
        "                input_size=super_in,\n",
        "                hidden_size=rnn_hidden,\n",
        "                num_layers=rnn_layers,\n",
        "                batch_first=True\n",
        "            )\n",
        "            self.supernet_out = nn.Sequential(\n",
        "                nn.Linear(rnn_hidden, rnn_hidden),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Linear(rnn_hidden, 4),\n",
        "                nn.Softplus()\n",
        "            )\n",
        "        else:\n",
        "            self.supernet = nn.Sequential(\n",
        "                nn.Linear(super_in, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_dim // 2, 4),\n",
        "                nn.Softplus()\n",
        "            )\n",
        "\n",
        "        # --------------------------\n",
        "        # Feature -> metric predictor\n",
        "        # --------------------------\n",
        "        self.predict_net = nn.Sequential(\n",
        "            nn.Linear(self.node_size, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim // 2, 4),\n",
        "            nn.Softplus()\n",
        "        )\n",
        "\n",
        "        # --------------------------\n",
        "        # Hardcoded columns\n",
        "        # --------------------------\n",
        "        self._cols = {\n",
        "            'travel_time': 0,\n",
        "            'load_utilization': 1,\n",
        "            'delivery_priority': 2,\n",
        "            'delay_probability': 3,\n",
        "            'congestion_score': 4,\n",
        "            'energy_level': 5,\n",
        "        }\n",
        "\n",
        "    # --------------------------\n",
        "    # Extract objectives\n",
        "    # --------------------------\n",
        "    def extract_objectives(self, node_idx):\n",
        "        N = self.data_matrix.shape[0]\n",
        "        r = self.data_matrix[node_idx % N]\n",
        "\n",
        "        travel_time = r[self._cols['travel_time']]\n",
        "        load_util = r[self._cols['load_utilization']]\n",
        "        priority = r[self._cols['delivery_priority']]\n",
        "        delay_prob = r[self._cols['delay_probability']]\n",
        "        congestion = r[self._cols['congestion_score']]\n",
        "        energy_level = r[self._cols['energy_level']] if 'energy_level' in self._cols else 0.5\n",
        "\n",
        "        travel_efficiency = 1.0 / (1.0 + travel_time)\n",
        "        vehicle_efficiency = load_util\n",
        "        scheduling_score = priority * (1.0 - delay_prob)\n",
        "        congestion_score = 1.0 / (1.0 + congestion)\n",
        "        energy_score = np.sqrt(energy_level + 0.1)\n",
        "\n",
        "        return torch.tensor([\n",
        "            travel_efficiency,\n",
        "            vehicle_efficiency,\n",
        "            scheduling_score,\n",
        "            congestion_score,\n",
        "            energy_score\n",
        "        ], dtype=torch.float32)\n",
        "\n",
        "\n",
        "    # -------------------------\n",
        "    # Compute node metrics (forward for a single node)\n",
        "    # -------------------------\n",
        "    def compute_node_metrics(self, node_idx, y=None):\n",
        "        # --- Extract node block safely ---\n",
        "        col_start = node_idx * self.node_size\n",
        "        node_block = self.data_matrix[:, col_start: col_start + self.node_size]\n",
        "\n",
        "        node_feats = node_block.mean(axis=0)\n",
        "        if len(node_feats) < self.node_size:\n",
        "            node_feats = np.pad(node_feats, (0, self.node_size - len(node_feats)), 'constant')\n",
        "\n",
        "        # --- Prepare external input y ---\n",
        "        if y is None:\n",
        "            y_arr = np.zeros(3)\n",
        "        else:\n",
        "            y_arr = np.array(y[:3])\n",
        "            if len(y_arr) < 3:\n",
        "                y_arr = np.pad(y_arr, (0, 3 - len(y_arr)), 'constant', constant_values=0.5)\n",
        "\n",
        "        # --- Build input for subnet ---\n",
        "        nn_input = torch.tensor(\n",
        "            np.concatenate([node_feats, y_arr, self.prev_metrics.numpy()]),\n",
        "            dtype=torch.float32\n",
        "        )\n",
        "\n",
        "        # --- Compute raw metrics from subnets ---\n",
        "        raw = torch.stack([\n",
        "            self.subnets['wait'](nn_input).squeeze(),\n",
        "            self.subnets['throughput'](nn_input).squeeze(),\n",
        "            self.subnets['util'](nn_input).squeeze(),\n",
        "            self.subnets['patience'](nn_input).squeeze()\n",
        "        ]) if self.subnets else torch.zeros(4)\n",
        "\n",
        "        # --- Supernet processing ---\n",
        "        super_in = torch.cat([torch.tensor(node_feats, dtype=torch.float32), raw, self.prev_metrics], dim=0)\n",
        "\n",
        "        if self.use_cnn and self.supernet is not None:\n",
        "            x = super_in.unsqueeze(0).unsqueeze(0)  # (1,1,L)\n",
        "            metrics = self.supernet(x).squeeze(0)\n",
        "        elif self.use_rnn and self.supernet_rnn is not None and self.supernet_out is not None:\n",
        "            x = super_in.unsqueeze(0).unsqueeze(1)  # (1,1,L)\n",
        "            rnn_out, _ = self.supernet_rnn(x)\n",
        "            metrics = self.supernet_out(rnn_out.squeeze(1)).squeeze(0)\n",
        "        elif self.supernet is not None:\n",
        "            metrics = self.supernet(super_in)\n",
        "        else:\n",
        "            metrics = raw  # fallback\n",
        "\n",
        "        # --- Optional graph propagation ---\n",
        "        if self.adj_matrices is not None and self.mi_weights is not None:\n",
        "            agg = metrics.clone()\n",
        "            for L, adj in enumerate(self.adj_matrices):\n",
        "                w = self.mi_weights[L]\n",
        "                row = adj[node_idx]\n",
        "                norm = row / (row.sum() + 1e-8)\n",
        "                neighbor = torch.matmul(norm, metrics.unsqueeze(0))\n",
        "                alpha = self.prop_factor * w\n",
        "                agg = (1 - alpha) * agg + alpha * neighbor.squeeze()\n",
        "            metrics = agg\n",
        "\n",
        "        # --- Update previous metrics ---\n",
        "        self.prev_metrics = metrics.detach()\n",
        "\n",
        "        # --- Aggregate score ---\n",
        "        score = metrics[0] + metrics[1] + metrics[2] + metrics[3]\n",
        "\n",
        "        return {\n",
        "            \"wait\": float(metrics[0].item()),\n",
        "            \"throughput\": float(metrics[1].item()),\n",
        "            \"util\": float(metrics[2].item()),\n",
        "            \"patience\": float(metrics[3].item()),\n",
        "            \"score\": float(score.item())\n",
        "\n",
        "        }\n",
        "\n",
        "    # -------------------------\n",
        "    # Training loop\n",
        "    # -------------------------\n",
        "    def train_from_system(self, epochs=250, lr=0.001,\n",
        "                      lambda_coord=0.1, lambda_act=0.01,\n",
        "                      lambda_corr=0.1, lambda_deco=0.1,\n",
        "                      lambda_pred=0.2, lambda_rel=0.05,\n",
        "                      lambda_feat=0.5, lambda_r2=0.5, verbose=True):\n",
        "        \"\"\"\n",
        "        Trains the subnets + supernet to predict the four objective labels extracted from DATA_MATRIX.\n",
        "        Loss combines:\n",
        "         - obj_loss (MSE to true objectives)\n",
        "         - -score (maximize score)\n",
        "         - coord_loss (consistency across nodes)\n",
        "         - act_loss   (activation penalty)\n",
        "         - corr_loss  (feature->metric alignment)\n",
        "         - deco_loss  (decorrelation across nodes)\n",
        "         - pred_loss  (feature->metric predictor match)\n",
        "         - rel_loss   (relative variance encouragement)\n",
        "         - r2_loss    (maximize R between predictions and true objectives)\n",
        "        \"\"\"\n",
        "\n",
        "        num_nodes = max(1, self.data_matrix.shape[1] // self.node_size)\n",
        "\n",
        "        # collect parameters\n",
        "        params = list(self.subnets.parameters()) + list(self.predict_net.parameters())\n",
        "        if self.use_cnn:\n",
        "            params += list(self.supernet.parameters())\n",
        "        elif self.use_rnn:\n",
        "            params += list(self.supernet_rnn.parameters()) + list(self.supernet_out.parameters())\n",
        "        else:\n",
        "            params += list(self.supernet.parameters())\n",
        "\n",
        "        optimizer = optim.Adam(params, lr=lr)\n",
        "        mse = nn.MSELoss()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self.prev_metrics = torch.zeros(4)\n",
        "            all_preds = []\n",
        "            obj_loss = torch.tensor(0.0, dtype=torch.float32)\n",
        "\n",
        "            # Forward pass: sequential per-node to respect prev_metrics conditioning\n",
        "            for node_idx in range(num_nodes):\n",
        "                start = node_idx * self.node_size\n",
        "                block = self.data_matrix[:, start:start + self.node_size]\n",
        "                feats = block.mean(axis=0)\n",
        "                feats = np.pad(feats, (0, max(0, self.node_size - len(feats))), 'constant')\n",
        "\n",
        "                y = np.zeros(3)\n",
        "                nn_input = torch.tensor(np.concatenate([feats, y, self.prev_metrics.numpy()]), dtype=torch.float32)\n",
        "\n",
        "                # Subnets\n",
        "                raw = torch.stack([\n",
        "                    self.subnets['wait'](nn_input).squeeze(),\n",
        "                    self.subnets['throughput'](nn_input).squeeze(),\n",
        "                    self.subnets['util'](nn_input).squeeze(),\n",
        "                    self.subnets['patience'](nn_input).squeeze()\n",
        "                ])\n",
        "\n",
        "                # Supernet\n",
        "                super_in = torch.cat([torch.tensor(feats, dtype=torch.float32), raw, self.prev_metrics], dim=0)\n",
        "                if self.use_cnn:\n",
        "                    x = super_in.unsqueeze(0).unsqueeze(0)\n",
        "                    preds = self.supernet(x).squeeze(0)\n",
        "                elif self.use_rnn:\n",
        "                    x = super_in.unsqueeze(0).unsqueeze(1)\n",
        "                    rnn_out, _ = self.supernet_rnn(x)\n",
        "                    preds = self.supernet_out(rnn_out.squeeze(1)).squeeze(0)\n",
        "                else:\n",
        "                    preds = self.supernet(super_in)\n",
        "\n",
        "                # Graph propagation\n",
        "                if self.adj_matrices is not None and self.mi_weights is not None:\n",
        "                    combined = preds.clone()\n",
        "                    for L, adj in enumerate(self.adj_matrices):\n",
        "                        w = self.mi_weights[L]\n",
        "                        row = adj[node_idx]\n",
        "                        norm = row / (row.sum() + 1e-8)\n",
        "                        neighbor = torch.matmul(norm, preds.unsqueeze(0))\n",
        "                        alpha = self.prop_factor * w\n",
        "                        combined = (1 - alpha) * combined + alpha * neighbor.squeeze()\n",
        "                    preds = combined\n",
        "\n",
        "                all_preds.append(preds)\n",
        "                self.prev_metrics = preds.detach()\n",
        "\n",
        "                # True objectives target\n",
        "                true_obj = self.extract_objectives(node_idx)\n",
        "                obj_loss = obj_loss + mse(preds, true_obj[:4])  # first 4 objectives\n",
        "\n",
        "            all_preds = torch.stack(all_preds)  # shape (num_nodes, 4)\n",
        "\n",
        "            # Score (maximize)\n",
        "            score = all_preds[-1, :4].sum()\n",
        "\n",
        "            # Coordination loss\n",
        "            coord_loss = torch.mean((all_preds.unsqueeze(1) - all_preds.unsqueeze(2)) ** 2)\n",
        "\n",
        "            # Activation penalty\n",
        "            act_loss = torch.mean(all_preds ** 2)\n",
        "\n",
        "            # Feature correlation\n",
        "            corr_loss = torch.tensor(0.0, dtype=torch.float32)\n",
        "            for node_idx in range(num_nodes):\n",
        "                start = node_idx * self.node_size\n",
        "                feats = self.data_matrix[:, start:start + self.node_size].mean(axis=0)\n",
        "                feats = np.pad(feats, (0, max(0, self.node_size - len(feats))), 'constant')\n",
        "                feats_t = torch.tensor(feats, dtype=torch.float32)\n",
        "                denom = (torch.norm(all_preds[node_idx]) * torch.norm(feats_t) + 1e-8)\n",
        "                cos_sim = torch.dot(all_preds[node_idx], feats_t) / denom\n",
        "                corr_loss += (1.0 - cos_sim)\n",
        "            corr_loss /= float(max(1, num_nodes))\n",
        "\n",
        "            # Decorrelation across nodes\n",
        "            deco_loss = torch.tensor(0.0, dtype=torch.float32)\n",
        "            if num_nodes > 1:\n",
        "                count = 0\n",
        "                for i in range(num_nodes):\n",
        "                    for j in range(i + 1, num_nodes):\n",
        "                        denom = (torch.norm(all_preds[i]) * torch.norm(all_preds[j]) + 1e-8)\n",
        "                        sim = torch.dot(all_preds[i], all_preds[j]) / denom\n",
        "                        deco_loss += sim ** 2\n",
        "                        count += 1\n",
        "                deco_loss /= float(max(1, count))\n",
        "\n",
        "        # Feature -> metric predictor loss\n",
        "            pred_loss = torch.tensor(0.0, dtype=torch.float32)\n",
        "            for node_idx in range(num_nodes):\n",
        "                start = node_idx * self.node_size\n",
        "                feats = self.data_matrix[:, start:start + self.node_size].mean(axis=0)\n",
        "                feats = np.pad(feats, (0, max(0, self.node_size - len(feats))), 'constant')\n",
        "                feats_t = torch.tensor(feats, dtype=torch.float32)\n",
        "                f2m = self.predict_net(feats_t)\n",
        "                pred_loss += mse(f2m, all_preds[node_idx].detach())\n",
        "            pred_loss /= float(max(1, num_nodes))\n",
        "\n",
        "            # Relative variance\n",
        "            rel_loss = torch.var(all_preds, dim=0).mean()\n",
        "\n",
        "            # --- R maximization ---\n",
        "            true_all = torch.stack([self.extract_objectives(i)[:4] for i in range(num_nodes)])\n",
        "            ss_res = torch.sum((true_all - all_preds) ** 2)\n",
        "            ss_tot = torch.sum((true_all - true_all.mean(dim=0)) ** 2)\n",
        "            r2 = 1 - ss_res / (ss_tot + 1e-8)\n",
        "\n",
        "            # Total loss\n",
        "            total_loss = (\n",
        "                obj_loss\n",
        "                - score\n",
        "                + lambda_coord * coord_loss\n",
        "                + lambda_act * act_loss\n",
        "                + lambda_corr * corr_loss\n",
        "                + lambda_deco * deco_loss\n",
        "                + lambda_pred * pred_loss\n",
        "                - lambda_rel * rel_loss\n",
        "                + lambda_feat * corr_loss\n",
        "                - lambda_r2 * r2  # maximize R\n",
        "            )\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if verbose and (epoch % 10 == 0 or epoch == epochs - 1):\n",
        "                print(f\"Epoch {epoch:03d} | TotalLoss {total_loss.item():.6f} | ObjLoss {obj_loss.item():.6f} | Score {score.item():.6f} | R2 {r2.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjGR3bsmeElG"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "# ---------------- CONFIG ----------------nsion\n",
        "#candidate_dims = [4, 3, 2, 3]  # D_graph=5\n",
        "#D_graph = len(candidate_dims)\n",
        "\n",
        "inner_archive_size = 80\n",
        "inner_offspring = 40\n",
        "outer_archive_size = 40\n",
        "outer_offspring = 40\n",
        "inner_iters_per_outer = 50\n",
        "outer_generations = 10\n",
        "outer_cost_limit = 10000\n",
        "inner_learning = 0.1\n",
        "gamma_interlayer = 1\n",
        "top_k = 263\n",
        "np.random.seed()\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class InterLayer:\n",
        "    def __init__(self, D_graph, max_inner_dim, inter_dim=None, edge_threshold=0.02, gamma=1.0, seed=42):\n",
        "        np.random.seed(seed)\n",
        "        self.D_graph = D_graph\n",
        "        self.max_input = 2 * max_inner_dim\n",
        "        self.edge_threshold = edge_threshold\n",
        "        self.gamma = gamma\n",
        "        self.inter_dim = inter_dim if inter_dim is not None else max_inner_dim\n",
        "\n",
        "        # Initialize weights proportional to synthetic correlation between nodes\n",
        "        self.weights = {}\n",
        "        self.bias = {}\n",
        "        for i in range(D_graph):\n",
        "            for j in range(D_graph):\n",
        "                if i != j:\n",
        "                    # small random + slight bias towards correlation\n",
        "                    w_init = np.random.uniform(-0.1, 0.1, (self.inter_dim, self.max_input))\n",
        "                    self.weights[(i,j)] = w_init\n",
        "                    self.bias[(i,j)] = np.zeros(self.inter_dim)\n",
        "\n",
        "    def compute_edge_activation(self, i, j, nested_reps):\n",
        "        concat = np.concatenate([nested_reps[i], nested_reps[j]])\n",
        "        concat = np.pad(concat, (0, max(0, self.max_input - len(concat))))[:self.max_input]\n",
        "\n",
        "        # Normalize input to improve correlation\n",
        "        concat = (concat - np.mean(concat)) / (np.std(concat) + 1e-12)\n",
        "\n",
        "        # Compute activation\n",
        "        v = self.weights[(i,j)].dot(concat) + self.bias[(i,j)]\n",
        "\n",
        "        # Scale by correlation strength with input signals\n",
        "        input_strength = np.clip(np.mean(np.abs(concat)), 0, 1)\n",
        "        v = v * input_strength\n",
        "\n",
        "        return 1 / (1 + np.exp(-v))\n",
        "\n",
        "    def build_activations(self, Gmat, nested_reps):\n",
        "        acts = {}\n",
        "        for i in range(self.D_graph):\n",
        "            for j in range(self.D_graph):\n",
        "                if i == j:\n",
        "                    continue\n",
        "                if abs(Gmat[i,j]) > self.edge_threshold:\n",
        "                    acts[(i,j)] = self.compute_edge_activation(i, j, nested_reps)\n",
        "        return acts\n",
        "\n",
        "    @staticmethod\n",
        "    def pairwise_squared_corr(acts):\n",
        "        if len(acts) < 2:\n",
        "            return 0.0\n",
        "        A = np.stack(list(acts.values()))\n",
        "        A_centered = A - A.mean(axis=1, keepdims=True)\n",
        "        stds = np.sqrt(np.sum(A_centered**2, axis=1) / (A.shape[1]-1) + 1e-12)\n",
        "        cov = A_centered @ A_centered.T / (A.shape[1]-1)\n",
        "        corr = cov / (np.outer(stds, stds) + 1e-12)\n",
        "        np.fill_diagonal(corr, 0)\n",
        "        return float((corr**2).sum())\n",
        "\n",
        "    def mi_for_graph(self, Gmat, nested_reps):\n",
        "        acts = self.build_activations(Gmat, nested_reps)\n",
        "        if not acts:\n",
        "            return 0.0\n",
        "        return self.gamma * self.pairwise_squared_corr(acts)\n",
        "\n",
        "    def correlate_shrink_interlayer(self, fmt_bounds=None, interaction_tensor=None, metrics_keys=None, verbose=True):\n",
        "        \"\"\"\n",
        "        Compute Pearson correlation per node & metric between:\n",
        "            - shrink factor (adaptive FMT)\n",
        "            - mean outgoing inter-layer activations\n",
        "        Returns: {node_idx: {metric: {'r':..., 'p':...}}}\n",
        "        \"\"\"\n",
        "        from scipy.stats import pearsonr\n",
        "\n",
        "        if metrics_keys is None:\n",
        "            metrics_keys = ['wait','throughput','util','patience']\n",
        "\n",
        "        D = self.D_graph\n",
        "\n",
        "        # 1. Compute FMT bounds if not given\n",
        "        if fmt_bounds is None:\n",
        "            fmt_bounds = self.compute_fmt_with_bounds_adaptive(top_k=top_k)\n",
        "\n",
        "        # 2. Get inter-layer activations if not provided\n",
        "        if interaction_tensor is None:\n",
        "            interaction_tensor = self.print_interactions(return_tensor=True, verbose=False)\n",
        "\n",
        "        inter_mean = interaction_tensor.mean(axis=2)  # (D,D)\n",
        "        shrink_factors = self.compute_fmt_shrink_factor(fmt_bounds, metrics_keys)  # (D, num_metrics)\n",
        "\n",
        "        correlations = {}\n",
        "\n",
        "        for i in range(D):\n",
        "            correlations[i] = {}\n",
        "            for k, key in enumerate(metrics_keys):\n",
        "                # FMT shrink for node i (broadcasted across outgoing edges)\n",
        "                shrink_vec = shrink_factors[i, k] * np.ones(D)\n",
        "                # Outgoing inter-layer activations from node i\n",
        "                inter_vec = inter_mean[i, :]\n",
        "                # Remove self-loop\n",
        "                mask = np.arange(D) != i\n",
        "                shrink_vec = shrink_vec[mask]\n",
        "                inter_vec = inter_vec[mask]\n",
        "\n",
        "                # Compute Pearson correlation\n",
        "                if np.std(inter_vec) > 1e-8:  # valid correlation\n",
        "                    r, p = pearsonr(shrink_vec, inter_vec)\n",
        "                else:\n",
        "                    r, p = 0.0, 1.0  # no variability\n",
        "\n",
        "                correlations[i][key] = {'r': r, 'p': p}\n",
        "                if verbose:\n",
        "                    print(f\"Node {i} | {key} shrink vs inter-layer: r={r:.3f}, p={p:.3e}\")\n",
        "\n",
        "        return correlations\n",
        "\n",
        "\n",
        "\n",
        "# ---------------- UNIFIED ACOR MULTIPLEX ----------------\n",
        "class GDFCM:\n",
        "    def __init__(self, candidate_dims, D_graph, inner_archive_size, inner_offspring,\n",
        "                 outer_archive_size, outer_offspring, synthetic_targets, inner_learning,\n",
        "                 gamma_interlayer=1.0, causal_flag=True):\n",
        "        self.candidate_dims = candidate_dims\n",
        "        self.D_graph = D_graph\n",
        "        self.inner_archive_size = inner_archive_size\n",
        "        self.inner_offspring = inner_offspring\n",
        "        self.outer_archive_size = outer_archive_size\n",
        "        self.outer_offspring = outer_offspring\n",
        "        self.synthetic_targets = synthetic_targets\n",
        "        self.inner_learning = inner_learning\n",
        "        self.causal_flag = causal_flag\n",
        "        self.best_dim_per_node = [len(t)-1 for t in synthetic_targets]  # last element as best dim\n",
        "\n",
        "        self.nested_reps = [np.zeros(max(candidate_dims)) for _ in range(D_graph)]\n",
        "      #  self.best_dim_per_node = [candidate_dims[0] for _ in range(D_graph)]\n",
        "        self.inter_layer = InterLayer(D_graph, max_inner_dim=max(candidate_dims), gamma=gamma_interlayer)\n",
        "        self.chosen_Gmat = np.random.uniform(-0.5,0.5,(D_graph,D_graph))\n",
        "        np.fill_diagonal(self.chosen_Gmat,0)\n",
        "        self.l2_before, self.l2_after = [], []\n",
        "\n",
        "    # ---------- INNER LOOP (FCM) ----------\n",
        "    def run_inner(self, node_idx, target, D_fcm,\n",
        "              steps=100, lr_x=0.001, lr_y=0.001, lr_W=0.001,\n",
        "              decorrelate_metrics=True):\n",
        "\n",
        "      # --- Initialize activations ---\n",
        "        # Inside run_inner\n",
        "        x = target.copy()\n",
        "        y = target.copy()\n",
        "\n",
        "        # Pad target for L2 computation\n",
        "        target_padded = np.pad(target, (0, len(self.nested_reps[node_idx]) - len(target)),\n",
        "                            mode='constant', constant_values=0.5)\n",
        "        self.l2_before.append(np.linalg.norm(self.nested_reps[node_idx] - target_padded))\n",
        "\n",
        "        # FCM updates\n",
        "        W = np.random.uniform(-0.6, 0.6, (D_fcm, D_fcm))\n",
        "        np.fill_diagonal(W, 0)\n",
        "\n",
        "        for _ in range(steps):\n",
        "            z = W.dot(x)\n",
        "            Theta_grad_z = 2*z - target\n",
        "            Theta_grad_x = Theta_grad_z @ W + (y+1)\n",
        "            Theta_grad_y = x + 1\n",
        "            Theta_grad_W = np.outer(Theta_grad_z, x)\n",
        "\n",
        "            x -= lr_x * np.clip(Theta_grad_x, -0.05, 0.05)\n",
        "            y -= lr_y * np.clip(Theta_grad_y, -0.05, 0.05)\n",
        "            x = np.clip(x, 0, 1)\n",
        "            y = np.clip(y, 0, 1)\n",
        "            W -= lr_W * np.clip(Theta_grad_W, -0.01, 0.01)\n",
        "            np.fill_diagonal(W, 0)\n",
        "            W = np.clip(W, -1, 1)\n",
        "\n",
        "        # Pad FCM output to max dim for nested_reps\n",
        "        x_padded = np.pad(x, (0, len(self.nested_reps[node_idx]) - len(x)),\n",
        "                        mode='constant', constant_values=0.5)\n",
        "        self.nested_reps[node_idx] = x_padded\n",
        "        self.l2_after.append(np.linalg.norm(x_padded - target_padded))\n",
        "\n",
        "\n",
        "      # --- Metric decoupling ---\n",
        "        metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "\n",
        "        if decorrelate_metrics:\n",
        "            # Neutralized input\n",
        "            neutral_y = np.full_like(x, 0.5)\n",
        "            metrics = metrics_evaluator.compute_node_metrics(node_idx, y=neutral_y)\n",
        "\n",
        "            # Optional: orthogonalize against inter-layer mean\n",
        "                    # Optional: orthogonalize against inter-layer mean\n",
        "        inter_tensor = self.inter_layer.build_activations(self.chosen_Gmat, self.nested_reps)\n",
        "        if inter_tensor:\n",
        "                D = self.D_graph\n",
        "                inter_mat = np.zeros((D, D))\n",
        "                for (i,j), vec in inter_tensor.items():\n",
        "                    inter_mat[i,j] = vec.mean()  # mean scalar\n",
        "\n",
        "                node_vec = np.array([metrics[k] for k in ['wait','throughput','util','patience']])\n",
        "                for i in range(D):\n",
        "                    f_scalar = inter_mat[i,:].mean()  # mean over row\n",
        "                    proj = f_scalar * (node_vec.mean() / (1e-12 + 1))  # scale by node_vec mean\n",
        "                    node_vec -= proj\n",
        "\n",
        "\n",
        "            # --- Fully decorrelated score ---\n",
        "                metrics['score'] = metrics['wait'] + metrics['throughput'] + metrics['util'] + metrics['patience']\n",
        "\n",
        "        else:\n",
        "            # Compute normally if no decorrelation\n",
        "            metrics = metrics_evaluator.compute_node_metrics(node_idx, y=x)\n",
        "\n",
        "        # --- Compute MI score for inter-layer ---\n",
        "        mi_score = self.inter_layer.mi_for_graph(self.chosen_Gmat, self.nested_reps)\n",
        "\n",
        "        return x, y, W, mi_score, metrics\n",
        "\n",
        "    # ---------- OUTER LOOP ----------\n",
        "    def run_outer(self, outer_cost_limit=1000):\n",
        "      metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "      node_metrics_list = []\n",
        "      raw_scores = []\n",
        "\n",
        "      # --- Compute node metrics per node ---\n",
        "      for i, y in enumerate(self.nested_reps):\n",
        "          metrics = metrics_evaluator.compute_node_metrics(i, y=y)\n",
        "          node_metrics_list.append(metrics)\n",
        "          raw_scores.append(metrics['score'])\n",
        "\n",
        "      raw_scores = np.array(raw_scores)\n",
        "      total_raw = raw_scores.sum()\n",
        "\n",
        "      # --- Apply cap to raw metrics ---\n",
        "      capped_total_raw = total_raw\n",
        "      if total_raw > outer_cost_limit:\n",
        "          scale_factor = outer_cost_limit / total_raw\n",
        "          for metrics in node_metrics_list:\n",
        "              for key in ['wait', 'throughput', 'util', 'patience', 'score']:\n",
        "                  metrics[key] *= scale_factor\n",
        "          raw_scores *= scale_factor\n",
        "          capped_total_raw = outer_cost_limit\n",
        "\n",
        "      # --- Compute Fuzzy Metric Tensor contribution ---\n",
        "      fuzzy_tensor = self.compute_fuzzy_metric_tensor(normalize=True)\n",
        "      D = self.D_graph\n",
        "\n",
        "      # Only consider off-diagonal entries for inter-node interactions\n",
        "      off_diag_mask = np.ones((D, D), dtype=bool)\n",
        "      np.fill_diagonal(off_diag_mask, 0)\n",
        "      fuzzy_score_offdiag = fuzzy_tensor[off_diag_mask].sum()\n",
        "\n",
        "      # --- Compute per-node contribution ---\n",
        "      node_contributions = np.zeros(D)\n",
        "      for i in range(D):\n",
        "          # Contribution from own metrics\n",
        "          own_score = raw_scores[i]\n",
        "\n",
        "          # Contribution from FMT interactions (row i -> others)\n",
        "          fmt_contrib = fuzzy_tensor[i, :, :].sum() - fuzzy_tensor[i, i, :].sum()  # exclude self\n",
        "          node_contributions[i] = own_score + self.inter_layer.gamma * fmt_contrib\n",
        "\n",
        "      # --- Strong decoupling: correlation penalty ---\n",
        "      inter_tensor = self.print_interactions(return_tensor=True, verbose=False)\n",
        "      if inter_tensor is None or inter_tensor.shape[2] == 0:\n",
        "          inter_mean = np.zeros((D, D))\n",
        "      else:\n",
        "          inter_mean = inter_tensor.mean(axis=2)  # shape (D,D)\n",
        "\n",
        "      fmt_mean = fuzzy_tensor.mean(axis=2)  # shape (D,D)\n",
        "\n",
        "      corr_penalty = 0.0\n",
        "      for i in range(D):\n",
        "          fmt_vec = fmt_mean[i, :]\n",
        "          inter_vec = inter_mean[i, :]\n",
        "          if np.std(fmt_vec) > 1e-8 and np.std(inter_vec) > 1e-8:\n",
        "              corr = np.corrcoef(fmt_vec, inter_vec)[0, 1]\n",
        "              corr_penalty += abs(corr) ** 2\n",
        "\n",
        "      corr_penalty /= D\n",
        "      combined_score = node_contributions.sum() - corr_penalty * 500  # strong decorrelation\n",
        "\n",
        "      # --- Store for plotting / further analysis ---\n",
        "      self.capped_node_metrics = node_metrics_list\n",
        "      self.node_score_contributions = node_contributions\n",
        "      self.correlation_penalty = corr_penalty\n",
        "\n",
        "      return node_metrics_list, combined_score, node_contributions\n",
        "\n",
        "\n",
        "\n",
        "    def run(self, outer_generations=outer_generations):\n",
        "        final_metrics = None\n",
        "\n",
        "        for gen in range(outer_generations):\n",
        "            mi_scores = []\n",
        "            node_metrics_list = []\n",
        "\n",
        "            for node_idx in range(self.D_graph):\n",
        "                # Full target vector for this node\n",
        "                full_target = self.synthetic_targets[node_idx]['target']\n",
        "\n",
        "                # Use the candidate dimension assigned to this node\n",
        "                D_fcm = self.candidate_dims[node_idx]\n",
        "                target = full_target[:D_fcm]  # slice according to candidate_dims\n",
        "\n",
        "                # Run inner FCM\n",
        "                _, _, _, mi_score, metrics = self.run_inner(node_idx, target, D_fcm)\n",
        "                mi_scores.append(mi_score)\n",
        "                node_metrics_list.append(metrics)\n",
        "\n",
        "\n",
        "            # --- Outer loop uses decorrelated metrics ---\n",
        "            self.capped_node_metrics = node_metrics_list\n",
        "            _, capped_score, node_contributions = self.run_outer()  # uses self.capped_node_metrics\n",
        "\n",
        "            print(f\"\\n--- Generation {gen} Metrics ---\")\n",
        "            for i, m in enumerate(node_metrics_list):\n",
        "                print(f\"Node {i} | \" + \" | \".join([f\"{k}: {v:.2f}\" for k,v in m.items()]))\n",
        "\n",
        "            print(f\"\\n--- Generation {gen} Node Contributions ---\")\n",
        "            for i, c in enumerate(node_contributions):\n",
        "                print(f\"Node {i}: Contribution = {c:.4f}\")\n",
        "\n",
        "            print(f\"Outer Score (capped): {capped_score:.3f}\")\n",
        "\n",
        "            final_metrics = node_metrics_list\n",
        "\n",
        "        return final_metrics\n",
        "\n",
        "\n",
        "    # ---------- VISUALIZATIONS ----------\n",
        "    # ---------- VISUALIZATIONS ----------\n",
        "    def plot_pointwise_minmax_elite(self, top_k=21):\n",
        "        plt.figure(figsize=(14,3))\n",
        "        for i in range(self.D_graph):\n",
        "            # Node's actual dimension\n",
        "            dim_i = self.candidate_dims[i]\n",
        "            base = self.nested_reps[i][:dim_i]  # slice to candidate dim\n",
        "            reps = np.clip(base + np.random.normal(0,0.05,(top_k,len(base))),0,1)\n",
        "            y_min, y_max = reps.min(axis=0), reps.max(axis=0)\n",
        "            y_sel = base\n",
        "\n",
        "            # True target for this node, sliced to candidate dim\n",
        "            y_true = self.synthetic_targets[i]['target'][:len(y_sel)]\n",
        "            if len(y_true) < len(y_sel):\n",
        "                y_true = np.pad(y_true, (0, len(y_sel)-len(y_true)), \"constant\")\n",
        "            else:\n",
        "                y_true = y_true[:len(y_sel)]\n",
        "\n",
        "            plt.subplot(1,self.D_graph,i+1)\n",
        "            plt.fill_between(range(len(y_min)),y_min,y_max,color='skyblue',alpha=0.4,label='Elite Interval')\n",
        "            plt.plot(y_sel,'k-',lw=2,label='Estimated')\n",
        "            plt.plot(y_true,'r--',lw=2,label='True')\n",
        "            plt.ylim(0,1.05)\n",
        "            plt.title(f\"Node {i+1}\")\n",
        "            if i==0: plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def plot_nested_activations(self):\n",
        "        plt.figure(figsize=(12,3))\n",
        "        for i,rep in enumerate(self.nested_reps):\n",
        "            dim_i = self.candidate_dims[i]\n",
        "            rep_i = rep[:dim_i]  # slice to candidate dim\n",
        "            plt.subplot(1,self.D_graph,i+1)\n",
        "            plt.bar(range(len(rep_i)), rep_i, color=plt.cm.plasma(rep_i))\n",
        "            plt.ylim(0,1)\n",
        "            plt.title(f\"Node {i+1}\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def plot_outer_fuzzy_graph(self):\n",
        "        G = nx.DiGraph()\n",
        "        for i in range(self.D_graph): G.add_node(i)\n",
        "        for i in range(self.D_graph):\n",
        "            for j in range(self.D_graph):\n",
        "                if i!=j and abs(self.chosen_Gmat[i,j])>0.02:\n",
        "                    G.add_edge(i,j,weight=self.chosen_Gmat[i,j])\n",
        "        node_sizes = [self.best_dim_per_node[i]*200 for i in range(self.D_graph)]\n",
        "        edge_colors = ['green' if d['weight']>0 else 'red' for _,_,d in G.edges(data=True)]\n",
        "        edge_widths = [abs(d['weight'])*3 for _,_,d in G.edges(data=True)]\n",
        "        pos = nx.circular_layout(G)\n",
        "        plt.figure(figsize=(6,6))\n",
        "        nx.draw(G,pos,node_size=node_sizes,node_color='skyblue',\n",
        "                edge_color=edge_colors,width=edge_widths,arrows=True,with_labels=True)\n",
        "        plt.title(\"Outer Fuzzy Multiplex Graph\")\n",
        "        plt.show()\n",
        "# ---------------- INTERACTIONS INSPECTOR ----------------\n",
        "\n",
        "    def print_interactions(self, return_tensor=True, verbose=True):\n",
        "            D_graph = self.D_graph\n",
        "            inter_dim = self.inter_layer.inter_dim\n",
        "            inter_tensor = np.zeros((D_graph, D_graph, inter_dim))\n",
        "\n",
        "            acts = self.inter_layer.build_activations(self.chosen_Gmat, self.nested_reps)\n",
        "            if not acts:\n",
        "                if verbose:\n",
        "                    print(\"No active edges above threshold.\")\n",
        "                return inter_tensor if return_tensor else None\n",
        "\n",
        "            for (i, j), vec in acts.items():\n",
        "                inter_tensor[i, j, :] = vec\n",
        "                if verbose:\n",
        "                    act_str = \", \".join([f\"{v:.3f}\" for v in vec])\n",
        "                    print(f\"Node {i} -> Node {j}: [{act_str}]\")\n",
        "            return inter_tensor if return_tensor else None\n",
        "\n",
        "        # Move these outside of print_interactions (class-level)\n",
        "    def print_l2_summary(self):\n",
        "            print(\"\\nL2 Distances to Target per Node:\")\n",
        "            for idx, (before, after) in enumerate(zip(self.l2_before, self.l2_after)):\n",
        "                print(f\"Node {idx}: Before={before:.4f}, After={after:.4f}\")\n",
        "\n",
        "    def compute_fuzzy_metric_tensor(self, normalize=True, verbose=False):\n",
        "            \"\"\"\n",
        "            Computes a Fuzzy Metric Tensor (D_graph x D_graph x num_metrics)\n",
        "            using current nested reps and node metrics.\n",
        "            Each slice [i,j,:] represents metrics of node j (optionally weighted by Gmat[i,j])\n",
        "            \"\"\"\n",
        "            metrics_keys = ['wait', 'throughput', 'util', 'patience']\n",
        "            D = self.D_graph\n",
        "            num_metrics = len(metrics_keys)\n",
        "            tensor = np.zeros((D, D, num_metrics))\n",
        "\n",
        "            metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "\n",
        "            node_metrics = []\n",
        "            for i, rep in enumerate(self.nested_reps):\n",
        "                metrics = metrics_evaluator.compute_node_metrics(i, y=rep)\n",
        "                node_metrics.append(np.array([metrics[k] for k in metrics_keys]))\n",
        "            node_metrics = np.array(node_metrics)  # (D, num_metrics)\n",
        "\n",
        "            for i in range(D):\n",
        "                for j in range(D):\n",
        "                    if i==j:\n",
        "                        tensor[i,j,:] = node_metrics[j]\n",
        "                    else:\n",
        "                        weight = np.clip(abs(self.chosen_Gmat[i,j]), 0, 1)\n",
        "                        tensor[i,j,:] = weight * node_metrics[j]\n",
        "\n",
        "            if normalize:\n",
        "                tensor = (tensor - tensor.min()) / (tensor.max() - tensor.min() + 1e-12)\n",
        "\n",
        "            if verbose:\n",
        "                print(\"Fuzzy Metric Tensor shape:\", tensor.shape)\n",
        "\n",
        "            return tensor\n",
        "\n",
        "\n",
        "\n",
        "    def compute_fmt_shrink_factor(self, fmt_bounds, metrics_keys=None):\n",
        "        \"\"\"\n",
        "        Returns shrink factor per node and metric.\n",
        "        shrink_factor = 1 - (current_interval / original_interval)\n",
        "        \"\"\"\n",
        "        if metrics_keys is None:\n",
        "            metrics_keys = ['wait','throughput','util','patience']\n",
        "\n",
        "        D = self.D_graph\n",
        "        num_metrics = len(metrics_keys)\n",
        "        shrink_factors = np.zeros((D, num_metrics))\n",
        "\n",
        "        for i in range(D):\n",
        "            for k in range(num_metrics):\n",
        "                lower, upper = fmt_bounds[i, i, k, 0], fmt_bounds[i, i, k, 1]  # self-node interval\n",
        "                interval_width = upper - lower + 1e-12  # normalized [0,1]\n",
        "                shrink_factors[i, k] = 1 - interval_width  # more shrink = higher value\n",
        "\n",
        "        return shrink_factors\n",
        "\n",
        "    def compute_fmt_with_bounds_adaptive(self, top_k=21, max_shrink=0.5, metrics_keys=None):\n",
        "        \"\"\"\n",
        "        Computes FMT bounds using pointwise min/max across elite solutions,\n",
        "        and applies dynamic adaptive shrinking where variability is low.\n",
        "        Returns tensor shape (D,D,num_metrics,2) [lower, upper].\n",
        "        \"\"\"\n",
        "        if metrics_keys is None:\n",
        "            metrics_keys = ['wait','throughput','util','patience']\n",
        "\n",
        "        D = self.D_graph\n",
        "        num_metrics = len(metrics_keys)\n",
        "        tensor_bounds = np.zeros((D, D, num_metrics, 2))\n",
        "        metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "\n",
        "        variability = np.zeros((D, num_metrics))\n",
        "\n",
        "        # Step 1: compute bounds from perturbations\n",
        "        for i in range(D):\n",
        "            base = self.nested_reps[i]\n",
        "            reps = np.clip(base + np.random.normal(0, 0.05, (top_k, len(base))), 0, 1)\n",
        "            metrics_matrix = np.zeros((top_k, num_metrics))\n",
        "            for idx, rep in enumerate(reps):\n",
        "                m = metrics_evaluator.compute_node_metrics(i, y=rep)\n",
        "                metrics_matrix[idx, :] = [m[k] for k in metrics_keys]\n",
        "\n",
        "            lower_i = metrics_matrix.min(axis=0)\n",
        "            upper_i = metrics_matrix.max(axis=0)\n",
        "            tensor_bounds[i, :, :, 0] = lower_i[np.newaxis, :]  # broadcast to all j\n",
        "            tensor_bounds[i, :, :, 1] = upper_i[np.newaxis, :]\n",
        "            variability[i, :] = metrics_matrix.std(axis=0)\n",
        "\n",
        "        # Step 2: adaptive shrinking\n",
        "        for i in range(D):\n",
        "            for j in range(D):\n",
        "                for k in range(num_metrics):\n",
        "                    lower, upper = tensor_bounds[i,j,k,0], tensor_bounds[i,j,k,1]\n",
        "                    mean = (lower + upper)/2\n",
        "                    var_norm = min(1.0, variability[i,k]/(upper-lower + 1e-12))\n",
        "                    shrink_factor = max_shrink * (1 - var_norm)\n",
        "                    tensor_bounds[i,j,k,0] = mean - shrink_factor*(mean - lower)\n",
        "                    tensor_bounds[i,j,k,1] = mean + shrink_factor*(upper - mean)\n",
        "\n",
        "        return tensor_bounds\n",
        "\n",
        "\n",
        "\n",
        "    def plot_fuzzy_metric_tensor_heatmaps(self, fuzzy_tensor=None, metrics_keys=['wait','throughput','util','patience']):\n",
        "        \"\"\"\n",
        "        Plot a heatmap panel for each metric in the FMT.\n",
        "        Rows: source node i\n",
        "        Columns: target node j\n",
        "        \"\"\"\n",
        "        if fuzzy_tensor is None:\n",
        "            fuzzy_tensor = self.compute_fuzzy_metric_tensor(normalize=True)\n",
        "\n",
        "        D = self.D_graph\n",
        "        num_metrics = len(metrics_keys)\n",
        "\n",
        "        fig, axes = plt.subplots(1, num_metrics, figsize=(4*num_metrics,4))\n",
        "        if num_metrics == 1: axes = [axes]\n",
        "\n",
        "        for k, key in enumerate(metrics_keys):\n",
        "            data = fuzzy_tensor[:,:,k]\n",
        "            im = axes[k].imshow(data, cmap='viridis', vmin=0, vmax=1)\n",
        "            for i in range(D):\n",
        "                for j in range(D):\n",
        "                    axes[k].text(j,i,f\"{data[i,j]:.2f}\",ha='center',va='center',color='white',fontsize=9)\n",
        "            axes[k].set_xticks(range(D))\n",
        "            axes[k].set_yticks(range(D))\n",
        "            axes[k].set_xticklabels([f'Node {j}' for j in range(D)])\n",
        "            axes[k].set_yticklabels([f'Node {i}' for i in range(D)])\n",
        "            axes[k].set_title(f'FMT - {key}')\n",
        "\n",
        "        fig.colorbar(im, ax=axes, orientation='vertical', fraction=0.025, pad=0.04, label='Normalized Metric Value')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def compute_fmt_with_elite_bounds(self, top_k=21):\n",
        "        \"\"\"\n",
        "        Computes FMT bounds using pointwise min/max across elite solutions.\n",
        "        Returns tensor shape (D,D,num_metrics,2) [lower, upper].\n",
        "        \"\"\"\n",
        "        metrics_keys = ['wait','throughput','util','patience']\n",
        "        D = self.D_graph\n",
        "        num_metrics = len(metrics_keys)\n",
        "        tensor_bounds = np.zeros((D,D,num_metrics,2))\n",
        "\n",
        "        metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "\n",
        "        for i in range(D):\n",
        "            # Generate top_k perturbations around current nested_rep (like in plot_pointwise_minmax_elite)\n",
        "            base = self.nested_reps[i]\n",
        "            reps = np.clip(base + np.random.normal(0,0.05,(top_k,len(base))),0,1)\n",
        "\n",
        "            # Compute node metrics for each perturbed solution\n",
        "            metrics_matrix = np.zeros((top_k, num_metrics))\n",
        "            for idx, rep in enumerate(reps):\n",
        "                m = metrics_evaluator.compute_node_metrics(i, y=rep)\n",
        "                metrics_matrix[idx,:] = [m[k] for k in metrics_keys]\n",
        "\n",
        "            # Compute pointwise min/max across elite solutions\n",
        "            lower_i = metrics_matrix.min(axis=0)\n",
        "            upper_i = metrics_matrix.max(axis=0)\n",
        "\n",
        "            # Fill bounds tensor for all source nodes (i->j)\n",
        "            for j in range(D):\n",
        "                tensor_bounds[i,j,:,0] = lower_i\n",
        "                tensor_bounds[i,j,:,1] = upper_i\n",
        "\n",
        "        return tensor_bounds\n",
        "\n",
        "\n",
        "    def plot_fmt_with_bounds(self, fmt_tensor_bounds):\n",
        "        D = self.D_graph\n",
        "        metrics_keys = ['wait','throughput','util','patience']\n",
        "        num_metrics = len(metrics_keys)\n",
        "\n",
        "        fig, axes = plt.subplots(1, num_metrics, figsize=(4*num_metrics,4))\n",
        "        if num_metrics == 1: axes = [axes]\n",
        "\n",
        "        for k, key in enumerate(metrics_keys):\n",
        "            lower = fmt_tensor_bounds[:,:,k,0]\n",
        "            upper = fmt_tensor_bounds[:,:,k,1]\n",
        "            mean_vals = (lower+upper)/2\n",
        "            range_vals = upper-lower\n",
        "            max_range = range_vals.max() if range_vals.max()>0 else 1.0\n",
        "            alphas = 0.2 + 0.8 * range_vals/max_range\n",
        "\n",
        "            im = axes[k].imshow(mean_vals, cmap='viridis', vmin=0, vmax=mean_vals.max())\n",
        "            for i in range(D):\n",
        "                for j in range(D):\n",
        "                    alpha_val = np.clip(1-alphas[i,j],0,1)\n",
        "                    rect = plt.Rectangle((j-0.5,i-0.5),1,1,color='white',alpha=alpha_val)\n",
        "                    axes[k].add_patch(rect)\n",
        "                    axes[k].text(j,i,f\"{lower[i,j]:.1f}\\n{upper[i,j]:.1f}\",ha='center',va='center',fontsize=8)\n",
        "            axes[k].set_title(f'FMT Bounds - {key}')\n",
        "            axes[k].set_xticks(range(D))\n",
        "            axes[k].set_yticks(range(D))\n",
        "            axes[k].set_xticklabels([f'Node {j}' for j in range(D)])\n",
        "            axes[k].set_yticklabels([f'Node {i}' for i in range(D)])\n",
        "\n",
        "        fig.colorbar(im, ax=axes, orientation='vertical', fraction=0.025, pad=0.04, label='Mean Metric Value')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    def plot_node_score_contribution(self, metrics_keys=['wait','throughput','util','patience']):\n",
        "        \"\"\"\n",
        "        Plot per-node total score contribution in the SAME STYLE as the FMT plots:\n",
        "            - uses imshow\n",
        "            - one panel for: raw, FMT, and total stacked\n",
        "            - diagonal masked\n",
        "            - annotated cells\n",
        "            - node contribution highlighted like your FMT code\n",
        "        \"\"\"\n",
        "        # ---------------------------------------------------------------------\n",
        "        # 1. Collect node contributions from run_outer()\n",
        "        # ---------------------------------------------------------------------\n",
        "        _, _, node_contributions = self.run_outer()\n",
        "        node_contributions = np.array(node_contributions)\n",
        "        D = len(node_contributions)\n",
        "\n",
        "        # ---------------------------------------------------------------------\n",
        "        # 2. Recompute FMT influence (same style as your FMT plots)\n",
        "        # ---------------------------------------------------------------------\n",
        "        fuzzy_tensor = self.compute_fuzzy_metric_tensor(normalize=True)\n",
        "        total_tensor = fuzzy_tensor.sum(axis=2)           # sum over metrics\n",
        "        fmt_tensor = total_tensor.copy()\n",
        "        np.fill_diagonal(fmt_tensor, 0)                   # mask diagonal\n",
        "\n",
        "        fmt_per_node = fmt_tensor.sum(axis=1)             # row sum\n",
        "        raw_per_node = node_contributions - fmt_per_node  # everything else\n",
        "\n",
        "        # Construct matrices for plotting (DD)\n",
        "        raw_matrix = np.zeros((D, D))\n",
        "        fmt_matrix = fmt_tensor\n",
        "        total_matrix = raw_matrix + fmt_matrix            # raw only on diagonal? no  distribute raw as row diag\n",
        "        np.fill_diagonal(raw_matrix, raw_per_node)\n",
        "        total_matrix = raw_matrix + fmt_matrix\n",
        "\n",
        "        # ---------------------------------------------------------------------\n",
        "        # 3. Plot - 3 subplots in SAME STYLE as FMT panels\n",
        "        # ---------------------------------------------------------------------\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "        matrices = [raw_matrix, fmt_matrix, total_matrix]\n",
        "        titles = [\"Raw Node Contribution\", \"FMT Interaction Contribution\", \"Total Contribution\"]\n",
        "\n",
        "        for ax, mat, title in zip(axes, matrices, titles):\n",
        "\n",
        "            im = ax.imshow(mat, cmap='viridis', vmin=np.min(mat), vmax=np.max(mat))\n",
        "\n",
        "            # annotate values\n",
        "            for i in range(D):\n",
        "                for j in range(D):\n",
        "                    val = mat[i, j]\n",
        "                    ax.text(j, i, f\"{val:.2f}\", ha='center',\n",
        "                            va='center', color='white', fontsize=8)\n",
        "\n",
        "            ax.set_title(title)\n",
        "            ax.set_xticks(range(D))\n",
        "            ax.set_xticklabels([f\"Node {i+1}\" for i in range(D)])\n",
        "            ax.set_yticks(range(D))\n",
        "            ax.set_yticklabels([f\"Node {i+1}\" for i in range(D)])\n",
        "\n",
        "        fig.colorbar(im, ax=axes, orientation='vertical', fraction=0.025, pad=0.04,\n",
        "                    label='Contribution Value')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def correlate_fmt_interactions_per_node(self, fmt_bounds=None, interaction_tensor=None, verbose=True):\n",
        "        \"\"\"\n",
        "        Correlate the FMT bounds with inter-layer interactions per node and per metric.\n",
        "        Returns a dict of shape: {node_idx: {metric: {'r':..., 'p':...}}}.\n",
        "        \"\"\"\n",
        "        from scipy.stats import pearsonr\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        metrics_keys = ['wait','throughput','util','patience']\n",
        "        D = self.D_graph\n",
        "\n",
        "        # Compute tensors if not provided\n",
        "        if fmt_bounds is None:\n",
        "            fmt_bounds = self.compute_fmt_with_elite_bounds(top_k=21)\n",
        "        if interaction_tensor is None:\n",
        "            interaction_tensor = self.print_interactions(return_tensor=True, verbose=False)\n",
        "\n",
        "        # Reduce interaction tensor along inter_dim\n",
        "        inter_mean = interaction_tensor.mean(axis=2)  # (D,D)\n",
        "\n",
        "        node_correlations = {}\n",
        "\n",
        "        for i in range(D):\n",
        "            node_correlations[i] = {}\n",
        "            for k, key in enumerate(metrics_keys):\n",
        "                # FMT bounds for target node j from source i (mean of lower/upper)\n",
        "                fmt_mean = fmt_bounds[i,:,k,:].mean(axis=1)  # shape (D,)\n",
        "                # Interaction tensor for edges from node i to j\n",
        "                inter_vec = inter_mean[i,:]  # shape (D,)\n",
        "                # Pearson correlation\n",
        "                corr, pval = pearsonr(fmt_mean, inter_vec)\n",
        "                node_correlations[i][key] = {'r': corr, 'p': pval}\n",
        "\n",
        "                if verbose:\n",
        "                    print(f\"Node {i} | {key}: r = {corr:.3f}, p = {pval:.3e}\")\n",
        "                    plt.figure(figsize=(4,3))\n",
        "                    plt.scatter(fmt_mean, inter_vec, alpha=0.7, edgecolor='k', color='skyblue')\n",
        "                    plt.xlabel(f\"FMT {key} (Node {i} -> others)\")\n",
        "                    plt.ylabel(f\"Interaction mean (Node {i} -> others)\")\n",
        "                    plt.title(f\"Node {i} | {key} correlation: r={corr:.3f}\")\n",
        "                    plt.grid(True)\n",
        "                    plt.show()\n",
        "\n",
        "        return node_correlations\n",
        "\n",
        "    def correlation_penalty(self, fmt_bounds=None, interaction_tensor=None):\n",
        "        \"\"\"\n",
        "        Computes a penalty term that is high if per-node FMT metrics correlate with interactions.\n",
        "        Returns total penalty to subtract from the outer score.\n",
        "        \"\"\"\n",
        "        from scipy.stats import pearsonr\n",
        "\n",
        "        D = self.D_graph\n",
        "        metrics_keys = ['wait','throughput','util','patience']\n",
        "\n",
        "        if fmt_bounds is None:\n",
        "            fmt_bounds = self.compute_fmt_with_elite_bounds(top_k=top_k)\n",
        "        if interaction_tensor is None:\n",
        "            interaction_tensor = self.print_interactions(return_tensor=True, verbose=False)\n",
        "\n",
        "        inter_mean = interaction_tensor.mean(axis=2)\n",
        "        total_penalty = 0.0\n",
        "\n",
        "        for i in range(D):\n",
        "            for k in range(len(metrics_keys)):\n",
        "                fmt_mean = fmt_bounds[i,:,k,:].mean(axis=1)\n",
        "                inter_vec = inter_mean[i,:]\n",
        "                if np.std(fmt_mean) > 1e-8 and np.std(inter_vec) > 1e-8:\n",
        "                    corr, _ = pearsonr(fmt_mean, inter_vec)\n",
        "                    total_penalty += abs(corr)  # penalize high correlation\n",
        "\n",
        "        # normalize by number of nodes  metrics\n",
        "        total_penalty /= (D * len(metrics_keys))**2\n",
        "        return total_penalty\n",
        "\n",
        "\n",
        "# ---------------- USAGE ----------------\n",
        "if __name__ == \"__main__\":\n",
        "    optimizer = GDFCM(\n",
        "        candidate_dims, D_graph,\n",
        "        inner_archive_size, inner_offspring,\n",
        "        outer_archive_size, outer_offspring,\n",
        "        synthetic_targets,\n",
        "        inner_learning, gamma_interlayer=1,\n",
        "        causal_flag=False\n",
        "    )\n",
        "    metrics_list = optimizer.run()\n",
        "    optimizer.plot_pointwise_minmax_elite()\n",
        "    optimizer.plot_nested_activations()\n",
        "    # Compute FMT with elite bounds\n",
        "    fmt_elite_bounds = optimizer.compute_fmt_with_elite_bounds(top_k=top_k+10)\n",
        "\n",
        "# Plot as heatmaps\n",
        "    optimizer.plot_fmt_with_bounds(fmt_elite_bounds)\n",
        "\n",
        "    # Compute fuzzy multiplex tensor\n",
        "    fmt_tensor = optimizer.compute_fuzzy_metric_tensor(normalize=True)\n",
        "    optimizer.plot_fuzzy_metric_tensor_heatmaps(fmt_tensor)\n",
        "\n",
        "    # Compute FMT with bounds (minimax elite intervals)\n",
        "    optimizer.plot_node_score_contribution()\n",
        "    optimizer.plot_outer_fuzzy_graph()\n",
        "  #  optimizer.print_interactions()\n",
        "    tensor = optimizer.print_interactions()\n",
        "\n",
        "    print(\"Tensor shape:\", tensor.shape,'\\n',tensor)\n",
        "    # Compute tensors first\n",
        "    fmt_elite_bounds = optimizer.compute_fmt_with_elite_bounds(top_k=top_k)\n",
        "    interaction_tensor = optimizer.print_interactions(return_tensor=True, verbose=False)\n",
        "\n",
        "\n",
        "    interaction_tensor = optimizer.print_interactions(return_tensor=True, verbose=False)\n",
        "\n",
        "    # Get per-node, per-metric correlations\n",
        "   # node_metric_corrs = optimizer.correlate_fmt_interactions_per_node(\n",
        "    #    fmt_bounds=fmt_elite_bounds,\n",
        "     #   interaction_tensor=interaction_tensor\n",
        "    #)\n",
        "    import networkx as nx\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    from mpl_toolkits.mplot3d import Axes3D\n",
        "D_graph = len(optimizer.nested_reps)\n",
        "tensor = optimizer.print_interactions(return_tensor=True, verbose=False)\n",
        "\n",
        "# ---------------- Outer nodes (hubs) ----------------\n",
        "G_outer = nx.DiGraph()\n",
        "for i in range(D_graph):\n",
        "    G_outer.add_node(i)\n",
        "for i in range(D_graph):\n",
        "    for j in range(D_graph):\n",
        "        if i != j and np.any(tensor[i,j,:] != 0):\n",
        "            # Shift to signed weights: 0.5 -> 0, <0.5 negative, >0.5 positive\n",
        "            mean_weight = 2 * (np.mean(tensor[i,j,:]) - 0.5)\n",
        "            G_outer.add_edge(i, j, weight=mean_weight)\n",
        "\n",
        "# Outer spring layout\n",
        "pos_outer_2d = nx.circular_layout(G_outer, scale=5)\n",
        "pos_outer = np.array([[x, y, 0] for x, y in pos_outer_2d.values()])\n",
        "\n",
        "fig = plt.figure(figsize=(10,8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Plot outer nodes\n",
        "for i in range(D_graph):\n",
        "    ax.scatter(*pos_outer[i], s=300, color='skyblue')\n",
        "    ax.text(*pos_outer[i], f'Node {i}', color='black')\n",
        "\n",
        "# Plot outer edges with positive/negative colors\n",
        "\n",
        "for i, j, data in G_outer.edges(data=True):\n",
        "    x_vals = [pos_outer[i,0], pos_outer[j,0]]\n",
        "    y_vals = [pos_outer[i,1], pos_outer[j,1]]\n",
        "    z_vals = [pos_outer[i,2], pos_outer[j,2]]\n",
        "\n",
        "    # Positive = bright green, Negative = bright red\n",
        "    color = 'green' if data['weight'] > 0 else 'red'\n",
        "    linewidth = 2 + 4*abs(data['weight'])  # scale width by magnitude\n",
        "    ax.plot(x_vals, y_vals, z_vals, color=color, linewidth=linewidth)\n",
        "# ---------------- Inner FCMs (small circular around hub) ----------------\n",
        "for i, rep in enumerate(optimizer.nested_reps):\n",
        "    dims = len(rep)\n",
        "    angle = np.linspace(0, 2*np.pi, dims, endpoint=False)\n",
        "    radius = 0.8  # small circle\n",
        "    xs = pos_outer[i,0] + radius * np.cos(angle)\n",
        "    ys = pos_outer[i,1] + radius * np.sin(angle)\n",
        "    zs = pos_outer[i,2] + rep  # activation as height\n",
        "\n",
        "    # Plot inner nodes\n",
        "    ax.scatter(xs, ys, zs, c=rep, cmap='plasma', s=50)\n",
        "\n",
        "    # Connect inner nodes in circle\n",
        "    for k in range(dims):\n",
        "        ax.plot([xs[k], xs[(k+1)%dims]], [ys[k], ys[(k+1)%dims]], [zs[k], zs[(k+1)%dims]], color='gray', alpha=0.5)\n",
        "\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Activation')\n",
        "ax.set_title('Outer Nodes with Inner FCMs (Signed correlations)')\n",
        "plt.show()\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class GDFCMPredictorForward:\n",
        "    \"\"\"\n",
        "    Forward predictor for GDFCM.\n",
        "    Can handle new input data (DATA_MATRIX) and compute:\n",
        "        - Node activations\n",
        "        - Node metrics\n",
        "        - Inter-layer MI scores\n",
        "        - Total contributions and outer score\n",
        "    \"\"\"\n",
        "    def __init__(self, trained_gdfcm: GDFCM):\n",
        "        self.gdfcm = trained_gdfcm\n",
        "        self.D_graph = trained_gdfcm.D_graph\n",
        "        self.nested_reps = trained_gdfcm.nested_reps\n",
        "        self.chosen_Gmat = trained_gdfcm.chosen_Gmat\n",
        "        self.inter_layer = trained_gdfcm.inter_layer\n",
        "\n",
        "    def predict_node(self, node_idx, data_row):\n",
        "        \"\"\"\n",
        "        Predict metrics and activations for a single node given a new data row.\n",
        "        \"\"\"\n",
        "        # Use stored nested_rep as starting point\n",
        "        x = self.nested_reps[node_idx].copy()\n",
        "        # Optional: you could combine with data_row to modify x\n",
        "        # For now, we just compute metrics using data_row as the input\n",
        "        metrics_evaluator = MetricsEvaluator(np.array([data_row]))\n",
        "        metrics = metrics_evaluator.compute_node_metrics(0, y=x)\n",
        "\n",
        "        # Inter-layer MI\n",
        "        mi_score = self.inter_layer.mi_for_graph(self.chosen_Gmat, self.nested_reps)\n",
        "\n",
        "        return {'activations': x, 'metrics': metrics, 'mi_score': mi_score}\n",
        "\n",
        "    def predict_all_nodes(self, new_data_matrix):\n",
        "        \"\"\"\n",
        "        Predict metrics and activations for all nodes using new data.\n",
        "        new_data_matrix: shape (D_graph, num_features)\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        for node_idx in range(self.D_graph):\n",
        "            data_row = new_data_matrix[node_idx % len(new_data_matrix)]\n",
        "            node_result = self.predict_node(node_idx, data_row)\n",
        "            results.append(node_result)\n",
        "        return results\n",
        "\n",
        "    def predict_scores(self):\n",
        "        \"\"\"\n",
        "        Compute per-node contributions and total outer score.\n",
        "        \"\"\"\n",
        "        _, total_score, node_contributions = self.gdfcm.run_outer()\n",
        "        return {'node_contributions': node_contributions, 'total_score': total_score}\n",
        "# Suppose optimizer is your trained GDFCM\n",
        "predictor = GDFCMPredictorForward(optimizer)\n",
        "\n",
        "# New data: same number of nodes, each with same num_features\n",
        "new_DATA_MATRIX = np.random.rand(D_graph, DATA_MATRIX.shape[1])\n",
        "\n",
        "# Predict all nodes\n",
        "predictions = predictor.predict_all_nodes(new_DATA_MATRIX)\n",
        "\n",
        "for idx, node_pred in enumerate(predictions):\n",
        "    print(f\"Node {idx} Metrics:\", node_pred['metrics'])\n",
        "    print(f\"Node {idx} Activations:\", node_pred['activations'])\n",
        "    print(f\"Node {idx} MI Score:\", node_pred['mi_score'])\n",
        "\n",
        "# Optional: compute total contributions\n",
        "score_info = predictor.predict_scores()\n",
        "print(\"Node Contributions:\", score_info['node_contributions'])\n",
        "print(\"Total Score:\", score_info['total_score'])\n",
        "\n",
        "class GDFCMPredictorAdaptive:\n",
        "    \"\"\"\n",
        "    Adaptive forward predictor for GDFCM.\n",
        "    - Accepts new data per node.\n",
        "    - Updates activations slightly using a mini inner-loop.\n",
        "    - Computes metrics, inter-layer MI, and outer score contributions.\n",
        "    \"\"\"\n",
        "    def __init__(self, trained_gdfcm: GDFCM, lr_x=0.01, lr_steps=10):\n",
        "        self.gdfcm = trained_gdfcm\n",
        "        self.D_graph = trained_gdfcm.D_graph\n",
        "        self.nested_reps = [rep.copy() for rep in trained_gdfcm.nested_reps]\n",
        "        self.chosen_Gmat = trained_gdfcm.chosen_Gmat\n",
        "        self.inter_layer = trained_gdfcm.inter_layer\n",
        "        self.lr_x = lr_x\n",
        "        self.lr_steps = lr_steps\n",
        "\n",
        "    def adapt_node(self, node_idx, data_row):\n",
        "        \"\"\"\n",
        "        Mini inner-loop: slightly update activations based on new data row.\n",
        "        Handles shape mismatch by projecting new data to node activation dim.\n",
        "        \"\"\"\n",
        "        x = self.nested_reps[node_idx].copy()\n",
        "        dim = len(x)\n",
        "\n",
        "        # Simple linear projection of new data to activation space\n",
        "        if len(data_row) != dim:\n",
        "            # Use mean pooling to reduce or slice if too large\n",
        "            if len(data_row) > dim:\n",
        "                target = data_row[:dim]\n",
        "            else:\n",
        "                # Pad with 0.5\n",
        "                target = np.pad(data_row, (0, dim - len(data_row)), 'constant', constant_values=0.5)\n",
        "        else:\n",
        "            target = data_row\n",
        "\n",
        "        target = np.clip(target, 0, 1)\n",
        "\n",
        "        # Mini inner-loop update\n",
        "        for _ in range(self.lr_steps):\n",
        "            grad = x - target\n",
        "            x -= self.lr_x * grad\n",
        "            x = np.clip(x, 0, 1)\n",
        "\n",
        "        self.nested_reps[node_idx] = x\n",
        "\n",
        "        # Compute metrics\n",
        "        metrics_evaluator = MetricsEvaluator(np.array([data_row]))\n",
        "        metrics = metrics_evaluator.compute_node_metrics(0, y=x)\n",
        "\n",
        "        # Inter-layer MI\n",
        "        mi_score = self.inter_layer.mi_for_graph(self.chosen_Gmat, self.nested_reps)\n",
        "\n",
        "        return {'activations': x, 'metrics': metrics, 'mi_score': mi_score}\n",
        "\n",
        "\n",
        "    def adapt_all_nodes(self, new_data_matrix):\n",
        "        \"\"\"\n",
        "        Update activations for all nodes given new data matrix.\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        for node_idx in range(self.D_graph):\n",
        "            data_row = new_data_matrix[node_idx % len(new_data_matrix)]\n",
        "            node_result = self.adapt_node(node_idx, data_row)\n",
        "            results.append(node_result)\n",
        "        return results\n",
        "\n",
        "    def compute_outer_score(self):\n",
        "        \"\"\"\n",
        "        Compute node contributions and total outer score using current nested_reps.\n",
        "        \"\"\"\n",
        "        _, total_score, node_contributions = self.gdfcm.run_outer()\n",
        "        return {'node_contributions': node_contributions, 'total_score': total_score}\n",
        "# Initialize adaptive predictor\n",
        "adaptive_predictor = GDFCMPredictorAdaptive(optimizer, lr_x=0.02, lr_steps=15)\n",
        "\n",
        "# New data\n",
        "new_DATA_MATRIX = np.random.rand(D_graph, DATA_MATRIX.shape[1])\n",
        "\n",
        "# Adapt activations to new data\n",
        "predictions = adaptive_predictor.adapt_all_nodes(new_DATA_MATRIX)\n",
        "\n",
        "for idx, node_pred in enumerate(predictions):\n",
        "    print(f\"Node {idx} Metrics:\", node_pred['metrics'])\n",
        "    print(f\"Node {idx} Activations:\", node_pred['activations'])\n",
        "    print(f\"Node {idx} MI Score:\", node_pred['mi_score'])\n",
        "\n",
        "# Compute outer node contributions after adaptation\n",
        "score_info = adaptive_predictor.compute_outer_score()\n",
        "print(\"Node Contributions:\", score_info['node_contributions'])\n",
        "print(\"Total Score:\", score_info['total_score'])\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "pred_metrics = []\n",
        "true_metrics = []\n",
        "for idx, data_row in enumerate(new_DATA_MATRIX):\n",
        "    pred = adaptive_predictor.adapt_node(idx, data_row)\n",
        "    pred_metrics.append([pred['metrics'][k] for k in ['wait','throughput','util','patience']])\n",
        "    true_metrics.append([MetricsEvaluator(new_DATA_MATRIX).compute_node_metrics(idx)[k]\n",
        "                         for k in ['wait','throughput','util','patience']])\n",
        "\n",
        "pred_metrics = np.array(pred_metrics)\n",
        "true_metrics = np.array(true_metrics)\n",
        "\n",
        "\n",
        "def evaluate_predictions(true_metrics, pred_metrics, metric_names=None):\n",
        "    \"\"\"\n",
        "    Evaluate multi-metric predictions per node and overall.\n",
        "\n",
        "    Args:\n",
        "        true_metrics: np.array, shape (num_nodes, num_metrics)\n",
        "        pred_metrics: np.array, same shape as true_metrics\n",
        "        metric_names: list of metric names (optional)\n",
        "\n",
        "    Returns:\n",
        "        dict with evaluation metrics\n",
        "    \"\"\"\n",
        "    true_metrics = np.array(true_metrics)\n",
        "    pred_metrics = np.array(pred_metrics)\n",
        "\n",
        "    if metric_names is None:\n",
        "        metric_names = [f'Metric {i}' for i in range(true_metrics.shape[1])]\n",
        "\n",
        "    num_nodes, num_metrics = true_metrics.shape\n",
        "\n",
        "    results = {'per_metric': {}, 'overall': {}}\n",
        "\n",
        "    # Per metric\n",
        "    for i, name in enumerate(metric_names):\n",
        "        t = true_metrics[:, i]\n",
        "        p = pred_metrics[:, i]\n",
        "        mae = mean_absolute_error(t, p)\n",
        "        rmse = np.sqrt(mean_squared_error(t, p))\n",
        "        r2 = r2_score(t, p)\n",
        "        corr = np.corrcoef(t, p)[0,1]\n",
        "        mape = np.mean(np.abs((t - p) / (t + 1e-12))) * 100\n",
        "        results['per_metric'][name] = {\n",
        "            'MAE': mae,\n",
        "            'RMSE': rmse,\n",
        "            'R2': r2,\n",
        "            'Pearson': corr,\n",
        "            'MAPE (%)': mape\n",
        "        }\n",
        "\n",
        "    # Overall\n",
        "    mae_overall = mean_absolute_error(true_metrics.flatten(), pred_metrics.flatten())\n",
        "    rmse_overall = np.sqrt(mean_squared_error(true_metrics.flatten(), pred_metrics.flatten()))\n",
        "    r2_overall = r2_score(true_metrics.flatten(), pred_metrics.flatten())\n",
        "\n",
        "    # Cosine similarity (averaged over nodes)\n",
        "    cos_sim = np.mean([cosine_similarity(t.reshape(1,-1), p.reshape(1,-1))[0,0]\n",
        "                       for t,p in zip(true_metrics, pred_metrics)])\n",
        "\n",
        "    results['overall'] = {\n",
        "        'MAE': mae_overall,\n",
        "        'RMSE': rmse_overall,\n",
        "        'R2': r2_overall,\n",
        "        'CosineSim': cos_sim\n",
        "    }\n",
        "\n",
        "    return results\n",
        "# Suppose true_metrics and pred_metrics have shape (D_graph, 4)\n",
        "metrics_eval = evaluate_predictions(true_metrics, pred_metrics, metric_names=['wait','throughput','util','patience'])\n",
        "\n",
        "# Print per-metric evaluation\n",
        "for metric, vals in metrics_eval['per_metric'].items():\n",
        "    print(f\"{metric}: {vals}\")\n",
        "\n",
        "# Print overall evaluation\n",
        "print(\"\\nOverall metrics:\", metrics_eval['overall'])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJJKLZdbh9FI"
      },
      "source": [
        "# MoE Neural Metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tT4Xz-tkpOs2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Full rewritten MetricsEvaluator with corrected CNN / RNN usage and training loop.\n",
        "# - Keeps external API identical: __init__(data_matrix, adj_matrices=None, ...)\n",
        "# - Internally the 4 subnets represent the 4 objectives (learned, no hardcoded numeric formulas)\n",
        "# - Training uses the dataset-derived objectives as targets (extracted from DATA_MATRIX via hardcoded columns)\n",
        "# - CNN and RNN handling fixed (shapes, squeezes) both in forward and train loop\n",
        "\n",
        "class MetricsEvaluator(nn.Module):\n",
        "    def __init__(self, data_matrix, adj_matrices=None, mi_weights=None,\n",
        "                 node_size=None, hidden_dim=512,\n",
        "                 use_cnn=True, cnn_channels=512, kernel_size=[2,3],\n",
        "                 use_rnn=False, rnn_hidden=256, prop_factor=0.0,num_metrics=4):\n",
        "        \"\"\"\n",
        "        data_matrix : numpy array (N_samples x N_features) containing stacked datasets\n",
        "                      (must contain the columns created in the synthetic generator).\n",
        "        adj_matrices : list of adjacency numpy arrays (optional)\n",
        "        mi_weights    : list of floats for weighting each adjacency layer (optional)\n",
        "        node_size     : how many columns per node to treat as its features (default 4)\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        self.num_metrics = num_metrics\n",
        "        # ---- public-like inputs (unchanged external API) ----\n",
        "        self.data_matrix = data_matrix\n",
        "        self.node_size = node_size or 4\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # ---- internal state ----\n",
        "        self.prev_metrics = torch.zeros(num_metrics)   # conditioning across nodes during forward/train\n",
        "        self.adj_matrices = adj_matrices\n",
        "        self.mi_weights = mi_weights\n",
        "        self.prop_factor = prop_factor\n",
        "\n",
        "        self.use_cnn = use_cnn\n",
        "        self.use_rnn = use_rnn\n",
        "\n",
        "        # ---------- Subnet input size ----------\n",
        "        self.sub_input_size = self.node_size + 3 + 4  # node_features + y(3) + prev_metrics(4)\n",
        "\n",
        "        # ---------- Objective-aligned subnet factories ----------\n",
        "        def subnet_wait():\n",
        "            return nn.Sequential(\n",
        "                nn.Linear(self.sub_input_size, hidden_dim),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(hidden_dim // 2, 1),\n",
        "                nn.Softplus()\n",
        "            )\n",
        "\n",
        "        def subnet_throughput():\n",
        "            return nn.Sequential(\n",
        "                nn.Linear(self.sub_input_size, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_dim, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_dim, 1),\n",
        "                nn.Softplus()\n",
        "            )\n",
        "\n",
        "        def subnet_util():\n",
        "            return nn.Sequential(\n",
        "                nn.Linear(self.sub_input_size, hidden_dim),\n",
        "                nn.Sigmoid(),\n",
        "                nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "                nn.Sigmoid(),\n",
        "                nn.Linear(hidden_dim // 2, 1),\n",
        "                nn.Softplus()\n",
        "            )\n",
        "\n",
        "        def subnet_patience():\n",
        "            return nn.Sequential(\n",
        "                nn.Linear(self.sub_input_size, hidden_dim),\n",
        "                nn.ELU(),\n",
        "                nn.Linear(hidden_dim, hidden_dim),\n",
        "                nn.ELU(),\n",
        "                nn.Linear(hidden_dim, 1),\n",
        "                nn.Softplus()\n",
        "            )\n",
        "\n",
        "        self.subnets = nn.ModuleDict({\n",
        "            \"wait\": subnet_wait(),\n",
        "            \"throughput\": subnet_throughput(),\n",
        "            \"util\": subnet_util(),\n",
        "            \"patience\": subnet_patience()\n",
        "        })\n",
        "\n",
        "        # ---------- Supernet ----------\n",
        "        super_in = self.node_size + 4 + 4  # node_features + raw_metrics + prev_metrics\n",
        "\n",
        "        if use_cnn:\n",
        "            # CNN pipeline: input shape -> (batch=1, channels=1, length=super_in)\n",
        "            # we will design convs to produce channel dimension = 4 and collapse length\n",
        "            self.supernet = nn.Sequential(\n",
        "                nn.Conv1d(1, cnn_channels, kernel_size, padding=kernel_size // 2),\n",
        "                nn.ReLU(),\n",
        "                nn.Conv1d(cnn_channels, 4, kernel_size=1),  # produce 4 channels\n",
        "                nn.AdaptiveAvgPool1d(1),                     # collapse length -> 1\n",
        "                nn.Flatten(),                                # shape (batch, 4)\n",
        "                nn.Softplus()\n",
        "            )\n",
        "        elif use_rnn:\n",
        "            self.supernet_rnn = nn.GRU(input_size=super_in, hidden_size=rnn_hidden, batch_first=True)\n",
        "            self.supernet_out = nn.Sequential(\n",
        "                nn.Linear(rnn_hidden, 4),\n",
        "                nn.Softplus()\n",
        "            )\n",
        "        else:\n",
        "            self.supernet = nn.Sequential(\n",
        "                nn.Linear(super_in, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_dim // 2, 4),\n",
        "                nn.Softplus()\n",
        "            )\n",
        "\n",
        "        # ---------- Feature -> Metric predictor ----------\n",
        "        self.predict_net = nn.Sequential(\n",
        "            nn.Linear(self.node_size, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim // 2, 4),\n",
        "            nn.Softplus()\n",
        "        )\n",
        "\n",
        "        # ---------- Hardcoded column indices (matches synthetic generation) ----------\n",
        "        # These indices were previously used in the notebook examples.\n",
        "        # If your DATA_MATRIX ordering differs, adjust accordingly.\n",
        "        self._cols = {\n",
        "    'travel_time': 0,\n",
        "    'load_utilization': 1,\n",
        "    'delivery_priority': 2,\n",
        "    'delay_probability': 3,\n",
        "    'congestion_score': 4,\n",
        "    'energy_level': 5,  # add this\n",
        "}\n",
        "\n",
        "     # -------------------------\n",
        "    # Extract ground-truth objectives for training\n",
        "    # -------------------------\n",
        "    def extract_objectives(self, node_idx):\n",
        "        N = self.data_matrix.shape[0]\n",
        "        r = self.data_matrix[node_idx % N]\n",
        "\n",
        "        travel_time = r[self._cols['travel_time']]\n",
        "        load_util = r[self._cols['load_utilization']]\n",
        "        priority = r[self._cols['delivery_priority']]\n",
        "        delay_prob = r[self._cols['delay_probability']]\n",
        "        congestion = r[self._cols['congestion_score']]\n",
        "        energy_level = r[self._cols['energy_level']] if 'energy_level' in self._cols else 0.5\n",
        "\n",
        "        # Original GA-style objectives\n",
        "        travel_efficiency = 1.0 / (1.0 + travel_time)\n",
        "        vehicle_efficiency = load_util\n",
        "        scheduling_score = priority * (1.0 - delay_prob)\n",
        "        congestion_score = 1.0 / (1.0 + congestion)\n",
        "\n",
        "        # GA-style energy contribution (synthetic, like in compute_node_metrics)\n",
        "        energy_score = np.sqrt(energy_level + 0.1)\n",
        "\n",
        "        return torch.tensor([\n",
        "            travel_efficiency,\n",
        "            vehicle_efficiency,\n",
        "            scheduling_score,\n",
        "            congestion_score,\n",
        "            energy_score  # matches GA contribution\n",
        "        ], dtype=torch.float32)\n",
        "\n",
        "    # -------------------------\n",
        "    # Compute node metrics (forward for a single node)\n",
        "    # -------------------------\n",
        "    def compute_node_metrics(self, node_idx, y=None):\n",
        "        # --- Extract node block safely ---\n",
        "        col_start = node_idx * self.node_size\n",
        "        node_block = self.data_matrix[:, col_start: col_start + self.node_size]\n",
        "\n",
        "        node_feats = node_block.mean(axis=0)\n",
        "        if len(node_feats) < self.node_size:\n",
        "            node_feats = np.pad(node_feats, (0, self.node_size - len(node_feats)), 'constant')\n",
        "\n",
        "        # --- Prepare external input y ---\n",
        "        if y is None:\n",
        "            y_arr = np.zeros(3)\n",
        "        else:\n",
        "            y_arr = np.array(y[:3])\n",
        "            if len(y_arr) < 3:\n",
        "                y_arr = np.pad(y_arr, (0, 3 - len(y_arr)), 'constant', constant_values=0.5)\n",
        "\n",
        "        # --- Build input for subnet ---\n",
        "        nn_input = torch.tensor(\n",
        "            np.concatenate([node_feats, y_arr, self.prev_metrics.numpy()]),\n",
        "            dtype=torch.float32\n",
        "        )\n",
        "\n",
        "        # --- Compute raw metrics from subnets ---\n",
        "        raw = torch.stack([\n",
        "            self.subnets['wait'](nn_input).squeeze(),\n",
        "            self.subnets['throughput'](nn_input).squeeze(),\n",
        "            self.subnets['util'](nn_input).squeeze(),\n",
        "            self.subnets['patience'](nn_input).squeeze()\n",
        "        ]) if self.subnets else torch.zeros(4)\n",
        "\n",
        "        # --- Supernet processing ---\n",
        "        super_in = torch.cat([torch.tensor(node_feats, dtype=torch.float32), raw, self.prev_metrics], dim=0)\n",
        "\n",
        "        if self.use_cnn and self.supernet is not None:\n",
        "            x = super_in.unsqueeze(0).unsqueeze(0)  # (1,1,L)\n",
        "            metrics = self.supernet(x).squeeze(0)\n",
        "        elif self.use_rnn and self.supernet_rnn is not None and self.supernet_out is not None:\n",
        "            x = super_in.unsqueeze(0).unsqueeze(1)  # (1,1,L)\n",
        "            rnn_out, _ = self.supernet_rnn(x)\n",
        "            metrics = self.supernet_out(rnn_out.squeeze(1)).squeeze(0)\n",
        "        elif self.supernet is not None:\n",
        "            metrics = self.supernet(super_in)\n",
        "        else:\n",
        "            metrics = raw  # fallback\n",
        "\n",
        "        # --- Optional graph propagation ---\n",
        "        if self.adj_matrices is not None and self.mi_weights is not None:\n",
        "            agg = metrics.clone()\n",
        "            for L, adj in enumerate(self.adj_matrices):\n",
        "                w = self.mi_weights[L]\n",
        "                row = adj[node_idx]\n",
        "                norm = row / (row.sum() + 1e-8)\n",
        "                neighbor = torch.matmul(norm, metrics.unsqueeze(0))\n",
        "                alpha = self.prop_factor * w\n",
        "                agg = (1 - alpha) * agg + alpha * neighbor.squeeze()\n",
        "            metrics = agg\n",
        "\n",
        "        # --- Update previous metrics ---\n",
        "        self.prev_metrics = metrics.detach()\n",
        "\n",
        "        # --- Aggregate score ---\n",
        "        score = metrics[0] + metrics[1] + metrics[2] + metrics[3]\n",
        "\n",
        "        return {\n",
        "            \"wait\": float(metrics[0].item()),\n",
        "            \"throughput\": float(metrics[1].item()),\n",
        "            \"util\": float(metrics[2].item()),\n",
        "            \"patience\": float(metrics[3].item()),\n",
        "            \"score\": float(score.item())\n",
        "\n",
        "        }\n",
        "\n",
        "    # -------------------------\n",
        "    # Training loop\n",
        "    # -------------------------\n",
        "    def train_from_system(self, epochs=500, lr=0.001,\n",
        "                          lambda_coord=0.1, lambda_act=0.01,\n",
        "                          lambda_corr=0.1, lambda_deco=0.1,\n",
        "                          lambda_pred=0.2, lambda_rel=0.05,\n",
        "                          lambda_feat=0.5, verbose=True):\n",
        "        \"\"\"\n",
        "        Trains the subnets + supernet to predict the four objective labels extracted from DATA_MATRIX.\n",
        "        Loss combines:\n",
        "         - obj_loss (MSE to true objectives)\n",
        "         - -score (maximize score)\n",
        "         - coord_loss (consistency across nodes)\n",
        "         - act_loss   (activation penalty)\n",
        "         - corr_loss  (feature->metric alignment)\n",
        "         - deco_loss  (decorrelation across nodes)\n",
        "         - pred_loss  (feature->metric predictor match)\n",
        "         - rel_loss   (relative variance encouragement)\n",
        "        \"\"\"\n",
        "\n",
        "        num_nodes = max(1, self.data_matrix.shape[1] // self.node_size)\n",
        "\n",
        "        # collect parameters\n",
        "        params = list(self.subnets.parameters()) + list(self.predict_net.parameters())\n",
        "        if self.use_cnn:\n",
        "            params += list(self.supernet.parameters())\n",
        "        elif self.use_rnn:\n",
        "            params += list(self.supernet_rnn.parameters()) + list(self.supernet_out.parameters())\n",
        "        else:\n",
        "            params += list(self.supernet.parameters())\n",
        "\n",
        "        optimizer = optim.Adam(params, lr=lr)\n",
        "        mse = nn.MSELoss()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self.prev_metrics = torch.zeros(4)\n",
        "            all_preds = []\n",
        "            obj_loss = torch.tensor(0.0, dtype=torch.float32)\n",
        "\n",
        "            # Forward pass: sequential per-node to respect prev_metrics conditioning\n",
        "            for node_idx in range(num_nodes):\n",
        "                # build node features\n",
        "                start = node_idx * self.node_size\n",
        "                block = self.data_matrix[:, start:start + self.node_size]\n",
        "                feats = block.mean(axis=0)\n",
        "                feats = np.pad(feats, (0, max(0, self.node_size - len(feats))), 'constant')\n",
        "\n",
        "                y = np.zeros(3)  # unchanged API - y generator could be added but kept simple here\n",
        "                nn_input = torch.tensor(np.concatenate([feats, y, self.prev_metrics.numpy()]), dtype=torch.float32)\n",
        "\n",
        "                # Subnets\n",
        "                raw = torch.stack([\n",
        "                    self.subnets['wait'](nn_input).squeeze(),\n",
        "                    self.subnets['throughput'](nn_input).squeeze(),\n",
        "                    self.subnets['util'](nn_input).squeeze(),\n",
        "                    self.subnets['patience'](nn_input).squeeze()\n",
        "                ])\n",
        "\n",
        "                # Supernet\n",
        "                super_in = torch.cat([torch.tensor(feats, dtype=torch.float32), raw, self.prev_metrics], dim=0)\n",
        "\n",
        "                if self.use_cnn:\n",
        "                    x = super_in.unsqueeze(0).unsqueeze(0)    # (1,1,L)\n",
        "                    preds = self.supernet(x).squeeze(0)       # (4,)\n",
        "                elif self.use_rnn:\n",
        "                    x = super_in.unsqueeze(0).unsqueeze(1)    # (1,1,L)\n",
        "                    rnn_out, _ = self.supernet_rnn(x)\n",
        "                    preds = self.supernet_out(rnn_out.squeeze(1)).squeeze(0)\n",
        "                else:\n",
        "                    preds = self.supernet(super_in)\n",
        "\n",
        "                # Graph propagation if available\n",
        "                if self.adj_matrices is not None and self.mi_weights is not None:\n",
        "                    combined = preds.clone()\n",
        "                    for L, adj in enumerate(self.adj_matrices):\n",
        "                        w = self.mi_weights[L]\n",
        "                        row = adj[node_idx]\n",
        "                        norm = row / (row.sum() + 1e-8)\n",
        "                        neighbor = torch.matmul(norm, preds.unsqueeze(0))\n",
        "                        alpha = self.prop_factor * w\n",
        "                        combined = (1 - alpha) * combined + alpha * neighbor.squeeze()\n",
        "                    preds = combined\n",
        "\n",
        "                all_preds.append(preds)\n",
        "                self.prev_metrics = preds.detach()\n",
        "\n",
        "                # True objectives target for this node\n",
        "                true_obj = self.extract_objectives(node_idx)\n",
        "                obj_loss = obj_loss + mse(preds, true_obj)\n",
        "\n",
        "            all_preds = torch.stack(all_preds)  # shape (num_nodes, 4)\n",
        "\n",
        "            # Score (we maximize score by minimizing -score)\n",
        "            score = all_preds[-1, 0] + all_preds[-1, 1] + all_preds[-1, 2] + all_preds[-1, 3]\n",
        "\n",
        "            # Coordination loss: encourage consistency across nodes\n",
        "            coord_loss = torch.mean((all_preds.unsqueeze(1) - all_preds.unsqueeze(2)) ** 2)\n",
        "\n",
        "            # Activation penalty\n",
        "            act_loss = torch.mean(all_preds ** 2)\n",
        "\n",
        "            # Feature correlation loss (feature -> metric alignment)\n",
        "            corr_loss = torch.tensor(0.0, dtype=torch.float32)\n",
        "            for node_idx in range(num_nodes):\n",
        "                start = node_idx * self.node_size\n",
        "                feats = self.data_matrix[:, start:start + self.node_size].mean(axis=0)\n",
        "                feats = np.pad(feats, (0, max(0, self.node_size - len(feats))), 'constant')\n",
        "                feats_t = torch.tensor(feats, dtype=torch.float32)\n",
        "\n",
        "                denom = (torch.norm(all_preds[node_idx]) * torch.norm(feats_t) + 1e-8)\n",
        "                cos_sim = torch.dot(all_preds[node_idx], feats_t) / denom\n",
        "                corr_loss = corr_loss + (1.0 - cos_sim)\n",
        "            corr_loss = corr_loss / float(max(1, num_nodes))\n",
        "\n",
        "            # Decorrelation across nodes\n",
        "            deco_loss = torch.tensor(0.0, dtype=torch.float32)\n",
        "            if num_nodes > 1:\n",
        "                count = 0\n",
        "                for i in range(num_nodes):\n",
        "                    for j in range(i + 1, num_nodes):\n",
        "                        denom = (torch.norm(all_preds[i]) * torch.norm(all_preds[j]) + 1e-8)\n",
        "                        sim = torch.dot(all_preds[i], all_preds[j]) / denom\n",
        "                        deco_loss = deco_loss + sim ** 2\n",
        "                        count += 1\n",
        "                deco_loss = deco_loss / float(max(1, count))\n",
        "\n",
        "            # Feature -> metric predictor loss\n",
        "            pred_loss = torch.tensor(0.0, dtype=torch.float32)\n",
        "            for node_idx in range(num_nodes):\n",
        "                start = node_idx * self.node_size\n",
        "                feats = self.data_matrix[:, start:start + self.node_size].mean(axis=0)\n",
        "                feats = np.pad(feats, (0, max(0, self.node_size - len(feats))), 'constant')\n",
        "                feats_t = torch.tensor(feats, dtype=torch.float32)\n",
        "                f2m = self.predict_net(feats_t)\n",
        "                pred_loss = pred_loss + mse(f2m, all_preds[node_idx].detach())\n",
        "            pred_loss = pred_loss / float(max(1, num_nodes))\n",
        "\n",
        "            # Relative variance (encourage diversity across nodes)\n",
        "            rel_loss = torch.var(all_preds, dim=0).mean()\n",
        "\n",
        "            total_loss = (\n",
        "                obj_loss\n",
        "                - score\n",
        "                + lambda_coord * coord_loss\n",
        "                + lambda_act * act_loss\n",
        "                + lambda_corr * corr_loss\n",
        "                + lambda_deco * deco_loss\n",
        "                + lambda_pred * pred_loss\n",
        "                - lambda_rel * rel_loss\n",
        "                + lambda_feat * corr_loss\n",
        "            )\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if verbose and (epoch % 10 == 0 or epoch == epochs - 1):\n",
        "                print(f\"Epoch {epoch:03d} | TotalLoss {total_loss.item():.6f} | ObjLoss {obj_loss.item():.6f} | Score {score.item():.6f}\")\n",
        "\n",
        "        return\n",
        "\n",
        "# Usage notes:\n",
        "# - Instantiate with MetricsEvaluator(DATA_MATRIX, node_size=4)\n",
        "# - Call compute_node_metrics(i) to get {'wait','throughput','util','patience','score'}\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Full rewritten MetricsEvaluator with corrected CNN / RNN usage and training loop.\n",
        "# - Keeps external API identical: __init__(data_matrix, adj_matrices=None, ...)\n",
        "# - Internally the 4 subnets represent the 4 objectives (learned, no hardcoded numeric formulas)\n",
        "# - Training uses the dataset-derived objectives as targets (extracted from DATA_MATRIX via hardcoded columns)\n",
        "# - CNN and RNN handling fixed (shapes, squeezes) both in forward and train loop\n",
        "\n",
        "class MetricsEvaluator(nn.Module):\n",
        "    def __init__(self, data_matrix, adj_matrices=None, mi_weights=None,\n",
        "                 node_size=None, hidden_dim=256,\n",
        "                 use_cnn=False, cnn_channels=64, kernel_size=2,\n",
        "                 use_rnn=True, rnn_hidden=256, prop_factor=0.0,num_metrics=4):\n",
        "        \"\"\"\n",
        "        data_matrix : numpy array (N_samples x N_features) containing stacked datasets\n",
        "                      (must contain the columns created in the synthetic generator).\n",
        "        adj_matrices : list of adjacency numpy arrays (optional)\n",
        "        mi_weights    : list of floats for weighting each adjacency layer (optional)\n",
        "        node_size     : how many columns per node to treat as its features (default 4)\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        self.num_metrics = num_metrics\n",
        "        # ---- public-like inputs (unchanged external API) ----\n",
        "        self.data_matrix = data_matrix\n",
        "        self.node_size = node_size or 4\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # ---- internal state ----\n",
        "        self.prev_metrics = torch.zeros(num_metrics)   # conditioning across nodes during forward/train\n",
        "        self.adj_matrices = adj_matrices\n",
        "        self.mi_weights = mi_weights\n",
        "        self.prop_factor = prop_factor\n",
        "\n",
        "        self.use_cnn = use_cnn\n",
        "        self.use_rnn = use_rnn\n",
        "\n",
        "        # ---------- Subnet input size ----------\n",
        "        self.sub_input_size = self.node_size + 3 + 4  # node_features + y(3) + prev_metrics(4)\n",
        "\n",
        "        # ---------- Objective-aligned subnet factories ----------\n",
        "        def subnet_wait():\n",
        "            return nn.Sequential(\n",
        "                nn.Linear(self.sub_input_size, hidden_dim),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(hidden_dim // 2, 1),\n",
        "                nn.Softplus()\n",
        "            )\n",
        "\n",
        "        def subnet_throughput():\n",
        "            return nn.Sequential(\n",
        "                nn.Linear(self.sub_input_size, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_dim, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_dim, 1),\n",
        "                nn.Softplus()\n",
        "            )\n",
        "\n",
        "        def subnet_util():\n",
        "            return nn.Sequential(\n",
        "                nn.Linear(self.sub_input_size, hidden_dim),\n",
        "                nn.Sigmoid(),\n",
        "                nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "                nn.Sigmoid(),\n",
        "                nn.Linear(hidden_dim // 2, 1),\n",
        "                nn.Softplus()\n",
        "            )\n",
        "\n",
        "        def subnet_patience():\n",
        "            return nn.Sequential(\n",
        "                nn.Linear(self.sub_input_size, hidden_dim),\n",
        "                nn.ELU(),\n",
        "                nn.Linear(hidden_dim, hidden_dim),\n",
        "                nn.ELU(),\n",
        "                nn.Linear(hidden_dim, 1),\n",
        "                nn.Softplus()\n",
        "            )\n",
        "\n",
        "        self.subnets = nn.ModuleDict({\n",
        "            \"wait\": subnet_wait(),\n",
        "            \"throughput\": subnet_throughput(),\n",
        "            \"util\": subnet_util(),\n",
        "            \"patience\": subnet_patience()\n",
        "        })\n",
        "\n",
        "        # ---------- Supernet ----------\n",
        "        super_in = self.node_size + 4 + 4  # node_features + raw_metrics + prev_metrics\n",
        "\n",
        "        if use_cnn:\n",
        "            # CNN pipeline: input shape -> (batch=1, channels=1, length=super_in)\n",
        "            # we will design convs to produce channel dimension = 4 and collapse length\n",
        "            self.supernet = nn.Sequential(\n",
        "                nn.Conv1d(1, cnn_channels, kernel_size, padding=kernel_size // 2),\n",
        "                nn.ReLU(),\n",
        "                nn.Conv1d(cnn_channels, 4, kernel_size=1),  # produce 4 channels\n",
        "                nn.AdaptiveAvgPool1d(1),                     # collapse length -> 1\n",
        "                nn.Flatten(),                                # shape (batch, 4)\n",
        "                nn.Softplus()\n",
        "            )\n",
        "        elif use_rnn:\n",
        "            self.supernet_rnn = nn.GRU(input_size=super_in, hidden_size=rnn_hidden, batch_first=True)\n",
        "            self.supernet_out = nn.Sequential(\n",
        "                nn.Linear(rnn_hidden, 4),\n",
        "                nn.Softplus()\n",
        "            )\n",
        "        else:\n",
        "            self.supernet = nn.Sequential(\n",
        "                nn.Linear(super_in, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_dim // 2, 4),\n",
        "                nn.Softplus()\n",
        "            )\n",
        "\n",
        "        # ---------- Feature -> Metric predictor ----------\n",
        "        self.predict_net = nn.Sequential(\n",
        "            nn.Linear(self.node_size, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim // 2, 4),\n",
        "            nn.Softplus()\n",
        "        )\n",
        "\n",
        "        # ---------- Hardcoded column indices (matches synthetic generation) ----------\n",
        "        # These indices were previously used in the notebook examples.\n",
        "        # If your DATA_MATRIX ordering differs, adjust accordingly.\n",
        "        self._cols = {\n",
        "    'travel_time': 0,\n",
        "    'load_utilization': 1,\n",
        "    'delivery_priority': 2,\n",
        "    'delay_probability': 3,\n",
        "    'congestion_score': 4,\n",
        "    'energy_level': 5,  # add this\n",
        "}\n",
        "\n",
        "     # -------------------------\n",
        "    # Extract ground-truth objectives for training\n",
        "    # -------------------------\n",
        "    def extract_objectives(self, node_idx):\n",
        "        N = self.data_matrix.shape[0]\n",
        "        r = self.data_matrix[node_idx % N]\n",
        "\n",
        "        travel_time = r[self._cols['travel_time']]\n",
        "        load_util = r[self._cols['load_utilization']]\n",
        "        priority = r[self._cols['delivery_priority']]\n",
        "        delay_prob = r[self._cols['delay_probability']]\n",
        "        congestion = r[self._cols['congestion_score']]\n",
        "        energy_level = r[self._cols['energy_level']] if 'energy_level' in self._cols else 0.5\n",
        "\n",
        "        # Original GA-style objectives\n",
        "        travel_efficiency = 1.0 / (1.0 + travel_time)\n",
        "        vehicle_efficiency = load_util\n",
        "        scheduling_score = priority * (1.0 - delay_prob)\n",
        "        congestion_score = 1.0 / (1.0 + congestion)\n",
        "\n",
        "        # GA-style energy contribution (synthetic, like in compute_node_metrics)\n",
        "        energy_score = np.sqrt(energy_level + 0.1)\n",
        "\n",
        "        return torch.tensor([\n",
        "            travel_efficiency,\n",
        "            vehicle_efficiency,\n",
        "            scheduling_score,\n",
        "            congestion_score,\n",
        "            energy_score  # matches GA contribution\n",
        "        ], dtype=torch.float32)\n",
        "\n",
        "    # -------------------------\n",
        "    # Compute node metrics (forward for a single node)\n",
        "    # -------------------------\n",
        "    def compute_node_metrics(self, node_idx, y=None):\n",
        "        # --- Extract node block safely ---\n",
        "        col_start = node_idx * self.node_size\n",
        "        node_block = self.data_matrix[:, col_start: col_start + self.node_size]\n",
        "\n",
        "        node_feats = node_block.mean(axis=0)\n",
        "        if len(node_feats) < self.node_size:\n",
        "            node_feats = np.pad(node_feats, (0, self.node_size - len(node_feats)), 'constant')\n",
        "\n",
        "        # --- Prepare external input y ---\n",
        "        if y is None:\n",
        "            y_arr = np.zeros(3)\n",
        "        else:\n",
        "            y_arr = np.array(y[:3])\n",
        "            if len(y_arr) < 3:\n",
        "                y_arr = np.pad(y_arr, (0, 3 - len(y_arr)), 'constant', constant_values=0.5)\n",
        "\n",
        "        # --- Build input for subnet ---\n",
        "        nn_input = torch.tensor(\n",
        "            np.concatenate([node_feats, y_arr, self.prev_metrics.numpy()]),\n",
        "            dtype=torch.float32\n",
        "        )\n",
        "\n",
        "        # --- Compute raw metrics from subnets ---\n",
        "        raw = torch.stack([\n",
        "            self.subnets['wait'](nn_input).squeeze(),\n",
        "            self.subnets['throughput'](nn_input).squeeze(),\n",
        "            self.subnets['util'](nn_input).squeeze(),\n",
        "            self.subnets['patience'](nn_input).squeeze()\n",
        "        ]) if self.subnets else torch.zeros(4)\n",
        "\n",
        "        # --- Supernet processing ---\n",
        "        super_in = torch.cat([torch.tensor(node_feats, dtype=torch.float32), raw, self.prev_metrics], dim=0)\n",
        "\n",
        "        if self.use_cnn and self.supernet is not None:\n",
        "            x = super_in.unsqueeze(0).unsqueeze(0)  # (1,1,L)\n",
        "            metrics = self.supernet(x).squeeze(0)\n",
        "        elif self.use_rnn and self.supernet_rnn is not None and self.supernet_out is not None:\n",
        "            x = super_in.unsqueeze(0).unsqueeze(1)  # (1,1,L)\n",
        "            rnn_out, _ = self.supernet_rnn(x)\n",
        "            metrics = self.supernet_out(rnn_out.squeeze(1)).squeeze(0)\n",
        "        elif self.supernet is not None:\n",
        "            metrics = self.supernet(super_in)\n",
        "        else:\n",
        "            metrics = raw  # fallback\n",
        "\n",
        "        # --- Optional graph propagation ---\n",
        "        if self.adj_matrices is not None and self.mi_weights is not None:\n",
        "            agg = metrics.clone()\n",
        "            for L, adj in enumerate(self.adj_matrices):\n",
        "                w = self.mi_weights[L]\n",
        "                row = adj[node_idx]\n",
        "                norm = row / (row.sum() + 1e-8)\n",
        "                neighbor = torch.matmul(norm, metrics.unsqueeze(0))\n",
        "                alpha = self.prop_factor * w\n",
        "                agg = (1 - alpha) * agg + alpha * neighbor.squeeze()\n",
        "            metrics = agg\n",
        "\n",
        "        # --- Update previous metrics ---\n",
        "        self.prev_metrics = metrics.detach()\n",
        "\n",
        "        # --- Aggregate score ---\n",
        "        score = metrics[0] + metrics[1] + metrics[2] + metrics[3]\n",
        "\n",
        "        return {\n",
        "            \"wait\": float(metrics[0].item()),\n",
        "            \"throughput\": float(metrics[1].item()),\n",
        "            \"util\": float(metrics[2].item()),\n",
        "            \"patience\": float(metrics[3].item()),\n",
        "            \"score\": float(score.item())\n",
        "\n",
        "        }\n",
        "\n",
        "    # -------------------------\n",
        "    # Training loop\n",
        "    # -------------------------\n",
        "    def train_from_system(self, epochs=250, lr=0.001,\n",
        "                      lambda_coord=0.1, lambda_act=0.01,\n",
        "                      lambda_corr=0.1, lambda_deco=0.1,\n",
        "                      lambda_pred=0.2, lambda_rel=0.05,\n",
        "                      lambda_feat=0.5, lambda_r2=0.5, verbose=True):\n",
        "        \"\"\"\n",
        "        Trains the subnets + supernet to predict the four objective labels extracted from DATA_MATRIX.\n",
        "        Loss combines:\n",
        "         - obj_loss (MSE to true objectives)\n",
        "         - -score (maximize score)\n",
        "         - coord_loss (consistency across nodes)\n",
        "         - act_loss   (activation penalty)\n",
        "         - corr_loss  (feature->metric alignment)\n",
        "         - deco_loss  (decorrelation across nodes)\n",
        "         - pred_loss  (feature->metric predictor match)\n",
        "         - rel_loss   (relative variance encouragement)\n",
        "         - r2_loss    (maximize R between predictions and true objectives)\n",
        "        \"\"\"\n",
        "\n",
        "        num_nodes = max(1, self.data_matrix.shape[1] // self.node_size)\n",
        "\n",
        "        # collect parameters\n",
        "        params = list(self.subnets.parameters()) + list(self.predict_net.parameters())\n",
        "        if self.use_cnn:\n",
        "            params += list(self.supernet.parameters())\n",
        "        elif self.use_rnn:\n",
        "            params += list(self.supernet_rnn.parameters()) + list(self.supernet_out.parameters())\n",
        "        else:\n",
        "            params += list(self.supernet.parameters())\n",
        "\n",
        "        optimizer = optim.Adam(params, lr=lr)\n",
        "        mse = nn.MSELoss()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self.prev_metrics = torch.zeros(4)\n",
        "            all_preds = []\n",
        "            obj_loss = torch.tensor(0.0, dtype=torch.float32)\n",
        "\n",
        "            # Forward pass: sequential per-node to respect prev_metrics conditioning\n",
        "            for node_idx in range(num_nodes):\n",
        "                start = node_idx * self.node_size\n",
        "                block = self.data_matrix[:, start:start + self.node_size]\n",
        "                feats = block.mean(axis=0)\n",
        "                feats = np.pad(feats, (0, max(0, self.node_size - len(feats))), 'constant')\n",
        "\n",
        "                y = np.zeros(3)\n",
        "                nn_input = torch.tensor(np.concatenate([feats, y, self.prev_metrics.numpy()]), dtype=torch.float32)\n",
        "\n",
        "                # Subnets\n",
        "                raw = torch.stack([\n",
        "                    self.subnets['wait'](nn_input).squeeze(),\n",
        "                    self.subnets['throughput'](nn_input).squeeze(),\n",
        "                    self.subnets['util'](nn_input).squeeze(),\n",
        "                    self.subnets['patience'](nn_input).squeeze()\n",
        "                ])\n",
        "\n",
        "                # Supernet\n",
        "                super_in = torch.cat([torch.tensor(feats, dtype=torch.float32), raw, self.prev_metrics], dim=0)\n",
        "                if self.use_cnn:\n",
        "                    x = super_in.unsqueeze(0).unsqueeze(0)\n",
        "                    preds = self.supernet(x).squeeze(0)\n",
        "                elif self.use_rnn:\n",
        "                    x = super_in.unsqueeze(0).unsqueeze(1)\n",
        "                    rnn_out, _ = self.supernet_rnn(x)\n",
        "                    preds = self.supernet_out(rnn_out.squeeze(1)).squeeze(0)\n",
        "                else:\n",
        "                    preds = self.supernet(super_in)\n",
        "\n",
        "                # Graph propagation\n",
        "                if self.adj_matrices is not None and self.mi_weights is not None:\n",
        "                    combined = preds.clone()\n",
        "                    for L, adj in enumerate(self.adj_matrices):\n",
        "                        w = self.mi_weights[L]\n",
        "                        row = adj[node_idx]\n",
        "                        norm = row / (row.sum() + 1e-8)\n",
        "                        neighbor = torch.matmul(norm, preds.unsqueeze(0))\n",
        "                        alpha = self.prop_factor * w\n",
        "                        combined = (1 - alpha) * combined + alpha * neighbor.squeeze()\n",
        "                    preds = combined\n",
        "\n",
        "                all_preds.append(preds)\n",
        "                self.prev_metrics = preds.detach()\n",
        "\n",
        "                # True objectives target\n",
        "                true_obj = self.extract_objectives(node_idx)\n",
        "                obj_loss = obj_loss + mse(preds, true_obj[:4])  # first 4 objectives\n",
        "\n",
        "            all_preds = torch.stack(all_preds)  # shape (num_nodes, 4)\n",
        "\n",
        "            # Score (maximize)\n",
        "            score = all_preds[-1, :4].sum()\n",
        "\n",
        "            # Coordination loss\n",
        "            coord_loss = torch.mean((all_preds.unsqueeze(1) - all_preds.unsqueeze(2)) ** 2)\n",
        "\n",
        "            # Activation penalty\n",
        "            act_loss = torch.mean(all_preds ** 2)\n",
        "\n",
        "            # Feature correlation\n",
        "            corr_loss = torch.tensor(0.0, dtype=torch.float32)\n",
        "            for node_idx in range(num_nodes):\n",
        "                start = node_idx * self.node_size\n",
        "                feats = self.data_matrix[:, start:start + self.node_size].mean(axis=0)\n",
        "                feats = np.pad(feats, (0, max(0, self.node_size - len(feats))), 'constant')\n",
        "                feats_t = torch.tensor(feats, dtype=torch.float32)\n",
        "                denom = (torch.norm(all_preds[node_idx]) * torch.norm(feats_t) + 1e-8)\n",
        "                cos_sim = torch.dot(all_preds[node_idx], feats_t) / denom\n",
        "                corr_loss += (1.0 - cos_sim)\n",
        "            corr_loss /= float(max(1, num_nodes))\n",
        "\n",
        "            # Decorrelation across nodes\n",
        "            deco_loss = torch.tensor(0.0, dtype=torch.float32)\n",
        "            if num_nodes > 1:\n",
        "                count = 0\n",
        "                for i in range(num_nodes):\n",
        "                    for j in range(i + 1, num_nodes):\n",
        "                        denom = (torch.norm(all_preds[i]) * torch.norm(all_preds[j]) + 1e-8)\n",
        "                        sim = torch.dot(all_preds[i], all_preds[j]) / denom\n",
        "                        deco_loss += sim ** 2\n",
        "                        count += 1\n",
        "                deco_loss /= float(max(1, count))\n",
        "\n",
        "        # Feature -> metric predictor loss\n",
        "            pred_loss = torch.tensor(0.0, dtype=torch.float32)\n",
        "            for node_idx in range(num_nodes):\n",
        "                start = node_idx * self.node_size\n",
        "                feats = self.data_matrix[:, start:start + self.node_size].mean(axis=0)\n",
        "                feats = np.pad(feats, (0, max(0, self.node_size - len(feats))), 'constant')\n",
        "                feats_t = torch.tensor(feats, dtype=torch.float32)\n",
        "                f2m = self.predict_net(feats_t)\n",
        "                pred_loss += mse(f2m, all_preds[node_idx].detach())\n",
        "            pred_loss /= float(max(1, num_nodes))\n",
        "\n",
        "            # Relative variance\n",
        "            rel_loss = torch.var(all_preds, dim=0).mean()\n",
        "\n",
        "            # --- R maximization ---\n",
        "            true_all = torch.stack([self.extract_objectives(i)[:4] for i in range(num_nodes)])\n",
        "            ss_res = torch.sum((true_all - all_preds) ** 2)\n",
        "            ss_tot = torch.sum((true_all - true_all.mean(dim=0)) ** 2)\n",
        "            r2 = 1 - ss_res / (ss_tot + 1e-8)\n",
        "\n",
        "            # Total loss\n",
        "            total_loss = (\n",
        "                obj_loss\n",
        "                - score\n",
        "                + lambda_coord * coord_loss\n",
        "                + lambda_act * act_loss\n",
        "                + lambda_corr * corr_loss\n",
        "                + lambda_deco * deco_loss\n",
        "                + lambda_pred * pred_loss\n",
        "                - lambda_rel * rel_loss\n",
        "                + lambda_feat * corr_loss\n",
        "                - lambda_r2 * r2  # maximize R\n",
        "            )\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if verbose and (epoch % 10 == 0 or epoch == epochs - 1):\n",
        "                print(f\"Epoch {epoch:03d} | TotalLoss {total_loss.item():.6f} | ObjLoss {obj_loss.item():.6f} | Score {score.item():.6f} | R2 {r2.item():.4f}\")\n",
        "\n",
        "# - Call train_from_system(...) to train subnets/supernet against the 4 true objectives\n",
        "#\n",
        "# If your DATA_MATRIX ordering or columns differ, update self._cols mapping accordingly.\n",
        "\n",
        "class MetricsEvaluator(MetricsEvaluator):\n",
        "    def __init__(self, *args, n_experts=4, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.n_experts = n_experts  # Number of experts per objective\n",
        "\n",
        "        # ---------- MoE: replace original subnets ----------\n",
        "        # Each original subnet becomes a set of experts\n",
        "        for key in self.subnets.keys():\n",
        "            expert_list = nn.ModuleList([\n",
        "                nn.Sequential(\n",
        "                    nn.Linear(self.sub_input_size, self.hidden_dim),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(self.hidden_dim, self.hidden_dim // 2),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(self.hidden_dim // 2, 1),\n",
        "                    nn.Softplus()\n",
        "                ) for _ in range(n_experts)\n",
        "            ])\n",
        "            setattr(self, f\"{key}_experts\", expert_list)\n",
        "\n",
        "        # Gating network per objective\n",
        "        self.gates = nn.ModuleDict({\n",
        "            key: nn.Sequential(\n",
        "                nn.Linear(self.sub_input_size, n_experts),\n",
        "                nn.Softmax(dim=-1)\n",
        "            ) for key in self.subnets.keys()\n",
        "        })\n",
        "\n",
        "        # Remove original subnets since replaced by MoE\n",
        "        self.subnets = None\n",
        "\n",
        "    def compute_node_metrics(self, node_idx, y=None):\n",
        "        # --- Extract node block safely ---\n",
        "        col_start = node_idx * self.node_size\n",
        "        node_block = self.data_matrix[:, col_start: col_start + self.node_size]\n",
        "\n",
        "        node_feats = node_block.mean(axis=0)\n",
        "        if len(node_feats) < self.node_size:\n",
        "            node_feats = np.pad(node_feats, (0, self.node_size - len(node_feats)), 'constant')\n",
        "\n",
        "        # --- Prepare external input y ---\n",
        "        if y is None:\n",
        "            y_arr = np.zeros(3)\n",
        "        else:\n",
        "            y_arr = np.array(y[:3])\n",
        "            if len(y_arr) < 3:\n",
        "                y_arr = np.pad(y_arr, (0, 3 - len(y_arr)), 'constant', constant_values=0.5)\n",
        "\n",
        "        # --- Build input for experts ---\n",
        "        nn_input = torch.tensor(\n",
        "            np.concatenate([node_feats, y_arr, self.prev_metrics.numpy()]),\n",
        "            dtype=torch.float32\n",
        "        )\n",
        "\n",
        "        # --- Compute MoE metrics per objective ---\n",
        "        raw = []\n",
        "        for key in ['wait', 'throughput', 'util', 'patience']:\n",
        "            expert_list = getattr(self, f\"{key}_experts\")\n",
        "            gate = self.gates[key](nn_input)  # shape (n_experts,)\n",
        "            expert_outputs = torch.stack([e(nn_input).squeeze() for e in expert_list])  # shape (n_experts,)\n",
        "            combined = (gate * expert_outputs).sum()\n",
        "            raw.append(combined)\n",
        "        raw = torch.stack(raw)\n",
        "\n",
        "        # --- Supernet processing (unchanged) ---\n",
        "        super_in = torch.cat([torch.tensor(node_feats, dtype=torch.float32), raw, self.prev_metrics], dim=0)\n",
        "        if self.use_cnn and self.supernet is not None:\n",
        "            x = super_in.unsqueeze(0).unsqueeze(0)  # (1,1,L)\n",
        "            metrics = self.supernet(x).squeeze(0)\n",
        "        elif self.use_rnn and self.supernet_rnn is not None and self.supernet_out is not None:\n",
        "            x = super_in.unsqueeze(0).unsqueeze(1)  # (1,1,L)\n",
        "            rnn_out, _ = self.supernet_rnn(x)\n",
        "            metrics = self.supernet_out(rnn_out.squeeze(1)).squeeze(0)\n",
        "        elif self.supernet is not None:\n",
        "            metrics = self.supernet(super_in)\n",
        "        else:\n",
        "            metrics = raw  # fallback\n",
        "\n",
        "        # --- Optional graph propagation ---\n",
        "        if self.adj_matrices is not None and self.mi_weights is not None:\n",
        "            agg = metrics.clone()\n",
        "            for L, adj in enumerate(self.adj_matrices):\n",
        "                w = self.mi_weights[L]\n",
        "                row = adj[node_idx]\n",
        "                norm = row / (row.sum() + 1e-8)\n",
        "                neighbor = torch.matmul(norm, metrics.unsqueeze(0))\n",
        "                alpha = self.prop_factor * w\n",
        "                agg = (1 - alpha) * agg + alpha * neighbor.squeeze()\n",
        "            metrics = agg\n",
        "\n",
        "        # --- Update previous metrics ---\n",
        "        self.prev_metrics = metrics.detach()\n",
        "\n",
        "        # --- Aggregate score ---\n",
        "        score = metrics[0] + metrics[1] + metrics[2] + metrics[3]\n",
        "\n",
        "        return {\n",
        "            \"wait\": float(metrics[0].item()),\n",
        "            \"throughput\": float(metrics[1].item()),\n",
        "            \"util\": float(metrics[2].item()),\n",
        "            \"patience\": float(metrics[3].item()),\n",
        "            \"score\": float(score.item())\n",
        "        }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLkjv6byg_Ie"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "# ---------------- CONFIG ----------------nsion\n",
        "#candidate_dims = [4, 3, 2, 3]  # D_graph=5\n",
        "#D_graph = len(candidate_dims)\n",
        "\n",
        "inner_archive_size = 80\n",
        "inner_offspring = 40\n",
        "outer_archive_size = 40\n",
        "outer_offspring = 40\n",
        "inner_iters_per_outer = 50\n",
        "outer_generations = 10\n",
        "outer_cost_limit = 10000\n",
        "inner_learning = 0.1\n",
        "gamma_interlayer = 1\n",
        "top_k = 263\n",
        "np.random.seed()\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class InterLayer:\n",
        "    def __init__(self, D_graph, max_inner_dim, inter_dim=None, edge_threshold=0.02, gamma=1.0, seed=42):\n",
        "        np.random.seed(seed)\n",
        "        self.D_graph = D_graph\n",
        "        self.max_input = 2 * max_inner_dim\n",
        "        self.edge_threshold = edge_threshold\n",
        "        self.gamma = gamma\n",
        "        self.inter_dim = inter_dim if inter_dim is not None else max_inner_dim\n",
        "\n",
        "        # Initialize weights proportional to synthetic correlation between nodes\n",
        "        self.weights = {}\n",
        "        self.bias = {}\n",
        "        for i in range(D_graph):\n",
        "            for j in range(D_graph):\n",
        "                if i != j:\n",
        "                    # small random + slight bias towards correlation\n",
        "                    w_init = np.random.uniform(-0.1, 0.1, (self.inter_dim, self.max_input))\n",
        "                    self.weights[(i,j)] = w_init\n",
        "                    self.bias[(i,j)] = np.zeros(self.inter_dim)\n",
        "\n",
        "    def compute_edge_activation(self, i, j, nested_reps):\n",
        "        concat = np.concatenate([nested_reps[i], nested_reps[j]])\n",
        "        concat = np.pad(concat, (0, max(0, self.max_input - len(concat))))[:self.max_input]\n",
        "\n",
        "        # Normalize input to improve correlation\n",
        "        concat = (concat - np.mean(concat)) / (np.std(concat) + 1e-12)\n",
        "\n",
        "        # Compute activation\n",
        "        v = self.weights[(i,j)].dot(concat) + self.bias[(i,j)]\n",
        "\n",
        "        # Scale by correlation strength with input signals\n",
        "        input_strength = np.clip(np.mean(np.abs(concat)), 0, 1)\n",
        "        v = v * input_strength\n",
        "\n",
        "        return 1 / (1 + np.exp(-v))\n",
        "\n",
        "    def build_activations(self, Gmat, nested_reps):\n",
        "        acts = {}\n",
        "        for i in range(self.D_graph):\n",
        "            for j in range(self.D_graph):\n",
        "                if i == j:\n",
        "                    continue\n",
        "                if abs(Gmat[i,j]) > self.edge_threshold:\n",
        "                    acts[(i,j)] = self.compute_edge_activation(i, j, nested_reps)\n",
        "        return acts\n",
        "\n",
        "    @staticmethod\n",
        "    def pairwise_squared_corr(acts):\n",
        "        if len(acts) < 2:\n",
        "            return 0.0\n",
        "        A = np.stack(list(acts.values()))\n",
        "        A_centered = A - A.mean(axis=1, keepdims=True)\n",
        "        stds = np.sqrt(np.sum(A_centered**2, axis=1) / (A.shape[1]-1) + 1e-12)\n",
        "        cov = A_centered @ A_centered.T / (A.shape[1]-1)\n",
        "        corr = cov / (np.outer(stds, stds) + 1e-12)\n",
        "        np.fill_diagonal(corr, 0)\n",
        "        return float((corr**2).sum())\n",
        "\n",
        "    def mi_for_graph(self, Gmat, nested_reps):\n",
        "        acts = self.build_activations(Gmat, nested_reps)\n",
        "        if not acts:\n",
        "            return 0.0\n",
        "        return self.gamma * self.pairwise_squared_corr(acts)\n",
        "\n",
        "    def correlate_shrink_interlayer(self, fmt_bounds=None, interaction_tensor=None, metrics_keys=None, verbose=True):\n",
        "        \"\"\"\n",
        "        Compute Pearson correlation per node & metric between:\n",
        "            - shrink factor (adaptive FMT)\n",
        "            - mean outgoing inter-layer activations\n",
        "        Returns: {node_idx: {metric: {'r':..., 'p':...}}}\n",
        "        \"\"\"\n",
        "        from scipy.stats import pearsonr\n",
        "\n",
        "        if metrics_keys is None:\n",
        "            metrics_keys = ['wait','throughput','util','patience']\n",
        "\n",
        "        D = self.D_graph\n",
        "\n",
        "        # 1. Compute FMT bounds if not given\n",
        "        if fmt_bounds is None:\n",
        "            fmt_bounds = self.compute_fmt_with_bounds_adaptive(top_k=top_k)\n",
        "\n",
        "        # 2. Get inter-layer activations if not provided\n",
        "        if interaction_tensor is None:\n",
        "            interaction_tensor = self.print_interactions(return_tensor=True, verbose=False)\n",
        "\n",
        "        inter_mean = interaction_tensor.mean(axis=2)  # (D,D)\n",
        "        shrink_factors = self.compute_fmt_shrink_factor(fmt_bounds, metrics_keys)  # (D, num_metrics)\n",
        "\n",
        "        correlations = {}\n",
        "\n",
        "        for i in range(D):\n",
        "            correlations[i] = {}\n",
        "            for k, key in enumerate(metrics_keys):\n",
        "                # FMT shrink for node i (broadcasted across outgoing edges)\n",
        "                shrink_vec = shrink_factors[i, k] * np.ones(D)\n",
        "                # Outgoing inter-layer activations from node i\n",
        "                inter_vec = inter_mean[i, :]\n",
        "                # Remove self-loop\n",
        "                mask = np.arange(D) != i\n",
        "                shrink_vec = shrink_vec[mask]\n",
        "                inter_vec = inter_vec[mask]\n",
        "\n",
        "                # Compute Pearson correlation\n",
        "                if np.std(inter_vec) > 1e-8:  # valid correlation\n",
        "                    r, p = pearsonr(shrink_vec, inter_vec)\n",
        "                else:\n",
        "                    r, p = 0.0, 1.0  # no variability\n",
        "\n",
        "                correlations[i][key] = {'r': r, 'p': p}\n",
        "                if verbose:\n",
        "                    print(f\"Node {i} | {key} shrink vs inter-layer: r={r:.3f}, p={p:.3e}\")\n",
        "\n",
        "        return correlations\n",
        "\n",
        "\n",
        "\n",
        "# ---------------- UNIFIED ACOR MULTIPLEX ----------------\n",
        "class GDFCM:\n",
        "    def __init__(self, candidate_dims, D_graph, inner_archive_size, inner_offspring,\n",
        "                 outer_archive_size, outer_offspring, synthetic_targets, inner_learning,\n",
        "                 gamma_interlayer=1.0, causal_flag=True):\n",
        "        self.candidate_dims = candidate_dims\n",
        "        self.D_graph = D_graph\n",
        "        self.inner_archive_size = inner_archive_size\n",
        "        self.inner_offspring = inner_offspring\n",
        "        self.outer_archive_size = outer_archive_size\n",
        "        self.outer_offspring = outer_offspring\n",
        "        self.synthetic_targets = synthetic_targets\n",
        "        self.inner_learning = inner_learning\n",
        "        self.causal_flag = causal_flag\n",
        "        self.best_dim_per_node = [len(t)-1 for t in synthetic_targets]  # last element as best dim\n",
        "\n",
        "        self.nested_reps = [np.zeros(max(candidate_dims)) for _ in range(D_graph)]\n",
        "      #  self.best_dim_per_node = [candidate_dims[0] for _ in range(D_graph)]\n",
        "        self.inter_layer = InterLayer(D_graph, max_inner_dim=max(candidate_dims), gamma=gamma_interlayer)\n",
        "        self.chosen_Gmat = np.random.uniform(-0.5,0.5,(D_graph,D_graph))\n",
        "        np.fill_diagonal(self.chosen_Gmat,0)\n",
        "        self.l2_before, self.l2_after = [], []\n",
        "\n",
        "    # ---------- INNER LOOP (FCM) ----------\n",
        "    def run_inner(self, node_idx, target, D_fcm,\n",
        "              steps=100, lr_x=0.001, lr_y=0.001, lr_W=0.001,\n",
        "              decorrelate_metrics=True):\n",
        "\n",
        "      # --- Initialize activations ---\n",
        "        # Inside run_inner\n",
        "        x = target.copy()\n",
        "        y = target.copy()\n",
        "\n",
        "        # Pad target for L2 computation\n",
        "        target_padded = np.pad(target, (0, len(self.nested_reps[node_idx]) - len(target)),\n",
        "                            mode='constant', constant_values=0.5)\n",
        "        self.l2_before.append(np.linalg.norm(self.nested_reps[node_idx] - target_padded))\n",
        "\n",
        "        # FCM updates\n",
        "        W = np.random.uniform(-0.6, 0.6, (D_fcm, D_fcm))\n",
        "        np.fill_diagonal(W, 0)\n",
        "\n",
        "        for _ in range(steps):\n",
        "            z = W.dot(x)\n",
        "            Theta_grad_z = 2*z - target\n",
        "            Theta_grad_x = Theta_grad_z @ W + (y+1)\n",
        "            Theta_grad_y = x + 1\n",
        "            Theta_grad_W = np.outer(Theta_grad_z, x)\n",
        "\n",
        "            x -= lr_x * np.clip(Theta_grad_x, -0.05, 0.05)\n",
        "            y -= lr_y * np.clip(Theta_grad_y, -0.05, 0.05)\n",
        "            x = np.clip(x, 0, 1)\n",
        "            y = np.clip(y, 0, 1)\n",
        "            W -= lr_W * np.clip(Theta_grad_W, -0.01, 0.01)\n",
        "            np.fill_diagonal(W, 0)\n",
        "            W = np.clip(W, -1, 1)\n",
        "\n",
        "        # Pad FCM output to max dim for nested_reps\n",
        "        x_padded = np.pad(x, (0, len(self.nested_reps[node_idx]) - len(x)),\n",
        "                        mode='constant', constant_values=0.5)\n",
        "        self.nested_reps[node_idx] = x_padded\n",
        "        self.l2_after.append(np.linalg.norm(x_padded - target_padded))\n",
        "\n",
        "\n",
        "      # --- Metric decoupling ---\n",
        "        metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "\n",
        "        if decorrelate_metrics:\n",
        "            # Neutralized input\n",
        "            neutral_y = np.full_like(x, 0.5)\n",
        "            metrics = metrics_evaluator.compute_node_metrics(node_idx, y=neutral_y)\n",
        "\n",
        "            # Optional: orthogonalize against inter-layer mean\n",
        "                    # Optional: orthogonalize against inter-layer mean\n",
        "        inter_tensor = self.inter_layer.build_activations(self.chosen_Gmat, self.nested_reps)\n",
        "        if inter_tensor:\n",
        "                D = self.D_graph\n",
        "                inter_mat = np.zeros((D, D))\n",
        "                for (i,j), vec in inter_tensor.items():\n",
        "                    inter_mat[i,j] = vec.mean()  # mean scalar\n",
        "\n",
        "                node_vec = np.array([metrics[k] for k in ['wait','throughput','util','patience']])\n",
        "                for i in range(D):\n",
        "                    f_scalar = inter_mat[i,:].mean()  # mean over row\n",
        "                    proj = f_scalar * (node_vec.mean() / (1e-12 + 1))  # scale by node_vec mean\n",
        "                    node_vec -= proj\n",
        "\n",
        "\n",
        "            # --- Fully decorrelated score ---\n",
        "                metrics['score'] = metrics['wait'] + metrics['throughput'] + metrics['util'] + metrics['patience']\n",
        "\n",
        "        else:\n",
        "            # Compute normally if no decorrelation\n",
        "            metrics = metrics_evaluator.compute_node_metrics(node_idx, y=x)\n",
        "\n",
        "        # --- Compute MI score for inter-layer ---\n",
        "        mi_score = self.inter_layer.mi_for_graph(self.chosen_Gmat, self.nested_reps)\n",
        "\n",
        "        return x, y, W, mi_score, metrics\n",
        "\n",
        "    # ---------- OUTER LOOP ----------\n",
        "    def run_outer(self, outer_cost_limit=1000):\n",
        "      metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "      node_metrics_list = []\n",
        "      raw_scores = []\n",
        "\n",
        "      # --- Compute node metrics per node ---\n",
        "      for i, y in enumerate(self.nested_reps):\n",
        "          metrics = metrics_evaluator.compute_node_metrics(i, y=y)\n",
        "          node_metrics_list.append(metrics)\n",
        "          raw_scores.append(metrics['score'])\n",
        "\n",
        "      raw_scores = np.array(raw_scores)\n",
        "      total_raw = raw_scores.sum()\n",
        "\n",
        "      # --- Apply cap to raw metrics ---\n",
        "      capped_total_raw = total_raw\n",
        "      if total_raw > outer_cost_limit:\n",
        "          scale_factor = outer_cost_limit / total_raw\n",
        "          for metrics in node_metrics_list:\n",
        "              for key in ['wait', 'throughput', 'util', 'patience', 'score']:\n",
        "                  metrics[key] *= scale_factor\n",
        "          raw_scores *= scale_factor\n",
        "          capped_total_raw = outer_cost_limit\n",
        "\n",
        "      # --- Compute Fuzzy Metric Tensor contribution ---\n",
        "      fuzzy_tensor = self.compute_fuzzy_metric_tensor(normalize=True)\n",
        "      D = self.D_graph\n",
        "\n",
        "      # Only consider off-diagonal entries for inter-node interactions\n",
        "      off_diag_mask = np.ones((D, D), dtype=bool)\n",
        "      np.fill_diagonal(off_diag_mask, 0)\n",
        "      fuzzy_score_offdiag = fuzzy_tensor[off_diag_mask].sum()\n",
        "\n",
        "      # --- Compute per-node contribution ---\n",
        "      node_contributions = np.zeros(D)\n",
        "      for i in range(D):\n",
        "          # Contribution from own metrics\n",
        "          own_score = raw_scores[i]\n",
        "\n",
        "          # Contribution from FMT interactions (row i -> others)\n",
        "          fmt_contrib = fuzzy_tensor[i, :, :].sum() - fuzzy_tensor[i, i, :].sum()  # exclude self\n",
        "          node_contributions[i] = own_score + self.inter_layer.gamma * fmt_contrib\n",
        "\n",
        "      # --- Strong decoupling: correlation penalty ---\n",
        "      inter_tensor = self.print_interactions(return_tensor=True, verbose=False)\n",
        "      if inter_tensor is None or inter_tensor.shape[2] == 0:\n",
        "          inter_mean = np.zeros((D, D))\n",
        "      else:\n",
        "          inter_mean = inter_tensor.mean(axis=2)  # shape (D,D)\n",
        "\n",
        "      fmt_mean = fuzzy_tensor.mean(axis=2)  # shape (D,D)\n",
        "\n",
        "      corr_penalty = 0.0\n",
        "      for i in range(D):\n",
        "          fmt_vec = fmt_mean[i, :]\n",
        "          inter_vec = inter_mean[i, :]\n",
        "          if np.std(fmt_vec) > 1e-8 and np.std(inter_vec) > 1e-8:\n",
        "              corr = np.corrcoef(fmt_vec, inter_vec)[0, 1]\n",
        "              corr_penalty += abs(corr) ** 2\n",
        "\n",
        "      corr_penalty /= D\n",
        "      combined_score = node_contributions.sum() - corr_penalty * 500  # strong decorrelation\n",
        "\n",
        "      # --- Store for plotting / further analysis ---\n",
        "      self.capped_node_metrics = node_metrics_list\n",
        "      self.node_score_contributions = node_contributions\n",
        "      self.correlation_penalty = corr_penalty\n",
        "\n",
        "      return node_metrics_list, combined_score, node_contributions\n",
        "\n",
        "\n",
        "\n",
        "    def run(self, outer_generations=outer_generations):\n",
        "        final_metrics = None\n",
        "\n",
        "        for gen in range(outer_generations):\n",
        "            mi_scores = []\n",
        "            node_metrics_list = []\n",
        "\n",
        "            for node_idx in range(self.D_graph):\n",
        "                # Full target vector for this node\n",
        "                full_target = self.synthetic_targets[node_idx]['target']\n",
        "\n",
        "                # Use the candidate dimension assigned to this node\n",
        "                D_fcm = self.candidate_dims[node_idx]\n",
        "                target = full_target[:D_fcm]  # slice according to candidate_dims\n",
        "\n",
        "                # Run inner FCM\n",
        "                _, _, _, mi_score, metrics = self.run_inner(node_idx, target, D_fcm)\n",
        "                mi_scores.append(mi_score)\n",
        "                node_metrics_list.append(metrics)\n",
        "\n",
        "\n",
        "            # --- Outer loop uses decorrelated metrics ---\n",
        "            self.capped_node_metrics = node_metrics_list\n",
        "            _, capped_score, node_contributions = self.run_outer()  # uses self.capped_node_metrics\n",
        "\n",
        "            print(f\"\\n--- Generation {gen} Metrics ---\")\n",
        "            for i, m in enumerate(node_metrics_list):\n",
        "                print(f\"Node {i} | \" + \" | \".join([f\"{k}: {v:.2f}\" for k,v in m.items()]))\n",
        "\n",
        "            print(f\"\\n--- Generation {gen} Node Contributions ---\")\n",
        "            for i, c in enumerate(node_contributions):\n",
        "                print(f\"Node {i}: Contribution = {c:.4f}\")\n",
        "\n",
        "            print(f\"Outer Score (capped): {capped_score:.3f}\")\n",
        "\n",
        "            final_metrics = node_metrics_list\n",
        "\n",
        "        return final_metrics\n",
        "\n",
        "\n",
        "    # ---------- VISUALIZATIONS ----------\n",
        "    # ---------- VISUALIZATIONS ----------\n",
        "    def plot_pointwise_minmax_elite(self, top_k=21):\n",
        "        plt.figure(figsize=(14,3))\n",
        "        for i in range(self.D_graph):\n",
        "            # Node's actual dimension\n",
        "            dim_i = self.candidate_dims[i]\n",
        "            base = self.nested_reps[i][:dim_i]  # slice to candidate dim\n",
        "            reps = np.clip(base + np.random.normal(0,0.05,(top_k,len(base))),0,1)\n",
        "            y_min, y_max = reps.min(axis=0), reps.max(axis=0)\n",
        "            y_sel = base\n",
        "\n",
        "            # True target for this node, sliced to candidate dim\n",
        "            y_true = self.synthetic_targets[i]['target'][:len(y_sel)]\n",
        "            if len(y_true) < len(y_sel):\n",
        "                y_true = np.pad(y_true, (0, len(y_sel)-len(y_true)), \"constant\")\n",
        "            else:\n",
        "                y_true = y_true[:len(y_sel)]\n",
        "\n",
        "            plt.subplot(1,self.D_graph,i+1)\n",
        "            plt.fill_between(range(len(y_min)),y_min,y_max,color='skyblue',alpha=0.4,label='Elite Interval')\n",
        "            plt.plot(y_sel,'k-',lw=2,label='Estimated')\n",
        "            plt.plot(y_true,'r--',lw=2,label='True')\n",
        "            plt.ylim(0,1.05)\n",
        "            plt.title(f\"Node {i+1}\")\n",
        "            if i==0: plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def plot_nested_activations(self):\n",
        "        plt.figure(figsize=(12,3))\n",
        "        for i,rep in enumerate(self.nested_reps):\n",
        "            dim_i = self.candidate_dims[i]\n",
        "            rep_i = rep[:dim_i]  # slice to candidate dim\n",
        "            plt.subplot(1,self.D_graph,i+1)\n",
        "            plt.bar(range(len(rep_i)), rep_i, color=plt.cm.plasma(rep_i))\n",
        "            plt.ylim(0,1)\n",
        "            plt.title(f\"Node {i+1}\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def plot_outer_fuzzy_graph(self):\n",
        "        G = nx.DiGraph()\n",
        "        for i in range(self.D_graph): G.add_node(i)\n",
        "        for i in range(self.D_graph):\n",
        "            for j in range(self.D_graph):\n",
        "                if i!=j and abs(self.chosen_Gmat[i,j])>0.02:\n",
        "                    G.add_edge(i,j,weight=self.chosen_Gmat[i,j])\n",
        "        node_sizes = [self.best_dim_per_node[i]*200 for i in range(self.D_graph)]\n",
        "        edge_colors = ['green' if d['weight']>0 else 'red' for _,_,d in G.edges(data=True)]\n",
        "        edge_widths = [abs(d['weight'])*3 for _,_,d in G.edges(data=True)]\n",
        "        pos = nx.circular_layout(G)\n",
        "        plt.figure(figsize=(6,6))\n",
        "        nx.draw(G,pos,node_size=node_sizes,node_color='skyblue',\n",
        "                edge_color=edge_colors,width=edge_widths,arrows=True,with_labels=True)\n",
        "        plt.title(\"Outer Fuzzy Multiplex Graph\")\n",
        "        plt.show()\n",
        "# ---------------- INTERACTIONS INSPECTOR ----------------\n",
        "\n",
        "    def print_interactions(self, return_tensor=True, verbose=True):\n",
        "            D_graph = self.D_graph\n",
        "            inter_dim = self.inter_layer.inter_dim\n",
        "            inter_tensor = np.zeros((D_graph, D_graph, inter_dim))\n",
        "\n",
        "            acts = self.inter_layer.build_activations(self.chosen_Gmat, self.nested_reps)\n",
        "            if not acts:\n",
        "                if verbose:\n",
        "                    print(\"No active edges above threshold.\")\n",
        "                return inter_tensor if return_tensor else None\n",
        "\n",
        "            for (i, j), vec in acts.items():\n",
        "                inter_tensor[i, j, :] = vec\n",
        "                if verbose:\n",
        "                    act_str = \", \".join([f\"{v:.3f}\" for v in vec])\n",
        "                    print(f\"Node {i} -> Node {j}: [{act_str}]\")\n",
        "            return inter_tensor if return_tensor else None\n",
        "\n",
        "        # Move these outside of print_interactions (class-level)\n",
        "    def print_l2_summary(self):\n",
        "            print(\"\\nL2 Distances to Target per Node:\")\n",
        "            for idx, (before, after) in enumerate(zip(self.l2_before, self.l2_after)):\n",
        "                print(f\"Node {idx}: Before={before:.4f}, After={after:.4f}\")\n",
        "\n",
        "    def compute_fuzzy_metric_tensor(self, normalize=True, verbose=False):\n",
        "            \"\"\"\n",
        "            Computes a Fuzzy Metric Tensor (D_graph x D_graph x num_metrics)\n",
        "            using current nested reps and node metrics.\n",
        "            Each slice [i,j,:] represents metrics of node j (optionally weighted by Gmat[i,j])\n",
        "            \"\"\"\n",
        "            metrics_keys = ['wait', 'throughput', 'util', 'patience']\n",
        "            D = self.D_graph\n",
        "            num_metrics = len(metrics_keys)\n",
        "            tensor = np.zeros((D, D, num_metrics))\n",
        "\n",
        "            metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "\n",
        "            node_metrics = []\n",
        "            for i, rep in enumerate(self.nested_reps):\n",
        "                metrics = metrics_evaluator.compute_node_metrics(i, y=rep)\n",
        "                node_metrics.append(np.array([metrics[k] for k in metrics_keys]))\n",
        "            node_metrics = np.array(node_metrics)  # (D, num_metrics)\n",
        "\n",
        "            for i in range(D):\n",
        "                for j in range(D):\n",
        "                    if i==j:\n",
        "                        tensor[i,j,:] = node_metrics[j]\n",
        "                    else:\n",
        "                        weight = np.clip(abs(self.chosen_Gmat[i,j]), 0, 1)\n",
        "                        tensor[i,j,:] = weight * node_metrics[j]\n",
        "\n",
        "            if normalize:\n",
        "                tensor = (tensor - tensor.min()) / (tensor.max() - tensor.min() + 1e-12)\n",
        "\n",
        "            if verbose:\n",
        "                print(\"Fuzzy Metric Tensor shape:\", tensor.shape)\n",
        "\n",
        "            return tensor\n",
        "\n",
        "\n",
        "\n",
        "    def compute_fmt_shrink_factor(self, fmt_bounds, metrics_keys=None):\n",
        "        \"\"\"\n",
        "        Returns shrink factor per node and metric.\n",
        "        shrink_factor = 1 - (current_interval / original_interval)\n",
        "        \"\"\"\n",
        "        if metrics_keys is None:\n",
        "            metrics_keys = ['wait','throughput','util','patience']\n",
        "\n",
        "        D = self.D_graph\n",
        "        num_metrics = len(metrics_keys)\n",
        "        shrink_factors = np.zeros((D, num_metrics))\n",
        "\n",
        "        for i in range(D):\n",
        "            for k in range(num_metrics):\n",
        "                lower, upper = fmt_bounds[i, i, k, 0], fmt_bounds[i, i, k, 1]  # self-node interval\n",
        "                interval_width = upper - lower + 1e-12  # normalized [0,1]\n",
        "                shrink_factors[i, k] = 1 - interval_width  # more shrink = higher value\n",
        "\n",
        "        return shrink_factors\n",
        "\n",
        "    def compute_fmt_with_bounds_adaptive(self, top_k=21, max_shrink=0.5, metrics_keys=None):\n",
        "        \"\"\"\n",
        "        Computes FMT bounds using pointwise min/max across elite solutions,\n",
        "        and applies dynamic adaptive shrinking where variability is low.\n",
        "        Returns tensor shape (D,D,num_metrics,2) [lower, upper].\n",
        "        \"\"\"\n",
        "        if metrics_keys is None:\n",
        "            metrics_keys = ['wait','throughput','util','patience']\n",
        "\n",
        "        D = self.D_graph\n",
        "        num_metrics = len(metrics_keys)\n",
        "        tensor_bounds = np.zeros((D, D, num_metrics, 2))\n",
        "        metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "\n",
        "        variability = np.zeros((D, num_metrics))\n",
        "\n",
        "        # Step 1: compute bounds from perturbations\n",
        "        for i in range(D):\n",
        "            base = self.nested_reps[i]\n",
        "            reps = np.clip(base + np.random.normal(0, 0.05, (top_k, len(base))), 0, 1)\n",
        "            metrics_matrix = np.zeros((top_k, num_metrics))\n",
        "            for idx, rep in enumerate(reps):\n",
        "                m = metrics_evaluator.compute_node_metrics(i, y=rep)\n",
        "                metrics_matrix[idx, :] = [m[k] for k in metrics_keys]\n",
        "\n",
        "            lower_i = metrics_matrix.min(axis=0)\n",
        "            upper_i = metrics_matrix.max(axis=0)\n",
        "            tensor_bounds[i, :, :, 0] = lower_i[np.newaxis, :]  # broadcast to all j\n",
        "            tensor_bounds[i, :, :, 1] = upper_i[np.newaxis, :]\n",
        "            variability[i, :] = metrics_matrix.std(axis=0)\n",
        "\n",
        "        # Step 2: adaptive shrinking\n",
        "        for i in range(D):\n",
        "            for j in range(D):\n",
        "                for k in range(num_metrics):\n",
        "                    lower, upper = tensor_bounds[i,j,k,0], tensor_bounds[i,j,k,1]\n",
        "                    mean = (lower + upper)/2\n",
        "                    var_norm = min(1.0, variability[i,k]/(upper-lower + 1e-12))\n",
        "                    shrink_factor = max_shrink * (1 - var_norm)\n",
        "                    tensor_bounds[i,j,k,0] = mean - shrink_factor*(mean - lower)\n",
        "                    tensor_bounds[i,j,k,1] = mean + shrink_factor*(upper - mean)\n",
        "\n",
        "        return tensor_bounds\n",
        "\n",
        "\n",
        "\n",
        "    def plot_fuzzy_metric_tensor_heatmaps(self, fuzzy_tensor=None, metrics_keys=['wait','throughput','util','patience']):\n",
        "        \"\"\"\n",
        "        Plot a heatmap panel for each metric in the FMT.\n",
        "        Rows: source node i\n",
        "        Columns: target node j\n",
        "        \"\"\"\n",
        "        if fuzzy_tensor is None:\n",
        "            fuzzy_tensor = self.compute_fuzzy_metric_tensor(normalize=True)\n",
        "\n",
        "        D = self.D_graph\n",
        "        num_metrics = len(metrics_keys)\n",
        "\n",
        "        fig, axes = plt.subplots(1, num_metrics, figsize=(4*num_metrics,4))\n",
        "        if num_metrics == 1: axes = [axes]\n",
        "\n",
        "        for k, key in enumerate(metrics_keys):\n",
        "            data = fuzzy_tensor[:,:,k]\n",
        "            im = axes[k].imshow(data, cmap='viridis', vmin=0, vmax=1)\n",
        "            for i in range(D):\n",
        "                for j in range(D):\n",
        "                    axes[k].text(j,i,f\"{data[i,j]:.2f}\",ha='center',va='center',color='white',fontsize=9)\n",
        "            axes[k].set_xticks(range(D))\n",
        "            axes[k].set_yticks(range(D))\n",
        "            axes[k].set_xticklabels([f'Node {j}' for j in range(D)])\n",
        "            axes[k].set_yticklabels([f'Node {i}' for i in range(D)])\n",
        "            axes[k].set_title(f'FMT - {key}')\n",
        "\n",
        "        fig.colorbar(im, ax=axes, orientation='vertical', fraction=0.025, pad=0.04, label='Normalized Metric Value')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def compute_fmt_with_elite_bounds(self, top_k=21):\n",
        "        \"\"\"\n",
        "        Computes FMT bounds using pointwise min/max across elite solutions.\n",
        "        Returns tensor shape (D,D,num_metrics,2) [lower, upper].\n",
        "        \"\"\"\n",
        "        metrics_keys = ['wait','throughput','util','patience']\n",
        "        D = self.D_graph\n",
        "        num_metrics = len(metrics_keys)\n",
        "        tensor_bounds = np.zeros((D,D,num_metrics,2))\n",
        "\n",
        "        metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "\n",
        "        for i in range(D):\n",
        "            # Generate top_k perturbations around current nested_rep (like in plot_pointwise_minmax_elite)\n",
        "            base = self.nested_reps[i]\n",
        "            reps = np.clip(base + np.random.normal(0,0.05,(top_k,len(base))),0,1)\n",
        "\n",
        "            # Compute node metrics for each perturbed solution\n",
        "            metrics_matrix = np.zeros((top_k, num_metrics))\n",
        "            for idx, rep in enumerate(reps):\n",
        "                m = metrics_evaluator.compute_node_metrics(i, y=rep)\n",
        "                metrics_matrix[idx,:] = [m[k] for k in metrics_keys]\n",
        "\n",
        "            # Compute pointwise min/max across elite solutions\n",
        "            lower_i = metrics_matrix.min(axis=0)\n",
        "            upper_i = metrics_matrix.max(axis=0)\n",
        "\n",
        "            # Fill bounds tensor for all source nodes (i->j)\n",
        "            for j in range(D):\n",
        "                tensor_bounds[i,j,:,0] = lower_i\n",
        "                tensor_bounds[i,j,:,1] = upper_i\n",
        "\n",
        "        return tensor_bounds\n",
        "\n",
        "\n",
        "    def plot_fmt_with_bounds(self, fmt_tensor_bounds):\n",
        "        D = self.D_graph\n",
        "        metrics_keys = ['wait','throughput','util','patience']\n",
        "        num_metrics = len(metrics_keys)\n",
        "\n",
        "        fig, axes = plt.subplots(1, num_metrics, figsize=(4*num_metrics,4))\n",
        "        if num_metrics == 1: axes = [axes]\n",
        "\n",
        "        for k, key in enumerate(metrics_keys):\n",
        "            lower = fmt_tensor_bounds[:,:,k,0]\n",
        "            upper = fmt_tensor_bounds[:,:,k,1]\n",
        "            mean_vals = (lower+upper)/2\n",
        "            range_vals = upper-lower\n",
        "            max_range = range_vals.max() if range_vals.max()>0 else 1.0\n",
        "            alphas = 0.2 + 0.8 * range_vals/max_range\n",
        "\n",
        "            im = axes[k].imshow(mean_vals, cmap='viridis', vmin=0, vmax=mean_vals.max())\n",
        "            for i in range(D):\n",
        "                for j in range(D):\n",
        "                    alpha_val = np.clip(1-alphas[i,j],0,1)\n",
        "                    rect = plt.Rectangle((j-0.5,i-0.5),1,1,color='white',alpha=alpha_val)\n",
        "                    axes[k].add_patch(rect)\n",
        "                    axes[k].text(j,i,f\"{lower[i,j]:.1f}\\n{upper[i,j]:.1f}\",ha='center',va='center',fontsize=8)\n",
        "            axes[k].set_title(f'FMT Bounds - {key}')\n",
        "            axes[k].set_xticks(range(D))\n",
        "            axes[k].set_yticks(range(D))\n",
        "            axes[k].set_xticklabels([f'Node {j}' for j in range(D)])\n",
        "            axes[k].set_yticklabels([f'Node {i}' for i in range(D)])\n",
        "\n",
        "        fig.colorbar(im, ax=axes, orientation='vertical', fraction=0.025, pad=0.04, label='Mean Metric Value')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    def plot_node_score_contribution(self, metrics_keys=['wait','throughput','util','patience']):\n",
        "        \"\"\"\n",
        "        Plot per-node total score contribution in the SAME STYLE as the FMT plots:\n",
        "            - uses imshow\n",
        "            - one panel for: raw, FMT, and total stacked\n",
        "            - diagonal masked\n",
        "            - annotated cells\n",
        "            - node contribution highlighted like your FMT code\n",
        "        \"\"\"\n",
        "        # ---------------------------------------------------------------------\n",
        "        # 1. Collect node contributions from run_outer()\n",
        "        # ---------------------------------------------------------------------\n",
        "        _, _, node_contributions = self.run_outer()\n",
        "        node_contributions = np.array(node_contributions)\n",
        "        D = len(node_contributions)\n",
        "\n",
        "        # ---------------------------------------------------------------------\n",
        "        # 2. Recompute FMT influence (same style as your FMT plots)\n",
        "        # ---------------------------------------------------------------------\n",
        "        fuzzy_tensor = self.compute_fuzzy_metric_tensor(normalize=True)\n",
        "        total_tensor = fuzzy_tensor.sum(axis=2)           # sum over metrics\n",
        "        fmt_tensor = total_tensor.copy()\n",
        "        np.fill_diagonal(fmt_tensor, 0)                   # mask diagonal\n",
        "\n",
        "        fmt_per_node = fmt_tensor.sum(axis=1)             # row sum\n",
        "        raw_per_node = node_contributions - fmt_per_node  # everything else\n",
        "\n",
        "        # Construct matrices for plotting (DD)\n",
        "        raw_matrix = np.zeros((D, D))\n",
        "        fmt_matrix = fmt_tensor\n",
        "        total_matrix = raw_matrix + fmt_matrix            # raw only on diagonal? no  distribute raw as row diag\n",
        "        np.fill_diagonal(raw_matrix, raw_per_node)\n",
        "        total_matrix = raw_matrix + fmt_matrix\n",
        "\n",
        "        # ---------------------------------------------------------------------\n",
        "        # 3. Plot - 3 subplots in SAME STYLE as FMT panels\n",
        "        # ---------------------------------------------------------------------\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "        matrices = [raw_matrix, fmt_matrix, total_matrix]\n",
        "        titles = [\"Raw Node Contribution\", \"FMT Interaction Contribution\", \"Total Contribution\"]\n",
        "\n",
        "        for ax, mat, title in zip(axes, matrices, titles):\n",
        "\n",
        "            im = ax.imshow(mat, cmap='viridis', vmin=np.min(mat), vmax=np.max(mat))\n",
        "\n",
        "            # annotate values\n",
        "            for i in range(D):\n",
        "                for j in range(D):\n",
        "                    val = mat[i, j]\n",
        "                    ax.text(j, i, f\"{val:.2f}\", ha='center',\n",
        "                            va='center', color='white', fontsize=8)\n",
        "\n",
        "            ax.set_title(title)\n",
        "            ax.set_xticks(range(D))\n",
        "            ax.set_xticklabels([f\"Node {i+1}\" for i in range(D)])\n",
        "            ax.set_yticks(range(D))\n",
        "            ax.set_yticklabels([f\"Node {i+1}\" for i in range(D)])\n",
        "\n",
        "        fig.colorbar(im, ax=axes, orientation='vertical', fraction=0.025, pad=0.04,\n",
        "                    label='Contribution Value')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def correlate_fmt_interactions_per_node(self, fmt_bounds=None, interaction_tensor=None, verbose=True):\n",
        "        \"\"\"\n",
        "        Correlate the FMT bounds with inter-layer interactions per node and per metric.\n",
        "        Returns a dict of shape: {node_idx: {metric: {'r':..., 'p':...}}}.\n",
        "        \"\"\"\n",
        "        from scipy.stats import pearsonr\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        metrics_keys = ['wait','throughput','util','patience']\n",
        "        D = self.D_graph\n",
        "\n",
        "        # Compute tensors if not provided\n",
        "        if fmt_bounds is None:\n",
        "            fmt_bounds = self.compute_fmt_with_elite_bounds(top_k=21)\n",
        "        if interaction_tensor is None:\n",
        "            interaction_tensor = self.print_interactions(return_tensor=True, verbose=False)\n",
        "\n",
        "        # Reduce interaction tensor along inter_dim\n",
        "        inter_mean = interaction_tensor.mean(axis=2)  # (D,D)\n",
        "\n",
        "        node_correlations = {}\n",
        "\n",
        "        for i in range(D):\n",
        "            node_correlations[i] = {}\n",
        "            for k, key in enumerate(metrics_keys):\n",
        "                # FMT bounds for target node j from source i (mean of lower/upper)\n",
        "                fmt_mean = fmt_bounds[i,:,k,:].mean(axis=1)  # shape (D,)\n",
        "                # Interaction tensor for edges from node i to j\n",
        "                inter_vec = inter_mean[i,:]  # shape (D,)\n",
        "                # Pearson correlation\n",
        "                corr, pval = pearsonr(fmt_mean, inter_vec)\n",
        "                node_correlations[i][key] = {'r': corr, 'p': pval}\n",
        "\n",
        "                if verbose:\n",
        "                    print(f\"Node {i} | {key}: r = {corr:.3f}, p = {pval:.3e}\")\n",
        "                    plt.figure(figsize=(4,3))\n",
        "                    plt.scatter(fmt_mean, inter_vec, alpha=0.7, edgecolor='k', color='skyblue')\n",
        "                    plt.xlabel(f\"FMT {key} (Node {i} -> others)\")\n",
        "                    plt.ylabel(f\"Interaction mean (Node {i} -> others)\")\n",
        "                    plt.title(f\"Node {i} | {key} correlation: r={corr:.3f}\")\n",
        "                    plt.grid(True)\n",
        "                    plt.show()\n",
        "\n",
        "        return node_correlations\n",
        "\n",
        "    def correlation_penalty(self, fmt_bounds=None, interaction_tensor=None):\n",
        "        \"\"\"\n",
        "        Computes a penalty term that is high if per-node FMT metrics correlate with interactions.\n",
        "        Returns total penalty to subtract from the outer score.\n",
        "        \"\"\"\n",
        "        from scipy.stats import pearsonr\n",
        "\n",
        "        D = self.D_graph\n",
        "        metrics_keys = ['wait','throughput','util','patience']\n",
        "\n",
        "        if fmt_bounds is None:\n",
        "            fmt_bounds = self.compute_fmt_with_elite_bounds(top_k=top_k)\n",
        "        if interaction_tensor is None:\n",
        "            interaction_tensor = self.print_interactions(return_tensor=True, verbose=False)\n",
        "\n",
        "        inter_mean = interaction_tensor.mean(axis=2)\n",
        "        total_penalty = 0.0\n",
        "\n",
        "        for i in range(D):\n",
        "            for k in range(len(metrics_keys)):\n",
        "                fmt_mean = fmt_bounds[i,:,k,:].mean(axis=1)\n",
        "                inter_vec = inter_mean[i,:]\n",
        "                if np.std(fmt_mean) > 1e-8 and np.std(inter_vec) > 1e-8:\n",
        "                    corr, _ = pearsonr(fmt_mean, inter_vec)\n",
        "                    total_penalty += abs(corr)  # penalize high correlation\n",
        "\n",
        "        # normalize by number of nodes  metrics\n",
        "        total_penalty /= (D * len(metrics_keys))**2\n",
        "        return total_penalty\n",
        "\n",
        "\n",
        "# ---------------- USAGE ----------------\n",
        "if __name__ == \"__main__\":\n",
        "    optimizer = GDFCM(\n",
        "        candidate_dims, D_graph,\n",
        "        inner_archive_size, inner_offspring,\n",
        "        outer_archive_size, outer_offspring,\n",
        "        synthetic_targets,\n",
        "        inner_learning, gamma_interlayer=1,\n",
        "        causal_flag=False\n",
        "    )\n",
        "    metrics_list = optimizer.run()\n",
        "    optimizer.plot_pointwise_minmax_elite()\n",
        "    optimizer.plot_nested_activations()\n",
        "    # Compute FMT with elite bounds\n",
        "    fmt_elite_bounds = optimizer.compute_fmt_with_elite_bounds(top_k=top_k+10)\n",
        "\n",
        "# Plot as heatmaps\n",
        "    optimizer.plot_fmt_with_bounds(fmt_elite_bounds)\n",
        "\n",
        "    # Compute fuzzy multiplex tensor\n",
        "    fmt_tensor = optimizer.compute_fuzzy_metric_tensor(normalize=True)\n",
        "    optimizer.plot_fuzzy_metric_tensor_heatmaps(fmt_tensor)\n",
        "\n",
        "    # Compute FMT with bounds (minimax elite intervals)\n",
        "    optimizer.plot_node_score_contribution()\n",
        "    optimizer.plot_outer_fuzzy_graph()\n",
        "  #  optimizer.print_interactions()\n",
        "    tensor = optimizer.print_interactions()\n",
        "\n",
        "    print(\"Tensor shape:\", tensor.shape,'\\n',tensor)\n",
        "    # Compute tensors first\n",
        "    fmt_elite_bounds = optimizer.compute_fmt_with_elite_bounds(top_k=top_k)\n",
        "    interaction_tensor = optimizer.print_interactions(return_tensor=True, verbose=False)\n",
        "\n",
        "\n",
        "    interaction_tensor = optimizer.print_interactions(return_tensor=True, verbose=False)\n",
        "\n",
        "    # Get per-node, per-metric correlations\n",
        "   # node_metric_corrs = optimizer.correlate_fmt_interactions_per_node(\n",
        "    #    fmt_bounds=fmt_elite_bounds,\n",
        "     #   interaction_tensor=interaction_tensor\n",
        "    #)\n",
        "    import networkx as nx\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    from mpl_toolkits.mplot3d import Axes3D\n",
        "D_graph = len(optimizer.nested_reps)\n",
        "tensor = optimizer.print_interactions(return_tensor=True, verbose=False)\n",
        "\n",
        "# ---------------- Outer nodes (hubs) ----------------\n",
        "G_outer = nx.DiGraph()\n",
        "for i in range(D_graph):\n",
        "    G_outer.add_node(i)\n",
        "for i in range(D_graph):\n",
        "    for j in range(D_graph):\n",
        "        if i != j and np.any(tensor[i,j,:] != 0):\n",
        "            # Shift to signed weights: 0.5 -> 0, <0.5 negative, >0.5 positive\n",
        "            mean_weight = 2 * (np.mean(tensor[i,j,:]) - 0.5)\n",
        "            G_outer.add_edge(i, j, weight=mean_weight)\n",
        "\n",
        "# Outer spring layout\n",
        "pos_outer_2d = nx.circular_layout(G_outer, scale=5)\n",
        "pos_outer = np.array([[x, y, 0] for x, y in pos_outer_2d.values()])\n",
        "\n",
        "fig = plt.figure(figsize=(10,8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Plot outer nodes\n",
        "for i in range(D_graph):\n",
        "    ax.scatter(*pos_outer[i], s=300, color='skyblue')\n",
        "    ax.text(*pos_outer[i], f'Node {i}', color='black')\n",
        "\n",
        "# Plot outer edges with positive/negative colors\n",
        "\n",
        "for i, j, data in G_outer.edges(data=True):\n",
        "    x_vals = [pos_outer[i,0], pos_outer[j,0]]\n",
        "    y_vals = [pos_outer[i,1], pos_outer[j,1]]\n",
        "    z_vals = [pos_outer[i,2], pos_outer[j,2]]\n",
        "\n",
        "    # Positive = bright green, Negative = bright red\n",
        "    color = 'green' if data['weight'] > 0 else 'red'\n",
        "    linewidth = 2 + 4*abs(data['weight'])  # scale width by magnitude\n",
        "    ax.plot(x_vals, y_vals, z_vals, color=color, linewidth=linewidth)\n",
        "# ---------------- Inner FCMs (small circular around hub) ----------------\n",
        "for i, rep in enumerate(optimizer.nested_reps):\n",
        "    dims = len(rep)\n",
        "    angle = np.linspace(0, 2*np.pi, dims, endpoint=False)\n",
        "    radius = 0.8  # small circle\n",
        "    xs = pos_outer[i,0] + radius * np.cos(angle)\n",
        "    ys = pos_outer[i,1] + radius * np.sin(angle)\n",
        "    zs = pos_outer[i,2] + rep  # activation as height\n",
        "\n",
        "    # Plot inner nodes\n",
        "    ax.scatter(xs, ys, zs, c=rep, cmap='plasma', s=50)\n",
        "\n",
        "    # Connect inner nodes in circle\n",
        "    for k in range(dims):\n",
        "        ax.plot([xs[k], xs[(k+1)%dims]], [ys[k], ys[(k+1)%dims]], [zs[k], zs[(k+1)%dims]], color='gray', alpha=0.5)\n",
        "\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Activation')\n",
        "ax.set_title('Outer Nodes with Inner FCMs (Signed correlations)')\n",
        "plt.show()\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class GDFCMPredictorForward:\n",
        "    \"\"\"\n",
        "    Forward predictor for GDFCM.\n",
        "    Can handle new input data (DATA_MATRIX) and compute:\n",
        "        - Node activations\n",
        "        - Node metrics\n",
        "        - Inter-layer MI scores\n",
        "        - Total contributions and outer score\n",
        "    \"\"\"\n",
        "    def __init__(self, trained_gdfcm: GDFCM):\n",
        "        self.gdfcm = trained_gdfcm\n",
        "        self.D_graph = trained_gdfcm.D_graph\n",
        "        self.nested_reps = trained_gdfcm.nested_reps\n",
        "        self.chosen_Gmat = trained_gdfcm.chosen_Gmat\n",
        "        self.inter_layer = trained_gdfcm.inter_layer\n",
        "\n",
        "    def predict_node(self, node_idx, data_row):\n",
        "        \"\"\"\n",
        "        Predict metrics and activations for a single node given a new data row.\n",
        "        \"\"\"\n",
        "        # Use stored nested_rep as starting point\n",
        "        x = self.nested_reps[node_idx].copy()\n",
        "        # Optional: you could combine with data_row to modify x\n",
        "        # For now, we just compute metrics using data_row as the input\n",
        "        metrics_evaluator = MetricsEvaluator(np.array([data_row]))\n",
        "        metrics = metrics_evaluator.compute_node_metrics(0, y=x)\n",
        "\n",
        "        # Inter-layer MI\n",
        "        mi_score = self.inter_layer.mi_for_graph(self.chosen_Gmat, self.nested_reps)\n",
        "\n",
        "        return {'activations': x, 'metrics': metrics, 'mi_score': mi_score}\n",
        "\n",
        "    def predict_all_nodes(self, new_data_matrix):\n",
        "        \"\"\"\n",
        "        Predict metrics and activations for all nodes using new data.\n",
        "        new_data_matrix: shape (D_graph, num_features)\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        for node_idx in range(self.D_graph):\n",
        "            data_row = new_data_matrix[node_idx % len(new_data_matrix)]\n",
        "            node_result = self.predict_node(node_idx, data_row)\n",
        "            results.append(node_result)\n",
        "        return results\n",
        "\n",
        "    def predict_scores(self):\n",
        "        \"\"\"\n",
        "        Compute per-node contributions and total outer score.\n",
        "        \"\"\"\n",
        "        _, total_score, node_contributions = self.gdfcm.run_outer()\n",
        "        return {'node_contributions': node_contributions, 'total_score': total_score}\n",
        "# Suppose optimizer is your trained GDFCM\n",
        "predictor = GDFCMPredictorForward(optimizer)\n",
        "\n",
        "# New data: same number of nodes, each with same num_features\n",
        "new_DATA_MATRIX = np.random.rand(D_graph, DATA_MATRIX.shape[1])\n",
        "\n",
        "# Predict all nodes\n",
        "predictions = predictor.predict_all_nodes(new_DATA_MATRIX)\n",
        "\n",
        "for idx, node_pred in enumerate(predictions):\n",
        "    print(f\"Node {idx} Metrics:\", node_pred['metrics'])\n",
        "    print(f\"Node {idx} Activations:\", node_pred['activations'])\n",
        "    print(f\"Node {idx} MI Score:\", node_pred['mi_score'])\n",
        "\n",
        "# Optional: compute total contributions\n",
        "score_info = predictor.predict_scores()\n",
        "print(\"Node Contributions:\", score_info['node_contributions'])\n",
        "print(\"Total Score:\", score_info['total_score'])\n",
        "\n",
        "class GDFCMPredictorAdaptive:\n",
        "    \"\"\"\n",
        "    Adaptive forward predictor for GDFCM.\n",
        "    - Accepts new data per node.\n",
        "    - Updates activations slightly using a mini inner-loop.\n",
        "    - Computes metrics, inter-layer MI, and outer score contributions.\n",
        "    \"\"\"\n",
        "    def __init__(self, trained_gdfcm: GDFCM, lr_x=0.01, lr_steps=10):\n",
        "        self.gdfcm = trained_gdfcm\n",
        "        self.D_graph = trained_gdfcm.D_graph\n",
        "        self.nested_reps = [rep.copy() for rep in trained_gdfcm.nested_reps]\n",
        "        self.chosen_Gmat = trained_gdfcm.chosen_Gmat\n",
        "        self.inter_layer = trained_gdfcm.inter_layer\n",
        "        self.lr_x = lr_x\n",
        "        self.lr_steps = lr_steps\n",
        "\n",
        "    def adapt_node(self, node_idx, data_row):\n",
        "        \"\"\"\n",
        "        Mini inner-loop: slightly update activations based on new data row.\n",
        "        Handles shape mismatch by projecting new data to node activation dim.\n",
        "        \"\"\"\n",
        "        x = self.nested_reps[node_idx].copy()\n",
        "        dim = len(x)\n",
        "\n",
        "        # Simple linear projection of new data to activation space\n",
        "        if len(data_row) != dim:\n",
        "            # Use mean pooling to reduce or slice if too large\n",
        "            if len(data_row) > dim:\n",
        "                target = data_row[:dim]\n",
        "            else:\n",
        "                # Pad with 0.5\n",
        "                target = np.pad(data_row, (0, dim - len(data_row)), 'constant', constant_values=0.5)\n",
        "        else:\n",
        "            target = data_row\n",
        "\n",
        "        target = np.clip(target, 0, 1)\n",
        "\n",
        "        # Mini inner-loop update\n",
        "        for _ in range(self.lr_steps):\n",
        "            grad = x - target\n",
        "            x -= self.lr_x * grad\n",
        "            x = np.clip(x, 0, 1)\n",
        "\n",
        "        self.nested_reps[node_idx] = x\n",
        "\n",
        "        # Compute metrics\n",
        "        metrics_evaluator = MetricsEvaluator(np.array([data_row]))\n",
        "        metrics = metrics_evaluator.compute_node_metrics(0, y=x)\n",
        "\n",
        "        # Inter-layer MI\n",
        "        mi_score = self.inter_layer.mi_for_graph(self.chosen_Gmat, self.nested_reps)\n",
        "\n",
        "        return {'activations': x, 'metrics': metrics, 'mi_score': mi_score}\n",
        "\n",
        "\n",
        "    def adapt_all_nodes(self, new_data_matrix):\n",
        "        \"\"\"\n",
        "        Update activations for all nodes given new data matrix.\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        for node_idx in range(self.D_graph):\n",
        "            data_row = new_data_matrix[node_idx % len(new_data_matrix)]\n",
        "            node_result = self.adapt_node(node_idx, data_row)\n",
        "            results.append(node_result)\n",
        "        return results\n",
        "\n",
        "    def compute_outer_score(self):\n",
        "        \"\"\"\n",
        "        Compute node contributions and total outer score using current nested_reps.\n",
        "        \"\"\"\n",
        "        _, total_score, node_contributions = self.gdfcm.run_outer()\n",
        "        return {'node_contributions': node_contributions, 'total_score': total_score}\n",
        "# Initialize adaptive predictor\n",
        "adaptive_predictor = GDFCMPredictorAdaptive(optimizer, lr_x=0.02, lr_steps=15)\n",
        "\n",
        "# New data\n",
        "new_DATA_MATRIX = np.random.rand(D_graph, DATA_MATRIX.shape[1])\n",
        "\n",
        "# Adapt activations to new data\n",
        "predictions = adaptive_predictor.adapt_all_nodes(new_DATA_MATRIX)\n",
        "\n",
        "for idx, node_pred in enumerate(predictions):\n",
        "    print(f\"Node {idx} Metrics:\", node_pred['metrics'])\n",
        "    print(f\"Node {idx} Activations:\", node_pred['activations'])\n",
        "    print(f\"Node {idx} MI Score:\", node_pred['mi_score'])\n",
        "\n",
        "# Compute outer node contributions after adaptation\n",
        "score_info = adaptive_predictor.compute_outer_score()\n",
        "print(\"Node Contributions:\", score_info['node_contributions'])\n",
        "print(\"Total Score:\", score_info['total_score'])\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "pred_metrics = []\n",
        "true_metrics = []\n",
        "for idx, data_row in enumerate(new_DATA_MATRIX):\n",
        "    pred = adaptive_predictor.adapt_node(idx, data_row)\n",
        "    pred_metrics.append([pred['metrics'][k] for k in ['wait','throughput','util','patience']])\n",
        "    true_metrics.append([MetricsEvaluator(new_DATA_MATRIX).compute_node_metrics(idx)[k]\n",
        "                         for k in ['wait','throughput','util','patience']])\n",
        "\n",
        "pred_metrics = np.array(pred_metrics)\n",
        "true_metrics = np.array(true_metrics)\n",
        "\n",
        "\n",
        "def evaluate_predictions(true_metrics, pred_metrics, metric_names=None):\n",
        "    \"\"\"\n",
        "    Evaluate multi-metric predictions per node and overall.\n",
        "\n",
        "    Args:\n",
        "        true_metrics: np.array, shape (num_nodes, num_metrics)\n",
        "        pred_metrics: np.array, same shape as true_metrics\n",
        "        metric_names: list of metric names (optional)\n",
        "\n",
        "    Returns:\n",
        "        dict with evaluation metrics\n",
        "    \"\"\"\n",
        "    true_metrics = np.array(true_metrics)\n",
        "    pred_metrics = np.array(pred_metrics)\n",
        "\n",
        "    if metric_names is None:\n",
        "        metric_names = [f'Metric {i}' for i in range(true_metrics.shape[1])]\n",
        "\n",
        "    num_nodes, num_metrics = true_metrics.shape\n",
        "\n",
        "    results = {'per_metric': {}, 'overall': {}}\n",
        "\n",
        "    # Per metric\n",
        "    for i, name in enumerate(metric_names):\n",
        "        t = true_metrics[:, i]\n",
        "        p = pred_metrics[:, i]\n",
        "        mae = mean_absolute_error(t, p)\n",
        "        rmse = np.sqrt(mean_squared_error(t, p))\n",
        "        r2 = r2_score(t, p)\n",
        "        corr = np.corrcoef(t, p)[0,1]\n",
        "        mape = np.mean(np.abs((t - p) / (t + 1e-12))) * 100\n",
        "        results['per_metric'][name] = {\n",
        "            'MAE': mae,\n",
        "            'RMSE': rmse,\n",
        "            'R2': r2,\n",
        "            'Pearson': corr,\n",
        "            'MAPE (%)': mape\n",
        "        }\n",
        "\n",
        "    # Overall\n",
        "    mae_overall = mean_absolute_error(true_metrics.flatten(), pred_metrics.flatten())\n",
        "    rmse_overall = np.sqrt(mean_squared_error(true_metrics.flatten(), pred_metrics.flatten()))\n",
        "    r2_overall = r2_score(true_metrics.flatten(), pred_metrics.flatten())\n",
        "\n",
        "    # Cosine similarity (averaged over nodes)\n",
        "    cos_sim = np.mean([cosine_similarity(t.reshape(1,-1), p.reshape(1,-1))[0,0]\n",
        "                       for t,p in zip(true_metrics, pred_metrics)])\n",
        "\n",
        "    results['overall'] = {\n",
        "        'MAE': mae_overall,\n",
        "        'RMSE': rmse_overall,\n",
        "        'R2': r2_overall,\n",
        "        'CosineSim': cos_sim\n",
        "    }\n",
        "\n",
        "    return results\n",
        "# Suppose true_metrics and pred_metrics have shape (D_graph, 4)\n",
        "metrics_eval = evaluate_predictions(true_metrics, pred_metrics, metric_names=['wait','throughput','util','patience'])\n",
        "\n",
        "# Print per-metric evaluation\n",
        "for metric, vals in metrics_eval['per_metric'].items():\n",
        "    print(f\"{metric}: {vals}\")\n",
        "\n",
        "# Print overall evaluation\n",
        "print(\"\\nOverall metrics:\", metrics_eval['overall'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKM9odmyrBJm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKvDe-cwhoaw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnCqC6DQgZf5"
      },
      "source": [
        "# Nested FCM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R65Ul_ZubuSu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# ================================================================\n",
        "#  SIMPLE FCM MODEL\n",
        "# ================================================================\n",
        "class SimpleFCM:\n",
        "    def __init__(self, n_nodes, lr=0.05, max_iter=200, seed=0):\n",
        "        self.n_nodes = n_nodes\n",
        "        self.lr = lr\n",
        "        self.max_iter = max_iter\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "        self.W = self.rng.normal(0, 0.3, size=(n_nodes, n_nodes))\n",
        "        np.fill_diagonal(self.W, 0.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def activation(x):\n",
        "        return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "    def propagate(self, state):\n",
        "        return self.activation(state @ self.W.T)\n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        for _ in range(self.max_iter):\n",
        "            idx = self.rng.integers(0, len(X))\n",
        "            x = X[idx]\n",
        "            y_true = Y[idx]\n",
        "            y_pred = self.propagate(x)\n",
        "            error = (y_true - y_pred)\n",
        "            deltaW = self.lr * np.outer(error, x)\n",
        "            np.fill_diagonal(deltaW, 0.0)\n",
        "            self.W += deltaW\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.activation(X @ self.W.T)\n",
        "\n",
        "# ================================================================\n",
        "#  METRIC COMPUTATION: WAIT, THROUGHPUT, UTILIZATION, PATIENCE\n",
        "# ================================================================\n",
        "def compute_metrics(raw_data):\n",
        "    \"\"\"\n",
        "    raw_data: (T, D, F) raw inputs for each node\n",
        "    Returns: (T, D, 4) metrics [wait, throughput, utilization, patience]\n",
        "    \"\"\"\n",
        "    T, D, F = raw_data.shape\n",
        "    metrics = np.zeros((T, D, 4))  # wait, throughput, utilization, patience\n",
        "\n",
        "    for t in range(T):\n",
        "        for d in range(D):\n",
        "            node_data = raw_data[t, d, :]\n",
        "\n",
        "            # ---------------- CUSTOM FORMULAS ----------------\n",
        "            # Replace these with your actual equations\n",
        "            wait = node_data[0]            # e.g., avg queue length / arrival rate\n",
        "            throughput = node_data[1]      # e.g., processed jobs / time interval\n",
        "            utilization = node_data[2]     # e.g., busy time / total time\n",
        "            patience = node_data[3]        # e.g., time until abandonment\n",
        "\n",
        "            metrics[t, d, 0] = wait\n",
        "            metrics[t, d, 1] = throughput\n",
        "            metrics[t, d, 2] = utilization\n",
        "            metrics[t, d, 3] = patience\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# ================================================================\n",
        "#  EVALUATION FUNCTION\n",
        "# ================================================================\n",
        "def evaluate_predictions(true_metrics, pred_metrics, metric_names=None):\n",
        "    true_metrics = np.asarray(true_metrics)\n",
        "    pred_metrics = np.asarray(pred_metrics)\n",
        "    results = {'per_metric': {}, 'overall': {}}\n",
        "\n",
        "    if true_metrics.ndim == 1:\n",
        "        true_metrics = true_metrics[:, None]\n",
        "    if pred_metrics.ndim == 1:\n",
        "        pred_metrics = pred_metrics[:, None]\n",
        "\n",
        "    num_nodes = true_metrics.shape[1]\n",
        "    if metric_names is None:\n",
        "        metric_names = [f'Node {i}' for i in range(num_nodes)]\n",
        "\n",
        "    for i, name in enumerate(metric_names):\n",
        "        t = true_metrics[:, i]\n",
        "        p = pred_metrics[:, i]\n",
        "        mae = mean_absolute_error(t, p)\n",
        "        rmse = np.sqrt(mean_squared_error(t, p))\n",
        "        r2 = r2_score(t, p)\n",
        "        corr = np.corrcoef(t, p)[0, 1] if np.std(t) * np.std(p) > 0 else np.nan\n",
        "        mape = np.mean(np.abs((t - p) / (t + 1e-12))) * 100\n",
        "        results['per_metric'][name] = {'MAE': mae, 'RMSE': rmse, 'R2': r2, 'Pearson': corr, 'MAPE (%)': mape}\n",
        "\n",
        "    mae_overall = mean_absolute_error(true_metrics.flatten(), pred_metrics.flatten())\n",
        "    rmse_overall = np.sqrt(mean_squared_error(true_metrics.flatten(), pred_metrics.flatten()))\n",
        "    r2_overall = r2_score(true_metrics.flatten(), pred_metrics.flatten())\n",
        "    cos_sim = np.mean([cosine_similarity(t.reshape(1, -1), p.reshape(1, -1))[0, 0]\n",
        "                       for t, p in zip(true_metrics, pred_metrics)])\n",
        "    results['overall'] = {'MAE': mae_overall, 'RMSE': rmse_overall, 'R2': r2_overall, 'CosineSim': cos_sim}\n",
        "\n",
        "    return results\n",
        "\n",
        "# ================================================================\n",
        "#  FCM PIPELINE\n",
        "# ================================================================\n",
        "def run_fcm_pipeline(micro_data, test_size=0.2, lr=0.05, max_iter=500):\n",
        "    T, D, M = micro_data.shape\n",
        "    N = D * M\n",
        "    # Flatten and normalize\n",
        "    DATA_MATRIX = micro_data.reshape(T, N)\n",
        "    DATA_MATRIX = (DATA_MATRIX - DATA_MATRIX.min(axis=0)) / (np.ptp(DATA_MATRIX, axis=0) + 1e-8)\n",
        "\n",
        "    X_train, X_test = train_test_split(DATA_MATRIX, test_size=test_size, random_state=42)\n",
        "    y_train, y_test = X_train.copy(), X_test.copy()\n",
        "\n",
        "    fcm = SimpleFCM(n_nodes=N, lr=lr, max_iter=max_iter)\n",
        "    fcm.fit(X_train, y_train)\n",
        "    y_pred = fcm.predict(X_test)\n",
        "\n",
        "    metric_names = [f\"N{n}-M{m}\" for n in range(D) for m in range(M)]\n",
        "    metrics_eval = evaluate_predictions(y_test, y_pred, metric_names)\n",
        "    return fcm, metrics_eval\n",
        "\n",
        "# ================================================================\n",
        "#  TWO-LAYER NESTED FCM PIPELINE\n",
        "# ================================================================\n",
        "def run_nested_fcm_pipeline(raw_data, test_size=0.2, lr=0.05, max_iter_layer1=500, max_iter_layer2=300):\n",
        "    \"\"\"\n",
        "    raw_data: (T, D, F) raw inputs per node\n",
        "    Layer 1: compute wait/throughput/util/patience and FCM\n",
        "    Layer 2: inter-node FCM on Layer 1 predictions\n",
        "    \"\"\"\n",
        "    # Compute Layer 1 metrics\n",
        "    micro_data_metrics = compute_metrics(raw_data)\n",
        "\n",
        "    # Layer 1 FCM\n",
        "    fcm_layer1, eval_layer1 = run_fcm_pipeline(micro_data_metrics, test_size=test_size,\n",
        "                                               lr=lr, max_iter=max_iter_layer1)\n",
        "\n",
        "    # Get Layer 1 predictions\n",
        "    T, D, M = micro_data_metrics.shape\n",
        "    X_flat = micro_data_metrics.reshape(T, D * M)\n",
        "    layer1_preds = fcm_layer1.predict(X_flat)\n",
        "    layer1_preds_norm = (layer1_preds - layer1_preds.min(axis=0)) / (np.ptp(layer1_preds, axis=0) + 1e-8)\n",
        "    layer1_preds_reshaped = layer1_preds_norm.reshape(T, D, M)\n",
        "\n",
        "    # Layer 2 FCM on flattened Layer 1 outputs\n",
        "    fcm_layer2, eval_layer2 = run_fcm_pipeline(layer1_preds_reshaped, test_size=test_size,\n",
        "                                               lr=lr, max_iter=max_iter_layer2)\n",
        "\n",
        "    return {\n",
        "        'fcm_layer1': fcm_layer1,\n",
        "        'eval_layer1': eval_layer1,\n",
        "        'fcm_layer2': fcm_layer2,\n",
        "        'eval_layer2': eval_layer2,\n",
        "        'layer1_preds': layer1_preds_reshaped\n",
        "    }\n",
        "\n",
        "# ================================================================\n",
        "#  Example Usage\n",
        "# ================================================================\n",
        "# raw_micro_data shape: (T, D, F)\n",
        "# Example: T=100, D=5 nodes, F=4 raw inputs per node\n",
        "T, D, F = 100, 5, 4\n",
        "#raw_micro_data = np.random.rand(T, D, F)  # replace with actual raw data\n",
        "# Suppose new_DATA_MATRIX shape is (T, D*F)\n",
        "T = new_DATA_MATRIX.shape[0]        # number of samples\n",
        "D = 5                               # number of nodes\n",
        "F = 4                               # metrics per node\n",
        "raw_micro_data = new_DATA_MATRIX[:, :D*F].reshape(T, D, F)\n",
        "\n",
        "# Now call the nested FCM pipeline\n",
        "nested_results = run_nested_fcm_pipeline(raw_micro_data)\n",
        "\n",
        "\n",
        "print(\"\\n--- Layer 1 Overall Metrics ---\")\n",
        "print(nested_results['eval_layer1']['overall'])\n",
        "\n",
        "print(\"\\n--- Layer 2 Overall Metrics ---\")\n",
        "print(nested_results['eval_layer2']['overall'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_z-QoKUODU0l"
      },
      "source": [
        "# Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvUSKi39DX0D"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LULoCpNwDekn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class MetricsEvaluator(nn.Module):\n",
        "    def __init__(self, data_matrix, adj_matrices=None, mi_weights=None,\n",
        "                 node_size=None, hidden_dim=256,\n",
        "                 use_cnn=False, cnn_channels=64, kernel_size=2,\n",
        "                 use_rnn=True, rnn_hidden=256, prop_factor=0.0,num_metrics=4,\n",
        "                 gamma=0.95, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.num_metrics = num_metrics\n",
        "        self.data_matrix = data_matrix\n",
        "        self.node_size = node_size or 4\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.prev_metrics = torch.zeros(num_metrics)\n",
        "        self.adj_matrices = adj_matrices\n",
        "        self.mi_weights = mi_weights\n",
        "        self.prop_factor = prop_factor\n",
        "        self.use_cnn = use_cnn\n",
        "        self.use_rnn = use_rnn\n",
        "        self.gamma = gamma\n",
        "        self.lr = lr\n",
        "\n",
        "        # Subnet input size: node_features + prev_metrics + optional y (3)\n",
        "        self.sub_input_size = self.node_size + 3 + 4\n",
        "\n",
        "        # ---------- Subnets as Q-heads ----------\n",
        "        def subnet_wait(): return nn.Sequential(nn.Linear(self.sub_input_size, hidden_dim),\n",
        "                                                nn.Tanh(),\n",
        "                                                nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "                                                nn.Tanh(),\n",
        "                                                nn.Linear(hidden_dim // 2, 1))\n",
        "        def subnet_throughput(): return nn.Sequential(nn.Linear(self.sub_input_size, hidden_dim),\n",
        "                                                       nn.ReLU(),\n",
        "                                                       nn.Linear(hidden_dim, hidden_dim),\n",
        "                                                       nn.ReLU(),\n",
        "                                                       nn.Linear(hidden_dim, 1))\n",
        "        def subnet_util(): return nn.Sequential(nn.Linear(self.sub_input_size, hidden_dim),\n",
        "                                                nn.Sigmoid(),\n",
        "                                                nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "                                                nn.Sigmoid(),\n",
        "                                                nn.Linear(hidden_dim // 2, 1))\n",
        "        def subnet_patience(): return nn.Sequential(nn.Linear(self.sub_input_size, hidden_dim),\n",
        "                                                    nn.ELU(),\n",
        "                                                    nn.Linear(hidden_dim, hidden_dim),\n",
        "                                                    nn.ELU(),\n",
        "                                                    nn.Linear(hidden_dim, 1))\n",
        "\n",
        "        self.subnets = nn.ModuleDict({\n",
        "            \"wait\": subnet_wait(),\n",
        "            \"throughput\": subnet_throughput(),\n",
        "            \"util\": subnet_util(),\n",
        "            \"patience\": subnet_patience()\n",
        "        })\n",
        "\n",
        "        # ---------- Supernet ----------\n",
        "        super_in = self.node_size + 4 + 4\n",
        "        if use_cnn:\n",
        "            self.supernet = nn.Sequential(\n",
        "                nn.Conv1d(1, cnn_channels, kernel_size, padding=kernel_size // 2),\n",
        "                nn.ReLU(),\n",
        "                nn.Conv1d(cnn_channels, 4, kernel_size=1),\n",
        "                nn.AdaptiveAvgPool1d(1),\n",
        "                nn.Flatten()\n",
        "            )\n",
        "        elif use_rnn:\n",
        "            self.supernet_rnn = nn.GRU(input_size=super_in, hidden_size=rnn_hidden, batch_first=True)\n",
        "            self.supernet_out = nn.Sequential(nn.Linear(rnn_hidden, 4))\n",
        "        else:\n",
        "            self.supernet = nn.Sequential(\n",
        "                nn.Linear(super_in, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_dim // 2, 4)\n",
        "            )\n",
        "\n",
        "        # ---------- Feature -> Metric predictor ----------\n",
        "        self.predict_net = nn.Sequential(\n",
        "            nn.Linear(self.node_size, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim // 2, 4)\n",
        "        )\n",
        "\n",
        "        # Hardcoded columns\n",
        "        self._cols = {\n",
        "            'travel_time': 0,\n",
        "            'load_utilization': 1,\n",
        "            'delivery_priority': 2,\n",
        "            'delay_probability': 3,\n",
        "            'congestion_score': 4,\n",
        "            'energy_level': 5\n",
        "        }\n",
        "\n",
        "        # Optimizer\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=self.lr)\n",
        "\n",
        "    # -------------------------\n",
        "    # Extract reward\n",
        "    # -------------------------\n",
        "    def extract_rewards(self, node_idx):\n",
        "        r = self.data_matrix[node_idx % self.data_matrix.shape[0]]\n",
        "        travel_time = r[self._cols['travel_time']]\n",
        "        load_util = r[self._cols['load_utilization']]\n",
        "        priority = r[self._cols['delivery_priority']]\n",
        "        delay_prob = r[self._cols['delay_probability']]\n",
        "        congestion = r[self._cols['congestion_score']]\n",
        "        energy_level = r[self._cols['energy_level']] if 'energy_level' in self._cols else 0.5\n",
        "\n",
        "        travel_efficiency = 1.0 / (1.0 + travel_time)\n",
        "        vehicle_efficiency = load_util\n",
        "        scheduling_score = priority * (1.0 - delay_prob)\n",
        "        congestion_score = 1.0 / (1.0 + congestion)\n",
        "        energy_score = np.sqrt(energy_level + 0.1)\n",
        "\n",
        "        return torch.tensor([\n",
        "            travel_efficiency,\n",
        "            vehicle_efficiency,\n",
        "            scheduling_score,\n",
        "            congestion_score\n",
        "        ], dtype=torch.float32)  # 4 metrics\n",
        "\n",
        "    # -------------------------\n",
        "    # Forward pass for a single node\n",
        "    # -------------------------\n",
        "    def compute_node_metrics(self, node_idx, y=None):\n",
        "        col_start = node_idx * self.node_size\n",
        "        node_block = self.data_matrix[:, col_start: col_start + self.node_size]\n",
        "        node_feats = node_block.mean(axis=0)\n",
        "        node_feats = np.pad(node_feats, (0, self.node_size - len(node_feats)), 'constant')\n",
        "\n",
        "        if y is None:\n",
        "            y_arr = np.zeros(3)\n",
        "        else:\n",
        "            y_arr = np.array(y[:3])\n",
        "            y_arr = np.pad(y_arr, (0, 3 - len(y_arr)), 'constant', constant_values=0.5)\n",
        "\n",
        "        nn_input = torch.tensor(np.concatenate([node_feats, y_arr, self.prev_metrics.numpy()]), dtype=torch.float32)\n",
        "\n",
        "        raw = torch.stack([self.subnets['wait'](nn_input).squeeze(),\n",
        "                           self.subnets['throughput'](nn_input).squeeze(),\n",
        "                           self.subnets['util'](nn_input).squeeze(),\n",
        "                           self.subnets['patience'](nn_input).squeeze()])\n",
        "\n",
        "        super_in = torch.cat([torch.tensor(node_feats, dtype=torch.float32), raw, self.prev_metrics], dim=0)\n",
        "\n",
        "        if self.use_cnn:\n",
        "            x = super_in.unsqueeze(0).unsqueeze(0)\n",
        "            metrics = self.supernet(x).squeeze(0)\n",
        "        elif self.use_rnn:\n",
        "            x = super_in.unsqueeze(0).unsqueeze(1)\n",
        "            rnn_out, _ = self.supernet_rnn(x)\n",
        "            metrics = self.supernet_out(rnn_out.squeeze(1)).squeeze(0)\n",
        "        else:\n",
        "            metrics = self.supernet(super_in)\n",
        "\n",
        "        if self.adj_matrices and self.mi_weights:\n",
        "            agg = metrics.clone()\n",
        "            for L, adj in enumerate(self.adj_matrices):\n",
        "                w = self.mi_weights[L]\n",
        "                row = adj[node_idx]\n",
        "                norm = row / (row.sum() + 1e-8)\n",
        "                neighbor = torch.matmul(norm, metrics.unsqueeze(0))\n",
        "                alpha = self.prop_factor * w\n",
        "                agg = (1 - alpha) * agg + alpha * neighbor.squeeze()\n",
        "            metrics = agg\n",
        "\n",
        "        self.prev_metrics = metrics.detach()\n",
        "        score = metrics.sum()\n",
        "        return {\n",
        "            \"wait\": float(metrics[0].item()),\n",
        "            \"throughput\": float(metrics[1].item()),\n",
        "            \"util\": float(metrics[2].item()),\n",
        "            \"patience\": float(metrics[3].item()),\n",
        "            \"score\": float(score.item())\n",
        "        }\n",
        "\n",
        "    # -------------------------\n",
        "    # Q-Learning Training Loop\n",
        "    # -------------------------\n",
        "    def train_q_learning(self, epochs=15000, noise_std=0.05):\n",
        "        num_nodes = max(1, self.data_matrix.shape[1] // self.node_size)\n",
        "        mse = nn.MSELoss()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self.prev_metrics = torch.zeros(self.num_metrics)\n",
        "            all_preds = []\n",
        "            rewards_list = []\n",
        "            total_loss = 0.0\n",
        "\n",
        "            for node_idx in range(num_nodes):\n",
        "                # --- Node features ---\n",
        "                col_start = node_idx * self.node_size\n",
        "                node_block = self.data_matrix[:, col_start: col_start + self.node_size]\n",
        "                feats = node_block.mean(axis=0)\n",
        "                feats = np.pad(feats, (0, self.node_size - len(feats)), 'constant')\n",
        "\n",
        "                nn_input = torch.tensor(\n",
        "                    np.concatenate([feats, np.zeros(3), self.prev_metrics.numpy()]),\n",
        "                    dtype=torch.float32\n",
        "                )\n",
        "\n",
        "                # --- Subnet Q-values ---\n",
        "                q_values = torch.stack([\n",
        "                    self.subnets['wait'](nn_input).squeeze(),\n",
        "                    self.subnets['throughput'](nn_input).squeeze(),\n",
        "                    self.subnets['util'](nn_input).squeeze(),\n",
        "                    self.subnets['patience'](nn_input).squeeze()\n",
        "                ])\n",
        "\n",
        "                # --- Supernet aggregation ---\n",
        "                super_in = torch.cat([torch.tensor(feats, dtype=torch.float32), q_values, self.prev_metrics], dim=0)\n",
        "\n",
        "                if self.use_cnn:\n",
        "                    x = super_in.unsqueeze(0).unsqueeze(0)\n",
        "                    q_agg = self.supernet(x).squeeze(0)\n",
        "                elif self.use_rnn:\n",
        "                    x = super_in.unsqueeze(0).unsqueeze(1)\n",
        "                    rnn_out, _ = self.supernet_rnn(x)\n",
        "                    q_agg = self.supernet_out(rnn_out.squeeze(1)).squeeze(0)\n",
        "                else:\n",
        "                    q_agg = self.supernet(super_in)\n",
        "\n",
        "                # --- Exploration noise ---\n",
        "                q_agg = q_agg + torch.randn_like(q_agg) * noise_std\n",
        "\n",
        "                # --- Graph propagation ---\n",
        "                if self.adj_matrices and self.mi_weights:\n",
        "                    agg = q_agg.clone()\n",
        "                    for L, adj in enumerate(self.adj_matrices):\n",
        "                        w = self.mi_weights[L]\n",
        "                        row = adj[node_idx]\n",
        "                        norm = row / (row.sum() + 1e-8)\n",
        "                        neighbor = torch.matmul(norm, q_agg.unsqueeze(0))\n",
        "                        alpha = self.prop_factor * w\n",
        "                        agg = (1 - alpha) * agg + alpha * neighbor.squeeze()\n",
        "                    q_agg = agg\n",
        "\n",
        "                all_preds.append(q_agg)\n",
        "                self.prev_metrics = q_agg.detach()\n",
        "\n",
        "                # --- Reward target ---\n",
        "                reward = self.extract_rewards(node_idx)\n",
        "                rewards_list.append(reward)\n",
        "                if node_idx < num_nodes - 1:\n",
        "                    prev_backup = self.prev_metrics.clone()\n",
        "                    next_metrics = self.compute_node_metrics(node_idx + 1)\n",
        "                    next_q_tensor = torch.tensor([\n",
        "                        next_metrics['wait'],\n",
        "                        next_metrics['throughput'],\n",
        "                        next_metrics['util'],\n",
        "                        next_metrics['patience']\n",
        "                    ], dtype=torch.float32)\n",
        "                    self.prev_metrics = prev_backup  # restore\n",
        "                else:\n",
        "                    next_q_tensor = torch.zeros_like(reward)\n",
        "\n",
        "                target_q = reward + self.gamma * next_q_tensor\n",
        "\n",
        "                # --- Base Q-loss ---\n",
        "                loss = mse(q_agg, target_q)\n",
        "                total_loss += loss\n",
        "\n",
        "        # --- Stack all node predictions ---\n",
        "        all_preds_tensor = torch.stack(all_preds)\n",
        "\n",
        "        # --- Coordination loss ---\n",
        "        coord_loss = torch.mean((all_preds_tensor.unsqueeze(1) - all_preds_tensor.unsqueeze(2)) ** 2)\n",
        "\n",
        "        # --- Feature correlation loss ---\n",
        "        corr_loss = 0.0\n",
        "        for node_idx in range(num_nodes):\n",
        "            col_start = node_idx * self.node_size\n",
        "            feats = self.data_matrix[:, col_start: col_start + self.node_size].mean(axis=0)\n",
        "            feats_t = torch.tensor(np.pad(feats, (0, self.node_size - len(feats)), 'constant'), dtype=torch.float32)\n",
        "            cos_sim = torch.dot(all_preds_tensor[node_idx], feats_t) / (torch.norm(all_preds_tensor[node_idx]) * torch.norm(feats_t) + 1e-8)\n",
        "            corr_loss += (1.0 - cos_sim)\n",
        "        corr_loss /= num_nodes\n",
        "\n",
        "        # --- Decorrelation across nodes ---\n",
        "        deco_loss = 0.0\n",
        "        if num_nodes > 1:\n",
        "            count = 0\n",
        "            for i in range(num_nodes):\n",
        "                for j in range(i + 1, num_nodes):\n",
        "                    sim = torch.dot(all_preds_tensor[i], all_preds_tensor[j]) / (torch.norm(all_preds_tensor[i]) * torch.norm(all_preds_tensor[j]) + 1e-8)\n",
        "                    deco_loss += sim ** 2\n",
        "                    count += 1\n",
        "            deco_loss /= max(1, count)\n",
        "\n",
        "        # --- Total loss with regularization ---\n",
        "        lambda_coord, lambda_corr, lambda_deco = 0.1, 0.1, 0.1\n",
        "        total_loss = total_loss + lambda_coord * coord_loss + lambda_corr * corr_loss + lambda_deco * deco_loss\n",
        "\n",
        "        # --- Backprop ---\n",
        "        self.optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # --- Logging ---\n",
        "        if epoch % 10 == 0 or epoch == epochs - 1:\n",
        "            total_reward = sum([r.sum().item() for r in rewards_list])\n",
        "            print(f\"Epoch {epoch:03d} | TotalLoss {total_loss.item():.6f} | TotalReward {total_reward:.6f}\")\n",
        "\n",
        "\n",
        "class MetricsEvaluator(MetricsEvaluator):\n",
        "    def __init__(self, *args, n_experts=4, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.n_experts = n_experts  # Number of experts per objective\n",
        "\n",
        "        # ---------- MoE: replace original subnets ----------\n",
        "        # Each original subnet becomes a set of experts\n",
        "        for key in self.subnets.keys():\n",
        "            expert_list = nn.ModuleList([\n",
        "                nn.Sequential(\n",
        "                    nn.Linear(self.sub_input_size, self.hidden_dim),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(self.hidden_dim, self.hidden_dim // 2),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(self.hidden_dim // 2, 1),\n",
        "                    nn.Softplus()\n",
        "                ) for _ in range(n_experts)\n",
        "            ])\n",
        "            setattr(self, f\"{key}_experts\", expert_list)\n",
        "\n",
        "        # Gating network per objective\n",
        "        self.gates = nn.ModuleDict({\n",
        "            key: nn.Sequential(\n",
        "                nn.Linear(self.sub_input_size, n_experts),\n",
        "                nn.Softmax(dim=-1)\n",
        "            ) for key in self.subnets.keys()\n",
        "        })\n",
        "\n",
        "        # Remove original subnets since replaced by MoE\n",
        "        self.subnets = None\n",
        "\n",
        "    def compute_node_metrics(self, node_idx, y=None):\n",
        "        # --- Extract node block safely ---\n",
        "        col_start = node_idx * self.node_size\n",
        "        node_block = self.data_matrix[:, col_start: col_start + self.node_size]\n",
        "\n",
        "        node_feats = node_block.mean(axis=0)\n",
        "        if len(node_feats) < self.node_size:\n",
        "            node_feats = np.pad(node_feats, (0, self.node_size - len(node_feats)), 'constant')\n",
        "\n",
        "        # --- Prepare external input y ---\n",
        "        if y is None:\n",
        "            y_arr = np.zeros(3)\n",
        "        else:\n",
        "            y_arr = np.array(y[:3])\n",
        "            if len(y_arr) < 3:\n",
        "                y_arr = np.pad(y_arr, (0, 3 - len(y_arr)), 'constant', constant_values=0.5)\n",
        "\n",
        "        # --- Build input for experts ---\n",
        "        nn_input = torch.tensor(\n",
        "            np.concatenate([node_feats, y_arr, self.prev_metrics.numpy()]),\n",
        "            dtype=torch.float32\n",
        "        )\n",
        "\n",
        "        # --- Compute MoE metrics per objective ---\n",
        "        raw = []\n",
        "        for key in ['wait', 'throughput', 'util', 'patience']:\n",
        "            expert_list = getattr(self, f\"{key}_experts\")\n",
        "            gate = self.gates[key](nn_input)  # shape (n_experts,)\n",
        "            expert_outputs = torch.stack([e(nn_input).squeeze() for e in expert_list])  # shape (n_experts,)\n",
        "            combined = (gate * expert_outputs).sum()\n",
        "            raw.append(combined)\n",
        "        raw = torch.stack(raw)\n",
        "\n",
        "        # --- Supernet processing (unchanged) ---\n",
        "        super_in = torch.cat([torch.tensor(node_feats, dtype=torch.float32), raw, self.prev_metrics], dim=0)\n",
        "        if self.use_cnn and self.supernet is not None:\n",
        "            x = super_in.unsqueeze(0).unsqueeze(0)  # (1,1,L)\n",
        "            metrics = self.supernet(x).squeeze(0)\n",
        "        elif self.use_rnn and self.supernet_rnn is not None and self.supernet_out is not None:\n",
        "            x = super_in.unsqueeze(0).unsqueeze(1)  # (1,1,L)\n",
        "            rnn_out, _ = self.supernet_rnn(x)\n",
        "            metrics = self.supernet_out(rnn_out.squeeze(1)).squeeze(0)\n",
        "        elif self.supernet is not None:\n",
        "            metrics = self.supernet(super_in)\n",
        "        else:\n",
        "            metrics = raw  # fallback\n",
        "\n",
        "        # --- Optional graph propagation ---\n",
        "        if self.adj_matrices is not None and self.mi_weights is not None:\n",
        "            agg = metrics.clone()\n",
        "            for L, adj in enumerate(self.adj_matrices):\n",
        "                w = self.mi_weights[L]\n",
        "                row = adj[node_idx]\n",
        "                norm = row / (row.sum() + 1e-8)\n",
        "                neighbor = torch.matmul(norm, metrics.unsqueeze(0))\n",
        "                alpha = self.prop_factor * w\n",
        "                agg = (1 - alpha) * agg + alpha * neighbor.squeeze()\n",
        "            metrics = agg\n",
        "\n",
        "        # --- Update previous metrics ---\n",
        "        self.prev_metrics = metrics.detach()\n",
        "\n",
        "        # --- Aggregate score ---\n",
        "        score = metrics[0] + metrics[1] + metrics[2] + metrics[3]\n",
        "\n",
        "        return {\n",
        "            \"wait\": float(metrics[0].item()),\n",
        "            \"throughput\": float(metrics[1].item()),\n",
        "            \"util\": float(metrics[2].item()),\n",
        "            \"patience\": float(metrics[3].item()),\n",
        "            \"score\": float(score.item())\n",
        "        }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hF9WBoJVgH6t"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "# ---------------- CONFIG ----------------nsion\n",
        "#candidate_dims = [4, 3, 2, 3]  # D_graph=5\n",
        "#D_graph = len(candidate_dims)\n",
        "\n",
        "inner_archive_size = 80\n",
        "inner_offspring = 40\n",
        "outer_archive_size = 40\n",
        "outer_offspring = 40\n",
        "inner_iters_per_outer = 50\n",
        "outer_generations = 10\n",
        "outer_cost_limit = 10000\n",
        "inner_learning = 0.1\n",
        "gamma_interlayer = 1\n",
        "top_k = 263\n",
        "np.random.seed()\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class InterLayer:\n",
        "    def __init__(self, D_graph, max_inner_dim, inter_dim=None, edge_threshold=0.02, gamma=1.0, seed=42):\n",
        "        np.random.seed(seed)\n",
        "        self.D_graph = D_graph\n",
        "        self.max_input = 2 * max_inner_dim\n",
        "        self.edge_threshold = edge_threshold\n",
        "        self.gamma = gamma\n",
        "        self.inter_dim = inter_dim if inter_dim is not None else max_inner_dim\n",
        "\n",
        "        # Initialize weights proportional to synthetic correlation between nodes\n",
        "        self.weights = {}\n",
        "        self.bias = {}\n",
        "        for i in range(D_graph):\n",
        "            for j in range(D_graph):\n",
        "                if i != j:\n",
        "                    # small random + slight bias towards correlation\n",
        "                    w_init = np.random.uniform(-0.1, 0.1, (self.inter_dim, self.max_input))\n",
        "                    self.weights[(i,j)] = w_init\n",
        "                    self.bias[(i,j)] = np.zeros(self.inter_dim)\n",
        "\n",
        "    def compute_edge_activation(self, i, j, nested_reps):\n",
        "        concat = np.concatenate([nested_reps[i], nested_reps[j]])\n",
        "        concat = np.pad(concat, (0, max(0, self.max_input - len(concat))))[:self.max_input]\n",
        "\n",
        "        # Normalize input to improve correlation\n",
        "        concat = (concat - np.mean(concat)) / (np.std(concat) + 1e-12)\n",
        "\n",
        "        # Compute activation\n",
        "        v = self.weights[(i,j)].dot(concat) + self.bias[(i,j)]\n",
        "\n",
        "        # Scale by correlation strength with input signals\n",
        "        input_strength = np.clip(np.mean(np.abs(concat)), 0, 1)\n",
        "        v = v * input_strength\n",
        "\n",
        "        return 1 / (1 + np.exp(-v))\n",
        "\n",
        "    def build_activations(self, Gmat, nested_reps):\n",
        "        acts = {}\n",
        "        for i in range(self.D_graph):\n",
        "            for j in range(self.D_graph):\n",
        "                if i == j:\n",
        "                    continue\n",
        "                if abs(Gmat[i,j]) > self.edge_threshold:\n",
        "                    acts[(i,j)] = self.compute_edge_activation(i, j, nested_reps)\n",
        "        return acts\n",
        "\n",
        "    @staticmethod\n",
        "    def pairwise_squared_corr(acts):\n",
        "        if len(acts) < 2:\n",
        "            return 0.0\n",
        "        A = np.stack(list(acts.values()))\n",
        "        A_centered = A - A.mean(axis=1, keepdims=True)\n",
        "        stds = np.sqrt(np.sum(A_centered**2, axis=1) / (A.shape[1]-1) + 1e-12)\n",
        "        cov = A_centered @ A_centered.T / (A.shape[1]-1)\n",
        "        corr = cov / (np.outer(stds, stds) + 1e-12)\n",
        "        np.fill_diagonal(corr, 0)\n",
        "        return float((corr**2).sum())\n",
        "\n",
        "    def mi_for_graph(self, Gmat, nested_reps):\n",
        "        acts = self.build_activations(Gmat, nested_reps)\n",
        "        if not acts:\n",
        "            return 0.0\n",
        "        return self.gamma * self.pairwise_squared_corr(acts)\n",
        "\n",
        "    def correlate_shrink_interlayer(self, fmt_bounds=None, interaction_tensor=None, metrics_keys=None, verbose=True):\n",
        "        \"\"\"\n",
        "        Compute Pearson correlation per node & metric between:\n",
        "            - shrink factor (adaptive FMT)\n",
        "            - mean outgoing inter-layer activations\n",
        "        Returns: {node_idx: {metric: {'r':..., 'p':...}}}\n",
        "        \"\"\"\n",
        "        from scipy.stats import pearsonr\n",
        "\n",
        "        if metrics_keys is None:\n",
        "            metrics_keys = ['wait','throughput','util','patience']\n",
        "\n",
        "        D = self.D_graph\n",
        "\n",
        "        # 1. Compute FMT bounds if not given\n",
        "        if fmt_bounds is None:\n",
        "            fmt_bounds = self.compute_fmt_with_bounds_adaptive(top_k=top_k)\n",
        "\n",
        "        # 2. Get inter-layer activations if not provided\n",
        "        if interaction_tensor is None:\n",
        "            interaction_tensor = self.print_interactions(return_tensor=True, verbose=False)\n",
        "\n",
        "        inter_mean = interaction_tensor.mean(axis=2)  # (D,D)\n",
        "        shrink_factors = self.compute_fmt_shrink_factor(fmt_bounds, metrics_keys)  # (D, num_metrics)\n",
        "\n",
        "        correlations = {}\n",
        "\n",
        "        for i in range(D):\n",
        "            correlations[i] = {}\n",
        "            for k, key in enumerate(metrics_keys):\n",
        "                # FMT shrink for node i (broadcasted across outgoing edges)\n",
        "                shrink_vec = shrink_factors[i, k] * np.ones(D)\n",
        "                # Outgoing inter-layer activations from node i\n",
        "                inter_vec = inter_mean[i, :]\n",
        "                # Remove self-loop\n",
        "                mask = np.arange(D) != i\n",
        "                shrink_vec = shrink_vec[mask]\n",
        "                inter_vec = inter_vec[mask]\n",
        "\n",
        "                # Compute Pearson correlation\n",
        "                if np.std(inter_vec) > 1e-8:  # valid correlation\n",
        "                    r, p = pearsonr(shrink_vec, inter_vec)\n",
        "                else:\n",
        "                    r, p = 0.0, 1.0  # no variability\n",
        "\n",
        "                correlations[i][key] = {'r': r, 'p': p}\n",
        "                if verbose:\n",
        "                    print(f\"Node {i} | {key} shrink vs inter-layer: r={r:.3f}, p={p:.3e}\")\n",
        "\n",
        "        return correlations\n",
        "\n",
        "\n",
        "\n",
        "# ---------------- UNIFIED ACOR MULTIPLEX ----------------\n",
        "class GDFCM:\n",
        "    def __init__(self, candidate_dims, D_graph, inner_archive_size, inner_offspring,\n",
        "                 outer_archive_size, outer_offspring, synthetic_targets, inner_learning,\n",
        "                 gamma_interlayer=1.0, causal_flag=True):\n",
        "        self.candidate_dims = candidate_dims\n",
        "        self.D_graph = D_graph\n",
        "        self.inner_archive_size = inner_archive_size\n",
        "        self.inner_offspring = inner_offspring\n",
        "        self.outer_archive_size = outer_archive_size\n",
        "        self.outer_offspring = outer_offspring\n",
        "        self.synthetic_targets = synthetic_targets\n",
        "        self.inner_learning = inner_learning\n",
        "        self.causal_flag = causal_flag\n",
        "        self.best_dim_per_node = [len(t)-1 for t in synthetic_targets]  # last element as best dim\n",
        "\n",
        "        self.nested_reps = [np.zeros(max(candidate_dims)) for _ in range(D_graph)]\n",
        "      #  self.best_dim_per_node = [candidate_dims[0] for _ in range(D_graph)]\n",
        "        self.inter_layer = InterLayer(D_graph, max_inner_dim=max(candidate_dims), gamma=gamma_interlayer)\n",
        "        self.chosen_Gmat = np.random.uniform(-0.5,0.5,(D_graph,D_graph))\n",
        "        np.fill_diagonal(self.chosen_Gmat,0)\n",
        "        self.l2_before, self.l2_after = [], []\n",
        "\n",
        "    # ---------- INNER LOOP (FCM) ----------\n",
        "    def run_inner(self, node_idx, target, D_fcm,\n",
        "              steps=100, lr_x=0.001, lr_y=0.001, lr_W=0.001,\n",
        "              decorrelate_metrics=True):\n",
        "\n",
        "      # --- Initialize activations ---\n",
        "        # Inside run_inner\n",
        "        x = target.copy()\n",
        "        y = target.copy()\n",
        "\n",
        "        # Pad target for L2 computation\n",
        "        target_padded = np.pad(target, (0, len(self.nested_reps[node_idx]) - len(target)),\n",
        "                            mode='constant', constant_values=0.5)\n",
        "        self.l2_before.append(np.linalg.norm(self.nested_reps[node_idx] - target_padded))\n",
        "\n",
        "        # FCM updates\n",
        "        W = np.random.uniform(-0.6, 0.6, (D_fcm, D_fcm))\n",
        "        np.fill_diagonal(W, 0)\n",
        "\n",
        "        for _ in range(steps):\n",
        "            z = W.dot(x)\n",
        "            Theta_grad_z = 2*z - target\n",
        "            Theta_grad_x = Theta_grad_z @ W + (y+1)\n",
        "            Theta_grad_y = x + 1\n",
        "            Theta_grad_W = np.outer(Theta_grad_z, x)\n",
        "\n",
        "            x -= lr_x * np.clip(Theta_grad_x, -0.05, 0.05)\n",
        "            y -= lr_y * np.clip(Theta_grad_y, -0.05, 0.05)\n",
        "            x = np.clip(x, 0, 1)\n",
        "            y = np.clip(y, 0, 1)\n",
        "            W -= lr_W * np.clip(Theta_grad_W, -0.01, 0.01)\n",
        "            np.fill_diagonal(W, 0)\n",
        "            W = np.clip(W, -1, 1)\n",
        "\n",
        "        # Pad FCM output to max dim for nested_reps\n",
        "        x_padded = np.pad(x, (0, len(self.nested_reps[node_idx]) - len(x)),\n",
        "                        mode='constant', constant_values=0.5)\n",
        "        self.nested_reps[node_idx] = x_padded\n",
        "        self.l2_after.append(np.linalg.norm(x_padded - target_padded))\n",
        "\n",
        "\n",
        "      # --- Metric decoupling ---\n",
        "        metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "\n",
        "        if decorrelate_metrics:\n",
        "            # Neutralized input\n",
        "            neutral_y = np.full_like(x, 0.5)\n",
        "            metrics = metrics_evaluator.compute_node_metrics(node_idx, y=neutral_y)\n",
        "\n",
        "            # Optional: orthogonalize against inter-layer mean\n",
        "                    # Optional: orthogonalize against inter-layer mean\n",
        "        inter_tensor = self.inter_layer.build_activations(self.chosen_Gmat, self.nested_reps)\n",
        "        if inter_tensor:\n",
        "                D = self.D_graph\n",
        "                inter_mat = np.zeros((D, D))\n",
        "                for (i,j), vec in inter_tensor.items():\n",
        "                    inter_mat[i,j] = vec.mean()  # mean scalar\n",
        "\n",
        "                node_vec = np.array([metrics[k] for k in ['wait','throughput','util','patience']])\n",
        "                for i in range(D):\n",
        "                    f_scalar = inter_mat[i,:].mean()  # mean over row\n",
        "                    proj = f_scalar * (node_vec.mean() / (1e-12 + 1))  # scale by node_vec mean\n",
        "                    node_vec -= proj\n",
        "\n",
        "\n",
        "            # --- Fully decorrelated score ---\n",
        "                metrics['score'] = metrics['wait'] + metrics['throughput'] + metrics['util'] + metrics['patience']\n",
        "\n",
        "        else:\n",
        "            # Compute normally if no decorrelation\n",
        "            metrics = metrics_evaluator.compute_node_metrics(node_idx, y=x)\n",
        "\n",
        "        # --- Compute MI score for inter-layer ---\n",
        "        mi_score = self.inter_layer.mi_for_graph(self.chosen_Gmat, self.nested_reps)\n",
        "\n",
        "        return x, y, W, mi_score, metrics\n",
        "\n",
        "    # ---------- OUTER LOOP ----------\n",
        "    def run_outer(self, outer_cost_limit=1000):\n",
        "      metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "      node_metrics_list = []\n",
        "      raw_scores = []\n",
        "\n",
        "      # --- Compute node metrics per node ---\n",
        "      for i, y in enumerate(self.nested_reps):\n",
        "          metrics = metrics_evaluator.compute_node_metrics(i, y=y)\n",
        "          node_metrics_list.append(metrics)\n",
        "          raw_scores.append(metrics['score'])\n",
        "\n",
        "      raw_scores = np.array(raw_scores)\n",
        "      total_raw = raw_scores.sum()\n",
        "\n",
        "      # --- Apply cap to raw metrics ---\n",
        "      capped_total_raw = total_raw\n",
        "      if total_raw > outer_cost_limit:\n",
        "          scale_factor = outer_cost_limit / total_raw\n",
        "          for metrics in node_metrics_list:\n",
        "              for key in ['wait', 'throughput', 'util', 'patience', 'score']:\n",
        "                  metrics[key] *= scale_factor\n",
        "          raw_scores *= scale_factor\n",
        "          capped_total_raw = outer_cost_limit\n",
        "\n",
        "      # --- Compute Fuzzy Metric Tensor contribution ---\n",
        "      fuzzy_tensor = self.compute_fuzzy_metric_tensor(normalize=True)\n",
        "      D = self.D_graph\n",
        "\n",
        "      # Only consider off-diagonal entries for inter-node interactions\n",
        "      off_diag_mask = np.ones((D, D), dtype=bool)\n",
        "      np.fill_diagonal(off_diag_mask, 0)\n",
        "      fuzzy_score_offdiag = fuzzy_tensor[off_diag_mask].sum()\n",
        "\n",
        "      # --- Compute per-node contribution ---\n",
        "      node_contributions = np.zeros(D)\n",
        "      for i in range(D):\n",
        "          # Contribution from own metrics\n",
        "          own_score = raw_scores[i]\n",
        "\n",
        "          # Contribution from FMT interactions (row i -> others)\n",
        "          fmt_contrib = fuzzy_tensor[i, :, :].sum() - fuzzy_tensor[i, i, :].sum()  # exclude self\n",
        "          node_contributions[i] = own_score + self.inter_layer.gamma * fmt_contrib\n",
        "\n",
        "      # --- Strong decoupling: correlation penalty ---\n",
        "      inter_tensor = self.print_interactions(return_tensor=True, verbose=False)\n",
        "      if inter_tensor is None or inter_tensor.shape[2] == 0:\n",
        "          inter_mean = np.zeros((D, D))\n",
        "      else:\n",
        "          inter_mean = inter_tensor.mean(axis=2)  # shape (D,D)\n",
        "\n",
        "      fmt_mean = fuzzy_tensor.mean(axis=2)  # shape (D,D)\n",
        "\n",
        "      corr_penalty = 0.0\n",
        "      for i in range(D):\n",
        "          fmt_vec = fmt_mean[i, :]\n",
        "          inter_vec = inter_mean[i, :]\n",
        "          if np.std(fmt_vec) > 1e-8 and np.std(inter_vec) > 1e-8:\n",
        "              corr = np.corrcoef(fmt_vec, inter_vec)[0, 1]\n",
        "              corr_penalty += abs(corr) ** 2\n",
        "\n",
        "      corr_penalty /= D\n",
        "      combined_score = node_contributions.sum() - corr_penalty * 500  # strong decorrelation\n",
        "\n",
        "      # --- Store for plotting / further analysis ---\n",
        "      self.capped_node_metrics = node_metrics_list\n",
        "      self.node_score_contributions = node_contributions\n",
        "      self.correlation_penalty = corr_penalty\n",
        "\n",
        "      return node_metrics_list, combined_score, node_contributions\n",
        "\n",
        "\n",
        "\n",
        "    def run(self, outer_generations=outer_generations):\n",
        "        final_metrics = None\n",
        "\n",
        "        for gen in range(outer_generations):\n",
        "            mi_scores = []\n",
        "            node_metrics_list = []\n",
        "\n",
        "            for node_idx in range(self.D_graph):\n",
        "                # Full target vector for this node\n",
        "                full_target = self.synthetic_targets[node_idx]['target']\n",
        "\n",
        "                # Use the candidate dimension assigned to this node\n",
        "                D_fcm = self.candidate_dims[node_idx]\n",
        "                target = full_target[:D_fcm]  # slice according to candidate_dims\n",
        "\n",
        "                # Run inner FCM\n",
        "                _, _, _, mi_score, metrics = self.run_inner(node_idx, target, D_fcm)\n",
        "                mi_scores.append(mi_score)\n",
        "                node_metrics_list.append(metrics)\n",
        "\n",
        "\n",
        "            # --- Outer loop uses decorrelated metrics ---\n",
        "            self.capped_node_metrics = node_metrics_list\n",
        "            _, capped_score, node_contributions = self.run_outer()  # uses self.capped_node_metrics\n",
        "\n",
        "            print(f\"\\n--- Generation {gen} Metrics ---\")\n",
        "            for i, m in enumerate(node_metrics_list):\n",
        "                print(f\"Node {i} | \" + \" | \".join([f\"{k}: {v:.2f}\" for k,v in m.items()]))\n",
        "\n",
        "            print(f\"\\n--- Generation {gen} Node Contributions ---\")\n",
        "            for i, c in enumerate(node_contributions):\n",
        "                print(f\"Node {i}: Contribution = {c:.4f}\")\n",
        "\n",
        "            print(f\"Outer Score (capped): {capped_score:.3f}\")\n",
        "\n",
        "            final_metrics = node_metrics_list\n",
        "\n",
        "        return final_metrics\n",
        "\n",
        "\n",
        "    # ---------- VISUALIZATIONS ----------\n",
        "    # ---------- VISUALIZATIONS ----------\n",
        "    def plot_pointwise_minmax_elite(self, top_k=21):\n",
        "        plt.figure(figsize=(14,3))\n",
        "        for i in range(self.D_graph):\n",
        "            # Node's actual dimension\n",
        "            dim_i = self.candidate_dims[i]\n",
        "            base = self.nested_reps[i][:dim_i]  # slice to candidate dim\n",
        "            reps = np.clip(base + np.random.normal(0,0.05,(top_k,len(base))),0,1)\n",
        "            y_min, y_max = reps.min(axis=0), reps.max(axis=0)\n",
        "            y_sel = base\n",
        "\n",
        "            # True target for this node, sliced to candidate dim\n",
        "            y_true = self.synthetic_targets[i]['target'][:len(y_sel)]\n",
        "            if len(y_true) < len(y_sel):\n",
        "                y_true = np.pad(y_true, (0, len(y_sel)-len(y_true)), \"constant\")\n",
        "            else:\n",
        "                y_true = y_true[:len(y_sel)]\n",
        "\n",
        "            plt.subplot(1,self.D_graph,i+1)\n",
        "            plt.fill_between(range(len(y_min)),y_min,y_max,color='skyblue',alpha=0.4,label='Elite Interval')\n",
        "            plt.plot(y_sel,'k-',lw=2,label='Estimated')\n",
        "            plt.plot(y_true,'r--',lw=2,label='True')\n",
        "            plt.ylim(0,1.05)\n",
        "            plt.title(f\"Node {i+1}\")\n",
        "            if i==0: plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def plot_nested_activations(self):\n",
        "        plt.figure(figsize=(12,3))\n",
        "        for i,rep in enumerate(self.nested_reps):\n",
        "            dim_i = self.candidate_dims[i]\n",
        "            rep_i = rep[:dim_i]  # slice to candidate dim\n",
        "            plt.subplot(1,self.D_graph,i+1)\n",
        "            plt.bar(range(len(rep_i)), rep_i, color=plt.cm.plasma(rep_i))\n",
        "            plt.ylim(0,1)\n",
        "            plt.title(f\"Node {i+1}\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def plot_outer_fuzzy_graph(self):\n",
        "        G = nx.DiGraph()\n",
        "        for i in range(self.D_graph): G.add_node(i)\n",
        "        for i in range(self.D_graph):\n",
        "            for j in range(self.D_graph):\n",
        "                if i!=j and abs(self.chosen_Gmat[i,j])>0.02:\n",
        "                    G.add_edge(i,j,weight=self.chosen_Gmat[i,j])\n",
        "        node_sizes = [self.best_dim_per_node[i]*200 for i in range(self.D_graph)]\n",
        "        edge_colors = ['green' if d['weight']>0 else 'red' for _,_,d in G.edges(data=True)]\n",
        "        edge_widths = [abs(d['weight'])*3 for _,_,d in G.edges(data=True)]\n",
        "        pos = nx.circular_layout(G)\n",
        "        plt.figure(figsize=(6,6))\n",
        "        nx.draw(G,pos,node_size=node_sizes,node_color='skyblue',\n",
        "                edge_color=edge_colors,width=edge_widths,arrows=True,with_labels=True)\n",
        "        plt.title(\"Outer Fuzzy Multiplex Graph\")\n",
        "        plt.show()\n",
        "# ---------------- INTERACTIONS INSPECTOR ----------------\n",
        "\n",
        "    def print_interactions(self, return_tensor=True, verbose=True):\n",
        "            D_graph = self.D_graph\n",
        "            inter_dim = self.inter_layer.inter_dim\n",
        "            inter_tensor = np.zeros((D_graph, D_graph, inter_dim))\n",
        "\n",
        "            acts = self.inter_layer.build_activations(self.chosen_Gmat, self.nested_reps)\n",
        "            if not acts:\n",
        "                if verbose:\n",
        "                    print(\"No active edges above threshold.\")\n",
        "                return inter_tensor if return_tensor else None\n",
        "\n",
        "            for (i, j), vec in acts.items():\n",
        "                inter_tensor[i, j, :] = vec\n",
        "                if verbose:\n",
        "                    act_str = \", \".join([f\"{v:.3f}\" for v in vec])\n",
        "                    print(f\"Node {i} -> Node {j}: [{act_str}]\")\n",
        "            return inter_tensor if return_tensor else None\n",
        "\n",
        "        # Move these outside of print_interactions (class-level)\n",
        "    def print_l2_summary(self):\n",
        "            print(\"\\nL2 Distances to Target per Node:\")\n",
        "            for idx, (before, after) in enumerate(zip(self.l2_before, self.l2_after)):\n",
        "                print(f\"Node {idx}: Before={before:.4f}, After={after:.4f}\")\n",
        "\n",
        "    def compute_fuzzy_metric_tensor(self, normalize=True, verbose=False):\n",
        "            \"\"\"\n",
        "            Computes a Fuzzy Metric Tensor (D_graph x D_graph x num_metrics)\n",
        "            using current nested reps and node metrics.\n",
        "            Each slice [i,j,:] represents metrics of node j (optionally weighted by Gmat[i,j])\n",
        "            \"\"\"\n",
        "            metrics_keys = ['wait', 'throughput', 'util', 'patience']\n",
        "            D = self.D_graph\n",
        "            num_metrics = len(metrics_keys)\n",
        "            tensor = np.zeros((D, D, num_metrics))\n",
        "\n",
        "            metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "\n",
        "            node_metrics = []\n",
        "            for i, rep in enumerate(self.nested_reps):\n",
        "                metrics = metrics_evaluator.compute_node_metrics(i, y=rep)\n",
        "                node_metrics.append(np.array([metrics[k] for k in metrics_keys]))\n",
        "            node_metrics = np.array(node_metrics)  # (D, num_metrics)\n",
        "\n",
        "            for i in range(D):\n",
        "                for j in range(D):\n",
        "                    if i==j:\n",
        "                        tensor[i,j,:] = node_metrics[j]\n",
        "                    else:\n",
        "                        weight = np.clip(abs(self.chosen_Gmat[i,j]), 0, 1)\n",
        "                        tensor[i,j,:] = weight * node_metrics[j]\n",
        "\n",
        "            if normalize:\n",
        "                tensor = (tensor - tensor.min()) / (tensor.max() - tensor.min() + 1e-12)\n",
        "\n",
        "            if verbose:\n",
        "                print(\"Fuzzy Metric Tensor shape:\", tensor.shape)\n",
        "\n",
        "            return tensor\n",
        "\n",
        "\n",
        "\n",
        "    def compute_fmt_shrink_factor(self, fmt_bounds, metrics_keys=None):\n",
        "        \"\"\"\n",
        "        Returns shrink factor per node and metric.\n",
        "        shrink_factor = 1 - (current_interval / original_interval)\n",
        "        \"\"\"\n",
        "        if metrics_keys is None:\n",
        "            metrics_keys = ['wait','throughput','util','patience']\n",
        "\n",
        "        D = self.D_graph\n",
        "        num_metrics = len(metrics_keys)\n",
        "        shrink_factors = np.zeros((D, num_metrics))\n",
        "\n",
        "        for i in range(D):\n",
        "            for k in range(num_metrics):\n",
        "                lower, upper = fmt_bounds[i, i, k, 0], fmt_bounds[i, i, k, 1]  # self-node interval\n",
        "                interval_width = upper - lower + 1e-12  # normalized [0,1]\n",
        "                shrink_factors[i, k] = 1 - interval_width  # more shrink = higher value\n",
        "\n",
        "        return shrink_factors\n",
        "\n",
        "    def compute_fmt_with_bounds_adaptive(self, top_k=21, max_shrink=0.5, metrics_keys=None):\n",
        "        \"\"\"\n",
        "        Computes FMT bounds using pointwise min/max across elite solutions,\n",
        "        and applies dynamic adaptive shrinking where variability is low.\n",
        "        Returns tensor shape (D,D,num_metrics,2) [lower, upper].\n",
        "        \"\"\"\n",
        "        if metrics_keys is None:\n",
        "            metrics_keys = ['wait','throughput','util','patience']\n",
        "\n",
        "        D = self.D_graph\n",
        "        num_metrics = len(metrics_keys)\n",
        "        tensor_bounds = np.zeros((D, D, num_metrics, 2))\n",
        "        metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "\n",
        "        variability = np.zeros((D, num_metrics))\n",
        "\n",
        "        # Step 1: compute bounds from perturbations\n",
        "        for i in range(D):\n",
        "            base = self.nested_reps[i]\n",
        "            reps = np.clip(base + np.random.normal(0, 0.05, (top_k, len(base))), 0, 1)\n",
        "            metrics_matrix = np.zeros((top_k, num_metrics))\n",
        "            for idx, rep in enumerate(reps):\n",
        "                m = metrics_evaluator.compute_node_metrics(i, y=rep)\n",
        "                metrics_matrix[idx, :] = [m[k] for k in metrics_keys]\n",
        "\n",
        "            lower_i = metrics_matrix.min(axis=0)\n",
        "            upper_i = metrics_matrix.max(axis=0)\n",
        "            tensor_bounds[i, :, :, 0] = lower_i[np.newaxis, :]  # broadcast to all j\n",
        "            tensor_bounds[i, :, :, 1] = upper_i[np.newaxis, :]\n",
        "            variability[i, :] = metrics_matrix.std(axis=0)\n",
        "\n",
        "        # Step 2: adaptive shrinking\n",
        "        for i in range(D):\n",
        "            for j in range(D):\n",
        "                for k in range(num_metrics):\n",
        "                    lower, upper = tensor_bounds[i,j,k,0], tensor_bounds[i,j,k,1]\n",
        "                    mean = (lower + upper)/2\n",
        "                    var_norm = min(1.0, variability[i,k]/(upper-lower + 1e-12))\n",
        "                    shrink_factor = max_shrink * (1 - var_norm)\n",
        "                    tensor_bounds[i,j,k,0] = mean - shrink_factor*(mean - lower)\n",
        "                    tensor_bounds[i,j,k,1] = mean + shrink_factor*(upper - mean)\n",
        "\n",
        "        return tensor_bounds\n",
        "\n",
        "\n",
        "\n",
        "    def plot_fuzzy_metric_tensor_heatmaps(self, fuzzy_tensor=None, metrics_keys=['wait','throughput','util','patience']):\n",
        "        \"\"\"\n",
        "        Plot a heatmap panel for each metric in the FMT.\n",
        "        Rows: source node i\n",
        "        Columns: target node j\n",
        "        \"\"\"\n",
        "        if fuzzy_tensor is None:\n",
        "            fuzzy_tensor = self.compute_fuzzy_metric_tensor(normalize=True)\n",
        "\n",
        "        D = self.D_graph\n",
        "        num_metrics = len(metrics_keys)\n",
        "\n",
        "        fig, axes = plt.subplots(1, num_metrics, figsize=(4*num_metrics,4))\n",
        "        if num_metrics == 1: axes = [axes]\n",
        "\n",
        "        for k, key in enumerate(metrics_keys):\n",
        "            data = fuzzy_tensor[:,:,k]\n",
        "            im = axes[k].imshow(data, cmap='viridis', vmin=0, vmax=1)\n",
        "            for i in range(D):\n",
        "                for j in range(D):\n",
        "                    axes[k].text(j,i,f\"{data[i,j]:.2f}\",ha='center',va='center',color='white',fontsize=9)\n",
        "            axes[k].set_xticks(range(D))\n",
        "            axes[k].set_yticks(range(D))\n",
        "            axes[k].set_xticklabels([f'Node {j}' for j in range(D)])\n",
        "            axes[k].set_yticklabels([f'Node {i}' for i in range(D)])\n",
        "            axes[k].set_title(f'FMT - {key}')\n",
        "\n",
        "        fig.colorbar(im, ax=axes, orientation='vertical', fraction=0.025, pad=0.04, label='Normalized Metric Value')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def compute_fmt_with_elite_bounds(self, top_k=21):\n",
        "        \"\"\"\n",
        "        Computes FMT bounds using pointwise min/max across elite solutions.\n",
        "        Returns tensor shape (D,D,num_metrics,2) [lower, upper].\n",
        "        \"\"\"\n",
        "        metrics_keys = ['wait','throughput','util','patience']\n",
        "        D = self.D_graph\n",
        "        num_metrics = len(metrics_keys)\n",
        "        tensor_bounds = np.zeros((D,D,num_metrics,2))\n",
        "\n",
        "        metrics_evaluator = MetricsEvaluator(DATA_MATRIX)\n",
        "\n",
        "        for i in range(D):\n",
        "            # Generate top_k perturbations around current nested_rep (like in plot_pointwise_minmax_elite)\n",
        "            base = self.nested_reps[i]\n",
        "            reps = np.clip(base + np.random.normal(0,0.05,(top_k,len(base))),0,1)\n",
        "\n",
        "            # Compute node metrics for each perturbed solution\n",
        "            metrics_matrix = np.zeros((top_k, num_metrics))\n",
        "            for idx, rep in enumerate(reps):\n",
        "                m = metrics_evaluator.compute_node_metrics(i, y=rep)\n",
        "                metrics_matrix[idx,:] = [m[k] for k in metrics_keys]\n",
        "\n",
        "            # Compute pointwise min/max across elite solutions\n",
        "            lower_i = metrics_matrix.min(axis=0)\n",
        "            upper_i = metrics_matrix.max(axis=0)\n",
        "\n",
        "            # Fill bounds tensor for all source nodes (i->j)\n",
        "            for j in range(D):\n",
        "                tensor_bounds[i,j,:,0] = lower_i\n",
        "                tensor_bounds[i,j,:,1] = upper_i\n",
        "\n",
        "        return tensor_bounds\n",
        "\n",
        "\n",
        "    def plot_fmt_with_bounds(self, fmt_tensor_bounds):\n",
        "        D = self.D_graph\n",
        "        metrics_keys = ['wait','throughput','util','patience']\n",
        "        num_metrics = len(metrics_keys)\n",
        "\n",
        "        fig, axes = plt.subplots(1, num_metrics, figsize=(4*num_metrics,4))\n",
        "        if num_metrics == 1: axes = [axes]\n",
        "\n",
        "        for k, key in enumerate(metrics_keys):\n",
        "            lower = fmt_tensor_bounds[:,:,k,0]\n",
        "            upper = fmt_tensor_bounds[:,:,k,1]\n",
        "            mean_vals = (lower+upper)/2\n",
        "            range_vals = upper-lower\n",
        "            max_range = range_vals.max() if range_vals.max()>0 else 1.0\n",
        "            alphas = 0.2 + 0.8 * range_vals/max_range\n",
        "\n",
        "            im = axes[k].imshow(mean_vals, cmap='viridis', vmin=0, vmax=mean_vals.max())\n",
        "            for i in range(D):\n",
        "                for j in range(D):\n",
        "                    alpha_val = np.clip(1-alphas[i,j],0,1)\n",
        "                    rect = plt.Rectangle((j-0.5,i-0.5),1,1,color='white',alpha=alpha_val)\n",
        "                    axes[k].add_patch(rect)\n",
        "                    axes[k].text(j,i,f\"{lower[i,j]:.1f}\\n{upper[i,j]:.1f}\",ha='center',va='center',fontsize=8)\n",
        "            axes[k].set_title(f'FMT Bounds - {key}')\n",
        "            axes[k].set_xticks(range(D))\n",
        "            axes[k].set_yticks(range(D))\n",
        "            axes[k].set_xticklabels([f'Node {j}' for j in range(D)])\n",
        "            axes[k].set_yticklabels([f'Node {i}' for i in range(D)])\n",
        "\n",
        "        fig.colorbar(im, ax=axes, orientation='vertical', fraction=0.025, pad=0.04, label='Mean Metric Value')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    def plot_node_score_contribution(self, metrics_keys=['wait','throughput','util','patience']):\n",
        "        \"\"\"\n",
        "        Plot per-node total score contribution in the SAME STYLE as the FMT plots:\n",
        "            - uses imshow\n",
        "            - one panel for: raw, FMT, and total stacked\n",
        "            - diagonal masked\n",
        "            - annotated cells\n",
        "            - node contribution highlighted like your FMT code\n",
        "        \"\"\"\n",
        "        # ---------------------------------------------------------------------\n",
        "        # 1. Collect node contributions from run_outer()\n",
        "        # ---------------------------------------------------------------------\n",
        "        _, _, node_contributions = self.run_outer()\n",
        "        node_contributions = np.array(node_contributions)\n",
        "        D = len(node_contributions)\n",
        "\n",
        "        # ---------------------------------------------------------------------\n",
        "        # 2. Recompute FMT influence (same style as your FMT plots)\n",
        "        # ---------------------------------------------------------------------\n",
        "        fuzzy_tensor = self.compute_fuzzy_metric_tensor(normalize=True)\n",
        "        total_tensor = fuzzy_tensor.sum(axis=2)           # sum over metrics\n",
        "        fmt_tensor = total_tensor.copy()\n",
        "        np.fill_diagonal(fmt_tensor, 0)                   # mask diagonal\n",
        "\n",
        "        fmt_per_node = fmt_tensor.sum(axis=1)             # row sum\n",
        "        raw_per_node = node_contributions - fmt_per_node  # everything else\n",
        "\n",
        "        # Construct matrices for plotting (DD)\n",
        "        raw_matrix = np.zeros((D, D))\n",
        "        fmt_matrix = fmt_tensor\n",
        "        total_matrix = raw_matrix + fmt_matrix            # raw only on diagonal? no  distribute raw as row diag\n",
        "        np.fill_diagonal(raw_matrix, raw_per_node)\n",
        "        total_matrix = raw_matrix + fmt_matrix\n",
        "\n",
        "        # ---------------------------------------------------------------------\n",
        "        # 3. Plot - 3 subplots in SAME STYLE as FMT panels\n",
        "        # ---------------------------------------------------------------------\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "        matrices = [raw_matrix, fmt_matrix, total_matrix]\n",
        "        titles = [\"Raw Node Contribution\", \"FMT Interaction Contribution\", \"Total Contribution\"]\n",
        "\n",
        "        for ax, mat, title in zip(axes, matrices, titles):\n",
        "\n",
        "            im = ax.imshow(mat, cmap='viridis', vmin=np.min(mat), vmax=np.max(mat))\n",
        "\n",
        "            # annotate values\n",
        "            for i in range(D):\n",
        "                for j in range(D):\n",
        "                    val = mat[i, j]\n",
        "                    ax.text(j, i, f\"{val:.2f}\", ha='center',\n",
        "                            va='center', color='white', fontsize=8)\n",
        "\n",
        "            ax.set_title(title)\n",
        "            ax.set_xticks(range(D))\n",
        "            ax.set_xticklabels([f\"Node {i+1}\" for i in range(D)])\n",
        "            ax.set_yticks(range(D))\n",
        "            ax.set_yticklabels([f\"Node {i+1}\" for i in range(D)])\n",
        "\n",
        "        fig.colorbar(im, ax=axes, orientation='vertical', fraction=0.025, pad=0.04,\n",
        "                    label='Contribution Value')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def correlate_fmt_interactions_per_node(self, fmt_bounds=None, interaction_tensor=None, verbose=True):\n",
        "        \"\"\"\n",
        "        Correlate the FMT bounds with inter-layer interactions per node and per metric.\n",
        "        Returns a dict of shape: {node_idx: {metric: {'r':..., 'p':...}}}.\n",
        "        \"\"\"\n",
        "        from scipy.stats import pearsonr\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        metrics_keys = ['wait','throughput','util','patience']\n",
        "        D = self.D_graph\n",
        "\n",
        "        # Compute tensors if not provided\n",
        "        if fmt_bounds is None:\n",
        "            fmt_bounds = self.compute_fmt_with_elite_bounds(top_k=21)\n",
        "        if interaction_tensor is None:\n",
        "            interaction_tensor = self.print_interactions(return_tensor=True, verbose=False)\n",
        "\n",
        "        # Reduce interaction tensor along inter_dim\n",
        "        inter_mean = interaction_tensor.mean(axis=2)  # (D,D)\n",
        "\n",
        "        node_correlations = {}\n",
        "\n",
        "        for i in range(D):\n",
        "            node_correlations[i] = {}\n",
        "            for k, key in enumerate(metrics_keys):\n",
        "                # FMT bounds for target node j from source i (mean of lower/upper)\n",
        "                fmt_mean = fmt_bounds[i,:,k,:].mean(axis=1)  # shape (D,)\n",
        "                # Interaction tensor for edges from node i to j\n",
        "                inter_vec = inter_mean[i,:]  # shape (D,)\n",
        "                # Pearson correlation\n",
        "                corr, pval = pearsonr(fmt_mean, inter_vec)\n",
        "                node_correlations[i][key] = {'r': corr, 'p': pval}\n",
        "\n",
        "                if verbose:\n",
        "                    print(f\"Node {i} | {key}: r = {corr:.3f}, p = {pval:.3e}\")\n",
        "                    plt.figure(figsize=(4,3))\n",
        "                    plt.scatter(fmt_mean, inter_vec, alpha=0.7, edgecolor='k', color='skyblue')\n",
        "                    plt.xlabel(f\"FMT {key} (Node {i} -> others)\")\n",
        "                    plt.ylabel(f\"Interaction mean (Node {i} -> others)\")\n",
        "                    plt.title(f\"Node {i} | {key} correlation: r={corr:.3f}\")\n",
        "                    plt.grid(True)\n",
        "                    plt.show()\n",
        "\n",
        "        return node_correlations\n",
        "\n",
        "    def correlation_penalty(self, fmt_bounds=None, interaction_tensor=None):\n",
        "        \"\"\"\n",
        "        Computes a penalty term that is high if per-node FMT metrics correlate with interactions.\n",
        "        Returns total penalty to subtract from the outer score.\n",
        "        \"\"\"\n",
        "        from scipy.stats import pearsonr\n",
        "\n",
        "        D = self.D_graph\n",
        "        metrics_keys = ['wait','throughput','util','patience']\n",
        "\n",
        "        if fmt_bounds is None:\n",
        "            fmt_bounds = self.compute_fmt_with_elite_bounds(top_k=top_k)\n",
        "        if interaction_tensor is None:\n",
        "            interaction_tensor = self.print_interactions(return_tensor=True, verbose=False)\n",
        "\n",
        "        inter_mean = interaction_tensor.mean(axis=2)\n",
        "        total_penalty = 0.0\n",
        "\n",
        "        for i in range(D):\n",
        "            for k in range(len(metrics_keys)):\n",
        "                fmt_mean = fmt_bounds[i,:,k,:].mean(axis=1)\n",
        "                inter_vec = inter_mean[i,:]\n",
        "                if np.std(fmt_mean) > 1e-8 and np.std(inter_vec) > 1e-8:\n",
        "                    corr, _ = pearsonr(fmt_mean, inter_vec)\n",
        "                    total_penalty += abs(corr)  # penalize high correlation\n",
        "\n",
        "        # normalize by number of nodes  metrics\n",
        "        total_penalty /= (D * len(metrics_keys))**2\n",
        "        return total_penalty\n",
        "\n",
        "\n",
        "# ---------------- USAGE ----------------\n",
        "if __name__ == \"__main__\":\n",
        "    optimizer = GDFCM(\n",
        "        candidate_dims, D_graph,\n",
        "        inner_archive_size, inner_offspring,\n",
        "        outer_archive_size, outer_offspring,\n",
        "        synthetic_targets,\n",
        "        inner_learning, gamma_interlayer=1,\n",
        "        causal_flag=False\n",
        "    )\n",
        "    metrics_list = optimizer.run()\n",
        "    optimizer.plot_pointwise_minmax_elite()\n",
        "    optimizer.plot_nested_activations()\n",
        "    # Compute FMT with elite bounds\n",
        "    fmt_elite_bounds = optimizer.compute_fmt_with_elite_bounds(top_k=top_k+10)\n",
        "\n",
        "# Plot as heatmaps\n",
        "    optimizer.plot_fmt_with_bounds(fmt_elite_bounds)\n",
        "\n",
        "    # Compute fuzzy multiplex tensor\n",
        "    fmt_tensor = optimizer.compute_fuzzy_metric_tensor(normalize=True)\n",
        "    optimizer.plot_fuzzy_metric_tensor_heatmaps(fmt_tensor)\n",
        "\n",
        "    # Compute FMT with bounds (minimax elite intervals)\n",
        "    optimizer.plot_node_score_contribution()\n",
        "    optimizer.plot_outer_fuzzy_graph()\n",
        "  #  optimizer.print_interactions()\n",
        "    tensor = optimizer.print_interactions()\n",
        "\n",
        "    print(\"Tensor shape:\", tensor.shape,'\\n',tensor)\n",
        "    # Compute tensors first\n",
        "    fmt_elite_bounds = optimizer.compute_fmt_with_elite_bounds(top_k=top_k)\n",
        "    interaction_tensor = optimizer.print_interactions(return_tensor=True, verbose=False)\n",
        "\n",
        "\n",
        "    interaction_tensor = optimizer.print_interactions(return_tensor=True, verbose=False)\n",
        "\n",
        "    # Get per-node, per-metric correlations\n",
        "   # node_metric_corrs = optimizer.correlate_fmt_interactions_per_node(\n",
        "    #    fmt_bounds=fmt_elite_bounds,\n",
        "     #   interaction_tensor=interaction_tensor\n",
        "    #)\n",
        "    import networkx as nx\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    from mpl_toolkits.mplot3d import Axes3D\n",
        "D_graph = len(optimizer.nested_reps)\n",
        "tensor = optimizer.print_interactions(return_tensor=True, verbose=False)\n",
        "\n",
        "# ---------------- Outer nodes (hubs) ----------------\n",
        "G_outer = nx.DiGraph()\n",
        "for i in range(D_graph):\n",
        "    G_outer.add_node(i)\n",
        "for i in range(D_graph):\n",
        "    for j in range(D_graph):\n",
        "        if i != j and np.any(tensor[i,j,:] != 0):\n",
        "            # Shift to signed weights: 0.5 -> 0, <0.5 negative, >0.5 positive\n",
        "            mean_weight = 2 * (np.mean(tensor[i,j,:]) - 0.5)\n",
        "            G_outer.add_edge(i, j, weight=mean_weight)\n",
        "\n",
        "# Outer spring layout\n",
        "pos_outer_2d = nx.circular_layout(G_outer, scale=5)\n",
        "pos_outer = np.array([[x, y, 0] for x, y in pos_outer_2d.values()])\n",
        "\n",
        "fig = plt.figure(figsize=(10,8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Plot outer nodes\n",
        "for i in range(D_graph):\n",
        "    ax.scatter(*pos_outer[i], s=300, color='skyblue')\n",
        "    ax.text(*pos_outer[i], f'Node {i}', color='black')\n",
        "\n",
        "# Plot outer edges with positive/negative colors\n",
        "\n",
        "for i, j, data in G_outer.edges(data=True):\n",
        "    x_vals = [pos_outer[i,0], pos_outer[j,0]]\n",
        "    y_vals = [pos_outer[i,1], pos_outer[j,1]]\n",
        "    z_vals = [pos_outer[i,2], pos_outer[j,2]]\n",
        "\n",
        "    # Positive = bright green, Negative = bright red\n",
        "    color = 'green' if data['weight'] > 0 else 'red'\n",
        "    linewidth = 2 + 4*abs(data['weight'])  # scale width by magnitude\n",
        "    ax.plot(x_vals, y_vals, z_vals, color=color, linewidth=linewidth)\n",
        "# ---------------- Inner FCMs (small circular around hub) ----------------\n",
        "for i, rep in enumerate(optimizer.nested_reps):\n",
        "    dims = len(rep)\n",
        "    angle = np.linspace(0, 2*np.pi, dims, endpoint=False)\n",
        "    radius = 0.8  # small circle\n",
        "    xs = pos_outer[i,0] + radius * np.cos(angle)\n",
        "    ys = pos_outer[i,1] + radius * np.sin(angle)\n",
        "    zs = pos_outer[i,2] + rep  # activation as height\n",
        "\n",
        "    # Plot inner nodes\n",
        "    ax.scatter(xs, ys, zs, c=rep, cmap='plasma', s=50)\n",
        "\n",
        "    # Connect inner nodes in circle\n",
        "    for k in range(dims):\n",
        "        ax.plot([xs[k], xs[(k+1)%dims]], [ys[k], ys[(k+1)%dims]], [zs[k], zs[(k+1)%dims]], color='gray', alpha=0.5)\n",
        "\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Activation')\n",
        "ax.set_title('Outer Nodes with Inner FCMs (Signed correlations)')\n",
        "plt.show()\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class GDFCMPredictorForward:\n",
        "    \"\"\"\n",
        "    Forward predictor for GDFCM.\n",
        "    Can handle new input data (DATA_MATRIX) and compute:\n",
        "        - Node activations\n",
        "        - Node metrics\n",
        "        - Inter-layer MI scores\n",
        "        - Total contributions and outer score\n",
        "    \"\"\"\n",
        "    def __init__(self, trained_gdfcm: GDFCM):\n",
        "        self.gdfcm = trained_gdfcm\n",
        "        self.D_graph = trained_gdfcm.D_graph\n",
        "        self.nested_reps = trained_gdfcm.nested_reps\n",
        "        self.chosen_Gmat = trained_gdfcm.chosen_Gmat\n",
        "        self.inter_layer = trained_gdfcm.inter_layer\n",
        "\n",
        "    def predict_node(self, node_idx, data_row):\n",
        "        \"\"\"\n",
        "        Predict metrics and activations for a single node given a new data row.\n",
        "        \"\"\"\n",
        "        # Use stored nested_rep as starting point\n",
        "        x = self.nested_reps[node_idx].copy()\n",
        "        # Optional: you could combine with data_row to modify x\n",
        "        # For now, we just compute metrics using data_row as the input\n",
        "        metrics_evaluator = MetricsEvaluator(np.array([data_row]))\n",
        "        metrics = metrics_evaluator.compute_node_metrics(0, y=x)\n",
        "\n",
        "        # Inter-layer MI\n",
        "        mi_score = self.inter_layer.mi_for_graph(self.chosen_Gmat, self.nested_reps)\n",
        "\n",
        "        return {'activations': x, 'metrics': metrics, 'mi_score': mi_score}\n",
        "\n",
        "    def predict_all_nodes(self, new_data_matrix):\n",
        "        \"\"\"\n",
        "        Predict metrics and activations for all nodes using new data.\n",
        "        new_data_matrix: shape (D_graph, num_features)\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        for node_idx in range(self.D_graph):\n",
        "            data_row = new_data_matrix[node_idx % len(new_data_matrix)]\n",
        "            node_result = self.predict_node(node_idx, data_row)\n",
        "            results.append(node_result)\n",
        "        return results\n",
        "\n",
        "    def predict_scores(self):\n",
        "        \"\"\"\n",
        "        Compute per-node contributions and total outer score.\n",
        "        \"\"\"\n",
        "        _, total_score, node_contributions = self.gdfcm.run_outer()\n",
        "        return {'node_contributions': node_contributions, 'total_score': total_score}\n",
        "# Suppose optimizer is your trained GDFCM\n",
        "predictor = GDFCMPredictorForward(optimizer)\n",
        "\n",
        "# New data: same number of nodes, each with same num_features\n",
        "new_DATA_MATRIX = np.random.rand(D_graph, DATA_MATRIX.shape[1])\n",
        "\n",
        "# Predict all nodes\n",
        "predictions = predictor.predict_all_nodes(new_DATA_MATRIX)\n",
        "\n",
        "for idx, node_pred in enumerate(predictions):\n",
        "    print(f\"Node {idx} Metrics:\", node_pred['metrics'])\n",
        "    print(f\"Node {idx} Activations:\", node_pred['activations'])\n",
        "    print(f\"Node {idx} MI Score:\", node_pred['mi_score'])\n",
        "\n",
        "# Optional: compute total contributions\n",
        "score_info = predictor.predict_scores()\n",
        "print(\"Node Contributions:\", score_info['node_contributions'])\n",
        "print(\"Total Score:\", score_info['total_score'])\n",
        "\n",
        "class GDFCMPredictorAdaptive:\n",
        "    \"\"\"\n",
        "    Adaptive forward predictor for GDFCM.\n",
        "    - Accepts new data per node.\n",
        "    - Updates activations slightly using a mini inner-loop.\n",
        "    - Computes metrics, inter-layer MI, and outer score contributions.\n",
        "    \"\"\"\n",
        "    def __init__(self, trained_gdfcm: GDFCM, lr_x=0.01, lr_steps=10):\n",
        "        self.gdfcm = trained_gdfcm\n",
        "        self.D_graph = trained_gdfcm.D_graph\n",
        "        self.nested_reps = [rep.copy() for rep in trained_gdfcm.nested_reps]\n",
        "        self.chosen_Gmat = trained_gdfcm.chosen_Gmat\n",
        "        self.inter_layer = trained_gdfcm.inter_layer\n",
        "        self.lr_x = lr_x\n",
        "        self.lr_steps = lr_steps\n",
        "\n",
        "    def adapt_node(self, node_idx, data_row):\n",
        "        \"\"\"\n",
        "        Mini inner-loop: slightly update activations based on new data row.\n",
        "        Handles shape mismatch by projecting new data to node activation dim.\n",
        "        \"\"\"\n",
        "        x = self.nested_reps[node_idx].copy()\n",
        "        dim = len(x)\n",
        "\n",
        "        # Simple linear projection of new data to activation space\n",
        "        if len(data_row) != dim:\n",
        "            # Use mean pooling to reduce or slice if too large\n",
        "            if len(data_row) > dim:\n",
        "                target = data_row[:dim]\n",
        "            else:\n",
        "                # Pad with 0.5\n",
        "                target = np.pad(data_row, (0, dim - len(data_row)), 'constant', constant_values=0.5)\n",
        "        else:\n",
        "            target = data_row\n",
        "\n",
        "        target = np.clip(target, 0, 1)\n",
        "\n",
        "        # Mini inner-loop update\n",
        "        for _ in range(self.lr_steps):\n",
        "            grad = x - target\n",
        "            x -= self.lr_x * grad\n",
        "            x = np.clip(x, 0, 1)\n",
        "\n",
        "        self.nested_reps[node_idx] = x\n",
        "\n",
        "        # Compute metrics\n",
        "        metrics_evaluator = MetricsEvaluator(np.array([data_row]))\n",
        "        metrics = metrics_evaluator.compute_node_metrics(0, y=x)\n",
        "\n",
        "        # Inter-layer MI\n",
        "        mi_score = self.inter_layer.mi_for_graph(self.chosen_Gmat, self.nested_reps)\n",
        "\n",
        "        return {'activations': x, 'metrics': metrics, 'mi_score': mi_score}\n",
        "\n",
        "\n",
        "    def adapt_all_nodes(self, new_data_matrix):\n",
        "        \"\"\"\n",
        "        Update activations for all nodes given new data matrix.\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        for node_idx in range(self.D_graph):\n",
        "            data_row = new_data_matrix[node_idx % len(new_data_matrix)]\n",
        "            node_result = self.adapt_node(node_idx, data_row)\n",
        "            results.append(node_result)\n",
        "        return results\n",
        "\n",
        "    def compute_outer_score(self):\n",
        "        \"\"\"\n",
        "        Compute node contributions and total outer score using current nested_reps.\n",
        "        \"\"\"\n",
        "        _, total_score, node_contributions = self.gdfcm.run_outer()\n",
        "        return {'node_contributions': node_contributions, 'total_score': total_score}\n",
        "# Initialize adaptive predictor\n",
        "adaptive_predictor = GDFCMPredictorAdaptive(optimizer, lr_x=0.02, lr_steps=15)\n",
        "\n",
        "# New data\n",
        "new_DATA_MATRIX = np.random.rand(D_graph, DATA_MATRIX.shape[1])\n",
        "\n",
        "# Adapt activations to new data\n",
        "predictions = adaptive_predictor.adapt_all_nodes(new_DATA_MATRIX)\n",
        "\n",
        "for idx, node_pred in enumerate(predictions):\n",
        "    print(f\"Node {idx} Metrics:\", node_pred['metrics'])\n",
        "    print(f\"Node {idx} Activations:\", node_pred['activations'])\n",
        "    print(f\"Node {idx} MI Score:\", node_pred['mi_score'])\n",
        "\n",
        "# Compute outer node contributions after adaptation\n",
        "score_info = adaptive_predictor.compute_outer_score()\n",
        "print(\"Node Contributions:\", score_info['node_contributions'])\n",
        "print(\"Total Score:\", score_info['total_score'])\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "pred_metrics = []\n",
        "true_metrics = []\n",
        "for idx, data_row in enumerate(new_DATA_MATRIX):\n",
        "    pred = adaptive_predictor.adapt_node(idx, data_row)\n",
        "    pred_metrics.append([pred['metrics'][k] for k in ['wait','throughput','util','patience']])\n",
        "    true_metrics.append([MetricsEvaluator(new_DATA_MATRIX).compute_node_metrics(idx)[k]\n",
        "                         for k in ['wait','throughput','util','patience']])\n",
        "\n",
        "pred_metrics = np.array(pred_metrics)\n",
        "true_metrics = np.array(true_metrics)\n",
        "\n",
        "\n",
        "def evaluate_predictions(true_metrics, pred_metrics, metric_names=None):\n",
        "    \"\"\"\n",
        "    Evaluate multi-metric predictions per node and overall.\n",
        "\n",
        "    Args:\n",
        "        true_metrics: np.array, shape (num_nodes, num_metrics)\n",
        "        pred_metrics: np.array, same shape as true_metrics\n",
        "        metric_names: list of metric names (optional)\n",
        "\n",
        "    Returns:\n",
        "        dict with evaluation metrics\n",
        "    \"\"\"\n",
        "    true_metrics = np.array(true_metrics)\n",
        "    pred_metrics = np.array(pred_metrics)\n",
        "\n",
        "    if metric_names is None:\n",
        "        metric_names = [f'Metric {i}' for i in range(true_metrics.shape[1])]\n",
        "\n",
        "    num_nodes, num_metrics = true_metrics.shape\n",
        "\n",
        "    results = {'per_metric': {}, 'overall': {}}\n",
        "\n",
        "\n",
        "    # Per metric\n",
        "    for i, name in enumerate(metric_names):\n",
        "        t = true_metrics[:, i]\n",
        "        p = pred_metrics[:, i]\n",
        "        mae = mean_absolute_error(t, p)\n",
        "        rmse = np.sqrt(mean_squared_error(t, p))\n",
        "        r2 = r2_score(t, p)\n",
        "        corr = np.corrcoef(t, p)[0,1]\n",
        "        mape = np.mean(np.abs((t - p) / (t + 1e-12))) * 100\n",
        "        results['per_metric'][name] = {\n",
        "            'MAE': mae,\n",
        "            'RMSE': rmse,\n",
        "            'R2': r2,\n",
        "            'Pearson': corr,\n",
        "            'MAPE (%)': mape\n",
        "        }\n",
        "\n",
        "    # Overall\n",
        "    mae_overall = mean_absolute_error(true_metrics.flatten(), pred_metrics.flatten())\n",
        "    rmse_overall = np.sqrt(mean_squared_error(true_metrics.flatten(), pred_metrics.flatten()))\n",
        "    r2_overall = r2_score(true_metrics.flatten(), pred_metrics.flatten())\n",
        "\n",
        "    # Cosine similarity (averaged over nodes)\n",
        "    cos_sim = np.mean([cosine_similarity(t.reshape(1,-1), p.reshape(1,-1))[0,0]\n",
        "                       for t,p in zip(true_metrics, pred_metrics)])\n",
        "\n",
        "    results['overall'] = {\n",
        "        'MAE': mae_overall,\n",
        "        'RMSE': rmse_overall,\n",
        "        'R2': r2_overall,\n",
        "        'CosineSim': cos_sim\n",
        "    }\n",
        "\n",
        "    return results\n",
        "# Suppose true_metrics and pred_metrics have shape (D_graph, 4)\n",
        "metrics_eval = evaluate_predictions(true_metrics, pred_metrics, metric_names=['wait','throughput','util','patience'])\n",
        "\n",
        "# Print per-metric evaluation\n",
        "for metric, vals in metrics_eval['per_metric'].items():\n",
        "    print(f\"{metric}: {vals}\")\n",
        "\n",
        "# Print overall evaluation\n",
        "print(\"\\nOverall metrics:\", metrics_eval['overall'])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtpJU1-GLrIz"
      },
      "source": [
        "# Q-MoE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmyULhsKLrAH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWA2vZGnDgr2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLxFixmzL9ao"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11 (XPython)",
      "language": "python",
      "name": "xpython"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}